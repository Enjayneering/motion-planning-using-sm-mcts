Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 550, 'sum_payoffs': 161.18775772329246, 'action': [2.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 141.92922358541486, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 136.68056369620697, 'action': [0.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 129.5038490457691, 'action': [0.0, 1.5707963267948966]}, {'num_count': 574, 'sum_payoffs': 170.39050284553798, 'action': [2.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 142.66238376653868, 'action': [1.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 134.05375821045234, 'action': [0.0, -1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 145.29034859542136, 'action': [1.0, -1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 156.9313041901127, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11953923060204304, 0.10845468376439904, 0.10541186698543795, 0.10128232992827646, 0.12475548793740492, 0.1088893718756792, 0.1038904585959574, 0.11041078026515974, 0.11714844599000217]
Actions to choose Agent 1: dict_values([{'num_count': 843, 'sum_payoffs': 242.72512719906675, 'action': [1.0, 0.0]}, {'num_count': 715, 'sum_payoffs': 197.1655991669954, 'action': [0.0, -1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 202.91313649695192, 'action': [0.0, 0.0]}, {'num_count': 801, 'sum_payoffs': 227.68099925602203, 'action': [1.0, 1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 230.6355595897271, 'action': [1.0, -1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 192.31903345892988, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18322103890458596, 0.15540099978265595, 0.1588785046728972, 0.17409258856770268, 0.1758313410128233, 0.15235818300369486]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 55.01903796195984 s
