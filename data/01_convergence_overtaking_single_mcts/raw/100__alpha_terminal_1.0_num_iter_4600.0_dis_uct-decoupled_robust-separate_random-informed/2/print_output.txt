Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 547, 'sum_payoffs': 160.2911453053627, 'action': [2.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 152.31356700089404, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 130.41511815593677, 'action': [0.0, 1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 165.8779879284851, 'action': [2.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 147.21828687038922, 'action': [1.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 132.2299252755425, 'action': [0.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 140.15398403128282, 'action': [0.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 147.3332100271452, 'action': [2.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 143.9283979586055, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1188871984351228, 0.11432297326668116, 0.10171701803955661, 0.12214735926972398, 0.11149750054336013, 0.102803738317757, 0.10736796348619865, 0.11149750054336013, 0.10954140404259943]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 203.65710526971398, 'action': [0.0, 0.0]}, {'num_count': 702, 'sum_payoffs': 192.69948211154326, 'action': [0.0, 1.5707963267948966]}, {'num_count': 791, 'sum_payoffs': 224.22566470357458, 'action': [1.0, 1.5707963267948966]}, {'num_count': 819, 'sum_payoffs': 234.228063072222, 'action': [1.0, -1.5707963267948966]}, {'num_count': 712, 'sum_payoffs': 196.27854707211063, 'action': [0.0, -1.5707963267948966]}, {'num_count': 843, 'sum_payoffs': 242.90077720922162, 'action': [1.0, 0.0]}])
Weights num count: [0.15931319278417735, 0.15257552705933491, 0.1719191480113019, 0.17800478156922409, 0.1547489676157357, 0.18322103890458596]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.8518488407135 s
