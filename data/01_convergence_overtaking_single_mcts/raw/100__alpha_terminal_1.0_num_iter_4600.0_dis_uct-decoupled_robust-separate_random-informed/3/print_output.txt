Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 493, 'sum_payoffs': 139.22888708738327, 'action': [0.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 151.70500423909454, 'action': [2.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 147.60202574092912, 'action': [1.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 153.2157638976744, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 137.044818333171, 'action': [0.0, 0.0]}, {'num_count': 561, 'sum_payoffs': 164.97847853665218, 'action': [2.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 134.0230242726267, 'action': [0.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 147.6018210027209, 'action': [1.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 139.6506868399341, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10715061943055858, 0.11432297326668116, 0.1119321886546403, 0.11519234948924147, 0.1058465550967181, 0.12193001521408389, 0.10410780265159748, 0.1119321886546403, 0.10736796348619865]
Actions to choose Agent 1: dict_values([{'num_count': 872, 'sum_payoffs': 253.4730469958322, 'action': [1.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 191.82676244124164, 'action': [0.0, -1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 197.1236368995973, 'action': [0.0, 1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 197.84157035324696, 'action': [0.0, 0.0]}, {'num_count': 804, 'sum_payoffs': 229.08996376192474, 'action': [1.0, 1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 225.85029551519588, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18952401651814824, 0.1519234948924147, 0.15518365572701587, 0.15561834383829604, 0.1747446207346229, 0.1727885242338622]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 55.341758251190186 s
