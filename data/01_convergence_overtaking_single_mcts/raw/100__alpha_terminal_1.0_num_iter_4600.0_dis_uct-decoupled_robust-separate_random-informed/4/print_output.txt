Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 476, 'sum_payoffs': 133.2636397433408, 'action': [0.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 150.99360562174746, 'action': [1.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 137.0460359382086, 'action': [0.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 133.01817102631594, 'action': [0.0, -1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 158.97699565770452, 'action': [2.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 148.34164182618846, 'action': [2.0, 1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 175.86213775249576, 'action': [2.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 141.58111228408734, 'action': [1.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 140.0819345825778, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10345577048467725, 0.11367094109976092, 0.10562921104107803, 0.10323842642903716, 0.11823516626820256, 0.11214953271028037, 0.127798304716366, 0.10823733970875897, 0.10736796348619865]
Actions to choose Agent 1: dict_values([{'num_count': 802, 'sum_payoffs': 228.24575099577072, 'action': [1.0, 1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 191.59070389479228, 'action': [0.0, 1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 197.31909937404956, 'action': [0.0, 0.0]}, {'num_count': 882, 'sum_payoffs': 256.8802804817626, 'action': [1.0, 0.0]}, {'num_count': 709, 'sum_payoffs': 195.20101608160826, 'action': [0.0, -1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 225.030575802571, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17430993262334274, 0.1519234948924147, 0.15540099978265595, 0.19169745707454902, 0.15409693544881548, 0.17235383612258204]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.74563670158386 s
