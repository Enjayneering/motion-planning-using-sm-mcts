Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 469, 'sum_payoffs': 130.4740791056253, 'action': [0.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 137.73238495985143, 'action': [0.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 139.54678628270548, 'action': [1.0, 1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 149.32144821778496, 'action': [1.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 150.11805452186076, 'action': [2.0, -1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 156.98177113510695, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 137.30287603298007, 'action': [0.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 145.62965844788502, 'action': [1.0, 0.0]}, {'num_count': 575, 'sum_payoffs': 170.52682463195111, 'action': [2.0, 0.0]}])
Weights num count: [0.1019343620951967, 0.10606389915235818, 0.10715061943055858, 0.11280156487720061, 0.11323625298848077, 0.11714844599000217, 0.1058465550967181, 0.11062812432079983, 0.12497283199304499]
Actions to choose Agent 1: dict_values([{'num_count': 863, 'sum_payoffs': 249.6331312543942, 'action': [1.0, 0.0]}, {'num_count': 800, 'sum_payoffs': 227.1547118483566, 'action': [1.0, -1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 224.5449174742245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 194.52915937403958, 'action': [0.0, 1.5707963267948966]}, {'num_count': 692, 'sum_payoffs': 188.78145384233085, 'action': [0.0, -1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 207.17975141318385, 'action': [0.0, 0.0]}])
Weights num count: [0.18756792001738753, 0.1738752445120626, 0.17235383612258204, 0.1538795913931754, 0.15040208650293416, 0.16170397739621822]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.10484194755554 s
