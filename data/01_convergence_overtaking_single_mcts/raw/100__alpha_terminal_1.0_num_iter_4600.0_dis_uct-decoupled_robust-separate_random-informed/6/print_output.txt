Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 479, 'sum_payoffs': 134.51743472509656, 'action': [0.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 141.92296970826533, 'action': [1.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 141.94292308469818, 'action': [0.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 147.6016274525099, 'action': [1.0, -1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 133.29136213740227, 'action': [0.0, -1.5707963267948966]}, {'num_count': 561, 'sum_payoffs': 165.46019823437067, 'action': [2.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 150.66603506264576, 'action': [2.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 150.26542471905833, 'action': [1.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 153.33891383900476, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10410780265159748, 0.10845468376439904, 0.10845468376439904, 0.11171484459900022, 0.10345577048467725, 0.12193001521408389, 0.11345359704412085, 0.11323625298848077, 0.1149750054336014]
Actions to choose Agent 1: dict_values([{'num_count': 841, 'sum_payoffs': 241.40921337738408, 'action': [1.0, 0.0]}, {'num_count': 785, 'sum_payoffs': 221.43363623274092, 'action': [1.0, -1.5707963267948966]}, {'num_count': 761, 'sum_payoffs': 212.97815850551683, 'action': [0.0, 0.0]}, {'num_count': 716, 'sum_payoffs': 197.00096605926308, 'action': [0.0, -1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 191.1141803303522, 'action': [0.0, 1.5707963267948966]}, {'num_count': 798, 'sum_payoffs': 226.05800521159983, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1827863507933058, 0.17061508367746142, 0.16539882634209954, 0.15561834383829604, 0.1519234948924147, 0.17344055640078243]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 56.80457806587219 s
