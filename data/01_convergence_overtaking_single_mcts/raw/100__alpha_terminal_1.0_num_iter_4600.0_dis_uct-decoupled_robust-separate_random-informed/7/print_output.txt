Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 499, 'sum_payoffs': 142.60674456820914, 'action': [0.0, 0.0]}, {'num_count': 546, 'sum_payoffs': 160.3135779802991, 'action': [2.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 144.11591450083455, 'action': [1.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 142.9491249385225, 'action': [1.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 128.62619986977407, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 126.3365977026222, 'action': [0.0, 1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 170.41867319296685, 'action': [2.0, 0.0]}, {'num_count': 551, 'sum_payoffs': 162.37054234814184, 'action': [2.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 147.06162114460867, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10845468376439904, 0.11866985437948271, 0.10932405998695936, 0.10867202782003912, 0.10041295370571615, 0.09910888937187567, 0.12432079982612476, 0.11975657465768311, 0.11106281243207998]
Actions to choose Agent 1: dict_values([{'num_count': 804, 'sum_payoffs': 228.17511698850478, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 196.9976005461608, 'action': [0.0, 0.0]}, {'num_count': 812, 'sum_payoffs': 231.02793527390367, 'action': [1.0, 1.5707963267948966]}, {'num_count': 704, 'sum_payoffs': 192.7395431279676, 'action': [0.0, -1.5707963267948966]}, {'num_count': 705, 'sum_payoffs': 193.0375847180042, 'action': [0.0, 1.5707963267948966]}, {'num_count': 859, 'sum_payoffs': 247.76716309016766, 'action': [1.0, 0.0]}])
Weights num count: [0.1747446207346229, 0.15561834383829604, 0.17648337317974352, 0.15301021517061508, 0.15322755922625517, 0.1866985437948272]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 56.08175468444824 s
