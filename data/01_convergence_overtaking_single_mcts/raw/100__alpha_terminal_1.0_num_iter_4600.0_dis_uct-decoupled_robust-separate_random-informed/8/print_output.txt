Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 509, 'sum_payoffs': 145.63186577045147, 'action': [1.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 153.2728092347233, 'action': [2.0, 1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 134.42481040766333, 'action': [0.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 168.77827583901828, 'action': [2.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 143.7428646195958, 'action': [0.0, 0.0]}, {'num_count': 543, 'sum_payoffs': 158.6003257237365, 'action': [2.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 147.1692970649744, 'action': [1.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 129.125278812987, 'action': [0.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 137.81407697456876, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11062812432079983, 0.1149750054336014, 0.10410780265159748, 0.1238861117148446, 0.10954140404259943, 0.11801782221256249, 0.11149750054336013, 0.10106498587263639, 0.10606389915235818]
Actions to choose Agent 1: dict_values([{'num_count': 819, 'sum_payoffs': 233.99242054051982, 'action': [1.0, 1.5707963267948966]}, {'num_count': 819, 'sum_payoffs': 234.09412971973472, 'action': [1.0, 0.0]}, {'num_count': 790, 'sum_payoffs': 223.70169823835985, 'action': [1.0, -1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 192.5732462005067, 'action': [0.0, 1.5707963267948966]}, {'num_count': 750, 'sum_payoffs': 209.49264598741658, 'action': [0.0, 0.0]}, {'num_count': 720, 'sum_payoffs': 198.87806884189348, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17800478156922409, 0.17800478156922409, 0.1717018039556618, 0.15257552705933491, 0.16300804173005867, 0.15648772006085634]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.063438177108765 s
