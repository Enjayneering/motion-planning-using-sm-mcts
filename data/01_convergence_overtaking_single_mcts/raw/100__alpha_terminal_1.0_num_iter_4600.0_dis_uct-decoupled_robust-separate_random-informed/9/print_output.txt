Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 501, 'sum_payoffs': 142.99952329499837, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 140.65156145625704, 'action': [0.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 152.4320980714082, 'action': [2.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 139.9152534553669, 'action': [1.0, 1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 132.78744564842995, 'action': [0.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 150.13195468955826, 'action': [1.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 153.925899038548, 'action': [2.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 171.90565394931954, 'action': [2.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 136.6026849282945, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1088893718756792, 0.10758530754183873, 0.11432297326668116, 0.10715061943055858, 0.10302108237339709, 0.11301890893284068, 0.11519234948924147, 0.12540752010432515, 0.10519452292979788]
Actions to choose Agent 1: dict_values([{'num_count': 841, 'sum_payoffs': 241.38100781923416, 'action': [1.0, 0.0]}, {'num_count': 817, 'sum_payoffs': 232.72728352780263, 'action': [1.0, 1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 203.3564437377021, 'action': [0.0, 0.0]}, {'num_count': 719, 'sum_payoffs': 197.9340445675665, 'action': [0.0, 1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 189.29140494668397, 'action': [0.0, -1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 224.9032885481403, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1827863507933058, 0.17757009345794392, 0.15953053683981744, 0.15627037600521626, 0.1508367746142143, 0.1727885242338622]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.57373332977295 s
