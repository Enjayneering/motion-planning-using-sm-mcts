Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 121, 'sum_payoffs': 33.903353121138835, 'action': [1.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 31.782627353653417, 'action': [1.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 37.198239877624324, 'action': [2.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 34.889682548816616, 'action': [1.0, -1.5707963267948966]}, {'num_count': 129, 'sum_payoffs': 37.63685487213237, 'action': [2.0, 1.5707963267948966]}, {'num_count': 111, 'sum_payoffs': 29.567011016310186, 'action': [0.0, 1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 34.74975635866393, 'action': [0.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 33.033682012252896, 'action': [0.0, -1.5707963267948966]}, {'num_count': 130, 'sum_payoffs': 38.00956180186837, 'action': [2.0, 0.0]}])
Weights num count: [0.10990009082652134, 0.10535876475930972, 0.11625794732061762, 0.11171662125340599, 0.11716621253405994, 0.1008174386920981, 0.11171662125340599, 0.1080835603996367, 0.11807447774750227]
Actions to choose Agent 1: dict_values([{'num_count': 177, 'sum_payoffs': 46.786682571829886, 'action': [0.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 44.73052067107975, 'action': [0.0, 1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 54.95203902280269, 'action': [1.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 47.93497921077994, 'action': [1.0, -1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 54.46092258214332, 'action': [1.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 47.08886088797528, 'action': [0.0, 0.0]}])
Weights num count: [0.16076294277929154, 0.15622161671207993, 0.17892824704813806, 0.16348773841961853, 0.17801998183469572, 0.16167120799273388]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.630022287368774 s
