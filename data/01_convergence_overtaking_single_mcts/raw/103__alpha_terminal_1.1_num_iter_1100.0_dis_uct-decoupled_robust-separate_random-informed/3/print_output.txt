Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 120, 'sum_payoffs': 33.47499616328695, 'action': [0.0, 1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 35.76647160926273, 'action': [1.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 35.336774182801406, 'action': [1.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 32.96591950403047, 'action': [0.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 33.38673559438229, 'action': [1.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 34.874272304696554, 'action': [2.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 35.82716991820347, 'action': [2.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 34.75259987719586, 'action': [2.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 33.922176463173756, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10899182561307902, 0.11353315168029064, 0.11262488646684832, 0.1080835603996367, 0.10899182561307902, 0.11171662125340599, 0.11353315168029064, 0.11171662125340599, 0.10990009082652134]
Actions to choose Agent 1: dict_values([{'num_count': 193, 'sum_payoffs': 53.388836127943975, 'action': [1.0, 1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 52.450382316953544, 'action': [1.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 47.19663713193645, 'action': [0.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 51.31744661993201, 'action': [1.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 44.67449703241357, 'action': [0.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 47.21764384312343, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17529518619436876, 0.1734786557674841, 0.16167120799273388, 0.17075386012715713, 0.15622161671207993, 0.16167120799273388]
Selected final action: [1.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 18.02583909034729 s
