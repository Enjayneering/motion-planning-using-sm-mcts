Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 132, 'sum_payoffs': 38.94265528609258, 'action': [2.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 33.66146447399175, 'action': [0.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 36.82803526731648, 'action': [2.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 34.408983744966996, 'action': [1.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 34.57923182214967, 'action': [2.0, 1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 34.943054777287756, 'action': [1.0, -1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 31.8156431183578, 'action': [0.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 33.69250399329028, 'action': [0.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 32.64863487732223, 'action': [1.0, 0.0]}])
Weights num count: [0.11989100817438691, 0.10899182561307902, 0.11534968210717529, 0.11080835603996367, 0.11080835603996367, 0.11171662125340599, 0.10535876475930972, 0.10899182561307902, 0.10717529518619437]
Actions to choose Agent 1: dict_values([{'num_count': 181, 'sum_payoffs': 48.47357779740672, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 52.20510737615717, 'action': [1.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 43.70718167163735, 'action': [0.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 52.16759102869382, 'action': [1.0, 1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 53.815699518477054, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 46.55664713536805, 'action': [0.0, 0.0]}])
Weights num count: [0.16439600363306087, 0.17257039055404177, 0.15349682107175294, 0.17257039055404177, 0.17620345140781107, 0.15985467756584923]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.683918476104736 s
