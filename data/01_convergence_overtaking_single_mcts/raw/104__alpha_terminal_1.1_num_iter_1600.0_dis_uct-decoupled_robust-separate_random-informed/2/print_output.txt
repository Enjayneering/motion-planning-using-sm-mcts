Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 182, 'sum_payoffs': 51.90911919093819, 'action': [1.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 52.72494176968437, 'action': [2.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 48.961530939959644, 'action': [0.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 50.69692412987964, 'action': [2.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 45.8897040028386, 'action': [0.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 50.65018299800368, 'action': [1.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 48.56659654626539, 'action': [1.0, 1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 53.1424100845981, 'action': [2.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 48.459542008827114, 'action': [0.0, 0.0]}])
Weights num count: [0.1136789506558401, 0.11492816989381636, 0.10930668332292318, 0.11180512179887571, 0.10493441599000625, 0.11180512179887571, 0.10868207370393504, 0.1155527795128045, 0.10868207370393504]
Actions to choose Agent 1: dict_values([{'num_count': 259, 'sum_payoffs': 68.73239864952278, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 75.64728827659678, 'action': [1.0, 1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 66.04478886916195, 'action': [0.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 77.50683859507299, 'action': [1.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 64.75488925279457, 'action': [0.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 77.1824474493849, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1617738913179263, 0.17301686445971268, 0.15740162398500937, 0.17613991255465333, 0.15552779512804496, 0.1755153029356652]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.776047229766846 s
