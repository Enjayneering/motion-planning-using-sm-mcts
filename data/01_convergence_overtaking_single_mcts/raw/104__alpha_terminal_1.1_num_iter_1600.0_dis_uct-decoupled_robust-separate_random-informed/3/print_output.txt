Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 179, 'sum_payoffs': 50.429632506833855, 'action': [1.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 51.33571603695418, 'action': [2.0, -1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 45.84369497215042, 'action': [0.0, 1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 44.59164521382366, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 51.72972902090943, 'action': [1.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 50.37132942674566, 'action': [1.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 51.69793465341156, 'action': [2.0, 1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 53.49692531110137, 'action': [2.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 50.017552077789695, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11180512179887571, 0.11305434103685197, 0.10493441599000625, 0.10306058713304185, 0.1136789506558401, 0.11180512179887571, 0.1136789506558401, 0.11617738913179262, 0.11118051217988757]
Actions to choose Agent 1: dict_values([{'num_count': 262, 'sum_payoffs': 70.3580075717138, 'action': [0.0, 0.0]}, {'num_count': 252, 'sum_payoffs': 66.50455983256971, 'action': [0.0, 1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 66.80510520367706, 'action': [0.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 73.74746628724772, 'action': [1.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 81.33857654660063, 'action': [1.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 74.27800414564959, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16364772017489068, 0.15740162398500937, 0.1580262336039975, 0.16926920674578388, 0.1811367895065584, 0.16989381636477202]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 25.86937975883484 s
