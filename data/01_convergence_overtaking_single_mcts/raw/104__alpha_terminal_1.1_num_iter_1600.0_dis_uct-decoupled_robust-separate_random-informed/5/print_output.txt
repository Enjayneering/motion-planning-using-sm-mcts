Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 46.861362484674586, 'action': [0.0, 1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 52.450895680755025, 'action': [2.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 48.51214519330217, 'action': [2.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 50.593173166085144, 'action': [1.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 50.69953014317723, 'action': [1.0, -1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 53.987078427315176, 'action': [2.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 50.29743692869902, 'action': [1.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 49.03220828174186, 'action': [0.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 48.984631981507604, 'action': [0.0, 0.0]}])
Weights num count: [0.1061836352279825, 0.11430356027482823, 0.10868207370393504, 0.11180512179887571, 0.11180512179887571, 0.11680199875078076, 0.11118051217988757, 0.10930668332292318, 0.10930668332292318]
Actions to choose Agent 1: dict_values([{'num_count': 260, 'sum_payoffs': 69.51093291782517, 'action': [0.0, -1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 71.03106283671744, 'action': [1.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 64.51645503050736, 'action': [0.0, 1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 69.5069103819363, 'action': [0.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 78.78130751997666, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 79.27277067983185, 'action': [1.0, 0.0]}])
Weights num count: [0.16239850093691444, 0.16489693941286696, 0.15427857589006871, 0.16239850093691444, 0.1773891317926296, 0.17801374141161774]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 25.935532331466675 s
