Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 240, 'sum_payoffs': 68.81000469379069, 'action': [1.0, -1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 64.61195412548122, 'action': [0.0, 0.0]}, {'num_count': 221, 'sum_payoffs': 60.924932578554944, 'action': [0.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 60.517176617924775, 'action': [0.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 70.4159184901704, 'action': [2.0, -1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 62.93731031992067, 'action': [1.0, 0.0]}, {'num_count': 250, 'sum_payoffs': 72.81074702526121, 'action': [2.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 63.360014818533095, 'action': [1.0, 1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 69.52182885409405, 'action': [2.0, 0.0]}])
Weights num count: [0.1142313184198001, 0.10947168015230842, 0.10518800571156592, 0.10471204188481675, 0.11613517372679677, 0.10756782484531176, 0.11899095668729176, 0.10804378867206092, 0.11518324607329843]
Actions to choose Agent 1: dict_values([{'num_count': 367, 'sum_payoffs': 99.98861970259175, 'action': [1.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 84.75731221262399, 'action': [0.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 83.99035146207747, 'action': [0.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 88.04826300587789, 'action': [0.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 100.32385461819653, 'action': [1.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 104.85596231375747, 'action': [1.0, 0.0]}])
Weights num count: [0.17467872441694432, 0.15516420752022847, 0.15421227986673014, 0.15944788196097096, 0.17515468824369348, 0.1808662541646835]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.097365617752075 s
