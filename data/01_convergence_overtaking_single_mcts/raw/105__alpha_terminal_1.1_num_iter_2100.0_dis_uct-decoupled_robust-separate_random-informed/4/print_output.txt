Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 235, 'sum_payoffs': 66.57943294976087, 'action': [2.0, -1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 61.16807286428956, 'action': [0.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 68.55821031767232, 'action': [2.0, 1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 70.6424285655112, 'action': [2.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 70.61095912740197, 'action': [1.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 62.34955584074937, 'action': [0.0, 0.0]}, {'num_count': 222, 'sum_payoffs': 61.22055149411669, 'action': [0.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 63.60028657443131, 'action': [1.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 67.7314219618749, 'action': [1.0, 0.0]}])
Weights num count: [0.11185149928605426, 0.10566396953831508, 0.1142313184198001, 0.11661113755354593, 0.11661113755354593, 0.10709186101856259, 0.10566396953831508, 0.10851975249881009, 0.11327939076630177]
Actions to choose Agent 1: dict_values([{'num_count': 330, 'sum_payoffs': 86.71863831069736, 'action': [0.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 98.2287044232374, 'action': [1.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 87.46321752952231, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 103.48145100190442, 'action': [1.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 87.81619835388899, 'action': [0.0, -1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 101.19689112810029, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15706806282722513, 0.1718229414564493, 0.15801999048072346, 0.17848643503093764, 0.15849595430747263, 0.17563065207044265]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.082050800323486 s
