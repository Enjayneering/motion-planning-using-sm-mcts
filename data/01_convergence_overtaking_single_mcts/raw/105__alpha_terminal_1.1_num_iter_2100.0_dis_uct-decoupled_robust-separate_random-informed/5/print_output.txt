Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 221, 'sum_payoffs': 60.28526867036564, 'action': [0.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 72.59158318895355, 'action': [2.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 65.60295439321229, 'action': [1.0, -1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 66.89534623466535, 'action': [2.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 68.56448498822417, 'action': [2.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 63.0870631516247, 'action': [1.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 64.40638670451234, 'action': [0.0, -1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 63.92397610853697, 'action': [1.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 62.666663624113205, 'action': [0.0, 0.0]}])
Weights num count: [0.10518800571156592, 0.11946692051404094, 0.11137553545930509, 0.1128034269395526, 0.11470728224654926, 0.10851975249881009, 0.1099476439790576, 0.10947168015230842, 0.10804378867206092]
Actions to choose Agent 1: dict_values([{'num_count': 336, 'sum_payoffs': 89.33549551122084, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 104.00236081992595, 'action': [1.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 87.40998506136383, 'action': [0.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 87.12172085683689, 'action': [0.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 103.51631153707923, 'action': [1.0, 1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 95.9330872399947, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15992384578772012, 0.17848643503093764, 0.1575440266539743, 0.15706806282722513, 0.17801047120418848, 0.16849119466920515]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.885340213775635 s
