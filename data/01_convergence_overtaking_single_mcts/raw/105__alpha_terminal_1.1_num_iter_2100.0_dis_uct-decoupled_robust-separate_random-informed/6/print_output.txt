Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 240, 'sum_payoffs': 68.05263704122864, 'action': [2.0, 1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 66.8623996243687, 'action': [1.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 61.48911957650153, 'action': [0.0, 0.0]}, {'num_count': 216, 'sum_payoffs': 58.25848808861614, 'action': [0.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 69.37359423583521, 'action': [2.0, 0.0]}, {'num_count': 251, 'sum_payoffs': 72.49949751913228, 'action': [2.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 61.98750055662054, 'action': [0.0, -1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 62.40871382496706, 'action': [1.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 67.23698831966102, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1142313184198001, 0.1128034269395526, 0.10661589719181343, 0.10280818657782008, 0.11565920990004759, 0.11946692051404094, 0.10709186101856259, 0.10756782484531176, 0.11327939076630177]
Actions to choose Agent 1: dict_values([{'num_count': 373, 'sum_payoffs': 103.08715022952975, 'action': [1.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 85.79267409660407, 'action': [0.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 81.780995856031, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 91.34743396354229, 'action': [0.0, 1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 104.16343797582684, 'action': [1.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 100.37340342932637, 'action': [1.0, 0.0]}])
Weights num count: [0.1775345073774393, 0.15564017134697763, 0.1504045692527368, 0.16277962874821514, 0.1789623988576868, 0.17420276059019515]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 36.53293967247009 s
