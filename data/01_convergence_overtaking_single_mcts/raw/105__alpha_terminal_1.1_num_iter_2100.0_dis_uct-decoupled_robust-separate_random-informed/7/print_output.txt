Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 221, 'sum_payoffs': 60.5123943351804, 'action': [0.0, 1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 70.39678924444132, 'action': [2.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 67.92349783780058, 'action': [2.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 61.79607692161313, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 67.87704281157619, 'action': [1.0, 1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 60.55826499294137, 'action': [0.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 65.85489100319266, 'action': [1.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 70.34382502868255, 'action': [2.0, 0.0]}, {'num_count': 232, 'sum_payoffs': 64.96946096749849, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10518800571156592, 0.11661113755354593, 0.11375535459305093, 0.10661589719181343, 0.11375535459305093, 0.10518800571156592, 0.11137553545930509, 0.11661113755354593, 0.11042360780580676]
Actions to choose Agent 1: dict_values([{'num_count': 371, 'sum_payoffs': 102.19491785188411, 'action': [1.0, -1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 91.57975919492162, 'action': [0.0, 0.0]}, {'num_count': 364, 'sum_payoffs': 99.52662975524714, 'action': [1.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 83.82562192684914, 'action': [0.0, 1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 82.70540878619128, 'action': [0.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 105.82110753969266, 'action': [1.0, 0.0]}])
Weights num count: [0.17658257972394098, 0.1632555925749643, 0.17325083293669682, 0.15326035221323178, 0.1518324607329843, 0.18134221799143266]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.31245231628418 s
