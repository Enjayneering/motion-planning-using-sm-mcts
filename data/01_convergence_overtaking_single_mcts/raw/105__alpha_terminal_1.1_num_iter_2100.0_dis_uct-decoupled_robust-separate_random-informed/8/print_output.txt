Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 229, 'sum_payoffs': 63.89076712934635, 'action': [1.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 66.77268416284056, 'action': [1.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 70.4910541554888, 'action': [2.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 65.45979687484495, 'action': [0.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 65.5788820377065, 'action': [1.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 68.22850529691947, 'action': [2.0, 1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 62.71818368458658, 'action': [0.0, -1.5707963267948966]}, {'num_count': 217, 'sum_payoffs': 59.02544697814828, 'action': [0.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 68.75307246237786, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10899571632555925, 0.11232746311280342, 0.11661113755354593, 0.11089957163255593, 0.11089957163255593, 0.1142313184198001, 0.10756782484531176, 0.10328415040456926, 0.11470728224654926]
Actions to choose Agent 1: dict_values([{'num_count': 341, 'sum_payoffs': 91.09617658939639, 'action': [0.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 98.14750654230123, 'action': [1.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 88.3785531337866, 'action': [0.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 87.32883891463423, 'action': [0.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 99.82855958907612, 'action': [1.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 102.02591932412707, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16230366492146597, 0.17134697762970014, 0.1589719181342218, 0.1575440266539743, 0.17325083293669682, 0.17610661589719181]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.618547677993774 s
