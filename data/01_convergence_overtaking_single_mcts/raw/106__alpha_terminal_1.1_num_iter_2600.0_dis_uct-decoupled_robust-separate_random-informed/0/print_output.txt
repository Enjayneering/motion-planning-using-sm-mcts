Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 79.00075338427791, 'action': [1.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 79.01992088291117, 'action': [1.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 74.95302471425592, 'action': [0.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 82.64316360226785, 'action': [1.0, -1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 75.50479524843236, 'action': [0.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 86.1608054314656, 'action': [2.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 77.73965310483453, 'action': [0.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 83.83819721124159, 'action': [2.0, 1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 89.81587882983534, 'action': [2.0, 0.0]}])
Weights num count: [0.10918877354863514, 0.10918877354863514, 0.1053440984236832, 0.11264898116109189, 0.1057285659361784, 0.11610918877354863, 0.10803537101114956, 0.11380238369857747, 0.11956939638600539]
Actions to choose Agent 1: dict_values([{'num_count': 412, 'sum_payoffs': 109.14013924866208, 'action': [0.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 106.81492360101063, 'action': [0.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 128.4011381613834, 'action': [1.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 119.33324374324684, 'action': [1.0, 1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 131.67359403464766, 'action': [1.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 105.90298397002704, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15840061514801998, 0.15609381007304882, 0.1787773933102653, 0.16916570549788543, 0.18223760092272204, 0.15494040753556323]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.96877932548523 s
