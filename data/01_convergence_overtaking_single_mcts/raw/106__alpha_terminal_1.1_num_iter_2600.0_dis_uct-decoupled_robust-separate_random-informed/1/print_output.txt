Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 288, 'sum_payoffs': 81.1336219471368, 'action': [1.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 91.87155821115195, 'action': [2.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 75.15476411148973, 'action': [0.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 88.8040267248412, 'action': [2.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 73.55624184727614, 'action': [0.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 77.56259550672034, 'action': [0.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 80.29956478277866, 'action': [1.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 81.60171201116336, 'action': [1.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 83.40915180987523, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11072664359861592, 0.12110726643598616, 0.104959630911188, 0.11803152633602461, 0.10342176086120723, 0.10726643598615918, 0.10995770857362552, 0.1111111111111111, 0.11303344867358708]
Actions to choose Agent 1: dict_values([{'num_count': 412, 'sum_payoffs': 108.00111305802076, 'action': [0.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 124.73217657270361, 'action': [1.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 123.19077051474548, 'action': [1.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 102.29513409069762, 'action': [0.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 125.86435385107049, 'action': [1.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 110.53479702250877, 'action': [0.0, 0.0]}])
Weights num count: [0.15840061514801998, 0.17608612072279892, 0.17454825067281815, 0.1522491349480969, 0.1772395232602845, 0.16109188773548636]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.81581473350525 s
