Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 280, 'sum_payoffs': 77.76831419808846, 'action': [0.0, 0.0]}, {'num_count': 322, 'sum_payoffs': 94.5867152316721, 'action': [2.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 84.17270060581754, 'action': [2.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 74.19426773027804, 'action': [0.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 82.10927832050798, 'action': [2.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 81.61153799075504, 'action': [1.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 80.9940773921603, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 78.95482444355974, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 77.40683990516898, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10765090349865436, 0.12379853902345252, 0.11380238369857747, 0.10419069588619762, 0.1118800461361015, 0.1114955786236063, 0.11072664359861592, 0.10880430603613994, 0.10726643598615918]
Actions to choose Agent 1: dict_values([{'num_count': 460, 'sum_payoffs': 126.10375104356092, 'action': [1.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 103.95547737101104, 'action': [0.0, -1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 109.7029761764265, 'action': [0.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 125.0739195800641, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 122.02494394305243, 'action': [1.0, -1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 111.47653283475316, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1768550557477893, 0.15340253748558247, 0.15955401768550556, 0.17570165321030373, 0.17262591311034217, 0.16147635524798154]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.60831904411316 s
