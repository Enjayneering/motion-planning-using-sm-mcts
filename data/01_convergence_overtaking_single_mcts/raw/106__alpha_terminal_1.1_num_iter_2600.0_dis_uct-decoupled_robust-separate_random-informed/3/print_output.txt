Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 298, 'sum_payoffs': 84.56027255833946, 'action': [2.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 78.51033343503751, 'action': [1.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 78.99865966112779, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 84.81989324439343, 'action': [2.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 81.76520786629649, 'action': [1.0, -1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 87.64649049843686, 'action': [2.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 78.56548984813749, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 76.14094878078285, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 77.01059006210973, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11457131872356786, 0.10880430603613994, 0.10918877354863514, 0.11495578623606305, 0.1118800461361015, 0.11764705882352941, 0.10880430603613994, 0.10649750096116878, 0.10726643598615918]
Actions to choose Agent 1: dict_values([{'num_count': 452, 'sum_payoffs': 123.49341151383638, 'action': [1.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 109.25578045012405, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 126.03539978524634, 'action': [1.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 125.99143498238247, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 106.36441479017047, 'action': [0.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 108.87670440860094, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17377931564782775, 0.1587850826605152, 0.17647058823529413, 0.17647058823529413, 0.15570934256055363, 0.15840061514801998]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.027812480926514 s
