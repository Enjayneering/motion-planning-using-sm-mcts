Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 83.07688197224485, 'action': [1.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 81.49986924828914, 'action': [1.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 76.33017016783674, 'action': [0.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 85.7897910340949, 'action': [2.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 81.45872902590372, 'action': [1.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 75.87284905808008, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 73.86738189311883, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 85.0423700984603, 'action': [2.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 86.24856179991195, 'action': [2.0, 0.0]}])
Weights num count: [0.11303344867358708, 0.1114955786236063, 0.10649750096116878, 0.11572472126105345, 0.1114955786236063, 0.1061130334486736, 0.10419069588619762, 0.11495578623606305, 0.11610918877354863]
Actions to choose Agent 1: dict_values([{'num_count': 466, 'sum_payoffs': 128.4057474227492, 'action': [1.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 113.79131955912987, 'action': [0.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 119.57271338181174, 'action': [1.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 106.8592936297417, 'action': [0.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 109.34478875802218, 'action': [0.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 120.73712644521895, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17916186082276048, 0.1637831603229527, 0.16993464052287582, 0.15647827758554403, 0.15916955017301038, 0.1710880430603614]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.794105052948 s
