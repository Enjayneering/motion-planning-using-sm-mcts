Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 282, 'sum_payoffs': 78.58350952657763, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 76.58892332916923, 'action': [0.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 80.27806893735601, 'action': [1.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 82.69621104872066, 'action': [1.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 84.24405783453886, 'action': [2.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 85.47426451647243, 'action': [2.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 74.72337652377769, 'action': [0.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 88.59823015066904, 'action': [2.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 81.50643051034365, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10841983852364476, 0.10649750096116878, 0.10995770857362552, 0.1122645136485967, 0.11380238369857747, 0.11495578623606305, 0.10457516339869281, 0.11803152633602461, 0.1111111111111111]
Actions to choose Agent 1: dict_values([{'num_count': 394, 'sum_payoffs': 102.09466953434676, 'action': [0.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 109.71705563426785, 'action': [0.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 102.49584246422076, 'action': [0.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 129.7904026556905, 'action': [1.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 131.61141120414106, 'action': [1.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 122.77918969301857, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1514801999231065, 0.15955401768550556, 0.1518646674356017, 0.18069973087274124, 0.18262206843521722, 0.17339484813533257]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.43096423149109 s
