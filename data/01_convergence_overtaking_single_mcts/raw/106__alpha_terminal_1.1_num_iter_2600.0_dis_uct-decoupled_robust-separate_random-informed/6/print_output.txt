Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 293, 'sum_payoffs': 82.5048923483724, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 72.9351024355826, 'action': [0.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 82.8969551268228, 'action': [2.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 77.36047847959578, 'action': [0.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 79.72011342317275, 'action': [0.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 85.27004560401218, 'action': [2.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 79.33285981870107, 'action': [1.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 82.44034083061295, 'action': [1.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 85.28247290413434, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11264898116109189, 0.10342176086120723, 0.11303344867358708, 0.10765090349865436, 0.10995770857362552, 0.11534025374855825, 0.10957324106113034, 0.11264898116109189, 0.11534025374855825]
Actions to choose Agent 1: dict_values([{'num_count': 466, 'sum_payoffs': 128.64295085262452, 'action': [1.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 127.27967201084344, 'action': [1.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 124.68858732014051, 'action': [1.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 106.51207443317473, 'action': [0.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 106.10282696822756, 'action': [0.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 107.5900658705004, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17916186082276048, 0.1776239907727797, 0.17493271818531334, 0.15570934256055363, 0.15532487504805845, 0.1568627450980392]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.73283410072327 s
