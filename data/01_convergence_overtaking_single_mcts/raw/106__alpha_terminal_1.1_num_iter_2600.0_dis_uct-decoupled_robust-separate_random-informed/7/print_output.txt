Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 299, 'sum_payoffs': 85.31429688772322, 'action': [2.0, -1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 85.76095832888925, 'action': [2.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 72.34933538441447, 'action': [0.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 80.24103717920306, 'action': [1.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 76.64669767020202, 'action': [0.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 83.36830864992916, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 92.57599462953456, 'action': [2.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 75.5360697380285, 'action': [0.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 80.57854468363618, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11495578623606305, 0.11534025374855825, 0.10226835832372165, 0.10995770857362552, 0.10649750096116878, 0.11303344867358708, 0.12187620146097655, 0.1053440984236832, 0.11034217608612072]
Actions to choose Agent 1: dict_values([{'num_count': 415, 'sum_payoffs': 109.48273022605606, 'action': [0.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 102.77413404093697, 'action': [0.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 105.9735102063762, 'action': [0.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 125.2602200564266, 'action': [1.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 128.91490997748076, 'action': [1.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 125.1884136019644, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15955401768550556, 0.1522491349480969, 0.15570934256055363, 0.17608612072279892, 0.17993079584775087, 0.17608612072279892]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.266379833221436 s
