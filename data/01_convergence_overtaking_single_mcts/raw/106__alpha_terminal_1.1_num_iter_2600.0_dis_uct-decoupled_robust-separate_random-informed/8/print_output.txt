Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 285, 'sum_payoffs': 79.48473741014435, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 73.91698443913025, 'action': [0.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 75.95900125389979, 'action': [0.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 84.67787327475703, 'action': [2.0, 1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 84.37348673865549, 'action': [1.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 77.56048546977306, 'action': [1.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 77.51354638280162, 'action': [0.0, -1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 89.16717739175779, 'action': [2.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 87.17210618846488, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10957324106113034, 0.10419069588619762, 0.1061130334486736, 0.11457131872356786, 0.11418685121107267, 0.10765090349865436, 0.10765090349865436, 0.11880046136101499, 0.11687812379853903]
Actions to choose Agent 1: dict_values([{'num_count': 459, 'sum_payoffs': 126.43066585096025, 'action': [1.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 110.71388008036648, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 112.63276840196546, 'action': [0.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 106.10050866062849, 'action': [0.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 123.21802316314316, 'action': [1.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 123.58603930305833, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17647058823529413, 0.15993848519800077, 0.16186082276047675, 0.15494040753556323, 0.17301038062283736, 0.17339484813533257]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.74323773384094 s
