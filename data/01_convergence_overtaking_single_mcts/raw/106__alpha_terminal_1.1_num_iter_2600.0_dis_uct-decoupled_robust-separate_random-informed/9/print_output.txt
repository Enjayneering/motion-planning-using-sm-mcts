Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 79.12408969277841, 'action': [1.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 80.76504461735988, 'action': [1.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 84.33712057706931, 'action': [1.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 78.3219289230703, 'action': [0.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 81.96086544528721, 'action': [2.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 88.33394356952739, 'action': [2.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 87.079085190001, 'action': [2.0, 0.0]}, {'num_count': 269, 'sum_payoffs': 73.2074683814551, 'action': [0.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 76.7285060447808, 'action': [0.0, 0.0]}])
Weights num count: [0.10918877354863514, 0.11072664359861592, 0.11418685121107267, 0.10841983852364476, 0.1118800461361015, 0.11803152633602461, 0.11687812379853903, 0.10342176086120723, 0.10688196847366398]
Actions to choose Agent 1: dict_values([{'num_count': 424, 'sum_payoffs': 113.27553269155148, 'action': [0.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 123.62395873124898, 'action': [1.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 105.73862174007631, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 122.50388915257476, 'action': [1.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 124.71845884426463, 'action': [1.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 110.83461318535154, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16301422529796233, 0.17377931564782775, 0.15494040753556323, 0.17262591311034217, 0.17493271818531334, 0.16032295271049596]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.46613311767578 s
