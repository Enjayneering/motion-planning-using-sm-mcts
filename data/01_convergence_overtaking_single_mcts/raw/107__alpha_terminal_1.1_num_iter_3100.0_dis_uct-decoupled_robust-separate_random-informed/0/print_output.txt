Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 335, 'sum_payoffs': 92.71557230438857, 'action': [0.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 106.68284637128819, 'action': [2.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 96.61798213683645, 'action': [2.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 98.95780293483267, 'action': [1.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 88.02277170891965, 'action': [0.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 91.08838111206919, 'action': [0.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 102.84985559268351, 'action': [2.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 97.01658611132265, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 93.52556775793087, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10802966784908094, 0.11963882618510158, 0.11125443405353112, 0.11318929377620122, 0.10415994840374072, 0.10673976136730087, 0.11641405998065141, 0.11157691067397614, 0.10867462108997097]
Actions to choose Agent 1: dict_values([{'num_count': 475, 'sum_payoffs': 124.05622814827049, 'action': [0.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 147.18800951194314, 'action': [1.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 132.14471909289188, 'action': [0.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 144.30915651059172, 'action': [1.0, -1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 157.6844434035276, 'action': [1.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 127.96298271380438, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15317639471138342, 0.17413737504030957, 0.16059335698161883, 0.17155756207674944, 0.1834891970332151, 0.15672363753627863]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.882545709609985 s
