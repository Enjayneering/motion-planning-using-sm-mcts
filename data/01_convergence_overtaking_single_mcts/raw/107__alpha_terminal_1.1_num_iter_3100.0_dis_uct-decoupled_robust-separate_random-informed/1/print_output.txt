Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 337, 'sum_payoffs': 93.32804873916194, 'action': [0.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 96.00923595455924, 'action': [1.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 99.47115459422763, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 92.88502311565479, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 96.39042546449724, 'action': [1.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 90.54416676012087, 'action': [0.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 104.22144963501506, 'action': [2.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 101.79462391480985, 'action': [2.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 91.02947282904611, 'action': [0.0, 0.0]}])
Weights num count: [0.10867462108997097, 0.1109319574330861, 0.11383424701709126, 0.10835214446952596, 0.11125443405353112, 0.10641728474685586, 0.11770396646243148, 0.11576910673976137, 0.10673976136730087]
Actions to choose Agent 1: dict_values([{'num_count': 542, 'sum_payoffs': 147.39953623448525, 'action': [1.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 143.88711264687905, 'action': [1.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 128.27598474577047, 'action': [0.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 135.63692659589353, 'action': [0.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 128.92954474771312, 'action': [0.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 146.32819552424803, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1747823282811996, 0.17155756207674944, 0.15736859077716867, 0.16414059980651402, 0.1580135440180587, 0.17381489841986456]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 48.297011375427246 s
