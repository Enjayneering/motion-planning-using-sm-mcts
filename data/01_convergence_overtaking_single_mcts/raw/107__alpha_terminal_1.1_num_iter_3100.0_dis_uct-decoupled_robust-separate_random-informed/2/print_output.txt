Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 334, 'sum_payoffs': 92.18666384181485, 'action': [0.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 94.04429792656866, 'action': [1.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 105.32658394472331, 'action': [2.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 101.1342173032511, 'action': [2.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 87.9548617942401, 'action': [0.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 95.71457458687591, 'action': [0.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 98.39540382514285, 'action': [2.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 92.6020799077914, 'action': [1.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 98.81098487703234, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10770719122863592, 0.10931957433086101, 0.11867139632376653, 0.11512415349887133, 0.10415994840374072, 0.11060948081264109, 0.11286681715575621, 0.10802966784908094, 0.11318929377620122]
Actions to choose Agent 1: dict_values([{'num_count': 566, 'sum_payoffs': 155.8088933271236, 'action': [1.0, 0.0]}, {'num_count': 535, 'sum_payoffs': 144.6201595116427, 'action': [1.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 131.2181807827772, 'action': [0.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 126.27087382615365, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 126.86106630333133, 'action': [0.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 144.35303406852267, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18252176717188004, 0.1725249919380845, 0.16027088036117382, 0.15575620767494355, 0.15640116091583361, 0.17220251531763947]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.11196517944336 s
