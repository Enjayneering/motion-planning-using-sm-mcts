Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 329, 'sum_payoffs': 90.43271559161666, 'action': [0.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 98.98598539363415, 'action': [2.0, 1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 102.5377395143314, 'action': [2.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 100.19644574864176, 'action': [1.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 99.34187547490943, 'action': [1.0, 0.0]}, {'num_count': 330, 'sum_payoffs': 90.784645434542, 'action': [0.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 98.15392162416359, 'action': [2.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 91.20868434658504, 'action': [0.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 96.13496796629778, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10609480812641084, 0.11318929377620122, 0.11609158336020639, 0.11415672363753628, 0.11351177039664624, 0.10641728474685586, 0.11254434053531119, 0.10673976136730087, 0.1109319574330861]
Actions to choose Agent 1: dict_values([{'num_count': 498, 'sum_payoffs': 131.76898662023888, 'action': [0.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 143.8678148630915, 'action': [1.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 134.93865532521687, 'action': [0.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 126.0587720997729, 'action': [0.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 141.65721471398805, 'action': [1.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 152.07130246532697, 'action': [1.0, 0.0]}])
Weights num count: [0.16059335698161883, 0.17155756207674944, 0.163495646565624, 0.15543373105449854, 0.16962270235407934, 0.17897452434698485]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.459946393966675 s
