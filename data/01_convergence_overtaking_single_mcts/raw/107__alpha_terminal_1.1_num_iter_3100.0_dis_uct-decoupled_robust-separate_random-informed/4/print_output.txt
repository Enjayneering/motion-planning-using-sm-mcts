Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 363, 'sum_payoffs': 103.4404041855412, 'action': [2.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 89.9277243915217, 'action': [0.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 93.79279388784127, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 93.88835624874945, 'action': [1.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 104.74370949062194, 'action': [2.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 95.06365031889608, 'action': [1.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 100.85023231638812, 'action': [2.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 93.51683751221208, 'action': [0.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 91.97953349971884, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11705901322154144, 0.10577233150596582, 0.10899709771041599, 0.10899709771041599, 0.11802644308287649, 0.10996452757175104, 0.11480167687842631, 0.10867462108997097, 0.1073847146081909]
Actions to choose Agent 1: dict_values([{'num_count': 507, 'sum_payoffs': 135.16466574436907, 'action': [0.0, 0.0]}, {'num_count': 548, 'sum_payoffs': 149.7296817634574, 'action': [1.0, 1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 147.94445207598665, 'action': [1.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 142.63338230738495, 'action': [1.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 133.26112752947986, 'action': [0.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 122.76820506501798, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.163495646565624, 0.1767171880038697, 0.17510480490164462, 0.17026765559496937, 0.1618832634633989, 0.15220896485004837]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.49149227142334 s
