Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 347, 'sum_payoffs': 97.48146725944879, 'action': [2.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 97.83830506256781, 'action': [1.0, 0.0]}, {'num_count': 347, 'sum_payoffs': 97.39370583243307, 'action': [1.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 96.16232514562846, 'action': [1.0, 1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 98.25002979830568, 'action': [2.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 90.12975245803379, 'action': [0.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 90.47393775196362, 'action': [0.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 93.86289436735518, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 106.40405073168868, 'action': [2.0, 0.0]}])
Weights num count: [0.11189938729442116, 0.11222186391486617, 0.11189938729442116, 0.1109319574330861, 0.11254434053531119, 0.10577233150596582, 0.10609480812641084, 0.10899709771041599, 0.11931634956465656]
Actions to choose Agent 1: dict_values([{'num_count': 507, 'sum_payoffs': 134.89091208960724, 'action': [0.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 127.5045147009791, 'action': [0.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 127.41039120036162, 'action': [0.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 146.7201644349897, 'action': [1.0, 0.0]}, {'num_count': 527, 'sum_payoffs': 142.05331269380574, 'action': [1.0, 1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 151.67678672662396, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.163495646565624, 0.15672363753627863, 0.15672363753627863, 0.17413737504030957, 0.16994517897452435, 0.17865204772653984]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.595433950424194 s
