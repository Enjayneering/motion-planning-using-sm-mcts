Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 335, 'sum_payoffs': 92.82185799909371, 'action': [0.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 90.4280778910331, 'action': [0.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 105.63058307891987, 'action': [2.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 88.19004361469194, 'action': [0.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 95.01517186132837, 'action': [1.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 99.84737294276154, 'action': [2.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 91.18354199007327, 'action': [1.0, 0.0]}, {'num_count': 363, 'sum_payoffs': 103.6753068682875, 'action': [2.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 101.37533147738547, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10802966784908094, 0.10609480812641084, 0.11867139632376653, 0.10415994840374072, 0.10996452757175104, 0.11383424701709126, 0.10673976136730087, 0.11705901322154144, 0.11512415349887133]
Actions to choose Agent 1: dict_values([{'num_count': 526, 'sum_payoffs': 141.69715509934434, 'action': [1.0, -1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 148.54026978919345, 'action': [1.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 129.33012920118767, 'action': [0.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 151.4187689589608, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 127.19853452060306, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 132.4182926664903, 'action': [0.0, 0.0]}])
Weights num count: [0.16962270235407934, 0.17574975814253466, 0.15833602063850372, 0.17832957110609482, 0.15640116091583361, 0.16123831022250887]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.06527781486511 s
