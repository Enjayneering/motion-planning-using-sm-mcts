Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 317, 'sum_payoffs': 85.59035729353113, 'action': [0.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 89.77018638479274, 'action': [0.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 104.59145690319352, 'action': [2.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 98.37178305661351, 'action': [0.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 96.06597395133774, 'action': [1.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 96.05314632303494, 'action': [1.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 97.60859747113113, 'action': [2.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 100.22167621555117, 'action': [2.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 97.53690058438097, 'action': [1.0, 0.0]}])
Weights num count: [0.10222508868107062, 0.10577233150596582, 0.11802644308287649, 0.11286681715575621, 0.1109319574330861, 0.1109319574330861, 0.11222186391486617, 0.11447920025798129, 0.11222186391486617]
Actions to choose Agent 1: dict_values([{'num_count': 545, 'sum_payoffs': 148.9450105163851, 'action': [1.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 131.794222976464, 'action': [0.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 132.86644828720307, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 145.38339762215534, 'action': [1.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 127.49859426550248, 'action': [0.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 146.4548833379378, 'action': [1.0, 0.0]}])
Weights num count: [0.17574975814253466, 0.16027088036117382, 0.16123831022250887, 0.1725249919380845, 0.15640116091583361, 0.17349242179941954]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.90814018249512 s
