Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 370, 'sum_payoffs': 105.74101729985831, 'action': [2.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 89.41049481058934, 'action': [0.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 94.92107655800548, 'action': [1.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 92.60834189460205, 'action': [0.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 90.17395136510892, 'action': [0.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 98.74997685994211, 'action': [2.0, -1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 95.2930777561583, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 96.04862364539913, 'action': [1.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 99.51138350152868, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11931634956465656, 0.10577233150596582, 0.11028700419219607, 0.10835214446952596, 0.10641728474685586, 0.11351177039664624, 0.11060948081264109, 0.11125443405353112, 0.11415672363753628]
Actions to choose Agent 1: dict_values([{'num_count': 508, 'sum_payoffs': 136.347013790666, 'action': [0.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 145.60302648226758, 'action': [1.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 130.59961227438578, 'action': [0.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 148.50483663596185, 'action': [1.0, 1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 149.64865319554767, 'action': [1.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 125.98864115379939, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.163818123186069, 0.17220251531763947, 0.15865849725894873, 0.1747823282811996, 0.17574975814253466, 0.15446630119316349]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.82987403869629 s
