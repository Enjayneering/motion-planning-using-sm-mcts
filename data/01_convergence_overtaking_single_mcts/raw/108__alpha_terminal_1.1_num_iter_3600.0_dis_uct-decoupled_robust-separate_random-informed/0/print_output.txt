Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 374, 'sum_payoffs': 101.0712254746739, 'action': [0.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 122.36088049083372, 'action': [2.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 116.99303706560244, 'action': [1.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 110.89751763384373, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 108.28953715764054, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 109.72849392813707, 'action': [1.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 105.56940365124963, 'action': [0.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 108.5889169817609, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 114.7296083811224, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10386003887808942, 0.11941127464593168, 0.11552346570397112, 0.11108025548458761, 0.10913635101360733, 0.1102471535684532, 0.10719244654262705, 0.1094140516523188, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 563, 'sum_payoffs': 146.19875350230507, 'action': [0.0, 0.0]}, {'num_count': 664, 'sum_payoffs': 181.51470929688193, 'action': [1.0, 0.0]}, {'num_count': 611, 'sum_payoffs': 162.92958078852942, 'action': [1.0, 1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 152.85211818850377, 'action': [0.0, -1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 146.01174674005168, 'action': [0.0, 1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 165.3930692319923, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15634545959455706, 0.18439322410441544, 0.16967509025270758, 0.16162177173007497, 0.1560677589558456, 0.17161899472368786]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.7887225151062 s
