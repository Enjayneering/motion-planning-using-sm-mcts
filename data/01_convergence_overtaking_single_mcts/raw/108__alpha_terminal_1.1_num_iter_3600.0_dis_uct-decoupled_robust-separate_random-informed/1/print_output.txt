Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 411, 'sum_payoffs': 114.93176653920527, 'action': [2.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 105.47035886559132, 'action': [0.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 105.07567267189181, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 111.42021002079704, 'action': [1.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 110.25766090478506, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 116.44465661149802, 'action': [2.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 120.10370830531761, 'action': [2.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 102.04222451563217, 'action': [0.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 110.66589017067375, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11413496251041377, 0.10719244654262705, 0.10691474590391557, 0.11163565676201055, 0.11080255484587614, 0.11524576506525964, 0.11802277145237434, 0.10469314079422383, 0.11108025548458761]
Actions to choose Agent 1: dict_values([{'num_count': 634, 'sum_payoffs': 171.8108014767114, 'action': [1.0, 0.0]}, {'num_count': 559, 'sum_payoffs': 145.6328318634562, 'action': [0.0, 0.0]}, {'num_count': 578, 'sum_payoffs': 152.2175615124236, 'action': [0.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 149.45228504640895, 'action': [0.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 171.47073641480551, 'action': [1.0, 1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 168.88287429422851, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17606220494307137, 0.1552346570397112, 0.1605109691752291, 0.15828936406553734, 0.1757845043043599, 0.17384059983337963]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.36815547943115 s
