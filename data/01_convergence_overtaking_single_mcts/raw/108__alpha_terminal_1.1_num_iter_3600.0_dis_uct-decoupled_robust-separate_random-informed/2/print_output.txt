Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 414, 'sum_payoffs': 116.19183829316057, 'action': [1.0, -1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 121.97372800272075, 'action': [2.0, 0.0]}, {'num_count': 384, 'sum_payoffs': 104.84745552240628, 'action': [0.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 101.4145648192522, 'action': [0.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 102.59741691420751, 'action': [0.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 114.25009755848941, 'action': [2.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 105.64798247432151, 'action': [1.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 121.57128652872345, 'action': [2.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 109.75088860949364, 'action': [1.0, 0.0]}])
Weights num count: [0.11496806442654818, 0.11913357400722022, 0.10663704526520411, 0.10413773951680089, 0.1049708414329353, 0.11357956123299083, 0.10719244654262705, 0.11885587336850875, 0.1102471535684532]
Actions to choose Agent 1: dict_values([{'num_count': 639, 'sum_payoffs': 172.6833394659812, 'action': [1.0, 1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 176.58652776374075, 'action': [1.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 150.76914240183493, 'action': [0.0, -1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 142.50080372324027, 'action': [0.0, 1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 148.3339131320421, 'action': [0.0, 0.0]}, {'num_count': 614, 'sum_payoffs': 163.94377926294777, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17745070813662872, 0.18050541516245489, 0.15995556789780616, 0.1532907525687309, 0.15801166342682588, 0.170508192168842]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.838491678237915 s
