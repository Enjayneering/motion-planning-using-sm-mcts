Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 386, 'sum_payoffs': 105.17875830849903, 'action': [0.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 105.68519862165282, 'action': [0.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 105.20341443747897, 'action': [1.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 118.76855758997061, 'action': [2.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 100.703901715361, 'action': [0.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 115.06449155028822, 'action': [2.0, -1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 116.2244466391082, 'action': [2.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 114.3736997632581, 'action': [1.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 113.51547606973793, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10719244654262705, 0.10747014718133852, 0.10719244654262705, 0.11718966953623994, 0.10386003887808942, 0.11441266314912524, 0.11524576506525964, 0.11385726187170231, 0.11330186059427937]
Actions to choose Agent 1: dict_values([{'num_count': 557, 'sum_payoffs': 144.8610776093724, 'action': [0.0, -1.5707963267948966]}, {'num_count': 561, 'sum_payoffs': 146.2442125802781, 'action': [0.0, 1.5707963267948966]}, {'num_count': 631, 'sum_payoffs': 170.5123070270232, 'action': [1.0, 0.0]}, {'num_count': 632, 'sum_payoffs': 170.93148576528478, 'action': [1.0, 1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 170.88332180990508, 'action': [1.0, -1.5707963267948966]}, {'num_count': 587, 'sum_payoffs': 155.2517469304789, 'action': [0.0, 0.0]}])
Weights num count: [0.15467925576228825, 0.15579005831713413, 0.17522910302693695, 0.17550680366564844, 0.17550680366564844, 0.16301027492363232]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.64749598503113 s
