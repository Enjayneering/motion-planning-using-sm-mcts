Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 418, 'sum_payoffs': 117.56116472628577, 'action': [2.0, -1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 111.88887525192061, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 126.99942792239351, 'action': [2.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 110.39989043393781, 'action': [1.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 110.34575172300083, 'action': [1.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 97.97566764821323, 'action': [0.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 107.99860449874804, 'action': [0.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 109.92243935943772, 'action': [2.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 103.62699683622742, 'action': [0.0, 0.0]}])
Weights num count: [0.11607886698139405, 0.11191335740072202, 0.12302138294918079, 0.11080255484587614, 0.11080255484587614, 0.10163843376839767, 0.10913635101360733, 0.11052485420716468, 0.1058039433490697]
Actions to choose Agent 1: dict_values([{'num_count': 621, 'sum_payoffs': 167.06253715369107, 'action': [1.0, 1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 169.50358831231966, 'action': [1.0, -1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 154.50622692650987, 'action': [0.0, 0.0]}, {'num_count': 556, 'sum_payoffs': 144.342384286078, 'action': [0.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 143.38682962243507, 'action': [0.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 179.62637711823433, 'action': [1.0, 0.0]}])
Weights num count: [0.17245209663982228, 0.17439600111080256, 0.1624548736462094, 0.15440155512357678, 0.1535684532074424, 0.18244931963343516]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.92979693412781 s
