Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 405, 'sum_payoffs': 112.91459051700974, 'action': [1.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 109.20160367068594, 'action': [0.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 101.72702634351444, 'action': [0.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 116.75619918534994, 'action': [2.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 110.76911245677391, 'action': [2.0, 1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 122.95008105871706, 'action': [2.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 106.11065349512592, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 109.99481308333306, 'action': [1.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 109.6077613820845, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11246875867814496, 0.10969175229103027, 0.10413773951680089, 0.11524576506525964, 0.11080255484587614, 0.11968897528464316, 0.10747014718133852, 0.1102471535684532, 0.10996945292974174]
Actions to choose Agent 1: dict_values([{'num_count': 551, 'sum_payoffs': 142.62505438634432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 151.57239378210096, 'action': [0.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 175.68288607137856, 'action': [1.0, 0.0]}, {'num_count': 633, 'sum_payoffs': 171.18794713614878, 'action': [1.0, 1.5707963267948966]}, {'num_count': 556, 'sum_payoffs': 144.25806034310713, 'action': [0.0, -1.5707963267948966]}, {'num_count': 637, 'sum_payoffs': 172.53537924478474, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15301305193001943, 0.16023326853651762, 0.179394612607609, 0.1757845043043599, 0.15440155512357678, 0.17689530685920576]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.08383631706238 s
