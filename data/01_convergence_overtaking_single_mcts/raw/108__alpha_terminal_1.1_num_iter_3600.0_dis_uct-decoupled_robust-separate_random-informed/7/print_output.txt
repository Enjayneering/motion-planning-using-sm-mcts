Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 401, 'sum_payoffs': 111.6521260046927, 'action': [1.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 122.85274774996195, 'action': [2.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 106.08677890121298, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 112.13144254513632, 'action': [1.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 111.79678811044832, 'action': [1.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 115.62642844661084, 'action': [2.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 101.49258815901901, 'action': [0.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 104.22532218548315, 'action': [0.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 116.70960280340971, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11135795612329909, 0.11941127464593168, 0.10719244654262705, 0.11163565676201055, 0.11135795612329909, 0.11413496251041377, 0.10386003887808942, 0.1058039433490697, 0.11496806442654818]
Actions to choose Agent 1: dict_values([{'num_count': 561, 'sum_payoffs': 146.2907809212026, 'action': [0.0, -1.5707963267948966]}, {'num_count': 631, 'sum_payoffs': 170.69804962792145, 'action': [1.0, -1.5707963267948966]}, {'num_count': 558, 'sum_payoffs': 145.19775870180678, 'action': [0.0, 1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 172.43157887043412, 'action': [1.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 168.79708478591263, 'action': [1.0, 1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 155.57639409215216, 'action': [0.0, 0.0]}])
Weights num count: [0.15579005831713413, 0.17522910302693695, 0.1549569564009997, 0.1766176062204943, 0.17384059983337963, 0.16328797556234378]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.33765888214111 s
