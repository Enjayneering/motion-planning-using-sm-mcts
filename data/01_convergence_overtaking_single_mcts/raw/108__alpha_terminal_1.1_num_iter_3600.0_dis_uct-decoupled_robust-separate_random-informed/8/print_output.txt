Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 386, 'sum_payoffs': 105.18601582814797, 'action': [0.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 113.53839356353045, 'action': [1.0, -1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 118.08707643412865, 'action': [2.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 104.3411995754849, 'action': [1.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 109.34313667529472, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 107.80554344348714, 'action': [1.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 117.61592646796076, 'action': [2.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 120.3381475229783, 'action': [2.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 98.11434467259011, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10719244654262705, 0.11330186059427937, 0.116634268258817, 0.10663704526520411, 0.1102471535684532, 0.10913635101360733, 0.11635656762010553, 0.11830047209108581, 0.10191613440710913]
Actions to choose Agent 1: dict_values([{'num_count': 614, 'sum_payoffs': 165.06353707357772, 'action': [1.0, 1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 170.35032292230608, 'action': [1.0, -1.5707963267948966]}, {'num_count': 565, 'sum_payoffs': 148.02355047943593, 'action': [0.0, 1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 152.84982478276632, 'action': [0.0, -1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 152.09281435750313, 'action': [0.0, 0.0]}, {'num_count': 636, 'sum_payoffs': 172.7003930121329, 'action': [1.0, 0.0]}])
Weights num count: [0.170508192168842, 0.17467370174951402, 0.15690086087198002, 0.16078866981394058, 0.16023326853651762, 0.1766176062204943]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.831526041030884 s
