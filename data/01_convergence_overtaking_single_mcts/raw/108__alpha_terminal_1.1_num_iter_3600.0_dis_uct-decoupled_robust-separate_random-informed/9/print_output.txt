Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 410, 'sum_payoffs': 115.0992424189034, 'action': [2.0, -1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 122.7537498571489, 'action': [2.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 111.6382135935888, 'action': [1.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 113.88792418594974, 'action': [1.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 117.36283359365794, 'action': [2.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 104.80676923766899, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 112.02983765938234, 'action': [1.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 105.21781360374835, 'action': [0.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 98.74255733730665, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11385726187170231, 0.11941127464593168, 0.11135795612329909, 0.1130241599555679, 0.11552346570397112, 0.10635934462649264, 0.11163565676201055, 0.10663704526520411, 0.10191613440710913]
Actions to choose Agent 1: dict_values([{'num_count': 564, 'sum_payoffs': 147.01424371216544, 'action': [0.0, 1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 167.33083560081653, 'action': [1.0, -1.5707963267948966]}, {'num_count': 644, 'sum_payoffs': 174.96979291970442, 'action': [1.0, 0.0]}, {'num_count': 572, 'sum_payoffs': 149.89293545795243, 'action': [0.0, -1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 149.80697286231984, 'action': [0.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 168.7198800140974, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15662316023326853, 0.17272979727853374, 0.17883921133018607, 0.1588447653429603, 0.1588447653429603, 0.17384059983337963]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.36876392364502 s
