Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 462, 'sum_payoffs': 127.4421027650759, 'action': [1.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 117.46110934776776, 'action': [1.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 134.85905491622157, 'action': [2.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 124.83422416857948, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 111.88666162048784, 'action': [0.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 111.51436059842915, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 120.01544865119762, 'action': [0.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 144.83938419920165, 'action': [2.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 132.5496058332423, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11265544989027067, 0.10607168983174835, 0.11753230919287978, 0.11094854913435748, 0.10241404535479151, 0.10217020238966106, 0.10777859058766155, 0.1241160692514021, 0.11606925140209705]
Actions to choose Agent 1: dict_values([{'num_count': 705, 'sum_payoffs': 187.35120716658253, 'action': [1.0, -1.5707963267948966]}, {'num_count': 723, 'sum_payoffs': 193.5373546895612, 'action': [1.0, 1.5707963267948966]}, {'num_count': 756, 'sum_payoffs': 204.8636919862905, 'action': [1.0, 0.0]}, {'num_count': 663, 'sum_payoffs': 173.00106963236325, 'action': [0.0, 0.0]}, {'num_count': 621, 'sum_payoffs': 158.76064644272307, 'action': [0.0, -1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 162.47788544897594, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17190929041697148, 0.17629846378931968, 0.18434528163862474, 0.1616678858814923, 0.15142648134601316, 0.1541087539624482]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.31744980812073 s
