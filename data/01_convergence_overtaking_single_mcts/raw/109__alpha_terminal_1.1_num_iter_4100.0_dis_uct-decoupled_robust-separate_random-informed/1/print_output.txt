Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 460, 'sum_payoffs': 126.89266638915481, 'action': [2.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 117.6899967440946, 'action': [0.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 127.2928496053656, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 124.36037619318864, 'action': [1.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 121.8030413781233, 'action': [1.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 133.93082542062348, 'action': [2.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 113.231884361143, 'action': [0.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 120.6892900962887, 'action': [0.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 141.8030547697179, 'action': [2.0, 0.0]}])
Weights num count: [0.11216776396000976, 0.10607168983174835, 0.11241160692514021, 0.11046086320409657, 0.10875396244818337, 0.11680078029748842, 0.10314557425018288, 0.10802243355279201, 0.12192148256522799]
Actions to choose Agent 1: dict_values([{'num_count': 636, 'sum_payoffs': 164.65177167342878, 'action': [0.0, 1.5707963267948966]}, {'num_count': 763, 'sum_payoffs': 208.31305502027803, 'action': [1.0, 0.0]}, {'num_count': 635, 'sum_payoffs': 164.26904145968274, 'action': [0.0, -1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 170.11379438187578, 'action': [0.0, 0.0]}, {'num_count': 705, 'sum_payoffs': 188.25765166325826, 'action': [1.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 189.65978607198156, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15508412582297001, 0.1860521823945379, 0.15484028285783955, 0.1589856132650573, 0.17190929041697148, 0.1728846622774933]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.94747734069824 s
