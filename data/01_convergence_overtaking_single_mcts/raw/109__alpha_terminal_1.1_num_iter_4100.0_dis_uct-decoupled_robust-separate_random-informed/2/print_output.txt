Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 483, 'sum_payoffs': 136.01849203104325, 'action': [2.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 114.87962594780342, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 136.75285533673883, 'action': [2.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 117.38436728449213, 'action': [0.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 119.19692717145213, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 130.7724502343779, 'action': [2.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 124.02015493139179, 'action': [1.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 127.84274268581126, 'action': [1.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 125.16742736898634, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11777615215801024, 0.10387710314557425, 0.11826383808827115, 0.10558400390148744, 0.10680321872713973, 0.11436235064618386, 0.10997317727383565, 0.11241160692514021, 0.11070470616922702]
Actions to choose Agent 1: dict_values([{'num_count': 651, 'sum_payoffs': 169.20816285191376, 'action': [0.0, 0.0]}, {'num_count': 649, 'sum_payoffs': 168.44460020723287, 'action': [0.0, -1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 159.37963887479796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 191.71159250249403, 'action': [1.0, -1.5707963267948966]}, {'num_count': 723, 'sum_payoffs': 193.7537903600865, 'action': [1.0, 1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 198.98343983829497, 'action': [1.0, 0.0]}])
Weights num count: [0.15874177029992684, 0.15825408436966593, 0.15167032431114363, 0.17483540599853695, 0.17629846378931968, 0.1799561082662765]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 61.022944688797 s
