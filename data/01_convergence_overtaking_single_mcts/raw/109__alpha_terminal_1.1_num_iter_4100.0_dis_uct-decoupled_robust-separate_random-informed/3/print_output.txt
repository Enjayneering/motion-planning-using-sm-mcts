Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 448, 'sum_payoffs': 122.55861809689645, 'action': [1.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 121.1025346414977, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 126.35769459519709, 'action': [1.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 129.19258314915385, 'action': [2.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 113.03733664582688, 'action': [0.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 133.75344850379028, 'action': [2.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 117.46450908776283, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 141.61441725441335, 'action': [2.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 123.72548700922808, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10924164837844429, 0.10826627651792246, 0.11168007802974884, 0.11363082175079249, 0.10290173128505242, 0.11655693733235796, 0.10582784686661789, 0.12167763960009753, 0.10997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 724, 'sum_payoffs': 195.0912966255201, 'action': [1.0, 0.0]}, {'num_count': 649, 'sum_payoffs': 169.35622464069812, 'action': [0.0, 1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 172.79824605835572, 'action': [0.0, -1.5707963267948966]}, {'num_count': 646, 'sum_payoffs': 168.34778574449123, 'action': [0.0, 0.0]}, {'num_count': 706, 'sum_payoffs': 188.92521773042924, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 192.3070356810988, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17654230675445012, 0.15825408436966593, 0.16069251402097048, 0.15752255547427457, 0.17215313338210192, 0.17459156303340648]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.1298086643219 s
