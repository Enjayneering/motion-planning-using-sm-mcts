Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 476, 'sum_payoffs': 133.0650755722321, 'action': [2.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 122.28114710515082, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 120.70935167900939, 'action': [0.0, 0.0]}, {'num_count': 474, 'sum_payoffs': 132.28061043775102, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 137.13261373805872, 'action': [2.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 113.04643458544349, 'action': [0.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 127.05384044950878, 'action': [1.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 127.45893800671908, 'action': [1.0, 0.0]}, {'num_count': 430, 'sum_payoffs': 115.90618676987242, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11606925140209705, 0.10899780541331383, 0.10802243355279201, 0.11558156547183614, 0.11875152401853206, 0.10290173128505242, 0.11216776396000976, 0.11241160692514021, 0.10485247500609607]
Actions to choose Agent 1: dict_values([{'num_count': 724, 'sum_payoffs': 195.12807099995914, 'action': [1.0, 0.0]}, {'num_count': 695, 'sum_payoffs': 185.0473955709015, 'action': [1.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 169.01696376284804, 'action': [0.0, 1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 191.64822147704618, 'action': [1.0, 1.5707963267948966]}, {'num_count': 670, 'sum_payoffs': 176.52366489165186, 'action': [0.0, 0.0]}, {'num_count': 649, 'sum_payoffs': 169.37317316921315, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17654230675445012, 0.16947086076566692, 0.15801024140453548, 0.17410387710314557, 0.1633747866374055, 0.15825408436966593]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.8327157497406 s
