Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 454, 'sum_payoffs': 125.21020513072264, 'action': [1.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 124.08814862621085, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 135.9966236157676, 'action': [2.0, 0.0]}, {'num_count': 478, 'sum_payoffs': 134.1243381104795, 'action': [2.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 127.74629957488179, 'action': [1.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 118.56009626946609, 'action': [0.0, 1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 132.338377793292, 'action': [2.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 120.78181713296672, 'action': [0.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 113.35224695909862, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11070470616922702, 0.10997317727383565, 0.11777615215801024, 0.11655693733235796, 0.11241160692514021, 0.1063155327968788, 0.11533772250670568, 0.10777859058766155, 0.10290173128505242]
Actions to choose Agent 1: dict_values([{'num_count': 767, 'sum_payoffs': 209.03144495965998, 'action': [1.0, 0.0]}, {'num_count': 661, 'sum_payoffs': 172.6023028681404, 'action': [0.0, 0.0]}, {'num_count': 690, 'sum_payoffs': 182.51610403862458, 'action': [1.0, -1.5707963267948966]}, {'num_count': 637, 'sum_payoffs': 164.45834065804743, 'action': [0.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 167.19098981020454, 'action': [0.0, 1.5707963267948966]}, {'num_count': 700, 'sum_payoffs': 185.9331081425491, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18702755425505974, 0.1611801999512314, 0.16825164594001463, 0.15532796878810046, 0.1572787125091441, 0.17069007559131918]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.521098613739014 s
