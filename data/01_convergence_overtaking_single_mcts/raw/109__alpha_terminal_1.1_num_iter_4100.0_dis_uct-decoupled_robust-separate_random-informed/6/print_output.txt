Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 449, 'sum_payoffs': 122.95118765913175, 'action': [1.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 121.83985962275467, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 143.81198178242317, 'action': [2.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 132.89696508142146, 'action': [2.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 128.92326793717424, 'action': [2.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 114.71221807936446, 'action': [0.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 127.40958987068059, 'action': [1.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 118.06029165658529, 'action': [0.0, 0.0]}, {'num_count': 435, 'sum_payoffs': 117.6704917342314, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10948549134357474, 0.10875396244818337, 0.12314069739088028, 0.11606925140209705, 0.11338697878566203, 0.1041209461107047, 0.11241160692514021, 0.1063155327968788, 0.10607168983174835]
Actions to choose Agent 1: dict_values([{'num_count': 719, 'sum_payoffs': 193.04181736195852, 'action': [1.0, 1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 161.91900336466026, 'action': [0.0, 1.5707963267948966]}, {'num_count': 700, 'sum_payoffs': 186.49636488388603, 'action': [1.0, -1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 167.0031829473183, 'action': [0.0, -1.5707963267948966]}, {'num_count': 656, 'sum_payoffs': 171.45409202143114, 'action': [0.0, 0.0]}, {'num_count': 754, 'sum_payoffs': 205.15602517028586, 'action': [1.0, 0.0]}])
Weights num count: [0.17532309192879786, 0.15313338210192637, 0.17069007559131918, 0.1567910265788832, 0.15996098512557913, 0.18385759570836382]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.26505088806152 s
