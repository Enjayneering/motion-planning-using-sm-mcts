Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 456, 'sum_payoffs': 125.20556923477301, 'action': [1.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 122.50815356485003, 'action': [1.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 117.41044602478155, 'action': [0.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 142.68312428568373, 'action': [2.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 126.63376149063677, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 116.36278489304213, 'action': [0.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 131.13123494107109, 'action': [2.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 121.11750052803788, 'action': [0.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 122.18969178648969, 'action': [1.0, 0.0]}])
Weights num count: [0.11119239209948793, 0.10948549134357474, 0.10607168983174835, 0.12265301146061935, 0.11216776396000976, 0.10534016093635698, 0.11509387954157523, 0.10851011948305292, 0.10924164837844429]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 197.6989576637022, 'action': [1.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 197.67724833502317, 'action': [1.0, 1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 169.89869087668492, 'action': [0.0, 1.5707963267948966]}, {'num_count': 624, 'sum_payoffs': 160.2929676918265, 'action': [0.0, -1.5707963267948966]}, {'num_count': 656, 'sum_payoffs': 171.2244362838533, 'action': [0.0, 0.0]}, {'num_count': 702, 'sum_payoffs': 186.9289501031432, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17873689344062424, 0.17873689344062424, 0.1589856132650573, 0.15215801024140455, 0.15996098512557913, 0.1711777615215801]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.54294037818909 s
