Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 502, 'sum_payoffs': 130.11494250705084, 'action': [1.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 97.61119438653061, 'action': [0.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.8915542067405, 'action': [0.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 174.6226886265683, 'action': [2.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 129.78510742464707, 'action': [1.0, 1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 174.25287353417627, 'action': [2.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 129.72513740966454, 'action': [1.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 173.90304844677837, 'action': [2.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 96.25187404692704, 'action': [0.0, 0.0]}])
Weights num count: [0.10910671593131928, 0.08867637470115192, 0.08824168658987176, 0.13627472288632905, 0.1088893718756792, 0.13605737883068897, 0.1088893718756792, 0.1358400347750489, 0.08780699847859161]
Actions to choose Agent 1: dict_values([{'num_count': 867, 'sum_payoffs': 283.32833578486265, 'action': [1.0, -1.5707963267948966]}, {'num_count': 667, 'sum_payoffs': 204.90754619273588, 'action': [0.0, 1.5707963267948966]}, {'num_count': 871, 'sum_payoffs': 284.9275361843963, 'action': [1.0, 1.5707963267948966]}, {'num_count': 864, 'sum_payoffs': 282.15892049270366, 'action': [1.0, 0.0]}, {'num_count': 664, 'sum_payoffs': 203.81809092055397, 'action': [0.0, -1.5707963267948966]}, {'num_count': 667, 'sum_payoffs': 204.9675162077185, 'action': [0.0, 0.0]}])
Weights num count: [0.18843729623994784, 0.1449684851119322, 0.18930667246250815, 0.1877852640730276, 0.14431645294501194, 0.1449684851119322]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5989065170288086 s
