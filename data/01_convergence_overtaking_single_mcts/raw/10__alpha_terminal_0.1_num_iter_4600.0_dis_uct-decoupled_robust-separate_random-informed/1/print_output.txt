Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 407, 'sum_payoffs': 97.24137929413845, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 129.39530232726077, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 96.52173911434834, 'action': [0.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 97.16141927416179, 'action': [0.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 128.6556721424765, 'action': [1.0, -1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 175.36231881135257, 'action': [2.0, 1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 175.34232880635852, 'action': [2.0, 0.0]}, {'num_count': 625, 'sum_payoffs': 173.90304844677834, 'action': [2.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 129.39530232726074, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.08845903064551185, 0.10867202782003912, 0.08802434253423169, 0.08845903064551185, 0.10823733970875897, 0.13670941099760922, 0.13670941099760922, 0.1358400347750489, 0.10867202782003912]
Actions to choose Agent 1: dict_values([{'num_count': 872, 'sum_payoffs': 285.3173412817826, 'action': [1.0, -1.5707963267948966]}, {'num_count': 872, 'sum_payoffs': 285.33733128677665, 'action': [1.0, 0.0]}, {'num_count': 664, 'sum_payoffs': 203.77811091056563, 'action': [0.0, -1.5707963267948966]}, {'num_count': 867, 'sum_payoffs': 283.3883057998453, 'action': [1.0, 1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 203.3683158081848, 'action': [0.0, 0.0]}, {'num_count': 662, 'sum_payoffs': 203.01849072078707, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18952401651814824, 0.18952401651814824, 0.14431645294501194, 0.18843729623994784, 0.14409910888937189, 0.1438817648337318]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.6185281276702881 s
