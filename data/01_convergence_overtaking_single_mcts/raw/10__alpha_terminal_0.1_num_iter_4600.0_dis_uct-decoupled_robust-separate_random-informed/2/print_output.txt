Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 498, 'sum_payoffs': 128.67566214747066, 'action': [1.0, 1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 173.90304844677829, 'action': [2.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 97.53123436655392, 'action': [0.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 97.18140927915596, 'action': [0.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 129.47526234723748, 'action': [1.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 173.90304844677834, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 97.63118439152477, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 129.41529233225495, 'action': [1.0, 0.0]}, {'num_count': 629, 'sum_payoffs': 175.3223388013643, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10823733970875897, 0.1358400347750489, 0.08867637470115192, 0.08845903064551185, 0.10867202782003912, 0.1358400347750489, 0.08867637470115192, 0.10867202782003912, 0.13670941099760922]
Actions to choose Agent 1: dict_values([{'num_count': 661, 'sum_payoffs': 202.60869561840647, 'action': [0.0, 1.5707963267948966]}, {'num_count': 668, 'sum_payoffs': 205.35732130510493, 'action': [0.0, 0.0]}, {'num_count': 673, 'sum_payoffs': 207.2863567870423, 'action': [0.0, -1.5707963267948966]}, {'num_count': 860, 'sum_payoffs': 280.55972009317014, 'action': [1.0, -1.5707963267948966]}, {'num_count': 868, 'sum_payoffs': 283.75812089223746, 'action': [1.0, 0.0]}, {'num_count': 870, 'sum_payoffs': 284.53773108700966, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14366442077809172, 0.14518582916757228, 0.14627254944577267, 0.18691588785046728, 0.18865464029558793, 0.18908932840686807]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.6154553890228271 s
