Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 629, 'sum_payoffs': 175.32233880136434, 'action': [2.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 128.7356321624532, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.93153421672882, 'action': [0.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.87156420174631, 'action': [0.0, -1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 175.30234879637027, 'action': [2.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 129.37531232226658, 'action': [1.0, 0.0]}, {'num_count': 627, 'sum_payoffs': 174.58270861658008, 'action': [2.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 129.0454772398628, 'action': [1.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.83158419175797, 'action': [0.0, 0.0]}])
Weights num count: [0.13670941099760922, 0.10823733970875897, 0.08824168658987176, 0.08824168658987176, 0.13670941099760922, 0.10867202782003912, 0.13627472288632905, 0.10845468376439904, 0.08824168658987176]
Actions to choose Agent 1: dict_values([{'num_count': 874, 'sum_payoffs': 286.1169414815496, 'action': [1.0, -1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 203.7381309005772, 'action': [0.0, 0.0]}, {'num_count': 865, 'sum_payoffs': 282.568715595084, 'action': [1.0, 1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 205.66716638251458, 'action': [0.0, 1.5707963267948966]}, {'num_count': 656, 'sum_payoffs': 200.71964014645755, 'action': [0.0, -1.5707963267948966]}, {'num_count': 872, 'sum_payoffs': 285.33733128677676, 'action': [1.0, 0.0]}])
Weights num count: [0.18995870462942838, 0.14431645294501194, 0.18800260812866768, 0.14540317322321233, 0.14257770049989132, 0.18952401651814824]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.588862419128418 s
