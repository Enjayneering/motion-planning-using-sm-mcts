Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 408, 'sum_payoffs': 97.53123436655392, 'action': [0.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 130.5047476044372, 'action': [1.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 173.90304844677826, 'action': [2.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 129.08545724985115, 'action': [1.0, 0.0]}, {'num_count': 622, 'sum_payoffs': 172.83358317959028, 'action': [2.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 129.8250874346354, 'action': [1.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 97.24137929413847, 'action': [0.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 97.20139928415011, 'action': [0.0, 0.0]}, {'num_count': 628, 'sum_payoffs': 175.01249372395452, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.08867637470115192, 0.10932405998695936, 0.1358400347750489, 0.10845468376439904, 0.13518800260812866, 0.1088893718756792, 0.08845903064551185, 0.08845903064551185, 0.13649206694196914]
Actions to choose Agent 1: dict_values([{'num_count': 860, 'sum_payoffs': 280.5997001031582, 'action': [1.0, -1.5707963267948966]}, {'num_count': 671, 'sum_payoffs': 206.48675658727544, 'action': [0.0, -1.5707963267948966]}, {'num_count': 869, 'sum_payoffs': 284.18790599961187, 'action': [1.0, 1.5707963267948966]}, {'num_count': 865, 'sum_payoffs': 282.60869560507234, 'action': [1.0, 0.0]}, {'num_count': 663, 'sum_payoffs': 203.3483258031905, 'action': [0.0, 0.0]}, {'num_count': 672, 'sum_payoffs': 206.8565716796677, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18691588785046728, 0.1458378613344925, 0.18887198435122798, 0.18800260812866768, 0.14409910888937189, 0.1460552053901326]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.6113197803497314 s
