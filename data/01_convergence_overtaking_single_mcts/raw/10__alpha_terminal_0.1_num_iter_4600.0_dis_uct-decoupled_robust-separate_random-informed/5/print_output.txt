Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 405, 'sum_payoffs': 96.56171912433668, 'action': [0.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 96.50174910935418, 'action': [0.0, 0.0]}, {'num_count': 628, 'sum_payoffs': 174.9925037189605, 'action': [2.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 130.03498248707422, 'action': [1.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.87156420174632, 'action': [0.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 130.09495250205669, 'action': [1.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 128.98550722488028, 'action': [1.0, 1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 174.97251371396638, 'action': [2.0, 1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 173.90304844677837, 'action': [2.0, 0.0]}])
Weights num count: [0.08802434253423169, 0.08802434253423169, 0.13649206694196914, 0.10910671593131928, 0.08824168658987176, 0.10910671593131928, 0.10845468376439904, 0.13649206694196914, 0.1358400347750489]
Actions to choose Agent 1: dict_values([{'num_count': 870, 'sum_payoffs': 284.57771109699826, 'action': [1.0, 1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 203.03848072578126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 670, 'sum_payoffs': 206.07696148489507, 'action': [0.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 201.06946523385525, 'action': [0.0, 0.0]}, {'num_count': 876, 'sum_payoffs': 286.8965516763223, 'action': [1.0, -1.5707963267948966]}, {'num_count': 865, 'sum_payoffs': 282.5287355850957, 'action': [1.0, 0.0]}])
Weights num count: [0.18908932840686807, 0.1438817648337318, 0.14562051727885242, 0.1427950445555314, 0.19039339274070854, 0.18800260812866768]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.6467084884643555 s
