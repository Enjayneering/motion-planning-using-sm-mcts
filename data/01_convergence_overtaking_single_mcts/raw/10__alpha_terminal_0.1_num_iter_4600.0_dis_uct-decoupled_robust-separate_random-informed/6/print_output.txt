Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 502, 'sum_payoffs': 130.07496249706253, 'action': [1.0, 1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 173.90304844677826, 'action': [2.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 97.24137929413845, 'action': [0.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 128.715642157459, 'action': [1.0, 0.0]}, {'num_count': 627, 'sum_payoffs': 174.6426786315626, 'action': [2.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.91154421173465, 'action': [0.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 131.5342328616369, 'action': [1.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 96.50174910935417, 'action': [0.0, 1.5707963267948966]}, {'num_count': 624, 'sum_payoffs': 173.51324334939196, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10910671593131928, 0.1358400347750489, 0.08845903064551185, 0.10823733970875897, 0.13627472288632905, 0.08824168658987176, 0.10997609215387959, 0.08802434253423169, 0.13562269071940883]
Actions to choose Agent 1: dict_values([{'num_count': 664, 'sum_payoffs': 203.7781109105654, 'action': [0.0, -1.5707963267948966]}, {'num_count': 858, 'sum_payoffs': 279.7801098983974, 'action': [1.0, 1.5707963267948966]}, {'num_count': 876, 'sum_payoffs': 286.95652169130454, 'action': [1.0, 0.0]}, {'num_count': 668, 'sum_payoffs': 205.3373313001108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 204.5377311003438, 'action': [0.0, 0.0]}, {'num_count': 868, 'sum_payoffs': 283.7581208922372, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14431645294501194, 0.18648119973918714, 0.19039339274070854, 0.14518582916757228, 0.1447511410562921, 0.18865464029558793]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5385174751281738 s
