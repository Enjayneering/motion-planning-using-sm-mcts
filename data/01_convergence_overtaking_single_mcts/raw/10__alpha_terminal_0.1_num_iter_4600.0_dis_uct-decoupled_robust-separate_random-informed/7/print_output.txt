Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 628, 'sum_payoffs': 174.95252370897228, 'action': [2.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 129.39530232726077, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 96.56171912433668, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 130.09495250205669, 'action': [1.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 129.7051474046704, 'action': [1.0, -1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 174.95252370897208, 'action': [2.0, 0.0]}, {'num_count': 629, 'sum_payoffs': 175.28235879137603, 'action': [2.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 96.21189403693872, 'action': [0.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 95.84207894454657, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.13649206694196914, 0.10867202782003912, 0.08802434253423169, 0.10910671593131928, 0.1088893718756792, 0.13649206694196914, 0.13670941099760922, 0.08780699847859161, 0.08758965442295154]
Actions to choose Agent 1: dict_values([{'num_count': 661, 'sum_payoffs': 202.54872560342426, 'action': [0.0, -1.5707963267948966]}, {'num_count': 876, 'sum_payoffs': 286.91654168131606, 'action': [1.0, -1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 202.60869561840673, 'action': [0.0, 1.5707963267948966]}, {'num_count': 868, 'sum_payoffs': 283.7381308872432, 'action': [1.0, 0.0]}, {'num_count': 873, 'sum_payoffs': 285.7271363841629, 'action': [1.0, 1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 202.6086956184062, 'action': [0.0, 0.0]}])
Weights num count: [0.14366442077809172, 0.19039339274070854, 0.14366442077809172, 0.18865464029558793, 0.18974136057378832, 0.14366442077809172]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5264766216278076 s
