Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 501, 'sum_payoffs': 129.80509742964125, 'action': [1.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 174.23288352918212, 'action': [2.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 128.38580707505523, 'action': [1.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 174.6626686365567, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 97.51124436155975, 'action': [0.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 97.2213892891443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 173.90304844677829, 'action': [2.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 97.20139928415011, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 130.0949525020567, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1088893718756792, 0.13605737883068897, 0.10801999565311889, 0.13627472288632905, 0.08867637470115192, 0.08845903064551185, 0.1358400347750489, 0.08845903064551185, 0.10910671593131928]
Actions to choose Agent 1: dict_values([{'num_count': 872, 'sum_payoffs': 285.3573212917709, 'action': [1.0, 0.0]}, {'num_count': 666, 'sum_payoffs': 204.51774109534983, 'action': [0.0, 1.5707963267948966]}, {'num_count': 872, 'sum_payoffs': 285.337331286777, 'action': [1.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 203.83808092554793, 'action': [0.0, -1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 205.72713639749693, 'action': [0.0, 0.0]}, {'num_count': 857, 'sum_payoffs': 279.3903048010109, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18952401651814824, 0.1447511410562921, 0.18952401651814824, 0.14431645294501194, 0.14540317322321233, 0.18626385568354706]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5366055965423584 s
