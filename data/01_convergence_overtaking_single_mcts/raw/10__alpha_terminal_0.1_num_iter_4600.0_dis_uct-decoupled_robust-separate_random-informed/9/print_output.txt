Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 408, 'sum_payoffs': 97.6111943865306, 'action': [0.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 128.69565215246485, 'action': [1.0, 1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 175.03248372894882, 'action': [2.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 129.7651174196529, 'action': [1.0, 0.0]}, {'num_count': 624, 'sum_payoffs': 173.57321336437462, 'action': [2.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 97.18140927915594, 'action': [0.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 98.27086455133822, 'action': [0.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 129.45527234224326, 'action': [1.0, -1.5707963267948966]}, {'num_count': 624, 'sum_payoffs': 173.51324334939204, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.08867637470115192, 0.10823733970875897, 0.13649206694196914, 0.1088893718756792, 0.13562269071940883, 0.08845903064551185, 0.08911106281243208, 0.10867202782003912, 0.13562269071940883]
Actions to choose Agent 1: dict_values([{'num_count': 859, 'sum_payoffs': 280.18990500077774, 'action': [1.0, 1.5707963267948966]}, {'num_count': 864, 'sum_payoffs': 282.1989005026922, 'action': [1.0, -1.5707963267948966]}, {'num_count': 667, 'sum_payoffs': 204.94752620272453, 'action': [0.0, 1.5707963267948966]}, {'num_count': 871, 'sum_payoffs': 284.90754617940223, 'action': [1.0, 0.0]}, {'num_count': 672, 'sum_payoffs': 206.89655168965598, 'action': [0.0, -1.5707963267948966]}, {'num_count': 667, 'sum_payoffs': 204.98750621271267, 'action': [0.0, 0.0]}])
Weights num count: [0.1866985437948272, 0.1877852640730276, 0.1449684851119322, 0.18930667246250815, 0.1460552053901326, 0.1449684851119322]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5247433185577393 s
