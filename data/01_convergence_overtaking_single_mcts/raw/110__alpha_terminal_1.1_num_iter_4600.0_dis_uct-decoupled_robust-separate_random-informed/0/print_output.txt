Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 504, 'sum_payoffs': 137.10134366738893, 'action': [1.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 146.55873418537448, 'action': [2.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 136.43407713737412, 'action': [1.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 150.58633701493125, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 132.0343606970021, 'action': [0.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 146.50637776668128, 'action': [1.0, -1.5707963267948966]}, {'num_count': 547, 'sum_payoffs': 152.8806021984727, 'action': [2.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 128.79091613536465, 'action': [0.0, -1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 126.62268629795223, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10954140404259943, 0.11519234948924147, 0.10910671593131928, 0.11758313410128234, 0.10649858726363834, 0.11519234948924147, 0.1188871984351228, 0.10454249076287764, 0.10323842642903716]
Actions to choose Agent 1: dict_values([{'num_count': 714, 'sum_payoffs': 183.69981744250703, 'action': [0.0, -1.5707963267948966]}, {'num_count': 749, 'sum_payoffs': 195.37605340455212, 'action': [0.0, 0.0]}, {'num_count': 790, 'sum_payoffs': 209.1939191871558, 'action': [1.0, -1.5707963267948966]}, {'num_count': 794, 'sum_payoffs': 210.59545754022884, 'action': [1.0, 1.5707963267948966]}, {'num_count': 847, 'sum_payoffs': 228.52920748052662, 'action': [1.0, 0.0]}, {'num_count': 706, 'sum_payoffs': 181.04776284090448, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15518365572701587, 0.16279069767441862, 0.1717018039556618, 0.17257118017822212, 0.18409041512714627, 0.15344490328189525]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.03955674171448 s
