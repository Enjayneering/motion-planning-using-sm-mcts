Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 504, 'sum_payoffs': 137.4545893479144, 'action': [1.0, -1.5707963267948966]}, {'num_count': 565, 'sum_payoffs': 159.86528951032966, 'action': [2.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 144.02236355499303, 'action': [1.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 138.5243736099811, 'action': [0.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 127.68844427389939, 'action': [0.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 135.284213133525, 'action': [1.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 144.75225677469243, 'action': [2.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 141.8829589691362, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 131.30424999051124, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10954140404259943, 0.12279939143664421, 0.11345359704412085, 0.11019343620951967, 0.10367311454031732, 0.10823733970875897, 0.113888285155401, 0.11214953271028037, 0.1058465550967181]
Actions to choose Agent 1: dict_values([{'num_count': 717, 'sum_payoffs': 184.85276487795687, 'action': [0.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 191.189270431634, 'action': [0.0, 0.0]}, {'num_count': 705, 'sum_payoffs': 180.8621543226579, 'action': [0.0, -1.5707963267948966]}, {'num_count': 807, 'sum_payoffs': 215.11849371750026, 'action': [1.0, -1.5707963267948966]}, {'num_count': 789, 'sum_payoffs': 209.06856574823084, 'action': [1.0, 1.5707963267948966]}, {'num_count': 846, 'sum_payoffs': 228.38365621483672, 'action': [1.0, 0.0]}])
Weights num count: [0.1558356878939361, 0.15996522495109758, 0.15322755922625517, 0.17539665290154313, 0.17148445990002173, 0.1838730710715062]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.08407783508301 s
