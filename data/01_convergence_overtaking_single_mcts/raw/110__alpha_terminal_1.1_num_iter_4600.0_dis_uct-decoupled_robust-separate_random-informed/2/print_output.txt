Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 486, 'sum_payoffs': 130.86603845553844, 'action': [0.0, -1.5707963267948966]}, {'num_count': 556, 'sum_payoffs': 156.37500973790722, 'action': [2.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 133.67373511690514, 'action': [1.0, 1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 147.97494086288916, 'action': [2.0, -1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 144.31055088054137, 'action': [2.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 143.16718202821048, 'action': [1.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 142.80129504662523, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 131.89231959979898, 'action': [0.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 128.6432360011195, 'action': [0.0, 0.0]}])
Weights num count: [0.10562921104107803, 0.1208432949358835, 0.10736796348619865, 0.1158443816561617, 0.11367094109976092, 0.11301890893284068, 0.11280156487720061, 0.10628124320799825, 0.10432514670723755]
Actions to choose Agent 1: dict_values([{'num_count': 744, 'sum_payoffs': 193.5171427699318, 'action': [0.0, 0.0]}, {'num_count': 798, 'sum_payoffs': 211.7711224753886, 'action': [1.0, -1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 212.45079716002442, 'action': [1.0, 1.5707963267948966]}, {'num_count': 713, 'sum_payoffs': 183.15770287583186, 'action': [0.0, -1.5707963267948966]}, {'num_count': 707, 'sum_payoffs': 181.1672397190046, 'action': [0.0, 1.5707963267948966]}, {'num_count': 838, 'sum_payoffs': 225.30498223338273, 'action': [1.0, 0.0]}])
Weights num count: [0.16170397739621822, 0.17344055640078243, 0.1738752445120626, 0.15496631167137578, 0.1536622473375353, 0.18213431862638557]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.50757455825806 s
