Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 506, 'sum_payoffs': 138.12236497318315, 'action': [1.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 129.89433865616732, 'action': [0.0, -1.5707963267948966]}, {'num_count': 557, 'sum_payoffs': 156.8917829544278, 'action': [2.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 132.74753615007583, 'action': [1.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 143.55521401640166, 'action': [1.0, 0.0]}, {'num_count': 557, 'sum_payoffs': 156.96070437116535, 'action': [2.0, 0.0]}, {'num_count': 472, 'sum_payoffs': 125.905593657593, 'action': [0.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 132.72750816391405, 'action': [0.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 144.0408669177181, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10997609215387959, 0.10497717887415779, 0.12106063899152358, 0.10671593131927842, 0.11323625298848077, 0.12106063899152358, 0.10258639426211694, 0.10671593131927842, 0.11345359704412085]
Actions to choose Agent 1: dict_values([{'num_count': 775, 'sum_payoffs': 203.2909389735468, 'action': [1.0, -1.5707963267948966]}, {'num_count': 759, 'sum_payoffs': 197.92501307500555, 'action': [0.0, 0.0]}, {'num_count': 822, 'sum_payoffs': 219.08653654885993, 'action': [1.0, 0.0]}, {'num_count': 718, 'sum_payoffs': 184.11488175752555, 'action': [0.0, -1.5707963267948966]}, {'num_count': 817, 'sum_payoffs': 217.43487297303744, 'action': [1.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 181.13950957052262, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16844164312106064, 0.16496413823081937, 0.1786568137361443, 0.15605303194957618, 0.17757009345794392, 0.15409693544881548]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.4059271812439 s
