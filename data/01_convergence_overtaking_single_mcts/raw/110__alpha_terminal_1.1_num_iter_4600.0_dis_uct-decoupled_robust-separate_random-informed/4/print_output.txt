Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 517, 'sum_payoffs': 141.68997234095957, 'action': [1.0, -1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 151.62015389072198, 'action': [2.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 153.45132335412384, 'action': [2.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 141.34688451479815, 'action': [2.0, 1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 132.98776078198847, 'action': [1.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 141.733601891743, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 134.80560580942344, 'action': [0.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 132.29689628405748, 'action': [0.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 126.46529598046833, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11236687676592046, 0.11823516626820256, 0.11932188654640295, 0.11214953271028037, 0.10715061943055858, 0.11236687676592046, 0.10823733970875897, 0.10671593131927842, 0.10323842642903716]
Actions to choose Agent 1: dict_values([{'num_count': 714, 'sum_payoffs': 183.8067780275808, 'action': [0.0, -1.5707963267948966]}, {'num_count': 775, 'sum_payoffs': 204.26720778533846, 'action': [1.0, 1.5707963267948966]}, {'num_count': 815, 'sum_payoffs': 217.80238313989807, 'action': [1.0, -1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 182.84169101388068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 192.17447330546452, 'action': [0.0, 0.0]}, {'num_count': 846, 'sum_payoffs': 228.34786978365133, 'action': [1.0, 0.0]}])
Weights num count: [0.15518365572701587, 0.16844164312106064, 0.17713540534666378, 0.15453162356009564, 0.16061725711801783, 0.1838730710715062]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.47173619270325 s
