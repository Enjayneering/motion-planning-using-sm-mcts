Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 498, 'sum_payoffs': 134.81091774937448, 'action': [1.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 135.65484303222792, 'action': [0.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 146.20265433211978, 'action': [2.0, 1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 154.68544661661113, 'action': [2.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 139.64885028274705, 'action': [1.0, -1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 148.80867929638023, 'action': [2.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 123.02165330578362, 'action': [0.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 137.3819123082533, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 137.14695837371562, 'action': [0.0, 0.0]}])
Weights num count: [0.10823733970875897, 0.10867202782003912, 0.1149750054336014, 0.11997391871332319, 0.11106281243207998, 0.11649641382308194, 0.10106498587263639, 0.10975874809823952, 0.10954140404259943]
Actions to choose Agent 1: dict_values([{'num_count': 791, 'sum_payoffs': 209.54720266461425, 'action': [1.0, 1.5707963267948966]}, {'num_count': 843, 'sum_payoffs': 227.16137638472134, 'action': [1.0, 0.0]}, {'num_count': 723, 'sum_payoffs': 186.56697393510632, 'action': [0.0, 1.5707963267948966]}, {'num_count': 801, 'sum_payoffs': 212.943942635612, 'action': [1.0, -1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 183.9571087017609, 'action': [0.0, -1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 187.95978030364307, 'action': [0.0, 0.0]}])
Weights num count: [0.1719191480113019, 0.18322103890458596, 0.15713975222777657, 0.17409258856770268, 0.15540099978265595, 0.15800912845033688]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.18645906448364 s
