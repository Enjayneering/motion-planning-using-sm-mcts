Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 485, 'sum_payoffs': 130.30544177032738, 'action': [0.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 142.62246237125316, 'action': [2.0, 1.5707963267948966]}, {'num_count': 574, 'sum_payoffs': 162.74939532545355, 'action': [2.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 142.65895273341343, 'action': [1.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 123.79137987657069, 'action': [0.0, 1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 138.6605691967172, 'action': [1.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 150.64796038856318, 'action': [2.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 136.34396541581816, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 130.1949225553623, 'action': [0.0, 0.0]}])
Weights num count: [0.10541186698543795, 0.11280156487720061, 0.12475548793740492, 0.11280156487720061, 0.10149967398391654, 0.11041078026515974, 0.11758313410128234, 0.10910671593131928, 0.10541186698543795]
Actions to choose Agent 1: dict_values([{'num_count': 758, 'sum_payoffs': 198.36470159260506, 'action': [0.0, 0.0]}, {'num_count': 685, 'sum_payoffs': 173.88480128272204, 'action': [0.0, -1.5707963267948966]}, {'num_count': 830, 'sum_payoffs': 222.67317436997504, 'action': [1.0, 0.0]}, {'num_count': 796, 'sum_payoffs': 211.14273062105613, 'action': [1.0, -1.5707963267948966]}, {'num_count': 747, 'sum_payoffs': 194.61036944331025, 'action': [0.0, 1.5707963267948966]}, {'num_count': 784, 'sum_payoffs': 207.0870457920104, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16474679417517932, 0.1488806781134536, 0.18039556618126495, 0.1730058682895023, 0.16235600956313845, 0.17039773962182134]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.46440052986145 s
