Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 459, 'sum_payoffs': 121.28214659948802, 'action': [0.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 155.5290876645386, 'action': [2.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 147.88710410620874, 'action': [2.0, 1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 134.3472779987394, 'action': [0.0, 0.0]}, {'num_count': 538, 'sum_payoffs': 149.9981446744819, 'action': [2.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 133.20477922994297, 'action': [0.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 145.60205640290525, 'action': [1.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 140.14665222923315, 'action': [1.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 133.9569653490526, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.09976092153879591, 0.12019126276896328, 0.11562703760052162, 0.10758530754183873, 0.1169311019343621, 0.1069332753749185, 0.11432297326668116, 0.11106281243207998, 0.10736796348619865]
Actions to choose Agent 1: dict_values([{'num_count': 792, 'sum_payoffs': 209.58228271211271, 'action': [1.0, 1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 187.34214012651995, 'action': [0.0, 1.5707963267948966]}, {'num_count': 834, 'sum_payoffs': 223.73154613390335, 'action': [1.0, 0.0]}, {'num_count': 743, 'sum_payoffs': 193.09055476748551, 'action': [0.0, 0.0]}, {'num_count': 710, 'sum_payoffs': 182.0345136392611, 'action': [0.0, -1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 210.6004280552804, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17213649206694198, 0.1577917843946968, 0.18126494240382526, 0.16148663334057814, 0.15431427950445556, 0.1727885242338622]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 70.03284478187561 s
