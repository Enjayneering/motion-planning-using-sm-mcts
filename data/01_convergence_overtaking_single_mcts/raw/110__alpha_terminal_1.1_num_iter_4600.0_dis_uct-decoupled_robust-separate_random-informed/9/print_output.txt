Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 472, 'sum_payoffs': 125.86273727158658, 'action': [0.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 132.28496385839148, 'action': [0.0, -1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 159.430323977154, 'action': [2.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 133.86900189219264, 'action': [1.0, 1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 144.01557754998373, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 136.65888928047443, 'action': [0.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 147.33012643605338, 'action': [2.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 138.5055853282386, 'action': [1.0, -1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 142.55578135191917, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10258639426211694, 0.10649858726363834, 0.12258204738100413, 0.10736796348619865, 0.11345359704412085, 0.10910671593131928, 0.11540969354488155, 0.11019343620951967, 0.11258422082156053]
Actions to choose Agent 1: dict_values([{'num_count': 723, 'sum_payoffs': 186.11100522938904, 'action': [0.0, 1.5707963267948966]}, {'num_count': 733, 'sum_payoffs': 189.41414662166787, 'action': [0.0, 0.0]}, {'num_count': 808, 'sum_payoffs': 214.6713949952878, 'action': [1.0, 1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 179.09186528872976, 'action': [0.0, -1.5707963267948966]}, {'num_count': 796, 'sum_payoffs': 210.59790908511286, 'action': [1.0, -1.5707963267948966]}, {'num_count': 838, 'sum_payoffs': 224.77159414438572, 'action': [1.0, 0.0]}])
Weights num count: [0.15713975222777657, 0.15931319278417735, 0.17561399695718322, 0.15257552705933491, 0.1730058682895023, 0.18213431862638557]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 69.25421142578125 s
