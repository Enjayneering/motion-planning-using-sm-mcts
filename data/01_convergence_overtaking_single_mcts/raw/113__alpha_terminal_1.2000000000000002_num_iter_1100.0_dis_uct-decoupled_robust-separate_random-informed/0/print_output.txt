Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 121, 'sum_payoffs': 33.95659668575323, 'action': [1.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 37.185602090493916, 'action': [1.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 34.039067921631464, 'action': [0.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 33.93054775775219, 'action': [0.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 36.23604866488355, 'action': [2.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 33.55654463329627, 'action': [0.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 34.04753012542895, 'action': [2.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 33.60112466707124, 'action': [2.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 34.46649434752165, 'action': [1.0, 0.0]}])
Weights num count: [0.10990009082652134, 0.11625794732061762, 0.10990009082652134, 0.10990009082652134, 0.11444141689373297, 0.10899182561307902, 0.10990009082652134, 0.10899182561307902, 0.11080835603996367]
Actions to choose Agent 1: dict_values([{'num_count': 178, 'sum_payoffs': 47.106065759157836, 'action': [0.0, 1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 49.58011646144246, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 46.221952325460975, 'action': [0.0, -1.5707963267948966]}, {'num_count': 201, 'sum_payoffs': 56.45281582942376, 'action': [1.0, 0.0]}, {'num_count': 186, 'sum_payoffs': 50.43894068682199, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 45.98320846416724, 'action': [0.0, 0.0]}])
Weights num count: [0.16167120799273388, 0.16712079927338783, 0.15985467756584923, 0.18256130790190736, 0.16893732970027248, 0.1589464123524069]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 38.32260298728943 s
