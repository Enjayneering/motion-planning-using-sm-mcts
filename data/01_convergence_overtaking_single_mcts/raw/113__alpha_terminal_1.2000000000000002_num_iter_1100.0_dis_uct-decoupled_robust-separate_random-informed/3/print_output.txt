Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 117, 'sum_payoffs': 32.24493601370933, 'action': [0.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 36.880635790573194, 'action': [2.0, 1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 31.871371687591605, 'action': [0.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 33.099421265280334, 'action': [0.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 35.05455110788615, 'action': [1.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 36.32756217586853, 'action': [2.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 34.613042183202396, 'action': [1.0, 0.0]}, {'num_count': 125, 'sum_payoffs': 35.90918494229609, 'action': [2.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 35.892847972813705, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10626702997275204, 0.11534968210717529, 0.10535876475930972, 0.1080835603996367, 0.11171662125340599, 0.11444141689373297, 0.11080835603996367, 0.11353315168029064, 0.11353315168029064]
Actions to choose Agent 1: dict_values([{'num_count': 191, 'sum_payoffs': 52.458303914069496, 'action': [1.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 51.190868822501955, 'action': [0.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 48.36714976774965, 'action': [1.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 46.036972181396635, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 52.048125224238056, 'action': [1.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 45.96545138956066, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1734786557674841, 0.17075386012715713, 0.16439600363306087, 0.1589464123524069, 0.17257039055404177, 0.1589464123524069]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.18699550628662 s
