Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 118, 'sum_payoffs': 32.52425290890752, 'action': [0.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 36.45712826270836, 'action': [1.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 32.004207339586344, 'action': [0.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 33.740389927764284, 'action': [1.0, 0.0]}, {'num_count': 126, 'sum_payoffs': 35.94803240596023, 'action': [2.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 33.39095622892853, 'action': [0.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 36.144490375030976, 'action': [2.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 36.01770133650327, 'action': [2.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 32.98920068253083, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10717529518619437, 0.11534968210717529, 0.10626702997275204, 0.10990009082652134, 0.11444141689373297, 0.10899182561307902, 0.11444141689373297, 0.11444141689373297, 0.1080835603996367]
Actions to choose Agent 1: dict_values([{'num_count': 180, 'sum_payoffs': 48.22572828701257, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 52.27197608901231, 'action': [1.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 53.54618655654573, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 46.48498370045978, 'action': [0.0, 0.0]}, {'num_count': 192, 'sum_payoffs': 53.14778520234576, 'action': [1.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 43.8011426904476, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16348773841961853, 0.17257039055404177, 0.17529518619436876, 0.15985467756584923, 0.17438692098092642, 0.15349682107175294]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 38.112488746643066 s
