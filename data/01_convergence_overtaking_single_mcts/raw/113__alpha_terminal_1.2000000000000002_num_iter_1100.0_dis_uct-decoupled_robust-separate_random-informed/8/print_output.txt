Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 125, 'sum_payoffs': 35.54491345080731, 'action': [1.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 32.99724132006242, 'action': [0.0, -1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 31.546062217646952, 'action': [0.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 34.78177080048046, 'action': [2.0, -1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 36.984755100057356, 'action': [2.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 35.094006242883346, 'action': [2.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 35.20413512293442, 'action': [1.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 34.28380674185426, 'action': [1.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 32.968873480841864, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11353315168029064, 0.1080835603996367, 0.10535876475930972, 0.11171662125340599, 0.11625794732061762, 0.11262488646684832, 0.11262488646684832, 0.11080835603996367, 0.1080835603996367]
Actions to choose Agent 1: dict_values([{'num_count': 182, 'sum_payoffs': 48.88248046394811, 'action': [0.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 53.40109844866979, 'action': [1.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 49.06368849698975, 'action': [1.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 52.18506249796558, 'action': [1.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 44.57998936868806, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 48.88108488963855, 'action': [0.0, 0.0]}])
Weights num count: [0.16530426884650318, 0.17529518619436876, 0.16530426884650318, 0.17257039055404177, 0.1553133514986376, 0.16530426884650318]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 29.68945813179016 s
