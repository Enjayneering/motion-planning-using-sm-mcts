Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 174, 'sum_payoffs': 48.09150385512098, 'action': [1.0, -1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 45.58827190013762, 'action': [0.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 53.08082776584166, 'action': [2.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 47.290487242209615, 'action': [0.0, 1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 52.20735121559585, 'action': [2.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 54.473489344652265, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 48.89192299886365, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 48.874811689136195, 'action': [0.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 48.53707434754094, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10868207370393504, 0.10493441599000625, 0.11617738913179262, 0.10743285446595878, 0.11492816989381636, 0.11805121798875702, 0.1099312929419113, 0.1099312929419113, 0.10930668332292318]
Actions to choose Agent 1: dict_values([{'num_count': 250, 'sum_payoffs': 65.80068963779223, 'action': [0.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 78.55079475208606, 'action': [1.0, 0.0]}, {'num_count': 250, 'sum_payoffs': 65.80405834758726, 'action': [0.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 77.02081458565517, 'action': [1.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 78.5615220592044, 'action': [1.0, -1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 67.5927594491918, 'action': [0.0, 0.0]}])
Weights num count: [0.1561524047470331, 0.17676452217364147, 0.1561524047470331, 0.17426608369768895, 0.17676452217364147, 0.15927545284197375]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.195968627929688 s
