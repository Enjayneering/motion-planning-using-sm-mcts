Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 174, 'sum_payoffs': 48.920355388074576, 'action': [0.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 48.11751836511629, 'action': [0.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 48.50901550370434, 'action': [1.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 48.492804619892176, 'action': [1.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 53.60998171976009, 'action': [2.0, -1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 44.67474950831348, 'action': [0.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 55.867967842007054, 'action': [2.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 51.02926644433312, 'action': [1.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 55.77749140634577, 'action': [2.0, 0.0]}])
Weights num count: [0.10868207370393504, 0.10743285446595878, 0.1080574640849469, 0.1080574640849469, 0.1155527795128045, 0.10243597751405371, 0.11867582760774516, 0.11180512179887571, 0.11867582760774516]
Actions to choose Agent 1: dict_values([{'num_count': 270, 'sum_payoffs': 73.16453317955526, 'action': [1.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 79.32796312788558, 'action': [1.0, 0.0]}, {'num_count': 267, 'sum_payoffs': 71.89222680313077, 'action': [0.0, 0.0]}, {'num_count': 269, 'sum_payoffs': 72.64628234500468, 'action': [1.0, -1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 68.09699709899586, 'action': [0.0, -1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 65.7984023253416, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16864459712679575, 0.17863835103060588, 0.16677076826983137, 0.1680199875078076, 0.16052467207995003, 0.15677701436602123]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.393659353256226 s
