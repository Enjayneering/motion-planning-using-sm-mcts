Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 177, 'sum_payoffs': 49.494478758369, 'action': [2.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 50.904143267769044, 'action': [1.0, -1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 52.103806295704466, 'action': [2.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 47.04683999935899, 'action': [0.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 47.01950106180086, 'action': [0.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 47.509499775165175, 'action': [0.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 50.36869918859929, 'action': [1.0, 1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 52.94263435649037, 'action': [1.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 51.76605892538971, 'action': [2.0, 0.0]}])
Weights num count: [0.11055590256089944, 0.11242973141786383, 0.11430356027482823, 0.10680824484697064, 0.10680824484697064, 0.10743285446595878, 0.11180512179887571, 0.1155527795128045, 0.1136789506558401]
Actions to choose Agent 1: dict_values([{'num_count': 295, 'sum_payoffs': 82.99068589848612, 'action': [1.0, 0.0]}, {'num_count': 260, 'sum_payoffs': 69.41395637093889, 'action': [1.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 73.67341143640098, 'action': [1.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 67.84652033540762, 'action': [0.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 68.1552190238659, 'action': [0.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 69.73835430032382, 'action': [0.0, 0.0]}])
Weights num count: [0.18425983760149905, 0.16239850093691444, 0.16926920674578388, 0.1599000624609619, 0.16052467207995003, 0.16302311055590257]
Selected final action: [1.0, 0.0, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 25.106417655944824 s
