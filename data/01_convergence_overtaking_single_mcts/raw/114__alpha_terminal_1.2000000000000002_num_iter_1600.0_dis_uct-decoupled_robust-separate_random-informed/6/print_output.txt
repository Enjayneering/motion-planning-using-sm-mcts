Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 173, 'sum_payoffs': 48.079415130142074, 'action': [0.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 47.14607292298735, 'action': [1.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 48.35467769402333, 'action': [0.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 52.312358388043776, 'action': [2.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 50.17521693434005, 'action': [1.0, 1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 53.96802741759452, 'action': [1.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 51.89050467765593, 'action': [2.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 48.468686891854595, 'action': [0.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 50.144828056932035, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1080574640849469, 0.10680824484697064, 0.10868207370393504, 0.11430356027482823, 0.11118051217988757, 0.11680199875078076, 0.1136789506558401, 0.10868207370393504, 0.11118051217988757]
Actions to choose Agent 1: dict_values([{'num_count': 257, 'sum_payoffs': 68.3082829408671, 'action': [0.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 76.76909745426711, 'action': [1.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 82.70581668134439, 'action': [1.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 64.08123661463328, 'action': [0.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 73.61118991823822, 'action': [1.0, 1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 66.7593868725015, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16052467207995003, 0.17426608369768895, 0.18363522798251092, 0.15365396627108058, 0.16926920674578388, 0.1580262336039975]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 25.167632341384888 s
