Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 180, 'sum_payoffs': 50.973303332041475, 'action': [1.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 54.283394927409674, 'action': [2.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 47.55950224288648, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 51.833524362477455, 'action': [2.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 48.83950199249902, 'action': [1.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 46.29733844052291, 'action': [0.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 50.03017315175802, 'action': [1.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 53.04757414253849, 'action': [2.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 46.98824763942083, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11242973141786383, 0.1174266083697689, 0.10743285446595878, 0.1136789506558401, 0.10930668332292318, 0.10555902560899438, 0.11118051217988757, 0.1155527795128045, 0.10680824484697064]
Actions to choose Agent 1: dict_values([{'num_count': 288, 'sum_payoffs': 80.07936488242105, 'action': [1.0, 0.0]}, {'num_count': 265, 'sum_payoffs': 71.07536585521409, 'action': [0.0, 0.0]}, {'num_count': 244, 'sum_payoffs': 63.11830338067359, 'action': [0.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 77.68353925228648, 'action': [1.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 66.93625014403862, 'action': [0.0, -1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 71.8954670977617, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17988757026858213, 0.1655215490318551, 0.1524047470331043, 0.17613991255465333, 0.15865084322298564, 0.16677076826983137]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.8207426071167 s
