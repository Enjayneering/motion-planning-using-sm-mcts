Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 169, 'sum_payoffs': 46.27860738285668, 'action': [0.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 54.38439233613762, 'action': [2.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 46.706573050723534, 'action': [0.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 52.9641960896775, 'action': [2.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 50.81927360960384, 'action': [2.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 50.882544720468736, 'action': [1.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 48.76828829590245, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 51.718938799397996, 'action': [1.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 47.12200154931335, 'action': [1.0, 0.0]}])
Weights num count: [0.10555902560899438, 0.1174266083697689, 0.1061836352279825, 0.1155527795128045, 0.11242973141786383, 0.11242973141786383, 0.10930668332292318, 0.1136789506558401, 0.10680824484697064]
Actions to choose Agent 1: dict_values([{'num_count': 246, 'sum_payoffs': 63.78186434781677, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 76.17261373732062, 'action': [1.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 75.41126626685285, 'action': [1.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 79.7470249519343, 'action': [1.0, -1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 64.58294748580919, 'action': [0.0, 0.0]}, {'num_count': 265, 'sum_payoffs': 71.14540144551452, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15365396627108058, 0.1736414740787008, 0.17239225484072454, 0.17926296064959402, 0.15490318550905685, 0.1655215490318551]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 25.313542127609253 s
