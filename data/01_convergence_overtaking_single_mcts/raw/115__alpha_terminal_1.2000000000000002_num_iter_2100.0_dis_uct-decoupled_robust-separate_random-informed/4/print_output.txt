Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 232, 'sum_payoffs': 65.34806221119761, 'action': [1.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 69.06245586893749, 'action': [2.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 69.8475609686768, 'action': [2.0, 0.0]}, {'num_count': 247, 'sum_payoffs': 71.55850858783914, 'action': [2.0, -1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 60.468248406921475, 'action': [0.0, -1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 61.18233666690462, 'action': [0.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 69.05019454981587, 'action': [1.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 63.72662987009753, 'action': [1.0, 0.0]}, {'num_count': 226, 'sum_payoffs': 62.879912190837125, 'action': [0.0, 0.0]}])
Weights num count: [0.11042360780580676, 0.11470728224654926, 0.11565920990004759, 0.11756306520704426, 0.10471204188481675, 0.10566396953831508, 0.11470728224654926, 0.10851975249881009, 0.10756782484531176]
Actions to choose Agent 1: dict_values([{'num_count': 361, 'sum_payoffs': 98.15649647671277, 'action': [1.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 102.08036795494426, 'action': [1.0, 0.0]}, {'num_count': 330, 'sum_payoffs': 86.70299661509287, 'action': [0.0, 1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 86.55804877845358, 'action': [0.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 99.77554641465449, 'action': [1.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 91.49090934998324, 'action': [0.0, 0.0]}])
Weights num count: [0.1718229414564493, 0.17658257972394098, 0.15706806282722513, 0.15706806282722513, 0.173726796763446, 0.1632555925749643]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.39494776725769 s
