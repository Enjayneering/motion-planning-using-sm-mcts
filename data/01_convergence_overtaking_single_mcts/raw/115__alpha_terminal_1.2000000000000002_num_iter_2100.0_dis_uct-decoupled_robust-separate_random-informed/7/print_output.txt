Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 239, 'sum_payoffs': 67.6474517288709, 'action': [2.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 59.92030871111609, 'action': [0.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 66.32152432895954, 'action': [1.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 60.3399120174726, 'action': [0.0, -1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 63.91960817164816, 'action': [1.0, 1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 71.7626719794241, 'action': [2.0, -1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 72.62260395266061, 'action': [2.0, 0.0]}, {'num_count': 225, 'sum_payoffs': 61.8775132166425, 'action': [0.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 63.56307392331753, 'action': [1.0, 0.0]}])
Weights num count: [0.11375535459305093, 0.10471204188481675, 0.11232746311280342, 0.10518800571156592, 0.10947168015230842, 0.1185149928605426, 0.11946692051404094, 0.10709186101856259, 0.10899571632555925]
Actions to choose Agent 1: dict_values([{'num_count': 360, 'sum_payoffs': 98.04910597125821, 'action': [1.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 103.29779291732116, 'action': [1.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 90.60857540426395, 'action': [0.0, 0.0]}, {'num_count': 326, 'sum_payoffs': 85.4362902810397, 'action': [0.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 100.78883685300039, 'action': [1.0, 1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 88.05498952674668, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17134697762970014, 0.17801047120418848, 0.1618277010947168, 0.15516420752022847, 0.17467872441694432, 0.15849595430747263]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.09996271133423 s
