Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 237, 'sum_payoffs': 66.8658378135493, 'action': [1.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 64.80034758201765, 'action': [1.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 69.25771857176379, 'action': [2.0, 1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 62.37574031424727, 'action': [0.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 63.504583716500946, 'action': [1.0, 0.0]}, {'num_count': 216, 'sum_payoffs': 58.31689325934258, 'action': [0.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 63.20325688387053, 'action': [0.0, 0.0]}, {'num_count': 243, 'sum_payoffs': 69.23365103175043, 'action': [2.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 70.4715978728019, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1128034269395526, 0.11042360780580676, 0.11565920990004759, 0.10756782484531176, 0.10899571632555925, 0.10280818657782008, 0.10851975249881009, 0.11565920990004759, 0.1170871013802951]
Actions to choose Agent 1: dict_values([{'num_count': 326, 'sum_payoffs': 85.27676655941644, 'action': [0.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 100.9599638186895, 'action': [1.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 84.16583559730925, 'action': [0.0, 0.0]}, {'num_count': 363, 'sum_payoffs': 99.03119422578692, 'action': [1.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 109.90229407509393, 'action': [1.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 85.94934915353244, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15516420752022847, 0.17515468824369348, 0.15373631603998097, 0.17277486910994763, 0.1865778200856735, 0.1561161351737268]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 37.02836489677429 s
