Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 245, 'sum_payoffs': 70.34813818607198, 'action': [2.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 66.34548828430528, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 67.93976281733768, 'action': [1.0, 0.0]}, {'num_count': 244, 'sum_payoffs': 69.92190805192386, 'action': [2.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 60.591003261074704, 'action': [0.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 64.3100412781119, 'action': [1.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 67.43242480480033, 'action': [2.0, 1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 60.631032116230266, 'action': [0.0, 0.0]}, {'num_count': 227, 'sum_payoffs': 62.998460622172054, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11661113755354593, 0.11185149928605426, 0.11375535459305093, 0.11613517372679677, 0.10518800571156592, 0.10947168015230842, 0.11327939076630177, 0.10518800571156592, 0.10804378867206092]
Actions to choose Agent 1: dict_values([{'num_count': 368, 'sum_payoffs': 101.01554205264588, 'action': [1.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 98.45385108130341, 'action': [1.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 88.37050004276702, 'action': [0.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 105.62600766848121, 'action': [1.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 82.80652535858196, 'action': [0.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 89.76421400467719, 'action': [0.0, 0.0]}])
Weights num count: [0.17515468824369348, 0.1718229414564493, 0.1589719181342218, 0.1808662541646835, 0.1518324607329843, 0.16087577344121848]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.91905927658081 s
