Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 275, 'sum_payoffs': 75.35590746184685, 'action': [0.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 81.5576285904397, 'action': [1.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 85.26233064152707, 'action': [2.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 79.20329487748751, 'action': [0.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 79.64141479009157, 'action': [1.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 78.42096838341125, 'action': [1.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 74.84258115993788, 'action': [0.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 89.2519553670858, 'action': [2.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 83.50901801181696, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1057285659361784, 0.1118800461361015, 0.11534025374855825, 0.10957324106113034, 0.10995770857362552, 0.10880430603613994, 0.1053440984236832, 0.11918492887351019, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 459, 'sum_payoffs': 126.17586199442596, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 109.0501253960131, 'action': [0.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 106.16326585324059, 'action': [0.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 104.2949217760669, 'action': [0.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 129.79440208193478, 'action': [1.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 125.49846674559878, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17647058823529413, 0.15840061514801998, 0.15532487504805845, 0.15340253748558247, 0.18031526336024606, 0.17570165321030373]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 42.501713514328 s
