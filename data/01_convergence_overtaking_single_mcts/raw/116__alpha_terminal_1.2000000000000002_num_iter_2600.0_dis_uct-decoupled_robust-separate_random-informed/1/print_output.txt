Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 269, 'sum_payoffs': 73.15796416794647, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 80.9696696016073, 'action': [1.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 85.88789732364744, 'action': [2.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 79.11056572034569, 'action': [0.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 79.06274128828898, 'action': [0.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 77.91343969221525, 'action': [1.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 87.46404921367849, 'action': [2.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 80.71062324308448, 'action': [1.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 85.0567245120552, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10342176086120723, 0.1111111111111111, 0.11572472126105345, 0.10918877354863514, 0.10918877354863514, 0.10803537101114956, 0.11726259131103421, 0.11072664359861592, 0.11495578623606305]
Actions to choose Agent 1: dict_values([{'num_count': 469, 'sum_payoffs': 129.48512824444475, 'action': [1.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 122.80514172231248, 'action': [1.0, 0.0]}, {'num_count': 453, 'sum_payoffs': 123.67360371724395, 'action': [1.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 107.17381778052402, 'action': [0.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 110.55041584187423, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 105.10653528511166, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18031526336024606, 0.17339484813533257, 0.17416378316032297, 0.1568627450980392, 0.16032295271049596, 0.15455594002306805]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.927523136138916 s
