Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 285, 'sum_payoffs': 79.40146650966352, 'action': [1.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 84.49310063136907, 'action': [1.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 83.78177382764011, 'action': [2.0, 1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 87.81368750505952, 'action': [2.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 87.41551107769433, 'action': [2.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 77.05251546458027, 'action': [0.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 75.49686631601601, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 76.29842452354015, 'action': [0.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 77.09926572764434, 'action': [1.0, 0.0]}])
Weights num count: [0.10957324106113034, 0.11457131872356786, 0.11380238369857747, 0.11764705882352941, 0.11726259131103421, 0.10726643598615918, 0.1057285659361784, 0.10649750096116878, 0.10726643598615918]
Actions to choose Agent 1: dict_values([{'num_count': 459, 'sum_payoffs': 126.22941273904942, 'action': [1.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 133.98691434867547, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 125.17247363401549, 'action': [1.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 103.75040254487197, 'action': [0.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 104.42894478893477, 'action': [0.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 108.01451510927643, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17647058823529413, 0.1845444059976932, 0.17531718569780855, 0.15263360246059207, 0.15340253748558247, 0.1572472126105344]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.01675629615784 s
