Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 287, 'sum_payoffs': 80.70068710882691, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 85.51663559640468, 'action': [1.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 81.81362127179764, 'action': [2.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 81.4354441213357, 'action': [1.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 87.46483861691988, 'action': [2.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 82.6546241873495, 'action': [1.0, 0.0]}, {'num_count': 269, 'sum_payoffs': 73.5889306253172, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 74.7122129316114, 'action': [0.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 84.99277637532539, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11034217608612072, 0.11495578623606305, 0.1114955786236063, 0.1111111111111111, 0.11687812379853903, 0.1122645136485967, 0.10342176086120723, 0.10457516339869281, 0.11457131872356786]
Actions to choose Agent 1: dict_values([{'num_count': 459, 'sum_payoffs': 125.8403205145328, 'action': [1.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 127.6397059234267, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 108.34359129808527, 'action': [0.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 129.90151411124373, 'action': [1.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 104.69369333385369, 'action': [0.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 102.60162263602204, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17647058823529413, 0.17839292579777008, 0.1580161476355248, 0.18069973087274124, 0.15417147251057287, 0.1518646674356017]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.24089694023132 s
