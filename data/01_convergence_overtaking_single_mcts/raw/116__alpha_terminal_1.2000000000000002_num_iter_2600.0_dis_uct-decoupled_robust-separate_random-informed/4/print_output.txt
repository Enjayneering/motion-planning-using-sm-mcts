Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 296, 'sum_payoffs': 84.30516046558758, 'action': [1.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 83.89065482702493, 'action': [2.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 77.44826252477142, 'action': [0.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 81.43492644767433, 'action': [1.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 79.10667840416353, 'action': [1.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 73.89957251817435, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 76.67004543354089, 'action': [0.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 86.66188530396282, 'action': [2.0, -1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 89.5285565607879, 'action': [2.0, 0.0]}])
Weights num count: [0.11380238369857747, 0.11341791618608228, 0.10726643598615918, 0.1111111111111111, 0.10880430603613994, 0.10380622837370242, 0.10649750096116878, 0.11610918877354863, 0.11880046136101499]
Actions to choose Agent 1: dict_values([{'num_count': 422, 'sum_payoffs': 112.18212032764244, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 129.43500529479326, 'action': [1.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 123.86322252713009, 'action': [1.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 125.76578455290385, 'action': [1.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 99.95077880279108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 107.11931014794321, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16224529027297194, 0.18031526336024606, 0.17454825067281815, 0.17647058823529413, 0.14917339484813533, 0.1568627450980392]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 42.97345590591431 s
