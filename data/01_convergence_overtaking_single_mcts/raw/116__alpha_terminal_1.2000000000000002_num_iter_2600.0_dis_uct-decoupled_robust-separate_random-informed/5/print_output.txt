Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 278, 'sum_payoffs': 76.91802804941165, 'action': [0.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 82.03333672389891, 'action': [1.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 78.88746714966396, 'action': [1.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 84.8140159886716, 'action': [2.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 84.44476335236914, 'action': [2.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 83.91840696344059, 'action': [1.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 74.91417525345317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 89.28719245378655, 'action': [2.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 75.62186161295945, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10688196847366398, 0.1118800461361015, 0.10880430603613994, 0.11457131872356786, 0.11418685121107267, 0.11380238369857747, 0.104959630911188, 0.11880046136101499, 0.1057285659361784]
Actions to choose Agent 1: dict_values([{'num_count': 414, 'sum_payoffs': 109.7260660745095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 109.41256227854493, 'action': [0.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 107.28023886208398, 'action': [0.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 117.43025861962379, 'action': [1.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 126.62876755394038, 'action': [1.0, 1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 130.65356158532862, 'action': [1.0, 0.0]}])
Weights num count: [0.15916955017301038, 0.1587850826605152, 0.15647827758554403, 0.16724336793540945, 0.1768550557477893, 0.18108419838523646]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 41.43266797065735 s
