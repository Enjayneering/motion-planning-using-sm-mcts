Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 273, 'sum_payoffs': 74.77352389714441, 'action': [0.0, 1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 89.0224405302622, 'action': [2.0, -1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 87.85311070195007, 'action': [2.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 81.5273298154078, 'action': [1.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 77.95241087915693, 'action': [0.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 83.0750761933871, 'action': [2.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 75.99534350337521, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 79.39261257922792, 'action': [1.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 79.94145284915423, 'action': [1.0, 0.0]}])
Weights num count: [0.104959630911188, 0.11880046136101499, 0.11764705882352941, 0.1114955786236063, 0.10803537101114956, 0.11303344867358708, 0.1061130334486736, 0.10957324106113034, 0.10995770857362552]
Actions to choose Agent 1: dict_values([{'num_count': 441, 'sum_payoffs': 120.01787644688267, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 113.90839839042346, 'action': [0.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 122.27666428293698, 'action': [1.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 108.04539374098032, 'action': [0.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 127.82950673902928, 'action': [1.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 111.73363245775012, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1695501730103806, 0.16301422529796233, 0.17185697808535177, 0.1568627450980392, 0.1776239907727797, 0.16070742022299117]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 42.42355728149414 s
