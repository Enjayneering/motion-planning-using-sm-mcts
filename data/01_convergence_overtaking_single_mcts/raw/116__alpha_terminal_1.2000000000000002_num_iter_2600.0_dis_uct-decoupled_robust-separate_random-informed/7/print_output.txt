Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 299, 'sum_payoffs': 85.18039290488767, 'action': [2.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 80.39842945241463, 'action': [1.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 74.45467203699502, 'action': [0.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 81.50277036994416, 'action': [1.0, 0.0]}, {'num_count': 303, 'sum_payoffs': 86.733621511325, 'action': [2.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 76.81240766643357, 'action': [0.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 81.9641745132367, 'action': [1.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 75.24371305804915, 'action': [0.0, 1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 87.9458590943803, 'action': [2.0, 0.0]}])
Weights num count: [0.11495578623606305, 0.11034217608612072, 0.10457516339869281, 0.1114955786236063, 0.11649365628604383, 0.10688196847366398, 0.1118800461361015, 0.1053440984236832, 0.11764705882352941]
Actions to choose Agent 1: dict_values([{'num_count': 445, 'sum_payoffs': 120.4398535330672, 'action': [1.0, -1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 129.63951938030488, 'action': [1.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 108.2487166515383, 'action': [0.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 108.20621931309735, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 125.60065908251437, 'action': [1.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 105.68944685857915, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1710880430603614, 0.18069973087274124, 0.1580161476355248, 0.1580161476355248, 0.17647058823529413, 0.15532487504805845]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.04734444618225 s
