Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 278, 'sum_payoffs': 77.11197914558318, 'action': [0.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 75.51474949867436, 'action': [0.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 82.55223787071198, 'action': [1.0, 1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 88.19126125236325, 'action': [2.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 86.28581025064071, 'action': [2.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 75.43901026707739, 'action': [0.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 76.25657952577025, 'action': [1.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 83.41457602697969, 'action': [1.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 87.87150625287474, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10688196847366398, 0.1053440984236832, 0.1122645136485967, 0.11764705882352941, 0.11572472126105345, 0.1053440984236832, 0.1061130334486736, 0.11303344867358708, 0.11726259131103421]
Actions to choose Agent 1: dict_values([{'num_count': 417, 'sum_payoffs': 110.48635596112724, 'action': [0.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 126.59656842902142, 'action': [1.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 123.16994864998513, 'action': [1.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 110.44949459871468, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 105.16433609689834, 'action': [0.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 122.89349916610878, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16032295271049596, 0.1772395232602845, 0.17377931564782775, 0.16032295271049596, 0.15455594002306805, 0.17339484813533257]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.730414152145386 s
