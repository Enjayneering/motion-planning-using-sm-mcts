Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 298, 'sum_payoffs': 84.81162168435159, 'action': [2.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 83.9507266883219, 'action': [1.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 76.87019808274164, 'action': [1.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 85.61236116222453, 'action': [2.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 73.58356964471078, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 76.36487754653594, 'action': [0.0, 0.0]}, {'num_count': 305, 'sum_payoffs': 87.5651804140543, 'action': [2.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 81.21357385298451, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 80.40257730286106, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11457131872356786, 0.11380238369857747, 0.10688196847366398, 0.11534025374855825, 0.10380622837370242, 0.10649750096116878, 0.11726259131103421, 0.1111111111111111, 0.11034217608612072]
Actions to choose Agent 1: dict_values([{'num_count': 464, 'sum_payoffs': 127.14994875521724, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 105.73329877261473, 'action': [0.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 124.62670311874399, 'action': [1.0, 1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 104.31367330193551, 'action': [0.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 121.9171158274293, 'action': [1.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 112.18477799051178, 'action': [0.0, 0.0]}])
Weights num count: [0.17839292579777008, 0.15570934256055363, 0.17570165321030373, 0.15417147251057287, 0.17301038062283736, 0.16262975778546712]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.071399211883545 s
