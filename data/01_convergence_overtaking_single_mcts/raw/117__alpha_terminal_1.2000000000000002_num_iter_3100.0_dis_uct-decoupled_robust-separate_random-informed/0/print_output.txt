Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 332, 'sum_payoffs': 91.11071888923675, 'action': [0.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 91.18032404193396, 'action': [0.0, 0.0]}, {'num_count': 369, 'sum_payoffs': 105.47715885773069, 'action': [2.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 98.83041929585828, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 92.9811232537339, 'action': [1.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 99.1958894406549, 'action': [2.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 103.57966857173602, 'action': [2.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 93.0110087774322, 'action': [1.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 88.10429113918406, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10706223798774589, 0.10706223798774589, 0.11899387294421154, 0.11351177039664624, 0.10867462108997097, 0.11383424701709126, 0.11738148984198646, 0.10867462108997097, 0.10448242502418574]
Actions to choose Agent 1: dict_values([{'num_count': 478, 'sum_payoffs': 124.73394808616375, 'action': [0.0, 1.5707963267948966]}, {'num_count': 550, 'sum_payoffs': 150.49601880073237, 'action': [1.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 145.84979729698787, 'action': [1.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 129.36230949191486, 'action': [0.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 146.92521067406278, 'action': [1.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 134.0653030932474, 'action': [0.0, 0.0]}])
Weights num count: [0.15414382457271847, 0.17736214124475974, 0.17316994517897452, 0.15833602063850372, 0.17413737504030957, 0.16252821670428894]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.998239517211914 s
