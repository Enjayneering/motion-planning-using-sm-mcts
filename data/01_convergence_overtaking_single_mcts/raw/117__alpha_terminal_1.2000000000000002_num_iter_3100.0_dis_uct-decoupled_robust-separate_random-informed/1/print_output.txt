Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 349, 'sum_payoffs': 97.42550262814834, 'action': [1.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 96.99353528889458, 'action': [1.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 89.77197301342592, 'action': [0.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 93.62965878058947, 'action': [1.0, 0.0]}, {'num_count': 324, 'sum_payoffs': 87.90625011026178, 'action': [0.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 100.9294773632357, 'action': [2.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 102.4834571048112, 'action': [2.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 90.54079216588318, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 101.76732364821183, 'action': [2.0, 0.0]}])
Weights num count: [0.11254434053531119, 0.11222186391486617, 0.10609480812641084, 0.10931957433086101, 0.10448242502418574, 0.11544663011931634, 0.11673653660109642, 0.10673976136730087, 0.11609158336020639]
Actions to choose Agent 1: dict_values([{'num_count': 475, 'sum_payoffs': 124.51187498303756, 'action': [0.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 147.68375885598243, 'action': [1.0, 1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 156.05264348851676, 'action': [1.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 147.74326461014374, 'action': [1.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 133.03753596882754, 'action': [0.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 127.37440188837864, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15317639471138342, 0.17413737504030957, 0.181554337310545, 0.17413737504030957, 0.16091583360206385, 0.15575620767494355]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.38006520271301 s
