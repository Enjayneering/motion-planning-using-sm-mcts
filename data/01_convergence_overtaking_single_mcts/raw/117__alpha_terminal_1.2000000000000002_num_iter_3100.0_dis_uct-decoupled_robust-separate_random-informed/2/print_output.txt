Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 360, 'sum_payoffs': 102.18113385479258, 'action': [2.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 99.0133272482788, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 92.84264215382223, 'action': [1.0, 1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 97.77418346038087, 'action': [1.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 90.51384415762088, 'action': [0.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 93.2365307683189, 'action': [1.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 90.12718129405543, 'action': [0.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 94.76197514105269, 'action': [0.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 104.42920523775412, 'action': [2.0, 0.0]}])
Weights num count: [0.11609158336020639, 0.11351177039664624, 0.10835214446952596, 0.11254434053531119, 0.10641728474685586, 0.10867462108997097, 0.10609480812641084, 0.10996452757175104, 0.11802644308287649]
Actions to choose Agent 1: dict_values([{'num_count': 569, 'sum_payoffs': 157.0802375438752, 'action': [1.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 129.62169464385144, 'action': [0.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 141.58637797230728, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 134.58315695596107, 'action': [0.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 123.55736538553407, 'action': [0.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 143.80735525438547, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1834891970332151, 0.15865849725894873, 0.16962270235407934, 0.16317316994517897, 0.15317639471138342, 0.17155756207674944]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 52.86579346656799 s
