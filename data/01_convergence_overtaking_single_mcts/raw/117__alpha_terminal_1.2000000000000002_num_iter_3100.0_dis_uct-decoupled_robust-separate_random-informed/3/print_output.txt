Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 337, 'sum_payoffs': 93.46673280456639, 'action': [1.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 102.67179403586017, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 92.5867963433699, 'action': [0.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 99.61777064821298, 'action': [2.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 101.97575989360384, 'action': [2.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 96.93596663291318, 'action': [1.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 90.35647481511391, 'action': [0.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 89.52981772222522, 'action': [0.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 99.64084363698821, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10867462108997097, 0.11641405998065141, 0.10802966784908094, 0.11383424701709126, 0.11576910673976137, 0.11157691067397614, 0.10609480812641084, 0.1054498548855208, 0.11383424701709126]
Actions to choose Agent 1: dict_values([{'num_count': 530, 'sum_payoffs': 143.3601289549269, 'action': [1.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 143.7024959235013, 'action': [1.0, 1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 135.2016861293254, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 130.11919443917438, 'action': [0.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 127.98122925803823, 'action': [0.0, 1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 151.2279790877169, 'action': [1.0, 0.0]}])
Weights num count: [0.1709126088358594, 0.17123508545630442, 0.163495646565624, 0.15898097387939375, 0.15704611415672365, 0.1780070944856498]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.43017220497131 s
