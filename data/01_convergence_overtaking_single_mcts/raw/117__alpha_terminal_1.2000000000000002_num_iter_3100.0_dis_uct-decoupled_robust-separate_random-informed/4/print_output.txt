Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 327, 'sum_payoffs': 89.65969277035073, 'action': [0.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 99.01971168991165, 'action': [2.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 97.07752865784684, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 102.46845582320606, 'action': [2.0, 0.0]}, {'num_count': 347, 'sum_payoffs': 97.37045352127647, 'action': [1.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 92.34757946572633, 'action': [0.0, 0.0]}, {'num_count': 352, 'sum_payoffs': 99.40934722128209, 'action': [2.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 91.18184678472032, 'action': [0.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 99.37968126244101, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1054498548855208, 0.11318929377620122, 0.11157691067397614, 0.11609158336020639, 0.11189938729442116, 0.10770719122863592, 0.11351177039664624, 0.10673976136730087, 0.11351177039664624]
Actions to choose Agent 1: dict_values([{'num_count': 509, 'sum_payoffs': 135.6654942884193, 'action': [0.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 124.99474043968183, 'action': [0.0, 1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 159.58657391231694, 'action': [1.0, 0.0]}, {'num_count': 539, 'sum_payoffs': 146.36467642457183, 'action': [1.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 121.7897117468171, 'action': [0.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 141.99578765061568, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16414059980651402, 0.15446630119316349, 0.1857465333763302, 0.17381489841986456, 0.15156401160915833, 0.16994517897452435]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 48.04196524620056 s
