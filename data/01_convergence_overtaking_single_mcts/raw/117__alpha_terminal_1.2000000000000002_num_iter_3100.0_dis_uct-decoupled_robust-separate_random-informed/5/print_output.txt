Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 336, 'sum_payoffs': 92.88093534979578, 'action': [1.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 96.40498494377758, 'action': [2.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 99.05876599765452, 'action': [2.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 108.07149846858118, 'action': [2.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 90.59231221829225, 'action': [0.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 89.06201500742348, 'action': [0.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 96.46766589947474, 'action': [1.0, 0.0]}, {'num_count': 334, 'sum_payoffs': 92.15377270833045, 'action': [0.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 101.03069007323187, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10835214446952596, 0.11125443405353112, 0.11351177039664624, 0.12092873266688164, 0.10641728474685586, 0.10512737826507579, 0.11125443405353112, 0.10770719122863592, 0.11512415349887133]
Actions to choose Agent 1: dict_values([{'num_count': 492, 'sum_payoffs': 130.66016369136457, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 130.99771317260053, 'action': [0.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 128.1067477542773, 'action': [0.0, -1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 145.0290991975407, 'action': [1.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 147.17679708238896, 'action': [1.0, 1.5707963267948966]}, {'num_count': 560, 'sum_payoffs': 155.10352704055478, 'action': [1.0, 0.0]}])
Weights num count: [0.15865849725894873, 0.15898097387939375, 0.15640116091583361, 0.17155756207674944, 0.17349242179941954, 0.18058690744920994]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.538461685180664 s
