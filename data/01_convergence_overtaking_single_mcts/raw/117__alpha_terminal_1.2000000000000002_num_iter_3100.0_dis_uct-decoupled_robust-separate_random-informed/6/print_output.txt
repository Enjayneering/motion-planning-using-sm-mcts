Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 331, 'sum_payoffs': 91.31905183685356, 'action': [0.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 101.10414526824049, 'action': [2.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 103.00736189584286, 'action': [2.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 94.72702606706069, 'action': [1.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 88.18153603875255, 'action': [0.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 103.01387178223676, 'action': [2.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 98.34199998345878, 'action': [1.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 100.96526270717332, 'action': [1.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 88.29949219641568, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10673976136730087, 0.11480167687842631, 0.11641405998065141, 0.10964205095130602, 0.10415994840374072, 0.11641405998065141, 0.11254434053531119, 0.11480167687842631, 0.10415994840374072]
Actions to choose Agent 1: dict_values([{'num_count': 495, 'sum_payoffs': 130.37583325509382, 'action': [0.0, 1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 129.70205108071121, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 142.03909976211165, 'action': [1.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 126.87432141332339, 'action': [0.0, -1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 151.40902601755192, 'action': [1.0, 0.0]}, {'num_count': 545, 'sum_payoffs': 148.20948526420412, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15962592712028378, 0.15898097387939375, 0.17026765559496937, 0.15640116091583361, 0.17865204772653984, 0.17574975814253466]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.33020520210266 s
