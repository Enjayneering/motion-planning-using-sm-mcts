Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 341, 'sum_payoffs': 94.96544371445732, 'action': [1.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 89.65828191499175, 'action': [0.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 99.708020758737, 'action': [2.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 103.1619999910097, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 92.71123625300388, 'action': [1.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 90.3702699223258, 'action': [0.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 108.15756821535172, 'action': [2.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 95.76062424699269, 'action': [1.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 92.67320485509538, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10996452757175104, 0.1054498548855208, 0.11383424701709126, 0.11673653660109642, 0.10802966784908094, 0.10609480812641084, 0.12092873266688164, 0.11060948081264109, 0.10802966784908094]
Actions to choose Agent 1: dict_values([{'num_count': 555, 'sum_payoffs': 152.33815154854958, 'action': [1.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 133.07571578238083, 'action': [0.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 140.10179992358405, 'action': [1.0, 1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 148.40546607144827, 'action': [1.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 128.7199293811601, 'action': [0.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 129.08545043077515, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17897452434698485, 0.16156078684295389, 0.16801031925185425, 0.17542728152208964, 0.15769106739761368, 0.1580135440180587]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.68123698234558 s
