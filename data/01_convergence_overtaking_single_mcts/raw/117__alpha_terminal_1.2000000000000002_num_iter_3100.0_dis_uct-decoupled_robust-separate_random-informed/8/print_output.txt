Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 350, 'sum_payoffs': 98.30943924262606, 'action': [1.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 92.52458560240136, 'action': [0.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 101.8423827039115, 'action': [2.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 100.33657153391707, 'action': [2.0, 0.0]}, {'num_count': 334, 'sum_payoffs': 92.16375767503719, 'action': [1.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 97.62060967218403, 'action': [1.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 100.7494230077892, 'action': [2.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 87.90075788986027, 'action': [0.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 94.46799784781714, 'action': [0.0, 0.0]}])
Weights num count: [0.11286681715575621, 0.10802966784908094, 0.11576910673976137, 0.11447920025798129, 0.10770719122863592, 0.11222186391486617, 0.11480167687842631, 0.10415994840374072, 0.10964205095130602]
Actions to choose Agent 1: dict_values([{'num_count': 495, 'sum_payoffs': 130.84934623009718, 'action': [0.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 125.17609100679745, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 145.11714354165576, 'action': [1.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 128.3834211320547, 'action': [0.0, -1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 154.7860311404203, 'action': [1.0, 0.0]}, {'num_count': 541, 'sum_payoffs': 147.24390814134455, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15962592712028378, 0.15446630119316349, 0.1725249919380845, 0.15736859077716867, 0.18123186069009997, 0.1744598516607546]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.92205262184143 s
