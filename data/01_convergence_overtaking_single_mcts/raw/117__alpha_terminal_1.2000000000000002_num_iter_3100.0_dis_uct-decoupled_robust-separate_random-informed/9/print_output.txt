Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 96.5926043601435, 'action': [2.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 95.35426067585419, 'action': [1.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 96.54219939509751, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 104.72353551113771, 'action': [2.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 92.97649762219321, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 101.96748650826808, 'action': [2.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 90.24135178714828, 'action': [0.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 95.76798847612928, 'action': [1.0, 0.0]}, {'num_count': 327, 'sum_payoffs': 89.08374182675136, 'action': [0.0, 0.0]}])
Weights num count: [0.11157691067397614, 0.11060948081264109, 0.11157691067397614, 0.11834891970332151, 0.10867462108997097, 0.11609158336020639, 0.10641728474685586, 0.1109319574330861, 0.1054498548855208]
Actions to choose Agent 1: dict_values([{'num_count': 541, 'sum_payoffs': 147.83659480348842, 'action': [1.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 146.35000360880485, 'action': [1.0, -1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 125.68189863589349, 'action': [0.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 127.51962175489999, 'action': [0.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 134.22769116190696, 'action': [0.0, 0.0]}, {'num_count': 556, 'sum_payoffs': 153.23402757005422, 'action': [1.0, 0.0]}])
Weights num count: [0.1744598516607546, 0.17316994517897452, 0.15446630119316349, 0.1560786842953886, 0.16220574008384392, 0.17929700096742987]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.02309274673462 s
