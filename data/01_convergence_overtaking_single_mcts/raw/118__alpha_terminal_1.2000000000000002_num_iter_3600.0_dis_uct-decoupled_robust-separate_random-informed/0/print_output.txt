Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 405, 'sum_payoffs': 112.46913910493437, 'action': [1.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 112.2239551612298, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 108.06741742120593, 'action': [1.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 108.42977121319488, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 115.63550144672985, 'action': [2.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 105.45301324237384, 'action': [0.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 120.17235528380496, 'action': [2.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 111.09991856775515, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 102.75069852064847, 'action': [0.0, 0.0]}])
Weights num count: [0.11246875867814496, 0.1121910580394335, 0.10913635101360733, 0.1094140516523188, 0.11469036378783672, 0.10719244654262705, 0.11802277145237434, 0.11135795612329909, 0.10524854207164676]
Actions to choose Agent 1: dict_values([{'num_count': 569, 'sum_payoffs': 149.42981141025442, 'action': [0.0, -1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 169.26265745509016, 'action': [1.0, -1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 147.00175411660393, 'action': [0.0, 1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 172.47707402108492, 'action': [1.0, 1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 148.34427765766824, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 174.98145589195406, 'action': [1.0, 0.0]}])
Weights num count: [0.15801166342682588, 0.17384059983337963, 0.1560677589558456, 0.17633990558178284, 0.15717856151069148, 0.17828381005276311]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.69834494590759 s
