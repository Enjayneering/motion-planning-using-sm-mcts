Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 396, 'sum_payoffs': 109.41735785548862, 'action': [1.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 119.26978610295815, 'action': [2.0, -1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 100.69260946266797, 'action': [0.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 112.75371880890215, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 103.36675597106317, 'action': [0.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 123.91113513631592, 'action': [2.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 103.29220726669773, 'action': [0.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 112.45472218135338, 'action': [2.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 113.14639049638333, 'action': [1.0, 0.0]}])
Weights num count: [0.10996945292974174, 0.11718966953623994, 0.10358233823937794, 0.11246875867814496, 0.10552624271035824, 0.12052207720077757, 0.10552624271035824, 0.1121910580394335, 0.11274645931685642]
Actions to choose Agent 1: dict_values([{'num_count': 569, 'sum_payoffs': 148.71713967299505, 'action': [0.0, 1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 167.2723451894753, 'action': [1.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 176.3001337298481, 'action': [1.0, 0.0]}, {'num_count': 618, 'sum_payoffs': 165.78885894641698, 'action': [1.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 152.53609805332752, 'action': [0.0, 0.0]}, {'num_count': 563, 'sum_payoffs': 146.6372083855384, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15801166342682588, 0.17272979727853374, 0.17995001388503193, 0.17161899472368786, 0.16106637045265204, 0.15634545959455706]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.355817794799805 s
