Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 403, 'sum_payoffs': 111.79708088816736, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 104.64336122417113, 'action': [0.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 102.75437700165625, 'action': [0.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 107.6564657499121, 'action': [1.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 118.97439642284077, 'action': [2.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 104.19000509794957, 'action': [0.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 124.76962616836765, 'action': [2.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 112.58135368330001, 'action': [2.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 108.78684674385363, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11191335740072202, 0.10663704526520411, 0.10524854207164676, 0.10885865037489587, 0.11718966953623994, 0.10635934462649264, 0.12135517911691197, 0.11246875867814496, 0.10969175229103027]
Actions to choose Agent 1: dict_values([{'num_count': 565, 'sum_payoffs': 147.74655191323188, 'action': [0.0, 1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 173.83279073225617, 'action': [1.0, -1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 142.87768146960892, 'action': [0.0, -1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 171.78512143643673, 'action': [1.0, 1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 170.06292824337726, 'action': [1.0, 0.0]}, {'num_count': 581, 'sum_payoffs': 153.1783096711106, 'action': [0.0, 0.0]}])
Weights num count: [0.15690086087198002, 0.17772840877534019, 0.15301305193001943, 0.17606220494307137, 0.17467370174951402, 0.1613440710913635]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 56.55748963356018 s
