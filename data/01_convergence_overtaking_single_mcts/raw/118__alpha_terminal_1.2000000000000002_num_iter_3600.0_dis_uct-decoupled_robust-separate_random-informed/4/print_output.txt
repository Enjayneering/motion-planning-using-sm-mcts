Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 405, 'sum_payoffs': 112.55222644922364, 'action': [2.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 113.31348069396878, 'action': [1.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 103.95979481150144, 'action': [0.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 113.80676220220133, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 103.19982423384742, 'action': [0.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 116.08118727799807, 'action': [2.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 106.20564644742106, 'action': [1.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 120.63075597212126, 'action': [2.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 106.94565675427023, 'action': [0.0, 0.0]}])
Weights num count: [0.11246875867814496, 0.1130241599555679, 0.10608164398778117, 0.11330186059427937, 0.10552624271035824, 0.11496806442654818, 0.10774784782004998, 0.11830047209108581, 0.10830324909747292]
Actions to choose Agent 1: dict_values([{'num_count': 572, 'sum_payoffs': 150.1664701940039, 'action': [0.0, 0.0]}, {'num_count': 553, 'sum_payoffs': 143.63303405205625, 'action': [0.0, -1.5707963267948966]}, {'num_count': 567, 'sum_payoffs': 148.51833623233352, 'action': [0.0, 1.5707963267948966]}, {'num_count': 671, 'sum_payoffs': 184.78865763320275, 'action': [1.0, 0.0]}, {'num_count': 605, 'sum_payoffs': 161.60577884807157, 'action': [1.0, 1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 171.13806545118487, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1588447653429603, 0.1535684532074424, 0.15745626214940295, 0.18633712857539572, 0.16800888642043876, 0.17550680366564844]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 55.93179273605347 s
