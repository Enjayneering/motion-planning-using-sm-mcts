Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 409, 'sum_payoffs': 114.18430795475126, 'action': [2.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 110.66388570467363, 'action': [1.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 120.3272665781035, 'action': [2.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 99.93379458748443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 106.66746289094573, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 110.14060039303926, 'action': [1.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 123.7888523183882, 'action': [2.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 105.96745436277129, 'action': [0.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 105.95491220221973, 'action': [1.0, 0.0]}])
Weights num count: [0.11357956123299083, 0.11108025548458761, 0.11802277145237434, 0.10302693696195502, 0.10802554845876146, 0.11052485420716468, 0.12052207720077757, 0.10747014718133852, 0.10747014718133852]
Actions to choose Agent 1: dict_values([{'num_count': 557, 'sum_payoffs': 144.2670809900004, 'action': [0.0, -1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 177.34557085418078, 'action': [1.0, 0.0]}, {'num_count': 550, 'sum_payoffs': 141.75925269495522, 'action': [0.0, 1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 167.93773333717212, 'action': [1.0, 1.5707963267948966]}, {'num_count': 592, 'sum_payoffs': 156.39575724501614, 'action': [0.0, 0.0]}, {'num_count': 624, 'sum_payoffs': 167.4877269810906, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15467925576228825, 0.18106081643987781, 0.15273535129130797, 0.17356289919466814, 0.16439877811718967, 0.17328519855595667]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.19536304473877 s
