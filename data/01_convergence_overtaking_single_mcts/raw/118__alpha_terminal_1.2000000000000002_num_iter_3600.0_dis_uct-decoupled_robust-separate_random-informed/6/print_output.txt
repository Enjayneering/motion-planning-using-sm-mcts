Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 421, 'sum_payoffs': 118.46910011635093, 'action': [2.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 110.35794010662147, 'action': [1.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 111.22932604244501, 'action': [1.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 105.87051548715542, 'action': [0.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 116.500629436096, 'action': [2.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 99.91317794582918, 'action': [0.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 117.28690884470089, 'action': [2.0, -1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 111.63067302749863, 'action': [1.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 102.94725172251552, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11691196889752846, 0.11108025548458761, 0.11163565676201055, 0.10774784782004998, 0.11552346570397112, 0.10330463760066648, 0.11607886698139405, 0.11191335740072202, 0.10552624271035824]
Actions to choose Agent 1: dict_values([{'num_count': 639, 'sum_payoffs': 173.3871711772826, 'action': [1.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 174.0871409209656, 'action': [1.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 151.31909163519632, 'action': [0.0, -1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 163.57178715785972, 'action': [1.0, 1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 142.73186798721858, 'action': [0.0, 1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 153.48623894086438, 'action': [0.0, 0.0]}])
Weights num count: [0.17745070813662872, 0.17800610941405165, 0.15995556789780616, 0.16967509025270758, 0.15301305193001943, 0.16162177173007497]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 56.603458642959595 s
