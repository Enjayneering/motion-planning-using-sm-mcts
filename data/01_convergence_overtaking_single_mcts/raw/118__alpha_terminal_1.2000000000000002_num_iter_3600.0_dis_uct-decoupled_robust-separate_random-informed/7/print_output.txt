Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 389, 'sum_payoffs': 106.5168894214453, 'action': [0.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 121.33239832553548, 'action': [2.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 114.46723201327896, 'action': [2.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 110.63254947628016, 'action': [1.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 115.61950619778304, 'action': [2.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 103.49234537798324, 'action': [0.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 105.41348174919854, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 109.46609977469178, 'action': [1.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 109.17927606496342, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10802554845876146, 0.11885587336850875, 0.11385726187170231, 0.11108025548458761, 0.11469036378783672, 0.1058039433490697, 0.10719244654262705, 0.1102471535684532, 0.10996945292974174]
Actions to choose Agent 1: dict_values([{'num_count': 571, 'sum_payoffs': 149.8588191838907, 'action': [0.0, 1.5707963267948966]}, {'num_count': 630, 'sum_payoffs': 170.53436607622834, 'action': [1.0, -1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 181.41449301460378, 'action': [1.0, 0.0]}, {'num_count': 566, 'sum_payoffs': 148.14848227515895, 'action': [0.0, 0.0]}, {'num_count': 552, 'sum_payoffs': 143.33935166512197, 'action': [0.0, -1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 166.88097280456665, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15856706470424883, 0.17495140238822549, 0.18356012218828102, 0.15717856151069148, 0.1532907525687309, 0.1721743960011108]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 63.203124046325684 s
