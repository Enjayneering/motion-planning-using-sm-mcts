Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 403, 'sum_payoffs': 112.23898434901665, 'action': [2.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 105.7912661811059, 'action': [0.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 104.54592826251803, 'action': [0.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 111.44651078259592, 'action': [1.0, -1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 100.86436215230138, 'action': [0.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 106.09842830784133, 'action': [1.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 119.82991244930936, 'action': [2.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 111.05187643930394, 'action': [1.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 127.86436953123028, 'action': [2.0, 0.0]}])
Weights num count: [0.11191335740072202, 0.10719244654262705, 0.10635934462649264, 0.11135795612329909, 0.10358233823937794, 0.10747014718133852, 0.1174673701749514, 0.11108025548458761, 0.12329908358789225]
Actions to choose Agent 1: dict_values([{'num_count': 624, 'sum_payoffs': 168.09842937140388, 'action': [1.0, -1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 171.6173470487495, 'action': [1.0, 1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 145.40065825162262, 'action': [0.0, -1.5707963267948966]}, {'num_count': 639, 'sum_payoffs': 173.4482667210442, 'action': [1.0, 0.0]}, {'num_count': 581, 'sum_payoffs': 153.0841037544571, 'action': [0.0, 0.0]}, {'num_count': 563, 'sum_payoffs': 146.92492989488915, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17328519855595667, 0.17606220494307137, 0.1552346570397112, 0.17745070813662872, 0.1613440710913635, 0.15634545959455706]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 55.00296354293823 s
