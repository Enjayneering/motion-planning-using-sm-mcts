Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 389, 'sum_payoffs': 106.41635494138797, 'action': [0.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 107.5446333547057, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 115.07831459216555, 'action': [2.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 110.57548948715034, 'action': [1.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 105.26312132489356, 'action': [0.0, -1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 101.9257741878408, 'action': [0.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 114.58944911351647, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 114.66858826323643, 'action': [2.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 118.84933549645183, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10802554845876146, 0.10885865037489587, 0.11441266314912524, 0.11108025548458761, 0.10719244654262705, 0.10469314079422383, 0.11413496251041377, 0.11413496251041377, 0.11718966953623994]
Actions to choose Agent 1: dict_values([{'num_count': 626, 'sum_payoffs': 168.8627775367551, 'action': [1.0, 1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 178.28262286024085, 'action': [1.0, 0.0]}, {'num_count': 566, 'sum_payoffs': 147.88632460692585, 'action': [0.0, -1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 146.4964714323064, 'action': [0.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 151.03808821151082, 'action': [0.0, 0.0]}, {'num_count': 618, 'sum_payoffs': 166.0257060584284, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17384059983337963, 0.18133851707858928, 0.15717856151069148, 0.1560677589558456, 0.1596778672590947, 0.17161899472368786]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.45210909843445 s
