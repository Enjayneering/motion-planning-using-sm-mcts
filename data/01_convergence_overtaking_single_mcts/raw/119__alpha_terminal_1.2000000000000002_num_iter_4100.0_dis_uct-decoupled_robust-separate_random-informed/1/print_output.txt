Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 446, 'sum_payoffs': 121.85313886455044, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 125.55869513065534, 'action': [2.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 114.77663949126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 142.70985267959168, 'action': [2.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 135.54510723846013, 'action': [2.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 126.63004273854295, 'action': [1.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 118.12653960056684, 'action': [1.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 127.04155936932555, 'action': [1.0, -1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 116.3452730354914, 'action': [0.0, 0.0]}])
Weights num count: [0.10875396244818337, 0.11119239209948793, 0.1041209461107047, 0.1224091684954889, 0.11777615215801024, 0.1119239209948793, 0.1063155327968788, 0.11216776396000976, 0.10509631797122652]
Actions to choose Agent 1: dict_values([{'num_count': 695, 'sum_payoffs': 184.53433004200312, 'action': [1.0, 1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 166.89437404401113, 'action': [0.0, -1.5707963267948966]}, {'num_count': 668, 'sum_payoffs': 175.40827110375403, 'action': [0.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 197.69607904580147, 'action': [1.0, 0.0]}, {'num_count': 641, 'sum_payoffs': 166.21446479874183, 'action': [0.0, 1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 193.18630625963928, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16947086076566692, 0.1567910265788832, 0.1628871007071446, 0.17873689344062424, 0.15630334064862228, 0.1755669348939283]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 63.82844805717468 s
