Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 466, 'sum_payoffs': 129.17025792705033, 'action': [2.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 136.1927652453106, 'action': [2.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 142.97431956398387, 'action': [2.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 122.3981955194537, 'action': [1.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 126.15542519321052, 'action': [1.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 117.6977083883569, 'action': [0.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 118.07072988437544, 'action': [0.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 120.9429971233621, 'action': [1.0, 0.0]}, {'num_count': 425, 'sum_payoffs': 113.93492688533254, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11363082175079249, 0.11826383808827115, 0.12265301146061935, 0.10924164837844429, 0.11168007802974884, 0.10607168983174835, 0.1063155327968788, 0.10826627651792246, 0.10363326018044379]
Actions to choose Agent 1: dict_values([{'num_count': 707, 'sum_payoffs': 188.3156784979102, 'action': [1.0, -1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 201.11701447315005, 'action': [1.0, 0.0]}, {'num_count': 625, 'sum_payoffs': 160.48400188094143, 'action': [0.0, -1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 190.78475559892664, 'action': [1.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 172.28013135462032, 'action': [0.0, 0.0]}, {'num_count': 650, 'sum_payoffs': 168.87558316331263, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1723969763472324, 0.18141916605705927, 0.152401853206535, 0.17410387710314557, 0.16093635698610095, 0.1584979273347964]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 63.470884799957275 s
