Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 428, 'sum_payoffs': 115.00026015045776, 'action': [0.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 128.72956708060366, 'action': [2.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 126.05638143874609, 'action': [1.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 127.17553473819254, 'action': [1.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 128.31854376613572, 'action': [2.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 120.54541761387631, 'action': [0.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 120.20064552982323, 'action': [1.0, 1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 144.65767030018756, 'action': [2.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 116.10955662921911, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10436478907583516, 0.11338697878566203, 0.11168007802974884, 0.11241160692514021, 0.11314313582053158, 0.10802243355279201, 0.10777859058766155, 0.12387222628627165, 0.10509631797122652]
Actions to choose Agent 1: dict_values([{'num_count': 738, 'sum_payoffs': 199.5845482638566, 'action': [1.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 196.40118601842767, 'action': [1.0, 1.5707963267948966]}, {'num_count': 639, 'sum_payoffs': 165.577968576134, 'action': [0.0, -1.5707963267948966]}, {'num_count': 713, 'sum_payoffs': 190.94491380442977, 'action': [1.0, -1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 168.30330664016813, 'action': [0.0, 0.0]}, {'num_count': 634, 'sum_payoffs': 163.97166363235095, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1799561082662765, 0.17776152158010242, 0.15581565471836137, 0.17386003413801512, 0.15776639843940501, 0.1545964398927091]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.05805921554565 s
