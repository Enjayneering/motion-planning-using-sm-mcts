Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 485, 'sum_payoffs': 136.1743137152443, 'action': [2.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 125.08680085439357, 'action': [1.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 131.76656634283566, 'action': [2.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 113.3163480825251, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 122.49804536072551, 'action': [1.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 114.01345483633615, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 129.5654638277385, 'action': [1.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 116.85674406034305, 'action': [0.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 138.53576266714853, 'action': [2.0, 0.0]}])
Weights num count: [0.11826383808827115, 0.11094854913435748, 0.11533772250670568, 0.10314557425018288, 0.10924164837844429, 0.10363326018044379, 0.11387466471592295, 0.10558400390148744, 0.11972689587905389]
Actions to choose Agent 1: dict_values([{'num_count': 646, 'sum_payoffs': 167.74487950030908, 'action': [0.0, -1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 165.6857748152211, 'action': [0.0, 1.5707963267948966]}, {'num_count': 724, 'sum_payoffs': 194.42881219415548, 'action': [1.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 167.38483902865426, 'action': [0.0, 0.0]}, {'num_count': 738, 'sum_payoffs': 199.20440767854194, 'action': [1.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 188.53511508778814, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15752255547427457, 0.15605949768349184, 0.17654230675445012, 0.1572787125091441, 0.1799561082662765, 0.1723969763472324]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 64.60831570625305 s
