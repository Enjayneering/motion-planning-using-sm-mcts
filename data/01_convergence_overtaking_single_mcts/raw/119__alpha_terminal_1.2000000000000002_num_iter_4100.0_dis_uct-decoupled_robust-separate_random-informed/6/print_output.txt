Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 481, 'sum_payoffs': 134.91103060906212, 'action': [2.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 114.85387096845206, 'action': [0.0, 1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 139.35193865486283, 'action': [2.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 120.37189446080963, 'action': [0.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 123.07975463476437, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 119.28197167426632, 'action': [0.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 127.89060460388133, 'action': [1.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 124.1609234785766, 'action': [1.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 125.24162535595532, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11728846622774933, 0.1041209461107047, 0.1202145818093148, 0.10777859058766155, 0.10948549134357474, 0.10704706169227018, 0.11265544989027067, 0.11021702023896611, 0.11094854913435748]
Actions to choose Agent 1: dict_values([{'num_count': 657, 'sum_payoffs': 171.30323564680302, 'action': [0.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 167.2107528908597, 'action': [0.0, 0.0]}, {'num_count': 722, 'sum_payoffs': 193.51082406046459, 'action': [1.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 168.18618963870802, 'action': [0.0, 1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 191.83048588396295, 'action': [1.0, 0.0]}, {'num_count': 711, 'sum_payoffs': 189.74389374397805, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16020482809070957, 0.1572787125091441, 0.1760546208241892, 0.15801024140453548, 0.17483540599853695, 0.1733723482077542]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.62890696525574 s
