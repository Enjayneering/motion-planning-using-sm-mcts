Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 497, 'sum_payoffs': 141.19206467705519, 'action': [2.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 123.96567831398043, 'action': [1.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 119.24568206300724, 'action': [0.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 120.28678226467049, 'action': [1.0, 1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 134.38530226883273, 'action': [2.0, -1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 112.48766655548062, 'action': [0.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 132.9021313361556, 'action': [2.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 127.40309229676953, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 119.61026255109739, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12118995366983662, 0.10997317727383565, 0.10680321872713973, 0.1075347476225311, 0.11680078029748842, 0.10241404535479151, 0.11582540843696659, 0.11216776396000976, 0.10704706169227018]
Actions to choose Agent 1: dict_values([{'num_count': 708, 'sum_payoffs': 188.46485295022444, 'action': [1.0, -1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 171.75117193369863, 'action': [0.0, 0.0]}, {'num_count': 742, 'sum_payoffs': 200.04449315204096, 'action': [1.0, 0.0]}, {'num_count': 656, 'sum_payoffs': 170.74498676083465, 'action': [0.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 162.9184910681783, 'action': [0.0, -1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 186.4446905205586, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17264081931236283, 0.16069251402097048, 0.18093148012679836, 0.15996098512557913, 0.15435259692757863, 0.1711777615215801]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 62.5001585483551 s
