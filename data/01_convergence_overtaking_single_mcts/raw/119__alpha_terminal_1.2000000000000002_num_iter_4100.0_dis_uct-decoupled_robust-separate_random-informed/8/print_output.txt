Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 496, 'sum_payoffs': 140.7463961008949, 'action': [2.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 135.93901161577634, 'action': [2.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 119.16236392923769, 'action': [0.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 124.3442710723054, 'action': [1.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 125.39667475233358, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 119.550059095083, 'action': [0.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 115.12745345750801, 'action': [0.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 123.98293651692396, 'action': [1.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 126.99866434012202, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12094611070470616, 0.11777615215801024, 0.10680321872713973, 0.11021702023896611, 0.11094854913435748, 0.10704706169227018, 0.1041209461107047, 0.10997317727383565, 0.1119239209948793]
Actions to choose Agent 1: dict_values([{'num_count': 724, 'sum_payoffs': 193.6691500856868, 'action': [1.0, -1.5707963267948966]}, {'num_count': 637, 'sum_payoffs': 163.98503384075784, 'action': [0.0, 1.5707963267948966]}, {'num_count': 670, 'sum_payoffs': 175.14474992821306, 'action': [0.0, 0.0]}, {'num_count': 728, 'sum_payoffs': 194.92448728642094, 'action': [1.0, 0.0]}, {'num_count': 640, 'sum_payoffs': 164.9725876776542, 'action': [0.0, -1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 185.64631229419138, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17654230675445012, 0.15532796878810046, 0.1633747866374055, 0.17751767861497195, 0.15605949768349184, 0.17093391855644965]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 62.5047402381897 s
