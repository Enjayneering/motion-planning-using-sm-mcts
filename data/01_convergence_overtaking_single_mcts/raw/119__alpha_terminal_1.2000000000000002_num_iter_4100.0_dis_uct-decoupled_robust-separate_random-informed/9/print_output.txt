Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 454, 'sum_payoffs': 124.73965092116723, 'action': [1.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 123.61832999892322, 'action': [1.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 127.79009173613366, 'action': [2.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 135.51707864540094, 'action': [2.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 120.33527689252084, 'action': [0.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 141.22523954894623, 'action': [2.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 118.10832527644236, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 123.65682276833468, 'action': [1.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 113.32459526399562, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11070470616922702, 0.10997317727383565, 0.11265544989027067, 0.11777615215801024, 0.10777859058766155, 0.12143379663496708, 0.1063155327968788, 0.10997317727383565, 0.10314557425018288]
Actions to choose Agent 1: dict_values([{'num_count': 689, 'sum_payoffs': 182.00506430864175, 'action': [1.0, -1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 198.86059552598738, 'action': [1.0, 0.0]}, {'num_count': 722, 'sum_payoffs': 193.35406306927928, 'action': [1.0, 1.5707963267948966]}, {'num_count': 672, 'sum_payoffs': 176.16514944569298, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 166.0443184483334, 'action': [0.0, -1.5707963267948966]}, {'num_count': 637, 'sum_payoffs': 164.33270062576207, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16800780297488419, 0.1799561082662765, 0.1760546208241892, 0.16386247256766642, 0.15654718361375275, 0.15532796878810046]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 59.613821506500244 s
