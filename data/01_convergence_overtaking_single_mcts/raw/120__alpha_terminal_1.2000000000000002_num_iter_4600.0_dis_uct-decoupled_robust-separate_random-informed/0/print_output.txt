Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 541, 'sum_payoffs': 150.72289295627053, 'action': [2.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 132.48601831330154, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 138.57591126270893, 'action': [2.0, 1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 142.62375614205502, 'action': [1.0, 0.0]}, {'num_count': 571, 'sum_payoffs': 161.7823899625147, 'action': [2.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 123.02380198702099, 'action': [0.0, 1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 142.65840821689363, 'action': [1.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 134.6928802455151, 'action': [1.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 131.68736820398914, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11758313410128234, 0.10671593131927842, 0.11041078026515974, 0.11280156487720061, 0.12410345577048468, 0.10106498587263639, 0.11280156487720061, 0.10801999565311889, 0.10628124320799825]
Actions to choose Agent 1: dict_values([{'num_count': 703, 'sum_payoffs': 180.5530739253263, 'action': [0.0, -1.5707963267948966]}, {'num_count': 779, 'sum_payoffs': 206.14075693919065, 'action': [1.0, -1.5707963267948966]}, {'num_count': 785, 'sum_payoffs': 208.1702300196341, 'action': [1.0, 1.5707963267948966]}, {'num_count': 870, 'sum_payoffs': 237.0239354550096, 'action': [1.0, 0.0]}, {'num_count': 719, 'sum_payoffs': 185.80326352516988, 'action': [0.0, 1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 194.3292452229655, 'action': [0.0, 0.0]}])
Weights num count: [0.152792871114975, 0.16931101934362094, 0.17061508367746142, 0.18908932840686807, 0.15627037600521626, 0.16170397739621822]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.77642846107483 s
