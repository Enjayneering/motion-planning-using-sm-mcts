Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 513, 'sum_payoffs': 140.24285050188772, 'action': [1.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 132.3958093633514, 'action': [0.0, -1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 147.71984926175548, 'action': [2.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 161.28140408277227, 'action': [2.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 127.94480950778991, 'action': [1.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 141.12484659728446, 'action': [2.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 131.94198732295015, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 131.23547761671037, 'action': [0.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 143.31472331526157, 'action': [1.0, 0.0]}])
Weights num count: [0.11149750054336013, 0.10671593131927842, 0.1158443816561617, 0.1238861117148446, 0.10410780265159748, 0.1119321886546403, 0.10649858726363834, 0.10606389915235818, 0.11323625298848077]
Actions to choose Agent 1: dict_values([{'num_count': 840, 'sum_payoffs': 226.6494737863249, 'action': [1.0, 0.0]}, {'num_count': 791, 'sum_payoffs': 210.069226792276, 'action': [1.0, -1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 182.63527929530952, 'action': [0.0, 1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 190.8580179923333, 'action': [0.0, -1.5707963267948966]}, {'num_count': 746, 'sum_payoffs': 194.87800008773533, 'action': [0.0, 0.0]}, {'num_count': 779, 'sum_payoffs': 205.9310810146763, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1825690067376657, 0.1719191480113019, 0.15431427950445556, 0.15953053683981744, 0.16213866550749836, 0.16931101934362094]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 75.22934055328369 s
