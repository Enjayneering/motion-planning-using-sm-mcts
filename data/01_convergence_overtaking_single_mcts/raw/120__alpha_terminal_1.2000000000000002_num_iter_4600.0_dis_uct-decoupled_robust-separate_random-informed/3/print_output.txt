Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 489, 'sum_payoffs': 131.49693019159585, 'action': [0.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 130.4582661730989, 'action': [0.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 127.22340812983872, 'action': [0.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 133.37038487229066, 'action': [1.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 154.96710151929256, 'action': [2.0, -1.5707963267948966]}, {'num_count': 558, 'sum_payoffs': 156.73725031176562, 'action': [2.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 136.2757065322057, 'action': [1.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 143.13740880398962, 'action': [1.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 142.73008947575812, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10628124320799825, 0.10562921104107803, 0.10367311454031732, 0.10736796348619865, 0.12019126276896328, 0.12127798304716365, 0.10910671593131928, 0.11323625298848077, 0.11301890893284068]
Actions to choose Agent 1: dict_values([{'num_count': 761, 'sum_payoffs': 199.9119669840808, 'action': [0.0, 0.0]}, {'num_count': 725, 'sum_payoffs': 187.79371094725943, 'action': [0.0, 1.5707963267948966]}, {'num_count': 764, 'sum_payoffs': 200.8623388535484, 'action': [1.0, 1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 213.09118150429325, 'action': [1.0, -1.5707963267948966]}, {'num_count': 844, 'sum_payoffs': 228.06902618070987, 'action': [1.0, 0.0]}, {'num_count': 706, 'sum_payoffs': 181.38043139623505, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16539882634209954, 0.15757444033905674, 0.16605085850901977, 0.1738752445120626, 0.18343838296022605, 0.15344490328189525]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.53312993049622 s
