Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 500, 'sum_payoffs': 136.19450953867639, 'action': [1.0, 1.5707963267948966]}, {'num_count': 561, 'sum_payoffs': 158.67217916456298, 'action': [2.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 130.4541678527543, 'action': [0.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 132.60763243070102, 'action': [0.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 143.17255781566828, 'action': [2.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 151.24762550754312, 'action': [2.0, -1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 142.87185072383946, 'action': [1.0, 0.0]}, {'num_count': 512, 'sum_payoffs': 140.6416256621173, 'action': [1.0, -1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 127.21440989999387, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10867202782003912, 0.12193001521408389, 0.10519452292979788, 0.10649858726363834, 0.11280156487720061, 0.11758313410128234, 0.11258422082156053, 0.11128015648772006, 0.10323842642903716]
Actions to choose Agent 1: dict_values([{'num_count': 708, 'sum_payoffs': 180.98960182671183, 'action': [0.0, 1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 181.61689591998797, 'action': [0.0, -1.5707963267948966]}, {'num_count': 839, 'sum_payoffs': 225.10101928171326, 'action': [1.0, 0.0]}, {'num_count': 792, 'sum_payoffs': 209.10072956536442, 'action': [1.0, -1.5707963267948966]}, {'num_count': 801, 'sum_payoffs': 212.2126203336142, 'action': [1.0, 1.5707963267948966]}, {'num_count': 750, 'sum_payoffs': 195.0271138950949, 'action': [0.0, 0.0]}])
Weights num count: [0.1538795913931754, 0.15431427950445556, 0.18235166268202566, 0.17213649206694198, 0.17409258856770268, 0.16300804173005867]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.91780281066895 s
