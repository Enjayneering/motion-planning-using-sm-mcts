Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 456, 'sum_payoffs': 119.69246347492144, 'action': [0.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 143.9701769520731, 'action': [1.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 132.312241049299, 'action': [0.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 160.84495825923284, 'action': [2.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 145.0412118485891, 'action': [2.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 133.05219642369565, 'action': [0.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 146.52727580985817, 'action': [1.0, -1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 139.20708249092812, 'action': [2.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 136.2225249877503, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.09910888937187567, 0.11367094109976092, 0.10671593131927842, 0.12366876765920452, 0.11432297326668116, 0.10715061943055858, 0.11519234948924147, 0.11084546837643991, 0.10910671593131928]
Actions to choose Agent 1: dict_values([{'num_count': 700, 'sum_payoffs': 179.05740364655796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 724, 'sum_payoffs': 187.03875195754478, 'action': [0.0, 0.0]}, {'num_count': 815, 'sum_payoffs': 217.73543584860607, 'action': [1.0, 1.5707963267948966]}, {'num_count': 832, 'sum_payoffs': 223.50581300985138, 'action': [1.0, 0.0]}, {'num_count': 723, 'sum_payoffs': 186.7726013920972, 'action': [0.0, -1.5707963267948966]}, {'num_count': 806, 'sum_payoffs': 214.62562134394804, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15214083894805477, 0.15735709628341665, 0.17713540534666378, 0.1808302542925451, 0.15713975222777657, 0.17517930884590308]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 72.04608607292175 s
