Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 492, 'sum_payoffs': 132.89030468386565, 'action': [1.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 144.5650947274979, 'action': [1.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 130.05914523701492, 'action': [0.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 140.96168634154475, 'action': [1.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 147.1431966282423, 'action': [2.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 143.83766065351537, 'action': [2.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 126.69877015985521, 'action': [0.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 135.08316035698286, 'action': [0.0, 0.0]}, {'num_count': 560, 'sum_payoffs': 157.8259448685147, 'action': [2.0, 0.0]}])
Weights num count: [0.1069332753749185, 0.113888285155401, 0.10519452292979788, 0.11171484459900022, 0.11540969354488155, 0.11345359704412085, 0.10323842642903716, 0.10823733970875897, 0.12171267115844382]
Actions to choose Agent 1: dict_values([{'num_count': 731, 'sum_payoffs': 189.1060685052534, 'action': [0.0, -1.5707963267948966]}, {'num_count': 859, 'sum_payoffs': 232.31566994297594, 'action': [1.0, 0.0]}, {'num_count': 785, 'sum_payoffs': 207.26084791457336, 'action': [1.0, -1.5707963267948966]}, {'num_count': 750, 'sum_payoffs': 195.44133552254704, 'action': [0.0, 0.0]}, {'num_count': 765, 'sum_payoffs': 200.50633620292365, 'action': [1.0, 1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 182.06338575997435, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1588785046728972, 0.1866985437948272, 0.17061508367746142, 0.16300804173005867, 0.16626820256465985, 0.15431427950445556]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.86850309371948 s
