Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 509, 'sum_payoffs': 138.68658709174392, 'action': [1.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 134.02926755150034, 'action': [1.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 148.53417313839122, 'action': [2.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 125.7640726235045, 'action': [0.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 131.5212082162639, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 145.71653231337348, 'action': [1.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 128.2391898721639, 'action': [0.0, 1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 144.61372393589394, 'action': [2.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 158.92026022223016, 'action': [2.0, 0.0]}])
Weights num count: [0.11062812432079983, 0.10780265159747882, 0.11649641382308194, 0.102803738317757, 0.10628124320799825, 0.11475766137796131, 0.10432514670723755, 0.11410562921104107, 0.12258204738100413]
Actions to choose Agent 1: dict_values([{'num_count': 786, 'sum_payoffs': 208.22140240238005, 'action': [1.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 185.63596898742406, 'action': [0.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 184.60565872758187, 'action': [0.0, 1.5707963267948966]}, {'num_count': 833, 'sum_payoffs': 224.12356916457688, 'action': [1.0, 0.0]}, {'num_count': 812, 'sum_payoffs': 217.00061270665338, 'action': [1.0, -1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 190.6652867694881, 'action': [0.0, 0.0]}])
Weights num count: [0.1708324277331015, 0.15627037600521626, 0.15561834383829604, 0.18104759834818518, 0.17648337317974352, 0.15953053683981744]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.23974704742432 s
