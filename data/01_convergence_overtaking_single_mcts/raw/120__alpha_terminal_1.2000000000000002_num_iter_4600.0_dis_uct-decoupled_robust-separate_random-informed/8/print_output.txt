Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 473, 'sum_payoffs': 126.00345634493634, 'action': [0.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 143.8587779134725, 'action': [1.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 148.25012915760277, 'action': [2.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 131.0928452485345, 'action': [0.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 164.0248700032979, 'action': [2.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 134.3612951473315, 'action': [1.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 131.81133601937879, 'action': [0.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 140.89107017505899, 'action': [2.0, 1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 138.62847241481478, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.102803738317757, 0.11345359704412085, 0.11606172571180179, 0.1058465550967181, 0.12540752010432515, 0.10780265159747882, 0.10628124320799825, 0.11171484459900022, 0.11041078026515974]
Actions to choose Agent 1: dict_values([{'num_count': 777, 'sum_payoffs': 204.09496509782542, 'action': [1.0, -1.5707963267948966]}, {'num_count': 792, 'sum_payoffs': 209.1748607646928, 'action': [1.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 181.39396855247108, 'action': [0.0, -1.5707963267948966]}, {'num_count': 746, 'sum_payoffs': 193.64836509667157, 'action': [0.0, 0.0]}, {'num_count': 852, 'sum_payoffs': 229.4842450283987, 'action': [1.0, 0.0]}, {'num_count': 724, 'sum_payoffs': 186.40601503107814, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1688763312323408, 0.17213649206694198, 0.15409693544881548, 0.16213866550749836, 0.18517713540534667, 0.15735709628341665]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.91889309883118 s
