Searching game tree in timestep 0...
Max timehorizon: 8
Actions to choose Agent 0: dict_values([{'num_count': 501, 'sum_payoffs': 136.16998908726714, 'action': [1.0, 0.0]}, {'num_count': 555, 'sum_payoffs': 155.88642684710913, 'action': [2.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 147.4208658607257, 'action': [2.0, 1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 151.82510281943985, 'action': [2.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 136.8973391906253, 'action': [1.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 131.83059716783038, 'action': [0.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 130.3727956671916, 'action': [0.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 127.50221697236529, 'action': [0.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 140.93469845953766, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1088893718756792, 0.12062595088024343, 0.11562703760052162, 0.11823516626820256, 0.10932405998695936, 0.10628124320799825, 0.10541186698543795, 0.10367311454031732, 0.11171484459900022]
Actions to choose Agent 1: dict_values([{'num_count': 745, 'sum_payoffs': 193.81390685274093, 'action': [0.0, 0.0]}, {'num_count': 717, 'sum_payoffs': 184.4533712982049, 'action': [0.0, -1.5707963267948966]}, {'num_count': 801, 'sum_payoffs': 212.68723939725953, 'action': [1.0, 0.0]}, {'num_count': 737, 'sum_payoffs': 191.20645867562027, 'action': [0.0, 1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 212.41511887203444, 'action': [1.0, -1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 212.397170588245, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16192132145185828, 0.1558356878939361, 0.17409258856770268, 0.16018256900673766, 0.1738752445120626, 0.1738752445120626]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.37258291244507 s
