Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 120, 'sum_payoffs': 31.952512500375626, 'action': [0.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 32.82146144912533, 'action': [1.0, -1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 35.40416957876501, 'action': [2.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 33.152354564359634, 'action': [1.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 32.93125453512757, 'action': [2.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 33.34721877809821, 'action': [2.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 31.470939865787187, 'action': [0.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 32.85236651996965, 'action': [1.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 32.40821674899328, 'action': [0.0, 0.0]}])
Weights num count: [0.10899182561307902, 0.11080835603996367, 0.11625794732061762, 0.11171662125340599, 0.11080835603996367, 0.11171662125340599, 0.1080835603996367, 0.11080835603996367, 0.10990009082652134]
Actions to choose Agent 1: dict_values([{'num_count': 190, 'sum_payoffs': 49.137486657476686, 'action': [1.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 42.47183043325698, 'action': [0.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 44.89526674809976, 'action': [0.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 43.31246329645618, 'action': [0.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 50.48905031643324, 'action': [1.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 49.311662754753605, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17257039055404177, 0.15712988192552224, 0.16257947320617622, 0.1589464123524069, 0.17529518619436876, 0.17257039055404177]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.408080101013184 s
