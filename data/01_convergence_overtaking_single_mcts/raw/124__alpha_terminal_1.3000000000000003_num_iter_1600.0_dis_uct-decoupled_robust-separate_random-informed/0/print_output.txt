Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 173, 'sum_payoffs': 46.2748309824441, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 49.081484226040935, 'action': [0.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 48.29042306291424, 'action': [1.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 47.92873960954712, 'action': [1.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 46.76670399822695, 'action': [0.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 51.290243721060016, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 47.47182954438647, 'action': [2.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 46.75992811335272, 'action': [1.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 50.36142690891281, 'action': [2.0, 0.0]}])
Weights num count: [0.1080574640849469, 0.11242973141786383, 0.11118051217988757, 0.11055590256089944, 0.10868207370393504, 0.1155527795128045, 0.1099312929419113, 0.10868207370393504, 0.11430356027482823]
Actions to choose Agent 1: dict_values([{'num_count': 284, 'sum_payoffs': 74.11548430143849, 'action': [1.0, 0.0]}, {'num_count': 258, 'sum_payoffs': 64.2752028382562, 'action': [0.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 63.687022033595476, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 71.41494174460307, 'action': [1.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 69.24663514989243, 'action': [1.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 62.960016094293046, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1773891317926296, 0.16114928169893816, 0.1599000624609619, 0.17301686445971268, 0.16926920674578388, 0.15865084322298564]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.236368417739868 s
