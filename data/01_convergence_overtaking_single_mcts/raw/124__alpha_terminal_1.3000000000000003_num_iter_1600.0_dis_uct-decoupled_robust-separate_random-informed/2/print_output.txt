Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 178, 'sum_payoffs': 47.93549087986424, 'action': [1.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 48.77351089947036, 'action': [2.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 49.63014317983771, 'action': [1.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 45.44901520608003, 'action': [1.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 47.881536160382446, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 46.241926370173935, 'action': [0.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 44.20672784716683, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 49.56372300659672, 'action': [2.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 50.88886489304214, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11118051217988757, 0.11242973141786383, 0.1136789506558401, 0.10743285446595878, 0.11118051217988757, 0.10868207370393504, 0.10555902560899438, 0.1136789506558401, 0.1155527795128045]
Actions to choose Agent 1: dict_values([{'num_count': 280, 'sum_payoffs': 72.75680552798133, 'action': [1.0, -1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 78.41609256082606, 'action': [1.0, 0.0]}, {'num_count': 262, 'sum_payoffs': 65.98669128215742, 'action': [0.0, 0.0]}, {'num_count': 253, 'sum_payoffs': 62.611053815774035, 'action': [0.0, 1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 68.67459712597173, 'action': [1.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 58.286275503176334, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1748906933166771, 0.18425983760149905, 0.16364772017489068, 0.1580262336039975, 0.1680199875078076, 0.15053091817613992]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.99117660522461 s
