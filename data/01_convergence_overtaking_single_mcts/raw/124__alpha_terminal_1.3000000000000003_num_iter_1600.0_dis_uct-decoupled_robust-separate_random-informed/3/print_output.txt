Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 188, 'sum_payoffs': 52.042593746198236, 'action': [2.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 46.53810600733075, 'action': [0.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 50.339155284569415, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 45.022903466391185, 'action': [1.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 50.32495385800888, 'action': [2.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 45.862966249432155, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 47.08733543589799, 'action': [1.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 48.27298348708146, 'action': [1.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 44.607478334806416, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1174266083697689, 0.10930668332292318, 0.11492816989381636, 0.10680824484697064, 0.11492816989381636, 0.1080574640849469, 0.1099312929419113, 0.11180512179887571, 0.1061836352279825]
Actions to choose Agent 1: dict_values([{'num_count': 280, 'sum_payoffs': 72.48807871614126, 'action': [1.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 63.63608843403065, 'action': [0.0, -1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 66.59185613117212, 'action': [0.0, 0.0]}, {'num_count': 253, 'sum_payoffs': 62.39344914327161, 'action': [0.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 67.19789547676913, 'action': [1.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 72.77589243992077, 'action': [1.0, 0.0]}])
Weights num count: [0.1748906933166771, 0.1599000624609619, 0.16489693941286696, 0.1580262336039975, 0.16614615865084323, 0.1755153029356652]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.96931767463684 s
