Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 178, 'sum_payoffs': 48.27767817535328, 'action': [1.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 44.209671910661854, 'action': [0.0, 0.0]}, {'num_count': 166, 'sum_payoffs': 43.35571365523133, 'action': [0.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 52.48565809584392, 'action': [2.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 47.418040681541115, 'action': [1.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 45.44146418341115, 'action': [0.0, -1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 50.30078706580045, 'action': [1.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 53.5998003819757, 'action': [2.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 48.72848659848633, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11118051217988757, 0.10493441599000625, 0.10368519675202999, 0.1174266083697689, 0.1099312929419113, 0.10680824484697064, 0.11430356027482823, 0.1193004372267333, 0.11180512179887571]
Actions to choose Agent 1: dict_values([{'num_count': 257, 'sum_payoffs': 64.06396314930468, 'action': [0.0, -1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 67.49933370672541, 'action': [1.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 75.64180866537218, 'action': [1.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 69.94681627096995, 'action': [1.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 64.16384156319535, 'action': [0.0, 0.0]}, {'num_count': 259, 'sum_payoffs': 64.8077194048975, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16052467207995003, 0.16614615865084323, 0.17988757026858213, 0.17051842598376016, 0.16052467207995003, 0.1617738913179263]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.90915060043335 s
