Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 236, 'sum_payoffs': 63.96451437439369, 'action': [2.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 58.42512883800133, 'action': [0.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 64.20828824558151, 'action': [1.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 62.72063969816659, 'action': [1.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 59.119970958260346, 'action': [0.0, -1.5707963267948966]}, {'num_count': 219, 'sum_payoffs': 57.212877484827004, 'action': [0.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 69.49957503199252, 'action': [2.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 63.97058883839947, 'action': [1.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 66.68817903310361, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11232746311280342, 0.10566396953831508, 0.1128034269395526, 0.11089957163255593, 0.10661589719181343, 0.10423607805806759, 0.11899095668729176, 0.11232746311280342, 0.11565920990004759]
Actions to choose Agent 1: dict_values([{'num_count': 357, 'sum_payoffs': 91.11289891555103, 'action': [1.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 81.74159172470476, 'action': [0.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 81.07322379469527, 'action': [0.0, 0.0]}, {'num_count': 364, 'sum_payoffs': 93.61804879963145, 'action': [1.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 100.43606537481114, 'action': [1.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 83.60012069091692, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16991908614945264, 0.1575440266539743, 0.15659209900047596, 0.17325083293669682, 0.182294145644931, 0.15992384578772012]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.83013319969177 s
