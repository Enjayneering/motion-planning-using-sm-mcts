Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 241, 'sum_payoffs': 65.54660071747753, 'action': [2.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 60.38546860261391, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 57.29878690182429, 'action': [0.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 60.0497162847528, 'action': [0.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 61.184002382629316, 'action': [1.0, 1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 61.975701363938995, 'action': [0.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 64.39062092380352, 'action': [1.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 64.33344655471444, 'action': [2.0, -1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 67.60860612937654, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11470728224654926, 0.10851975249881009, 0.10471204188481675, 0.10804378867206092, 0.10947168015230842, 0.11042360780580676, 0.11327939076630177, 0.11327939076630177, 0.1170871013802951]
Actions to choose Agent 1: dict_values([{'num_count': 357, 'sum_payoffs': 92.19907418748282, 'action': [1.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 94.04124672435131, 'action': [1.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 85.10493781355652, 'action': [0.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 82.85765430308426, 'action': [0.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 96.95651153691563, 'action': [1.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 87.20188934281258, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16991908614945264, 0.17229890528319847, 0.1603998096144693, 0.1575440266539743, 0.17610661589719181, 0.1632555925749643]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.26961827278137 s
