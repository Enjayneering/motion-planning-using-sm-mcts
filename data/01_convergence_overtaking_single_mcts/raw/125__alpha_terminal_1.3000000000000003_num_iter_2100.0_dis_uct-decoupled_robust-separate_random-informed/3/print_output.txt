Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 223, 'sum_payoffs': 58.950007953695504, 'action': [0.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 58.902938787314916, 'action': [0.0, 0.0]}, {'num_count': 227, 'sum_payoffs': 60.55725231285683, 'action': [1.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 69.34425640788469, 'action': [2.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 68.11758322363046, 'action': [2.0, -1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 57.710382212390364, 'action': [0.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 64.01525210761713, 'action': [1.0, -1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 68.16606384171695, 'action': [2.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 61.71041635110163, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10613993336506425, 0.10613993336506425, 0.10804378867206092, 0.1185149928605426, 0.1170871013802951, 0.10471204188481675, 0.11232746311280342, 0.1170871013802951, 0.10947168015230842]
Actions to choose Agent 1: dict_values([{'num_count': 335, 'sum_payoffs': 83.35175325414681, 'action': [0.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 93.67607195128103, 'action': [1.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 98.87421329982396, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 92.34019142123545, 'action': [1.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 82.19737762047866, 'action': [0.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 81.95831152398988, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15944788196097096, 0.17325083293669682, 0.17991432651118516, 0.17134697762970014, 0.15801999048072346, 0.1575440266539743]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.405951499938965 s
