Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 226, 'sum_payoffs': 59.884460131852734, 'action': [0.0, -1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 67.75529675149804, 'action': [2.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 58.737888450899774, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 57.50268442272033, 'action': [0.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 65.48783048125294, 'action': [1.0, -1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 55.93735640687284, 'action': [0.0, 1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 64.15055739592135, 'action': [1.0, 1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 67.8147574536033, 'action': [2.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 67.85713225038678, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10756782484531176, 0.1170871013802951, 0.10613993336506425, 0.10471204188481675, 0.1142313184198001, 0.10280818657782008, 0.1128034269395526, 0.1170871013802951, 0.1170871013802951]
Actions to choose Agent 1: dict_values([{'num_count': 340, 'sum_payoffs': 85.24010979536088, 'action': [0.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 98.21681530815434, 'action': [1.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 82.30392604348566, 'action': [0.0, -1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 93.5184227539378, 'action': [1.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 89.5003586047997, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 84.17394996558636, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1618277010947168, 0.1789623988576868, 0.15801999048072346, 0.17277486910994763, 0.16753926701570682, 0.1603998096144693]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.56775212287903 s
