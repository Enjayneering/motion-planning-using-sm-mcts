Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 237, 'sum_payoffs': 64.41303145500113, 'action': [1.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 64.77120766835253, 'action': [1.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 58.34763552230538, 'action': [0.0, 0.0]}, {'num_count': 243, 'sum_payoffs': 66.74992228997415, 'action': [2.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 57.95111889014973, 'action': [0.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 63.91707551899755, 'action': [2.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 60.75271546429876, 'action': [0.0, -1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 60.028167226654226, 'action': [1.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 69.12716574770936, 'action': [2.0, 0.0]}])
Weights num count: [0.1128034269395526, 0.11327939076630177, 0.10566396953831508, 0.11565920990004759, 0.10518800571156592, 0.11232746311280342, 0.10851975249881009, 0.10756782484531176, 0.1185149928605426]
Actions to choose Agent 1: dict_values([{'num_count': 340, 'sum_payoffs': 85.08367300729758, 'action': [0.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 80.79516492085413, 'action': [0.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 81.87697041261833, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 92.27908890256045, 'action': [1.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 90.1177487822008, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 101.95805572149048, 'action': [1.0, 0.0]}])
Weights num count: [0.1618277010947168, 0.1561161351737268, 0.1575440266539743, 0.17134697762970014, 0.16849119466920515, 0.18419800095192765]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.28392434120178 s
