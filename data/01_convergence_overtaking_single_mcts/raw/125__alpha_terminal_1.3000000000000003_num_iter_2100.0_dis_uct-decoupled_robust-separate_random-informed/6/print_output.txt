Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 245, 'sum_payoffs': 67.54960303637552, 'action': [1.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 61.16791298517546, 'action': [2.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 63.98081907756493, 'action': [2.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 60.294572274579885, 'action': [0.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 63.108146471481426, 'action': [1.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 67.1639836978706, 'action': [2.0, 0.0]}, {'num_count': 226, 'sum_payoffs': 59.91828427630293, 'action': [0.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 63.4983336267708, 'action': [1.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 59.09792068079236, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11661113755354593, 0.10899571632555925, 0.11232746311280342, 0.10804378867206092, 0.11137553545930509, 0.11613517372679677, 0.10756782484531176, 0.11185149928605426, 0.10661589719181343]
Actions to choose Agent 1: dict_values([{'num_count': 324, 'sum_payoffs': 79.91565025536588, 'action': [0.0, -1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 96.36043016554385, 'action': [1.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 99.24663371083419, 'action': [1.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 91.74004065698936, 'action': [1.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 80.59889496882145, 'action': [0.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 87.39565359106338, 'action': [0.0, 0.0]}])
Weights num count: [0.15421227986673014, 0.17610661589719181, 0.17991432651118516, 0.16991908614945264, 0.15516420752022847, 0.16420752022846263]
Selected final action: [1.0, 0.0, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 35.140981912612915 s
