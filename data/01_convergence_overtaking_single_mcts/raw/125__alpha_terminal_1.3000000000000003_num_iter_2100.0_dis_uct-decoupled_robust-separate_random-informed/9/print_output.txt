Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 230, 'sum_payoffs': 61.569786513725774, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 66.33425631736237, 'action': [2.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 62.218864369821574, 'action': [0.0, -1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 63.097768055483805, 'action': [1.0, -1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 63.09826743475486, 'action': [2.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 61.835906454507345, 'action': [1.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 61.147389631912546, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 57.62227164633238, 'action': [0.0, 1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 68.76310765505832, 'action': [2.0, 0.0]}])
Weights num count: [0.10947168015230842, 0.11518324607329843, 0.11042360780580676, 0.11137553545930509, 0.11137553545930509, 0.1099476439790576, 0.10899571632555925, 0.10471204188481675, 0.11803902903379343]
Actions to choose Agent 1: dict_values([{'num_count': 366, 'sum_payoffs': 94.86215639848416, 'action': [1.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 96.61411108090012, 'action': [1.0, 1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 82.97066137961068, 'action': [0.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 83.267955295899, 'action': [0.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 98.77442378927944, 'action': [1.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 77.98041722584443, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17420276059019515, 0.17658257972394098, 0.15849595430747263, 0.1589719181342218, 0.17943836268443597, 0.1518324607329843]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.02306318283081 s
