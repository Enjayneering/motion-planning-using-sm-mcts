Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 310, 'sum_payoffs': 86.04786865283859, 'action': [2.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 73.19880167003035, 'action': [1.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 75.87992839678513, 'action': [0.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 73.93697166265369, 'action': [0.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 82.55059864170799, 'action': [2.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 78.25727612526516, 'action': [1.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 77.10415737269768, 'action': [1.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 72.86899542144353, 'action': [0.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 80.55836058299674, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11918492887351019, 0.10649750096116878, 0.10918877354863514, 0.10726643598615918, 0.11572472126105345, 0.1114955786236063, 0.11034217608612072, 0.1061130334486736, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 449, 'sum_payoffs': 115.3826786334859, 'action': [1.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 115.80815404575443, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 116.8304689638108, 'action': [1.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 104.93611731389959, 'action': [0.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 104.20893292375708, 'action': [0.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 102.44171797116473, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17262591311034217, 0.17301038062283736, 0.17416378316032297, 0.16109188773548636, 0.16032295271049596, 0.15840061514801998]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.57722234725952 s
