Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 299, 'sum_payoffs': 81.41192792639242, 'action': [2.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 74.77746653109736, 'action': [1.0, 1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 83.35075723183874, 'action': [2.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 73.23692474644172, 'action': [0.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 78.28414709697314, 'action': [0.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 75.58803815947093, 'action': [1.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 78.98719455093861, 'action': [1.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 80.62829495126232, 'action': [2.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 70.98925744155314, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11495578623606305, 0.10841983852364476, 0.11687812379853903, 0.10688196847366398, 0.1118800461361015, 0.10918877354863514, 0.11264898116109189, 0.11418685121107267, 0.10457516339869281]
Actions to choose Agent 1: dict_values([{'num_count': 463, 'sum_payoffs': 119.92070994515238, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 119.32789958498223, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 102.97409334931174, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 114.05525682753108, 'action': [1.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 98.70434923929541, 'action': [0.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 102.94672191577709, 'action': [0.0, 0.0]}])
Weights num count: [0.1780084582852749, 0.1772395232602845, 0.15916955017301038, 0.1714725105728566, 0.15455594002306805, 0.15916955017301038]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.252164125442505 s
