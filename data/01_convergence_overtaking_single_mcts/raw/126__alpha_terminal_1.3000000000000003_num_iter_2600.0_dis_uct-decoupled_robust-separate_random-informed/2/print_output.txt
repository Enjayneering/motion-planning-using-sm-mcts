Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 295, 'sum_payoffs': 80.28198042628405, 'action': [1.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 75.22757567075608, 'action': [0.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 79.10006097695955, 'action': [1.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 72.60104447526952, 'action': [0.0, 0.0]}, {'num_count': 305, 'sum_payoffs': 84.08155060787773, 'action': [2.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 81.76525891284217, 'action': [2.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 71.77798007187033, 'action': [1.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 83.72160150441644, 'action': [2.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 72.58855188972815, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11341791618608228, 0.10841983852364476, 0.1122645136485967, 0.1057285659361784, 0.11726259131103421, 0.11495578623606305, 0.104959630911188, 0.11687812379853903, 0.1057285659361784]
Actions to choose Agent 1: dict_values([{'num_count': 446, 'sum_payoffs': 113.51343008786414, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 101.50741341150642, 'action': [0.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 106.97509447871376, 'action': [0.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 98.75184817173974, 'action': [0.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 119.17099639017779, 'action': [1.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 115.3690883456166, 'action': [1.0, 0.0]}])
Weights num count: [0.1714725105728566, 0.1580161476355248, 0.16416762783544792, 0.15494040753556323, 0.1776239907727797, 0.17339484813533257]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.15969514846802 s
