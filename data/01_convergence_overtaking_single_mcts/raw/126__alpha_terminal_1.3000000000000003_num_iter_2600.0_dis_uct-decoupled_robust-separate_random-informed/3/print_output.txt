Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 275, 'sum_payoffs': 72.55860016687424, 'action': [0.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 81.4126864254999, 'action': [2.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 78.78689895703893, 'action': [1.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 71.79168067406867, 'action': [0.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 70.72503241477216, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 78.00927913526392, 'action': [1.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 86.13779464189204, 'action': [2.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 78.78534343999846, 'action': [1.0, -1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 83.38711657219217, 'action': [2.0, 0.0]}])
Weights num count: [0.1057285659361784, 0.11457131872356786, 0.1118800461361015, 0.104959630911188, 0.10380622837370242, 0.1111111111111111, 0.11918492887351019, 0.1118800461361015, 0.11649365628604383]
Actions to choose Agent 1: dict_values([{'num_count': 448, 'sum_payoffs': 114.94059160891106, 'action': [1.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 107.55635901503229, 'action': [0.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 97.06675019081325, 'action': [0.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 121.83952790419116, 'action': [1.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 102.67359392039317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 114.51222205909816, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.172241445597847, 0.16416762783544792, 0.15263360246059207, 0.17993079584775087, 0.1587850826605152, 0.17185697808535177]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.482377767562866 s
