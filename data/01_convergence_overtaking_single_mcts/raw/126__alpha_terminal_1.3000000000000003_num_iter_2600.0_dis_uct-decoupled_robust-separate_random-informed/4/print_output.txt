Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 291, 'sum_payoffs': 78.83253397460095, 'action': [1.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 75.3164723896819, 'action': [0.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 82.26331359972014, 'action': [2.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 78.39350389291887, 'action': [1.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 78.02762075375303, 'action': [1.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 72.67719763742674, 'action': [0.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 80.27611047987739, 'action': [2.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 81.550582454478, 'action': [2.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 74.60673092643823, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1118800461361015, 0.10841983852364476, 0.11534025374855825, 0.1114955786236063, 0.1111111111111111, 0.1057285659361784, 0.11341791618608228, 0.11457131872356786, 0.10765090349865436]
Actions to choose Agent 1: dict_values([{'num_count': 446, 'sum_payoffs': 114.0029358498257, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 102.17842508118665, 'action': [0.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 120.34841686660955, 'action': [1.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 116.13438643028618, 'action': [1.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 100.4822235582909, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 104.51146277148915, 'action': [0.0, 0.0]}])
Weights num count: [0.1714725105728566, 0.15840061514801998, 0.17839292579777008, 0.17377931564782775, 0.15647827758554403, 0.16109188773548636]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.5927848815918 s
