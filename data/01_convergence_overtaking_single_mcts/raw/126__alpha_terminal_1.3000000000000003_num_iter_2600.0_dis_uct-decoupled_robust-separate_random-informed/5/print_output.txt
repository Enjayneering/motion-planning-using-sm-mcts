Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 311, 'sum_payoffs': 86.09647557114484, 'action': [2.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 83.273243538887, 'action': [2.0, -1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 69.58649017933747, 'action': [0.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 77.32058698112907, 'action': [0.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 73.47899674117137, 'action': [1.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 79.9508916216349, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 76.14243597209736, 'action': [1.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 73.35071073041473, 'action': [0.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 79.19719502621268, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11956939638600539, 0.11687812379853903, 0.10303729334871203, 0.11072664359861592, 0.10688196847366398, 0.11341791618608228, 0.10957324106113034, 0.10688196847366398, 0.11264898116109189]
Actions to choose Agent 1: dict_values([{'num_count': 463, 'sum_payoffs': 120.34201684404302, 'action': [1.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 102.79922152146176, 'action': [0.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 100.47089685729847, 'action': [0.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 118.90305760406474, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 114.7032681421692, 'action': [1.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 102.51493102133611, 'action': [0.0, 0.0]}])
Weights num count: [0.1780084582852749, 0.1587850826605152, 0.15609381007304882, 0.17647058823529413, 0.17185697808535177, 0.15840061514801998]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.21503210067749 s
