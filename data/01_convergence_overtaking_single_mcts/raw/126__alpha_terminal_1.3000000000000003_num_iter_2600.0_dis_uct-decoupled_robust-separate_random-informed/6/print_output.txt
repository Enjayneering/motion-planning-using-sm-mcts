Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 283, 'sum_payoffs': 75.13183546328032, 'action': [1.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 80.06536509785427, 'action': [2.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 80.84924518429166, 'action': [2.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 74.31554405247303, 'action': [0.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 82.00409238060672, 'action': [2.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 78.54237950674653, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 75.78173372846591, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 75.12857231091952, 'action': [1.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 74.38063114994232, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10880430603613994, 0.11380238369857747, 0.11457131872356786, 0.10803537101114956, 0.11572472126105345, 0.1122645136485967, 0.10957324106113034, 0.10880430603613994, 0.10803537101114956]
Actions to choose Agent 1: dict_values([{'num_count': 452, 'sum_payoffs': 116.87701456623935, 'action': [1.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 101.82026190356885, 'action': [0.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 108.15932978189132, 'action': [0.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 98.76590590286149, 'action': [0.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 115.56231148688114, 'action': [1.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 121.09898745358808, 'action': [1.0, 0.0]}])
Weights num count: [0.17377931564782775, 0.1572472126105344, 0.16416762783544792, 0.15378700499807765, 0.172241445597847, 0.17839292579777008]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.48940420150757 s
