Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 281, 'sum_payoffs': 74.67459508693207, 'action': [0.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 81.17753189096355, 'action': [1.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 78.19773512853715, 'action': [2.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 73.45585098412347, 'action': [0.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 83.98008009431976, 'action': [2.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 75.85657186709756, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 77.02756135801357, 'action': [1.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 75.47450069199651, 'action': [1.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 79.62871075518251, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10803537101114956, 0.11457131872356786, 0.1114955786236063, 0.10688196847366398, 0.11726259131103421, 0.10918877354863514, 0.11034217608612072, 0.10880430603613994, 0.11303344867358708]
Actions to choose Agent 1: dict_values([{'num_count': 452, 'sum_payoffs': 116.65196801676323, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 104.72664266293692, 'action': [0.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 104.438002125515, 'action': [0.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 119.82795929466289, 'action': [1.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 97.4428372655626, 'action': [0.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 117.68655241156085, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17377931564782775, 0.16070742022299117, 0.16032295271049596, 0.1772395232602845, 0.15263360246059207, 0.17493271818531334]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.14410424232483 s
