Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 286, 'sum_payoffs': 76.45379277140553, 'action': [0.0, 0.0]}, {'num_count': 303, 'sum_payoffs': 83.15259355709983, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 74.23218728142801, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 76.05710075273872, 'action': [1.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 78.10056738334032, 'action': [1.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 78.05802500723141, 'action': [2.0, 1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 83.50559848243998, 'action': [2.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 71.41943603878886, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 77.72726104576154, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10995770857362552, 0.11649365628604383, 0.10765090349865436, 0.10957324106113034, 0.1114955786236063, 0.1114955786236063, 0.11687812379853903, 0.104959630911188, 0.1111111111111111]
Actions to choose Agent 1: dict_values([{'num_count': 413, 'sum_payoffs': 102.54390335520473, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 117.53036630188852, 'action': [1.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 104.19258281452174, 'action': [0.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 100.6978348952346, 'action': [0.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 117.06344919931834, 'action': [1.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 115.43007434233007, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1587850826605152, 0.17531718569780855, 0.16070742022299117, 0.1568627450980392, 0.17493271818531334, 0.17301038062283736]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.41211152076721 s
