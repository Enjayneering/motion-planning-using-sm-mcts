Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 280, 'sum_payoffs': 74.0317271916037, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 76.6317041353729, 'action': [1.0, 1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 80.51429845085694, 'action': [2.0, 0.0]}, {'num_count': 313, 'sum_payoffs': 86.71887474643813, 'action': [2.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 79.35996666927517, 'action': [1.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 78.24134155813726, 'action': [2.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 70.9055834257273, 'action': [0.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 76.67623404906908, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 73.55345007005161, 'action': [0.0, 0.0]}])
Weights num count: [0.10765090349865436, 0.11034217608612072, 0.11418685121107267, 0.12033833141099577, 0.11303344867358708, 0.1118800461361015, 0.10457516339869281, 0.11034217608612072, 0.10726643598615918]
Actions to choose Agent 1: dict_values([{'num_count': 410, 'sum_payoffs': 101.94825083092351, 'action': [0.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 104.46099977544094, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 122.6756101332076, 'action': [1.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 102.66752971300296, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 113.19506723041933, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 116.01563805407989, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1576316801230296, 0.16032295271049596, 0.18031526336024606, 0.15840061514801998, 0.16993464052287582, 0.17301038062283736]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.64380502700806 s
