Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 324, 'sum_payoffs': 84.59090211174336, 'action': [0.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 89.34462291257887, 'action': [1.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 94.31131494822296, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 91.22969063018732, 'action': [1.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 94.72796672793979, 'action': [1.0, -1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 103.06123088553926, 'action': [2.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 87.21665121727649, 'action': [0.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 96.9894562302611, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 88.66083990127397, 'action': [0.0, 0.0]}])
Weights num count: [0.10448242502418574, 0.10867462108997097, 0.11286681715575621, 0.11028700419219607, 0.11318929377620122, 0.12028377942599161, 0.10673976136730087, 0.11512415349887133, 0.10802966784908094]
Actions to choose Agent 1: dict_values([{'num_count': 483, 'sum_payoffs': 119.2783350266572, 'action': [0.0, 1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 142.46306611629367, 'action': [1.0, 0.0]}, {'num_count': 538, 'sum_payoffs': 138.0769110632859, 'action': [1.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 125.77926015341117, 'action': [0.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 125.09807876093232, 'action': [0.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 133.89369302509715, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15575620767494355, 0.17768461786520479, 0.17349242179941954, 0.1618832634633989, 0.16123831022250887, 0.16962270235407934]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.75566124916077 s
