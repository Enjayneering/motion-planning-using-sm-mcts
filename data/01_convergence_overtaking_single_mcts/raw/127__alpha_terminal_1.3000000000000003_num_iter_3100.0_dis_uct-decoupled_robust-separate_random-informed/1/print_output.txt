Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 333, 'sum_payoffs': 87.66559347815344, 'action': [0.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 94.70773397975486, 'action': [1.0, -1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 84.66346282594247, 'action': [0.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 97.7116193270238, 'action': [2.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 95.87779307338019, 'action': [2.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 91.37200435224638, 'action': [1.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 90.23276764180387, 'action': [1.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 88.46492322880506, 'action': [0.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 96.71236131587794, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1073847146081909, 0.11351177039664624, 0.10480490164463076, 0.11609158336020639, 0.11447920025798129, 0.11060948081264109, 0.10964205095130602, 0.10802966784908094, 0.11512415349887133]
Actions to choose Agent 1: dict_values([{'num_count': 542, 'sum_payoffs': 139.01016468602376, 'action': [1.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 122.7966086578334, 'action': [0.0, -1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 137.41142902458444, 'action': [1.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 123.36795125528631, 'action': [0.0, 1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 140.12177074779285, 'action': [1.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 120.07883015604523, 'action': [0.0, 0.0]}])
Weights num count: [0.1747823282811996, 0.15930345049983877, 0.17316994517897452, 0.1599484037407288, 0.17574975814253466, 0.15672363753627863]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.52449822425842 s
