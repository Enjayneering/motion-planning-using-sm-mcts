Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 335, 'sum_payoffs': 89.18840993350494, 'action': [0.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 93.06199206847973, 'action': [1.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 95.2868461964713, 'action': [1.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 96.77337979512906, 'action': [2.0, -1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 99.86604243287769, 'action': [2.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 91.42483182607175, 'action': [1.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 87.7667313809317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 86.59376871258824, 'action': [0.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 95.26621966383703, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10802966784908094, 0.11125443405353112, 0.11318929377620122, 0.11447920025798129, 0.11705901322154144, 0.10996452757175104, 0.10673976136730087, 0.10577233150596582, 0.11318929377620122]
Actions to choose Agent 1: dict_values([{'num_count': 525, 'sum_payoffs': 132.72674586439746, 'action': [1.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 138.838436912562, 'action': [1.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 124.13950986100491, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 121.83763135935105, 'action': [0.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 120.40996779163034, 'action': [0.0, -1.5707963267948966]}, {'num_count': 550, 'sum_payoffs': 141.2701575355943, 'action': [1.0, 0.0]}])
Weights num count: [0.16930022573363432, 0.17510480490164462, 0.16123831022250887, 0.15898097387939375, 0.15769106739761368, 0.17736214124475974]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.4892418384552 s
