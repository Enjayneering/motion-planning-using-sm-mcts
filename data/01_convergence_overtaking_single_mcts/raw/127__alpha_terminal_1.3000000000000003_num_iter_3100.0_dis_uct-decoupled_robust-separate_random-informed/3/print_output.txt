Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 348, 'sum_payoffs': 93.49308512285799, 'action': [1.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 87.77610193569436, 'action': [0.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 88.9178984374953, 'action': [1.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 99.42446469044485, 'action': [2.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 97.95957441907099, 'action': [2.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 90.12540011929444, 'action': [1.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 83.2606012521456, 'action': [0.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 96.0943454583483, 'action': [2.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 91.89520721562899, 'action': [0.0, 0.0]}])
Weights num count: [0.11222186391486617, 0.1073847146081909, 0.10835214446952596, 0.11738148984198646, 0.11609158336020639, 0.10931957433086101, 0.10351499516285069, 0.11447920025798129, 0.1109319574330861]
Actions to choose Agent 1: dict_values([{'num_count': 498, 'sum_payoffs': 124.21150695246799, 'action': [0.0, 1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 133.76275101801482, 'action': [1.0, 1.5707963267948966]}, {'num_count': 558, 'sum_payoffs': 144.6950282690269, 'action': [1.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 121.82808457666454, 'action': [0.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 121.54217660048619, 'action': [0.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 137.5895786008386, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16059335698161883, 0.16962270235407934, 0.1799419542083199, 0.15833602063850372, 0.1580135440180587, 0.17316994517897452]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.41551756858826 s
