Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 354, 'sum_payoffs': 95.8044972412766, 'action': [2.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 89.38407042063035, 'action': [0.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 92.34144378906387, 'action': [1.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 98.44213888452008, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 88.56819118203637, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 90.904718128515, 'action': [1.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 85.18045501540229, 'action': [0.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 91.61287777709292, 'action': [1.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 97.20810177240952, 'action': [2.0, 0.0]}])
Weights num count: [0.11415672363753628, 0.10867462108997097, 0.11125443405353112, 0.11641405998065141, 0.10802966784908094, 0.10996452757175104, 0.10512737826507579, 0.11060948081264109, 0.11544663011931634]
Actions to choose Agent 1: dict_values([{'num_count': 477, 'sum_payoffs': 116.75155803607419, 'action': [0.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 119.15745571624157, 'action': [0.0, -1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 139.94570899873568, 'action': [1.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 136.18213739007342, 'action': [1.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 126.2309335223109, 'action': [0.0, 0.0]}, {'num_count': 555, 'sum_payoffs': 143.28260252913788, 'action': [1.0, 0.0]}])
Weights num count: [0.15382134795227345, 0.1560786842953886, 0.17574975814253466, 0.17220251531763947, 0.16285069332473395, 0.17897452434698485]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.9818913936615 s
