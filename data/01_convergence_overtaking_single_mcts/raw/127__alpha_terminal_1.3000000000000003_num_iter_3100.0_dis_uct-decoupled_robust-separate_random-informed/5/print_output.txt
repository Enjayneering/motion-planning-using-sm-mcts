Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 327, 'sum_payoffs': 85.72782087012247, 'action': [0.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 87.14544314131554, 'action': [0.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 94.3430744456354, 'action': [1.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 91.26968054594623, 'action': [2.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 96.88817743724256, 'action': [2.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 89.7834667583005, 'action': [1.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 103.3766875310277, 'action': [2.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 95.879637542769, 'action': [1.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 85.71354124967931, 'action': [0.0, 0.0]}])
Weights num count: [0.1054498548855208, 0.10673976136730087, 0.11286681715575621, 0.11028700419219607, 0.11512415349887133, 0.10899709771041599, 0.12060625604643663, 0.11415672363753628, 0.1054498548855208]
Actions to choose Agent 1: dict_values([{'num_count': 518, 'sum_payoffs': 130.3749661780068, 'action': [1.0, -1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 135.8220430661176, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 125.56338876792353, 'action': [0.0, 0.0]}, {'num_count': 550, 'sum_payoffs': 141.31014619165245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 121.93787712477416, 'action': [0.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 124.64268244685951, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1670428893905192, 0.17220251531763947, 0.16252821670428894, 0.17736214124475974, 0.15898097387939375, 0.16156078684295389]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.70041346549988 s
