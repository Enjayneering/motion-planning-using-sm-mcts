Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 330, 'sum_payoffs': 86.1260346794424, 'action': [0.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 87.97282605764876, 'action': [0.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 98.07632516791116, 'action': [2.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 99.72059361487639, 'action': [2.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 88.446463832452, 'action': [1.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 86.98190153358033, 'action': [0.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 93.01260178417029, 'action': [2.0, 1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 93.35550334803821, 'action': [1.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 90.74017134744126, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10641728474685586, 0.10802966784908094, 0.11673653660109642, 0.11802644308287649, 0.10835214446952596, 0.10706223798774589, 0.11222186391486617, 0.11254434053531119, 0.11028700419219607]
Actions to choose Agent 1: dict_values([{'num_count': 483, 'sum_payoffs': 119.0983657372201, 'action': [0.0, -1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 137.52045857092824, 'action': [1.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 119.70172429430069, 'action': [0.0, 1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 141.61685090250202, 'action': [1.0, -1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 140.2248568427174, 'action': [1.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 125.17312050303428, 'action': [0.0, 0.0]}])
Weights num count: [0.15575620767494355, 0.17316994517897452, 0.15640116091583361, 0.17703966462431472, 0.17574975814253466, 0.16156078684295389]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.56389355659485 s
