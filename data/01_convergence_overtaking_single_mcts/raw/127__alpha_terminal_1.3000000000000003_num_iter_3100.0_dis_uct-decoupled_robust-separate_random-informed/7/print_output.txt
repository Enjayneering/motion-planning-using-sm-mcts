Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 363, 'sum_payoffs': 99.21738451244748, 'action': [2.0, 1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 87.83538436248838, 'action': [0.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 101.07663040330564, 'action': [2.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 90.79811387695513, 'action': [0.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 86.06104999893527, 'action': [1.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 84.06767271674596, 'action': [0.0, 1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 93.17831053809503, 'action': [1.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 98.80792058133778, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 88.52474690175099, 'action': [1.0, 0.0]}])
Weights num count: [0.11705901322154144, 0.1073847146081909, 0.11867139632376653, 0.10996452757175104, 0.10577233150596582, 0.10415994840374072, 0.11189938729442116, 0.11673653660109642, 0.10802966784908094]
Actions to choose Agent 1: dict_values([{'num_count': 486, 'sum_payoffs': 119.71554034540402, 'action': [0.0, -1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 145.94873920954268, 'action': [1.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 141.1471866100398, 'action': [1.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 124.38791842009203, 'action': [0.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 117.30278076830987, 'action': [0.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 132.27500853613262, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15672363753627863, 0.181554337310545, 0.17703966462431472, 0.16123831022250887, 0.15446630119316349, 0.16865527249274428]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.382750511169434 s
