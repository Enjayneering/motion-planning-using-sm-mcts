Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 349, 'sum_payoffs': 93.92197933354838, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 89.4748954437569, 'action': [0.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 94.36816122289379, 'action': [2.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 90.96394870289606, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 91.04952810826195, 'action': [1.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 96.30136210534961, 'action': [2.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 100.44894504942967, 'action': [2.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 89.4953409306697, 'action': [1.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 84.62988036180788, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11254434053531119, 0.10867462108997097, 0.11286681715575621, 0.10996452757175104, 0.10996452757175104, 0.11447920025798129, 0.11802644308287649, 0.10867462108997097, 0.10448242502418574]
Actions to choose Agent 1: dict_values([{'num_count': 506, 'sum_payoffs': 126.53372299375823, 'action': [0.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 124.9205967522143, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 134.0636310855747, 'action': [1.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 139.15605130523758, 'action': [1.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 121.54120372790953, 'action': [0.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 135.14304134197926, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16317316994517897, 0.16156078684295389, 0.17026765559496937, 0.17510480490164462, 0.15833602063850372, 0.17123508545630442]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.91807198524475 s
