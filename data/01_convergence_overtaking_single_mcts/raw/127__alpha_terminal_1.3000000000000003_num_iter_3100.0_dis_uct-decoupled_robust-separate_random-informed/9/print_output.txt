Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 339, 'sum_payoffs': 89.9893487001636, 'action': [0.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 88.99035881314016, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 89.6666490992523, 'action': [1.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 94.65647403316028, 'action': [2.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 90.10534674407997, 'action': [0.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 98.75423341059721, 'action': [2.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 89.73438092000723, 'action': [1.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 90.89650941106713, 'action': [2.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 96.49003734550652, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10931957433086101, 0.10835214446952596, 0.10899709771041599, 0.11318929377620122, 0.10931957433086101, 0.11673653660109642, 0.10899709771041599, 0.10996452757175104, 0.11480167687842631]
Actions to choose Agent 1: dict_values([{'num_count': 546, 'sum_payoffs': 140.5796220921635, 'action': [1.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 133.59450992667973, 'action': [1.0, 1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 142.25905814652754, 'action': [1.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 120.09500821634525, 'action': [0.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 123.03167003215174, 'action': [0.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 123.47706350370717, 'action': [0.0, 0.0]}])
Weights num count: [0.17607223476297967, 0.16962270235407934, 0.17768461786520479, 0.15672363753627863, 0.15962592712028378, 0.1599484037407288]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.881269216537476 s
