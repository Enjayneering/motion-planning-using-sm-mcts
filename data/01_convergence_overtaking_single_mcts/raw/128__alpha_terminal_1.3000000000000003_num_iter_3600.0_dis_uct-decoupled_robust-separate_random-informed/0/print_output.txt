Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 407, 'sum_payoffs': 108.82318689587883, 'action': [1.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 106.9703101050694, 'action': [2.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 98.99225140009854, 'action': [0.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 110.24288663023351, 'action': [2.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 112.10945668288895, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 98.94380885316278, 'action': [0.0, -1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 113.56691021695497, 'action': [2.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 103.71728615066871, 'action': [1.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 102.9100592875698, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1130241599555679, 0.11163565676201055, 0.10552624271035824, 0.11413496251041377, 0.11552346570397112, 0.10552624271035824, 0.116634268258817, 0.10913635101360733, 0.10858094973618439]
Actions to choose Agent 1: dict_values([{'num_count': 584, 'sum_payoffs': 144.85617050546293, 'action': [0.0, 1.5707963267948966]}, {'num_count': 613, 'sum_payoffs': 154.51935078718333, 'action': [1.0, -1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 156.0861460862329, 'action': [1.0, 1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 142.1398733850997, 'action': [0.0, 0.0]}, {'num_count': 644, 'sum_payoffs': 164.8027825062836, 'action': [1.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 138.4961384123541, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16217717300749793, 0.1702304915301305, 0.17161899472368786, 0.15995556789780616, 0.17883921133018607, 0.15690086087198002]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.601091623306274 s
