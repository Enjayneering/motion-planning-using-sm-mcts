Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 406, 'sum_payoffs': 107.75226674845858, 'action': [1.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 114.81647266044115, 'action': [2.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 99.02255126212063, 'action': [0.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 104.08079103421203, 'action': [2.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 109.98291835555797, 'action': [2.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 104.91655394991116, 'action': [1.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 108.85746687950137, 'action': [1.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 100.90042451437449, 'action': [0.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 100.08127117375614, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11274645931685642, 0.11802277145237434, 0.10608164398778117, 0.10996945292974174, 0.11441266314912524, 0.11052485420716468, 0.11357956123299083, 0.10747014718133852, 0.10691474590391557]
Actions to choose Agent 1: dict_values([{'num_count': 614, 'sum_payoffs': 155.65408160590366, 'action': [1.0, 1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 156.43258691295327, 'action': [1.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 167.18643162328448, 'action': [1.0, 0.0]}, {'num_count': 581, 'sum_payoffs': 144.64532149123198, 'action': [0.0, -1.5707963267948966]}, {'num_count': 567, 'sum_payoffs': 140.01548783189097, 'action': [0.0, 1.5707963267948966]}, {'num_count': 574, 'sum_payoffs': 142.36522774873703, 'action': [0.0, 0.0]}])
Weights num count: [0.170508192168842, 0.17106359344626493, 0.17995001388503193, 0.1613440710913635, 0.15745626214940295, 0.15940016662038323]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 59.051926612854004 s
