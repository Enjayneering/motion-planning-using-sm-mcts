Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 390, 'sum_payoffs': 102.38412358396634, 'action': [0.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 101.88688531980355, 'action': [0.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 97.90113101775401, 'action': [0.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 109.72405176359553, 'action': [1.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 116.71218393463536, 'action': [2.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 111.55734973870213, 'action': [2.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 102.7484385136166, 'action': [1.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 104.92158487503835, 'action': [2.0, 1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 106.41977983669953, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10830324909747292, 0.10802554845876146, 0.1049708414329353, 0.11385726187170231, 0.11913357400722022, 0.11524576506525964, 0.10858094973618439, 0.1102471535684532, 0.11135795612329909]
Actions to choose Agent 1: dict_values([{'num_count': 620, 'sum_payoffs': 157.16972185739934, 'action': [1.0, 1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 136.89867159728908, 'action': [0.0, 1.5707963267948966]}, {'num_count': 595, 'sum_payoffs': 148.8712581192554, 'action': [0.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 137.9543938032646, 'action': [0.0, -1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 156.22732722906304, 'action': [1.0, -1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 166.1812284550253, 'action': [1.0, 0.0]}])
Weights num count: [0.1721743960011108, 0.1552346570397112, 0.16523188003332406, 0.1560677589558456, 0.1713412940849764, 0.17967231324632046]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.883424282073975 s
