Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 390, 'sum_payoffs': 102.35143442461812, 'action': [0.0, -1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 107.20777181488644, 'action': [1.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 104.59362842873189, 'action': [1.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 104.135433900199, 'action': [0.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 96.51891519294344, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 113.07889470949992, 'action': [2.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 109.74200099951723, 'action': [2.0, -1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 107.12999602906629, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 109.61549033546999, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10830324909747292, 0.11191335740072202, 0.10996945292974174, 0.10969175229103027, 0.10386003887808942, 0.11635656762010553, 0.11385726187170231, 0.11191335740072202, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 565, 'sum_payoffs': 138.06912648151362, 'action': [0.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 156.6929177897407, 'action': [1.0, 1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 158.37999644750076, 'action': [1.0, -1.5707963267948966]}, {'num_count': 646, 'sum_payoffs': 165.12102196918934, 'action': [1.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 137.13394095307652, 'action': [0.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 143.12612917146436, 'action': [0.0, 0.0]}])
Weights num count: [0.15690086087198002, 0.17245209663982228, 0.17384059983337963, 0.179394612607609, 0.1560677589558456, 0.16106637045265204]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.7997682094574 s
