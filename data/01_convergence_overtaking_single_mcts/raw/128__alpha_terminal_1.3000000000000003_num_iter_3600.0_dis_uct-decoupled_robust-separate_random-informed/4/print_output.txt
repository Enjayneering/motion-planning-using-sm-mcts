Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 103.67710843836062, 'action': [1.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 105.94396169364478, 'action': [1.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 100.85010661378412, 'action': [0.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 112.53069848408417, 'action': [2.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 105.47940333691741, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 98.26741630254567, 'action': [0.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 114.28382660383295, 'action': [2.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 97.88045023178464, 'action': [0.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 114.3005382076241, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1094140516523188, 0.11108025548458761, 0.10719244654262705, 0.11607886698139405, 0.11080255484587614, 0.10524854207164676, 0.1174673701749514, 0.1049708414329353, 0.1174673701749514]
Actions to choose Agent 1: dict_values([{'num_count': 643, 'sum_payoffs': 164.48007063047865, 'action': [1.0, 1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 155.77300259428415, 'action': [1.0, -1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 134.56134097360183, 'action': [0.0, -1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 145.10035309503394, 'action': [0.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 139.81538094737806, 'action': [0.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 161.15214799186705, 'action': [1.0, 0.0]}])
Weights num count: [0.17856151069147458, 0.1713412940849764, 0.1535684532074424, 0.1624548736462094, 0.15801166342682588, 0.1757845043043599]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 59.032522439956665 s
