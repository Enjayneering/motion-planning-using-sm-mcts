Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 376, 'sum_payoffs': 97.42672096028537, 'action': [0.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 110.144445032193, 'action': [2.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 109.1191097633737, 'action': [2.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 106.16795846808017, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 100.34055773460786, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 110.2346751033835, 'action': [1.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 105.46435374092312, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 101.33200326177189, 'action': [0.0, 0.0]}, {'num_count': 425, 'sum_payoffs': 115.38541381242037, 'action': [2.0, 0.0]}])
Weights num count: [0.10441544015551235, 0.11413496251041377, 0.11330186059427937, 0.11108025548458761, 0.10663704526520411, 0.11413496251041377, 0.11052485420716468, 0.10747014718133852, 0.11802277145237434]
Actions to choose Agent 1: dict_values([{'num_count': 650, 'sum_payoffs': 167.50693045999162, 'action': [1.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 145.29394101840126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 156.03595921876197, 'action': [1.0, -1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 140.96659312212253, 'action': [0.0, -1.5707963267948966]}, {'num_count': 565, 'sum_payoffs': 139.1321417931991, 'action': [0.0, 0.0]}, {'num_count': 614, 'sum_payoffs': 155.4206500664939, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18050541516245489, 0.16217717300749793, 0.17106359344626493, 0.15856706470424883, 0.15690086087198002, 0.170508192168842]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.67096257209778 s
