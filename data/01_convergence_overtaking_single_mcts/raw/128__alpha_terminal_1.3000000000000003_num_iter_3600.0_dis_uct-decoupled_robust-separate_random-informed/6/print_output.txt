Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 392, 'sum_payoffs': 102.80996700466721, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 109.73985699705533, 'action': [2.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 99.90639115708784, 'action': [0.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 102.13938238398045, 'action': [1.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 114.17826330320523, 'action': [2.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 98.14592917990903, 'action': [0.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 104.32913899795919, 'action': [1.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 114.89936220782165, 'action': [2.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 105.7987417660109, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10885865037489587, 0.11413496251041377, 0.10663704526520411, 0.10830324909747292, 0.1174673701749514, 0.10524854207164676, 0.10996945292974174, 0.11802277145237434, 0.11108025548458761]
Actions to choose Agent 1: dict_values([{'num_count': 619, 'sum_payoffs': 156.46775657896214, 'action': [1.0, -1.5707963267948966]}, {'num_count': 590, 'sum_payoffs': 146.70462521609056, 'action': [0.0, -1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 144.49936952421766, 'action': [0.0, 0.0]}, {'num_count': 634, 'sum_payoffs': 161.53064278491976, 'action': [1.0, 0.0]}, {'num_count': 615, 'sum_payoffs': 155.11045205961457, 'action': [1.0, 1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 136.41911550795393, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17189669536239932, 0.16384337683976674, 0.16189947236878643, 0.17606220494307137, 0.17078589280755346, 0.1552346570397112]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.49679613113403 s
