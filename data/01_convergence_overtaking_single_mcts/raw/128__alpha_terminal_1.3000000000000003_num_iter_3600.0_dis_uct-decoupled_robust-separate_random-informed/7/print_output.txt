Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 383, 'sum_payoffs': 99.84527869128908, 'action': [0.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 112.27352605729452, 'action': [2.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 103.77951779139634, 'action': [1.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 99.87679240477733, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 105.30333160188005, 'action': [1.0, 0.0]}, {'num_count': 425, 'sum_payoffs': 115.30725828457157, 'action': [2.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 106.81472485047424, 'action': [1.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 101.66256799389747, 'action': [0.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 109.78809726779352, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10635934462649264, 0.11580116634268259, 0.1094140516523188, 0.10635934462649264, 0.11052485420716468, 0.11802277145237434, 0.11163565676201055, 0.10774784782004998, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 577, 'sum_payoffs': 142.51104121270612, 'action': [0.0, 0.0]}, {'num_count': 624, 'sum_payoffs': 158.05390032983453, 'action': [1.0, -1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 141.0834440729664, 'action': [0.0, 1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 155.10472372950463, 'action': [1.0, 1.5707963267948966]}, {'num_count': 631, 'sum_payoffs': 160.4887383808667, 'action': [1.0, 0.0]}, {'num_count': 580, 'sum_payoffs': 143.44287679227594, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16023326853651762, 0.17328519855595667, 0.15912246598167176, 0.17078589280755346, 0.17522910302693695, 0.16106637045265204]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.71715545654297 s
