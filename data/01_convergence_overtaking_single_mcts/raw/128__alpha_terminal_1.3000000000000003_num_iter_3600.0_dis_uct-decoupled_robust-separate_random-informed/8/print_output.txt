Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 381, 'sum_payoffs': 99.190486089789, 'action': [0.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 110.65570989292841, 'action': [1.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 110.97919221696131, 'action': [2.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 105.14502008016723, 'action': [1.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 105.82099270397714, 'action': [1.0, 0.0]}, {'num_count': 432, 'sum_payoffs': 118.03407575142744, 'action': [2.0, 0.0]}, {'num_count': 381, 'sum_payoffs': 99.20462624041001, 'action': [0.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 109.82206693424818, 'action': [2.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 97.1181248513132, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1058039433490697, 0.11441266314912524, 0.11469036378783672, 0.1102471535684532, 0.11080255484587614, 0.11996667592335462, 0.1058039433490697, 0.11385726187170231, 0.10413773951680089]
Actions to choose Agent 1: dict_values([{'num_count': 569, 'sum_payoffs': 140.03125527518117, 'action': [0.0, 0.0]}, {'num_count': 628, 'sum_payoffs': 159.65860001558605, 'action': [1.0, -1.5707963267948966]}, {'num_count': 642, 'sum_payoffs': 164.37229503564137, 'action': [1.0, 0.0]}, {'num_count': 573, 'sum_payoffs': 141.35119839412442, 'action': [0.0, -1.5707963267948966]}, {'num_count': 619, 'sum_payoffs': 156.71279625786755, 'action': [1.0, 1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 140.0594523861323, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15801166342682588, 0.17439600111080256, 0.17828381005276311, 0.15912246598167176, 0.17189669536239932, 0.15801166342682588]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.710363149642944 s
