Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 398, 'sum_payoffs': 105.28479634173108, 'action': [1.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 102.77813439810862, 'action': [1.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 116.39793373063385, 'action': [2.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 99.07885986250746, 'action': [0.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 98.33465206904359, 'action': [0.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 109.65484344226847, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 117.77423092573986, 'action': [2.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 106.39159614439083, 'action': [1.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 98.7766321204527, 'action': [0.0, 0.0]}])
Weights num count: [0.11052485420716468, 0.10858094973618439, 0.11885587336850875, 0.1058039433490697, 0.10524854207164676, 0.11385726187170231, 0.11996667592335462, 0.11135795612329909, 0.10552624271035824]
Actions to choose Agent 1: dict_values([{'num_count': 624, 'sum_payoffs': 158.64558307357933, 'action': [1.0, -1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 159.91104922112606, 'action': [1.0, 1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 142.59917246595234, 'action': [0.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 143.89551945474395, 'action': [0.0, 0.0]}, {'num_count': 559, 'sum_payoffs': 136.92606907249422, 'action': [0.0, -1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 161.5956460274177, 'action': [1.0, 0.0]}])
Weights num count: [0.17328519855595667, 0.17439600111080256, 0.15995556789780616, 0.16106637045265204, 0.1552346570397112, 0.1757845043043599]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.37473440170288 s
