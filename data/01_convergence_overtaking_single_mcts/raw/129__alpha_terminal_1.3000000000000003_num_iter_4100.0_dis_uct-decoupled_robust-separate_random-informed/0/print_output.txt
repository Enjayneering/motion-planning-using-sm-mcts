Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 115.86141895703872, 'action': [0.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 111.17935880204512, 'action': [0.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 129.86001289465733, 'action': [2.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 124.11826407614863, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 111.28828229036968, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 119.43519366312356, 'action': [1.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 128.139837119243, 'action': [2.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 119.13279669908982, 'action': [1.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 117.98152939463824, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10851011948305292, 0.10534016093635698, 0.1180199951231407, 0.1141185076810534, 0.10534016093635698, 0.11094854913435748, 0.11680078029748842, 0.11070470616922702, 0.10997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 646, 'sum_payoffs': 157.75891494733276, 'action': [0.0, 1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 180.58049369035032, 'action': [1.0, 1.5707963267948966]}, {'num_count': 735, 'sum_payoffs': 186.87661996942796, 'action': [1.0, 0.0]}, {'num_count': 634, 'sum_payoffs': 153.85670175122127, 'action': [0.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 166.52222407525608, 'action': [0.0, 0.0]}, {'num_count': 696, 'sum_payoffs': 174.06371328454102, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15752255547427457, 0.17459156303340648, 0.17922457937088515, 0.1545964398927091, 0.1641063155327969, 0.16971470373079736]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.95876383781433 s
