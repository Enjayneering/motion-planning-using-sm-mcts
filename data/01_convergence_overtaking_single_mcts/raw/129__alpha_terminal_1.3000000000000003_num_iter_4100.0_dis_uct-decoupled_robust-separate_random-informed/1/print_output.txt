Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 453, 'sum_payoffs': 118.84956942692497, 'action': [1.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 128.5634652608211, 'action': [2.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 120.5411790422938, 'action': [2.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 107.81920095778703, 'action': [0.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 121.73616889072855, 'action': [1.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 134.33524109825353, 'action': [2.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 120.31671205755552, 'action': [1.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 114.0664776844744, 'action': [0.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 111.65235045522749, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11046086320409657, 0.11704462326261887, 0.11168007802974884, 0.10290173128505242, 0.11241160692514021, 0.12094611070470616, 0.11143623506461839, 0.10729090465740064, 0.10558400390148744]
Actions to choose Agent 1: dict_values([{'num_count': 708, 'sum_payoffs': 178.26759048885336, 'action': [1.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 162.5880535743533, 'action': [0.0, 0.0]}, {'num_count': 736, 'sum_payoffs': 187.49913788073817, 'action': [1.0, 0.0]}, {'num_count': 644, 'sum_payoffs': 157.305778880895, 'action': [0.0, -1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 156.0622051121181, 'action': [0.0, 1.5707963267948966]}, {'num_count': 712, 'sum_payoffs': 179.54118259425888, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17264081931236283, 0.16093635698610095, 0.1794684223360156, 0.15703486954401366, 0.15605949768349184, 0.17361619117288465]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.93074584007263 s
