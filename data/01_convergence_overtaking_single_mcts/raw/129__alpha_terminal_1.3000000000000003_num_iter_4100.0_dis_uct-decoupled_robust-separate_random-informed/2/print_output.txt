Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 428, 'sum_payoffs': 109.99331700740682, 'action': [0.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 122.84717653463696, 'action': [1.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 114.58486329018822, 'action': [0.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 124.53534719973511, 'action': [2.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 130.42510522429123, 'action': [2.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 118.56675397651428, 'action': [1.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 128.15747508634604, 'action': [2.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 119.20201073059569, 'action': [1.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 109.9327005518954, 'action': [0.0, 0.0]}])
Weights num count: [0.10436478907583516, 0.11314313582053158, 0.1075347476225311, 0.11436235064618386, 0.11826383808827115, 0.11021702023896611, 0.11680078029748842, 0.11070470616922702, 0.10436478907583516]
Actions to choose Agent 1: dict_values([{'num_count': 657, 'sum_payoffs': 161.14977489205873, 'action': [0.0, 0.0]}, {'num_count': 641, 'sum_payoffs': 155.91330641757452, 'action': [0.0, 1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 180.07249198565802, 'action': [1.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 158.24813719930856, 'action': [0.0, -1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 175.77923962335552, 'action': [1.0, 1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 187.30910047976622, 'action': [1.0, 0.0]}])
Weights num count: [0.16020482809070957, 0.15630334064862228, 0.17434772006827604, 0.15801024140453548, 0.1711777615215801, 0.17971226530114606]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.19693064689636 s
