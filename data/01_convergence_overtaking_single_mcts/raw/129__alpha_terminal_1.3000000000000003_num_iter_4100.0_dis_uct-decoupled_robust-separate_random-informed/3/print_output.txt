Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 465, 'sum_payoffs': 123.50637207083179, 'action': [1.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 119.5439414688184, 'action': [1.0, 0.0]}, {'num_count': 432, 'sum_payoffs': 111.65021643826145, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 113.00854358114798, 'action': [0.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 127.84407439981419, 'action': [2.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 114.40161159876921, 'action': [0.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 120.56144785389407, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 131.33831461949939, 'action': [2.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 118.82937152449784, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11338697878566203, 0.11070470616922702, 0.10534016093635698, 0.1063155327968788, 0.1163130943672275, 0.10729090465740064, 0.11143623506461839, 0.11875152401853206, 0.11021702023896611]
Actions to choose Agent 1: dict_values([{'num_count': 708, 'sum_payoffs': 177.65784128003065, 'action': [1.0, 1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 181.0362135970758, 'action': [1.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 155.92908716401402, 'action': [0.0, 1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 187.19342071137845, 'action': [1.0, 0.0]}, {'num_count': 653, 'sum_payoffs': 159.78605844093, 'action': [0.0, 0.0]}, {'num_count': 643, 'sum_payoffs': 156.52898693431283, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17264081931236283, 0.1750792489636674, 0.15630334064862228, 0.17971226530114606, 0.15922945623018775, 0.1567910265788832]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.71898007392883 s
