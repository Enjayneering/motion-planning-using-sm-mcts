Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 465, 'sum_payoffs': 122.86934798086489, 'action': [1.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 112.29703678303738, 'action': [0.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 120.47934284709578, 'action': [2.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 127.6230043651967, 'action': [2.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 115.34061072233486, 'action': [0.0, 0.0]}, {'num_count': 432, 'sum_payoffs': 111.13781991167447, 'action': [0.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 117.8920583820732, 'action': [1.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 118.69890996293057, 'action': [1.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 129.85316036366015, 'action': [2.0, 0.0]}])
Weights num count: [0.11338697878566203, 0.10607168983174835, 0.11168007802974884, 0.11655693733235796, 0.10826627651792246, 0.10534016093635698, 0.10997317727383565, 0.11046086320409657, 0.1180199951231407]
Actions to choose Agent 1: dict_values([{'num_count': 712, 'sum_payoffs': 178.87847730124747, 'action': [1.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 155.68454155559334, 'action': [0.0, -1.5707963267948966]}, {'num_count': 671, 'sum_payoffs': 165.4866286899734, 'action': [0.0, 0.0]}, {'num_count': 726, 'sum_payoffs': 183.44962405955587, 'action': [1.0, 0.0]}, {'num_count': 721, 'sum_payoffs': 181.68682257055545, 'action': [1.0, 1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 151.8511426681096, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17361619117288465, 0.15630334064862228, 0.16361862960253595, 0.17702999268471104, 0.17581077785905877, 0.1533772250670568]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.18023467063904 s
