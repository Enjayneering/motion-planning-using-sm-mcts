Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 116.13430753039151, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 122.21307359148054, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 116.84778775765837, 'action': [1.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 116.42731765489127, 'action': [1.0, 1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 112.12300820047172, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 124.03207912783854, 'action': [2.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 125.1234032272543, 'action': [2.0, -1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 111.13844351230563, 'action': [0.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 135.15981765305094, 'action': [2.0, 0.0]}])
Weights num count: [0.10851011948305292, 0.11265544989027067, 0.10899780541331383, 0.10875396244818337, 0.10582784686661789, 0.11387466471592295, 0.11460619361131431, 0.10509631797122652, 0.12143379663496708]
Actions to choose Agent 1: dict_values([{'num_count': 714, 'sum_payoffs': 179.66619207404042, 'action': [1.0, -1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 161.6616019026268, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 156.13406001760282, 'action': [0.0, 1.5707963267948966]}, {'num_count': 735, 'sum_payoffs': 186.5818347501237, 'action': [1.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 157.4508359302082, 'action': [0.0, -1.5707963267948966]}, {'num_count': 704, 'sum_payoffs': 176.37084368034664, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17410387710314557, 0.16069251402097048, 0.15654718361375275, 0.17922457937088515, 0.15752255547427457, 0.171665447451841]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.8435001373291 s
