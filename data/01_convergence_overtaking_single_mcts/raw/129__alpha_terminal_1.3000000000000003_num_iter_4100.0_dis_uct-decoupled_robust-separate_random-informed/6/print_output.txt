Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 457, 'sum_payoffs': 120.26053687712388, 'action': [1.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 128.48943406542998, 'action': [2.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 113.86959191002806, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 114.21621662676436, 'action': [0.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 123.50460296758312, 'action': [1.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 111.61620186158822, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 118.50801174724596, 'action': [2.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 114.89078859234002, 'action': [1.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 132.47537024414652, 'action': [2.0, 0.0]}])
Weights num count: [0.11143623506461839, 0.11704462326261887, 0.10704706169227018, 0.10729090465740064, 0.11363082175079249, 0.10558400390148744, 0.11021702023896611, 0.10777859058766155, 0.11972689587905389]
Actions to choose Agent 1: dict_values([{'num_count': 716, 'sum_payoffs': 180.5032151302374, 'action': [1.0, -1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 164.20732339303228, 'action': [0.0, 0.0]}, {'num_count': 731, 'sum_payoffs': 185.43948968911678, 'action': [1.0, 0.0]}, {'num_count': 711, 'sum_payoffs': 178.86351709977092, 'action': [1.0, 1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 153.7605729268017, 'action': [0.0, -1.5707963267948966]}, {'num_count': 642, 'sum_payoffs': 156.4028269953598, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17459156303340648, 0.1623994147768837, 0.17824920751036333, 0.1733723482077542, 0.1545964398927091, 0.15654718361375275]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.92953395843506 s
