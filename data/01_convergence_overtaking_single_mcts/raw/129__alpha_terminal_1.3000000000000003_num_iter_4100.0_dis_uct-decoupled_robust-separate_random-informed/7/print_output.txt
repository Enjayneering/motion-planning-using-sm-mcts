Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 442, 'sum_payoffs': 114.92279463918247, 'action': [1.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 116.96096272484759, 'action': [1.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 109.60066103180245, 'action': [0.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 124.63638879589669, 'action': [2.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 120.96418719607225, 'action': [1.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 112.3579942138626, 'action': [0.0, 1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 133.9776389829153, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 116.36146742341641, 'action': [0.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 128.25449071190332, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10777859058766155, 0.10924164837844429, 0.1041209461107047, 0.11436235064618386, 0.1119239209948793, 0.10607168983174835, 0.12070226773957571, 0.10875396244818337, 0.11680078029748842]
Actions to choose Agent 1: dict_values([{'num_count': 646, 'sum_payoffs': 157.8138716882957, 'action': [0.0, 0.0]}, {'num_count': 654, 'sum_payoffs': 160.46287036298045, 'action': [0.0, 1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 178.76348063343787, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 180.75725163501633, 'action': [1.0, 1.5707963267948966]}, {'num_count': 654, 'sum_payoffs': 160.44607474706626, 'action': [0.0, -1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 182.01162987718882, 'action': [1.0, 0.0]}])
Weights num count: [0.15752255547427457, 0.15947329919531822, 0.17312850524262374, 0.17459156303340648, 0.15947329919531822, 0.1755669348939283]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.0051281452179 s
