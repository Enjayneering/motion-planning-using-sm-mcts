Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 451, 'sum_payoffs': 117.99363743636687, 'action': [1.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 109.84525322498394, 'action': [0.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 124.09108404809487, 'action': [2.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 118.0211985504587, 'action': [0.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 132.0379632576453, 'action': [2.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 112.5211041736344, 'action': [0.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 124.41006351773748, 'action': [2.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 116.13541140488304, 'action': [1.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 121.55750441691214, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10997317727383565, 0.10436478907583516, 0.1141185076810534, 0.10997317727383565, 0.11948305291392343, 0.1063155327968788, 0.11436235064618386, 0.10875396244818337, 0.11241160692514021]
Actions to choose Agent 1: dict_values([{'num_count': 729, 'sum_payoffs': 185.19892008653565, 'action': [1.0, 0.0]}, {'num_count': 726, 'sum_payoffs': 184.28304861822326, 'action': [1.0, 1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 175.07964068995474, 'action': [1.0, -1.5707963267948966]}, {'num_count': 644, 'sum_payoffs': 157.3669587598267, 'action': [0.0, 1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 156.10213360369013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 163.6119460105623, 'action': [0.0, 0.0]}])
Weights num count: [0.17776152158010242, 0.17702999268471104, 0.17020238966105827, 0.15703486954401366, 0.15605949768349184, 0.1616678858814923]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.94628953933716 s
