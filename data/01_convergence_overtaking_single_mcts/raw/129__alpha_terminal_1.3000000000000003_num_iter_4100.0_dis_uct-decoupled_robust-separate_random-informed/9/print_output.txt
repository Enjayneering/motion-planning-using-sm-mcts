Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 476, 'sum_payoffs': 126.87616102503803, 'action': [2.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 123.61615800425298, 'action': [2.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 131.16633732507222, 'action': [2.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 114.56785024657208, 'action': [0.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 108.95470010451864, 'action': [0.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 123.95166296792833, 'action': [1.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 114.61612347864985, 'action': [1.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 118.54807180644806, 'action': [1.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 113.20022023112507, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11606925140209705, 0.11387466471592295, 0.11899536698366252, 0.10777859058766155, 0.10387710314557425, 0.1141185076810534, 0.10777859058766155, 0.11046086320409657, 0.10680321872713973]
Actions to choose Agent 1: dict_values([{'num_count': 736, 'sum_payoffs': 187.32606194097912, 'action': [1.0, 0.0]}, {'num_count': 651, 'sum_payoffs': 159.48052847808972, 'action': [0.0, 0.0]}, {'num_count': 711, 'sum_payoffs': 179.12171282387962, 'action': [1.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 159.0400730348504, 'action': [0.0, -1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 181.38871268084606, 'action': [1.0, 1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 153.95589060580107, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1794684223360156, 0.15874177029992684, 0.1733723482077542, 0.1584979273347964, 0.1750792489636674, 0.1545964398927091]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.00102114677429 s
