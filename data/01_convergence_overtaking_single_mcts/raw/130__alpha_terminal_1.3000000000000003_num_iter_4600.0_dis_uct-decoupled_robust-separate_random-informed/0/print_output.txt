Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 476, 'sum_payoffs': 121.44753665636664, 'action': [0.0, 1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 128.09880707315875, 'action': [0.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 128.17298791612913, 'action': [1.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 138.39003228101814, 'action': [2.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 143.97314241861295, 'action': [2.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 130.23010117989952, 'action': [0.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 131.64993237746884, 'action': [1.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 145.12028274662484, 'action': [2.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 137.33412535039565, 'action': [1.0, 0.0]}])
Weights num count: [0.10345577048467725, 0.10758530754183873, 0.10758530754183873, 0.113888285155401, 0.11736579004564225, 0.1088893718756792, 0.10975874809823952, 0.11801782221256249, 0.11323625298848077]
Actions to choose Agent 1: dict_values([{'num_count': 732, 'sum_payoffs': 178.11676199695668, 'action': [0.0, -1.5707963267948966]}, {'num_count': 860, 'sum_payoffs': 219.31870250404975, 'action': [1.0, 0.0]}, {'num_count': 713, 'sum_payoffs': 171.96262571406658, 'action': [0.0, 1.5707963267948966]}, {'num_count': 774, 'sum_payoffs': 191.53735205581384, 'action': [1.0, -1.5707963267948966]}, {'num_count': 742, 'sum_payoffs': 181.24701704435964, 'action': [0.0, 0.0]}, {'num_count': 779, 'sum_payoffs': 193.14845317802735, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15909584872853727, 0.18691588785046728, 0.15496631167137578, 0.16822429906542055, 0.16126928928493806, 0.16931101934362094]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.61087918281555 s
