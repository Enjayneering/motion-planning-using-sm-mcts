Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 513, 'sum_payoffs': 134.55204215246386, 'action': [1.0, 0.0]}, {'num_count': 558, 'sum_payoffs': 150.5350702611278, 'action': [2.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 129.58056594543055, 'action': [1.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 125.4174617998528, 'action': [0.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 124.32814743288664, 'action': [0.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 138.13427916440185, 'action': [2.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 133.58517239472593, 'action': [1.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 145.19484340941233, 'action': [2.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 124.05069425815174, 'action': [0.0, 0.0]}])
Weights num count: [0.11149750054336013, 0.12127798304716365, 0.10845468376439904, 0.1058465550967181, 0.10519452292979788, 0.11367094109976092, 0.11084546837643991, 0.11801782221256249, 0.10497717887415779]
Actions to choose Agent 1: dict_values([{'num_count': 735, 'sum_payoffs': 179.11183761076347, 'action': [0.0, 1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 178.8284994380731, 'action': [0.0, 0.0]}, {'num_count': 800, 'sum_payoffs': 200.03566329706308, 'action': [1.0, -1.5707963267948966]}, {'num_count': 784, 'sum_payoffs': 194.91340845624555, 'action': [1.0, 1.5707963267948966]}, {'num_count': 832, 'sum_payoffs': 210.35151095316587, 'action': [1.0, 0.0]}, {'num_count': 715, 'sum_payoffs': 172.74565597129126, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15974788089545752, 0.15953053683981744, 0.1738752445120626, 0.17039773962182134, 0.1808302542925451, 0.15540099978265595]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.15064239501953 s
