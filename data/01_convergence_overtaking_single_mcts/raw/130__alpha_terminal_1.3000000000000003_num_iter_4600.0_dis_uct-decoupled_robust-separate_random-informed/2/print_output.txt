Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 486, 'sum_payoffs': 124.97083287877419, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 129.92430648598898, 'action': [0.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 141.5952149588967, 'action': [2.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 142.60399611547646, 'action': [2.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 144.02538788761564, 'action': [2.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 134.3888535727734, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 123.59482628008128, 'action': [0.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 128.46503079181645, 'action': [1.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 134.751638980609, 'action': [1.0, 0.0]}])
Weights num count: [0.10562921104107803, 0.10867202782003912, 0.1158443816561617, 0.11649641382308194, 0.11736579004564225, 0.11149750054336013, 0.10475983481851771, 0.10780265159747882, 0.11171484459900022]
Actions to choose Agent 1: dict_values([{'num_count': 737, 'sum_payoffs': 179.42383627039658, 'action': [0.0, 0.0]}, {'num_count': 724, 'sum_payoffs': 175.24061050354007, 'action': [0.0, 1.5707963267948966]}, {'num_count': 771, 'sum_payoffs': 190.30623970376368, 'action': [1.0, -1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 175.91592161227945, 'action': [0.0, -1.5707963267948966]}, {'num_count': 831, 'sum_payoffs': 209.69207292988438, 'action': [1.0, 0.0]}, {'num_count': 811, 'sum_payoffs': 203.20253223309092, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16018256900673766, 0.15735709628341665, 0.16757226689850033, 0.1577917843946968, 0.180612910236905, 0.17626602912410347]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 72.803142786026 s
