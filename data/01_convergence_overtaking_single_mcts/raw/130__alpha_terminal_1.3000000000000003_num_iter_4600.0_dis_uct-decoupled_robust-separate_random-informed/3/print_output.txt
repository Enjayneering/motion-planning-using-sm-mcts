Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 483, 'sum_payoffs': 123.48964145439894, 'action': [0.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 136.12379571639707, 'action': [1.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 134.39903496807327, 'action': [1.0, 0.0]}, {'num_count': 545, 'sum_payoffs': 145.18897409208742, 'action': [2.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 127.71403208772806, 'action': [0.0, 0.0]}, {'num_count': 471, 'sum_payoffs': 119.28973969173086, 'action': [0.0, 1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 142.76077558638298, 'action': [2.0, -1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 130.80860736824812, 'action': [1.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 140.32278986436233, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10497717887415779, 0.11280156487720061, 0.11171484459900022, 0.11845251032384264, 0.10758530754183873, 0.10236905020647685, 0.1169311019343621, 0.10954140404259943, 0.11540969354488155]
Actions to choose Agent 1: dict_values([{'num_count': 796, 'sum_payoffs': 199.25588321631952, 'action': [1.0, -1.5707963267948966]}, {'num_count': 741, 'sum_payoffs': 181.58322351207738, 'action': [0.0, 0.0]}, {'num_count': 831, 'sum_payoffs': 210.60545983797286, 'action': [1.0, 0.0]}, {'num_count': 787, 'sum_payoffs': 196.33946627371623, 'action': [1.0, 1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 177.03081193131254, 'action': [0.0, 1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 174.2248211449446, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1730058682895023, 0.16105194522929797, 0.180612910236905, 0.1710497717887416, 0.15800912845033688, 0.15605303194957618]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 72.94630312919617 s
