Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 519, 'sum_payoffs': 136.57229294629266, 'action': [1.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 139.74405221630994, 'action': [2.0, 1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 151.80725053105235, 'action': [2.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 117.27837505041381, 'action': [0.0, 1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 121.79387987675806, 'action': [0.0, -1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 133.33876043317042, 'action': [1.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 141.86988996741357, 'action': [2.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 128.41749716182713, 'action': [0.0, 0.0]}, {'num_count': 510, 'sum_payoffs': 133.35275760158166, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11280156487720061, 0.11475766137796131, 0.12214735926972398, 0.1008476418169963, 0.10367311454031732, 0.11084546837643991, 0.11606172571180179, 0.10780265159747882, 0.11084546837643991]
Actions to choose Agent 1: dict_values([{'num_count': 838, 'sum_payoffs': 211.8228031017336, 'action': [1.0, 0.0]}, {'num_count': 727, 'sum_payoffs': 176.18145953235842, 'action': [0.0, -1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 198.66978390370744, 'action': [1.0, 1.5707963267948966]}, {'num_count': 707, 'sum_payoffs': 169.7436682589058, 'action': [0.0, 1.5707963267948966]}, {'num_count': 779, 'sum_payoffs': 192.8536221760261, 'action': [1.0, -1.5707963267948966]}, {'num_count': 752, 'sum_payoffs': 184.1300165637127, 'action': [0.0, 0.0]}])
Weights num count: [0.18213431862638557, 0.15800912845033688, 0.17322321234514235, 0.1536622473375353, 0.16931101934362094, 0.16344272984133884]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.31781053543091 s
