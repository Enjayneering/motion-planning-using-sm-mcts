Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 551, 'sum_payoffs': 147.53217761502935, 'action': [2.0, 0.0]}, {'num_count': 474, 'sum_payoffs': 120.3891437876069, 'action': [0.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 136.92383313965036, 'action': [1.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 140.12635465647426, 'action': [2.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 121.82884644389067, 'action': [0.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 139.06044668968795, 'action': [2.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 131.32878663530116, 'action': [1.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 126.7620245700321, 'action': [0.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 137.2575214113627, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11975657465768311, 0.10302108237339709, 0.11323625298848077, 0.11519234948924147, 0.1038904585959574, 0.11454031732232123, 0.10975874809823952, 0.1069332753749185, 0.11345359704412085]
Actions to choose Agent 1: dict_values([{'num_count': 825, 'sum_payoffs': 208.5704167014005, 'action': [1.0, 0.0]}, {'num_count': 713, 'sum_payoffs': 172.58796908597108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 804, 'sum_payoffs': 201.72237266591665, 'action': [1.0, 1.5707963267948966]}, {'num_count': 778, 'sum_payoffs': 193.41365195824736, 'action': [1.0, -1.5707963267948966]}, {'num_count': 764, 'sum_payoffs': 188.9153847602477, 'action': [0.0, 0.0]}, {'num_count': 716, 'sum_payoffs': 173.47304081030484, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17930884590306456, 0.15496631167137578, 0.1747446207346229, 0.16909367528798086, 0.16605085850901977, 0.15561834383829604]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.5069899559021 s
