Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 477, 'sum_payoffs': 121.86247065144438, 'action': [0.0, 1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 137.63090188383856, 'action': [2.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 140.11184321088666, 'action': [2.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 149.3303251814335, 'action': [2.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 124.66039933718267, 'action': [0.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 132.3612260304122, 'action': [1.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 134.51582633226604, 'action': [1.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 127.02142365380938, 'action': [0.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 136.9533750567231, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10367311454031732, 0.11345359704412085, 0.1149750054336014, 0.12062595088024343, 0.10541186698543795, 0.11019343620951967, 0.11149750054336013, 0.1069332753749185, 0.11301890893284068]
Actions to choose Agent 1: dict_values([{'num_count': 708, 'sum_payoffs': 170.25091474413387, 'action': [0.0, 1.5707963267948966]}, {'num_count': 802, 'sum_payoffs': 200.33088331988577, 'action': [1.0, 1.5707963267948966]}, {'num_count': 749, 'sum_payoffs': 183.296022905609, 'action': [0.0, 0.0]}, {'num_count': 724, 'sum_payoffs': 175.31398633988263, 'action': [0.0, -1.5707963267948966]}, {'num_count': 789, 'sum_payoffs': 196.19758108874672, 'action': [1.0, -1.5707963267948966]}, {'num_count': 828, 'sum_payoffs': 208.77768300723437, 'action': [1.0, 0.0]}])
Weights num count: [0.1538795913931754, 0.17430993262334274, 0.16279069767441862, 0.15735709628341665, 0.17148445990002173, 0.1799608780699848]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.53327178955078 s
