Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 488, 'sum_payoffs': 125.4282506704065, 'action': [0.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 130.67541554048844, 'action': [1.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 137.39460877040224, 'action': [1.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 143.7973704524989, 'action': [2.0, -1.5707963267948966]}, {'num_count': 561, 'sum_payoffs': 151.23066053171132, 'action': [2.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 131.66586196123856, 'action': [1.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 121.2710951458127, 'action': [0.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 137.05860805150223, 'action': [2.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 123.60367376451622, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10606389915235818, 0.10932405998695936, 0.11345359704412085, 0.11736579004564225, 0.12193001521408389, 0.10997609215387959, 0.10345577048467725, 0.11323625298848077, 0.10497717887415779]
Actions to choose Agent 1: dict_values([{'num_count': 823, 'sum_payoffs': 207.8876355534821, 'action': [1.0, 0.0]}, {'num_count': 716, 'sum_payoffs': 173.53043175300508, 'action': [0.0, 1.5707963267948966]}, {'num_count': 789, 'sum_payoffs': 197.0161606938783, 'action': [1.0, 1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 180.55955062510017, 'action': [0.0, 0.0]}, {'num_count': 809, 'sum_payoffs': 203.44747914401376, 'action': [1.0, -1.5707963267948966]}, {'num_count': 725, 'sum_payoffs': 176.42971105490602, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1788741577917844, 0.15561834383829604, 0.17148445990002173, 0.16039991306237775, 0.1758313410128233, 0.15757444033905674]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.85215878486633 s
