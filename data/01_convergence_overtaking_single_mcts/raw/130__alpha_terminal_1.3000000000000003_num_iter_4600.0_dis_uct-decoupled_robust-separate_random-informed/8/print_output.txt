Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 506, 'sum_payoffs': 131.8464702672008, 'action': [1.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 131.10398296680427, 'action': [0.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 123.73847818752502, 'action': [0.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 139.23211729187716, 'action': [2.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 122.68383874072796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 134.58637093776065, 'action': [2.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 134.24210497495, 'action': [1.0, -1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 148.46782994569386, 'action': [2.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 136.7603824810449, 'action': [1.0, 0.0]}])
Weights num count: [0.10997609215387959, 0.10954140404259943, 0.10497717887415779, 0.11454031732232123, 0.10432514670723755, 0.11171484459900022, 0.11149750054336013, 0.12019126276896328, 0.11301890893284068]
Actions to choose Agent 1: dict_values([{'num_count': 746, 'sum_payoffs': 182.21800387414953, 'action': [0.0, 1.5707963267948966]}, {'num_count': 754, 'sum_payoffs': 184.83484721595528, 'action': [0.0, 0.0]}, {'num_count': 781, 'sum_payoffs': 193.48396349288424, 'action': [1.0, 1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 198.6663268994278, 'action': [1.0, -1.5707963267948966]}, {'num_count': 826, 'sum_payoffs': 207.99329355561198, 'action': [1.0, 0.0]}, {'num_count': 696, 'sum_payoffs': 166.35298368327702, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16213866550749836, 0.163877417952619, 0.1697457074549011, 0.17322321234514235, 0.17952618995870462, 0.15127146272549447]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.58328127861023 s
