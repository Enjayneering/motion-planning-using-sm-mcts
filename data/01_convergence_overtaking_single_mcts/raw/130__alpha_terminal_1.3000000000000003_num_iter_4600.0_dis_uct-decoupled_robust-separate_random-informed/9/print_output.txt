Searching game tree in timestep 0...
Max timehorizon: 9
Actions to choose Agent 0: dict_values([{'num_count': 498, 'sum_payoffs': 129.08453007554579, 'action': [0.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 122.75568118641532, 'action': [1.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 126.27247273105728, 'action': [0.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 128.78111170162566, 'action': [1.0, 0.0]}, {'num_count': 582, 'sum_payoffs': 158.9266366794622, 'action': [2.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 137.89743319565846, 'action': [2.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 137.15657161574484, 'action': [2.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 123.80987015408155, 'action': [0.0, 1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 138.79849905415765, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10823733970875897, 0.10432514670723755, 0.10649858726363834, 0.10801999565311889, 0.12649424038252555, 0.11367094109976092, 0.11323625298848077, 0.10497717887415779, 0.11432297326668116]
Actions to choose Agent 1: dict_values([{'num_count': 726, 'sum_payoffs': 176.19288863849363, 'action': [0.0, -1.5707963267948966]}, {'num_count': 860, 'sum_payoffs': 219.3166969008752, 'action': [1.0, 0.0]}, {'num_count': 794, 'sum_payoffs': 197.9731401641252, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 172.90724474515974, 'action': [0.0, 1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 179.61650319138022, 'action': [0.0, 0.0]}, {'num_count': 767, 'sum_payoffs': 189.1945919756656, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1577917843946968, 0.18691588785046728, 0.17257118017822212, 0.15561834383829604, 0.16018256900673766, 0.16670289067594002]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.10444641113281 s
