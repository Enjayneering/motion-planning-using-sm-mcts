Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 118, 'sum_payoffs': 29.531014484543558, 'action': [0.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 33.708312968160236, 'action': [2.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 31.68464811252827, 'action': [2.0, 0.0]}, {'num_count': 126, 'sum_payoffs': 33.007790579047956, 'action': [1.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 30.823182084250718, 'action': [1.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 30.843053032106052, 'action': [0.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 30.848066625694496, 'action': [1.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 31.1010335089607, 'action': [2.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 30.432902252980384, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10717529518619437, 0.11625794732061762, 0.11171662125340599, 0.11444141689373297, 0.10990009082652134, 0.10990009082652134, 0.10990009082652134, 0.11080835603996367, 0.10899182561307902]
Actions to choose Agent 1: dict_values([{'num_count': 188, 'sum_payoffs': 45.876436796599044, 'action': [0.0, 0.0]}, {'num_count': 187, 'sum_payoffs': 45.42767703957156, 'action': [1.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 46.645781104432814, 'action': [1.0, 1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 43.903489897414154, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 40.91797344495329, 'action': [0.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 41.73507576903891, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17075386012715713, 0.16984559491371481, 0.17257039055404177, 0.16621253405994552, 0.1589464123524069, 0.16076294277929154]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.102755308151245 s
