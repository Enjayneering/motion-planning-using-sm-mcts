Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 185, 'sum_payoffs': 48.809042554674896, 'action': [1.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 43.65603933035331, 'action': [1.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 45.58240026860527, 'action': [0.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 45.645328757417374, 'action': [2.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 47.68778959900854, 'action': [1.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 44.365329928433, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 45.14354279152985, 'action': [2.0, -1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 41.22530935045156, 'action': [0.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 51.18348315642948, 'action': [2.0, 0.0]}])
Weights num count: [0.1155527795128045, 0.10743285446595878, 0.11055590256089944, 0.11055590256089944, 0.1136789506558401, 0.10868207370393504, 0.1099312929419113, 0.10368519675202999, 0.1193004372267333]
Actions to choose Agent 1: dict_values([{'num_count': 253, 'sum_payoffs': 59.47819594677218, 'action': [0.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 69.54025386449045, 'action': [1.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 59.90934695556943, 'action': [0.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 72.92148799106647, 'action': [1.0, 0.0]}, {'num_count': 254, 'sum_payoffs': 59.827222550752424, 'action': [0.0, 0.0]}, {'num_count': 268, 'sum_payoffs': 64.92522206798161, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1580262336039975, 0.1755153029356652, 0.15865084322298564, 0.1811367895065584, 0.15865084322298564, 0.16739537788881947]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 30.5763201713562 s
