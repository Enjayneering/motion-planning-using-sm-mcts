Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 172, 'sum_payoffs': 43.467111688535816, 'action': [0.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 45.028692911241635, 'action': [0.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 48.25544803875714, 'action': [2.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 46.5612111194591, 'action': [1.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 48.706642607536764, 'action': [2.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 44.64376969447477, 'action': [0.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 44.657027512827874, 'action': [1.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 45.560710601615135, 'action': [2.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 45.13688196210948, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10743285446595878, 0.1099312929419113, 0.11492816989381636, 0.11242973141786383, 0.1155527795128045, 0.10930668332292318, 0.10930668332292318, 0.11055590256089944, 0.1099312929419113]
Actions to choose Agent 1: dict_values([{'num_count': 277, 'sum_payoffs': 67.84513055627376, 'action': [1.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 59.612596771171546, 'action': [0.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 68.5403134520293, 'action': [1.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 57.44351667080923, 'action': [0.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 69.31853744311746, 'action': [1.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 62.06966676759191, 'action': [0.0, 0.0]}])
Weights num count: [0.17301686445971268, 0.15865084322298564, 0.17426608369768895, 0.15490318550905685, 0.1755153029356652, 0.16302311055590257]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 30.56707525253296 s
