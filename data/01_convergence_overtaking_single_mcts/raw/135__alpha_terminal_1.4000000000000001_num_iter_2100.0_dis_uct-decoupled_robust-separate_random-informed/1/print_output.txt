Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 232, 'sum_payoffs': 59.82490692260891, 'action': [1.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 62.194099674794465, 'action': [1.0, -1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 66.01398577781643, 'action': [2.0, 0.0]}, {'num_count': 225, 'sum_payoffs': 57.16163856859665, 'action': [0.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 57.83699461793983, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 55.222983229012925, 'action': [0.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 62.99779638916028, 'action': [2.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 56.77006656829063, 'action': [0.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 65.30460370175216, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11042360780580676, 0.11327939076630177, 0.11803902903379343, 0.10709186101856259, 0.10804378867206092, 0.10471204188481675, 0.1142313184198001, 0.10661589719181343, 0.1170871013802951]
Actions to choose Agent 1: dict_values([{'num_count': 361, 'sum_payoffs': 88.23942212204561, 'action': [1.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 94.26051426493468, 'action': [1.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 75.22443261730835, 'action': [0.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 89.35246827102874, 'action': [1.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 76.89880507750327, 'action': [0.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 83.11395577082448, 'action': [0.0, 0.0]}])
Weights num count: [0.1718229414564493, 0.17991432651118516, 0.15373631603998097, 0.17325083293669682, 0.1561161351737268, 0.1646834840552118]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.17379355430603 s
