Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 244, 'sum_payoffs': 64.11874214941895, 'action': [2.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 63.420557974490634, 'action': [2.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 60.638261670357885, 'action': [1.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 56.45678793083396, 'action': [0.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 61.76752856748495, 'action': [2.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 57.98535022524091, 'action': [1.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 57.96445067378931, 'action': [1.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 57.978424129071165, 'action': [0.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 59.81859053800569, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11613517372679677, 0.11518324607329843, 0.11185149928605426, 0.10661589719181343, 0.11327939076630177, 0.10851975249881009, 0.10851975249881009, 0.10851975249881009, 0.11089957163255593]
Actions to choose Agent 1: dict_values([{'num_count': 332, 'sum_payoffs': 78.26340517499709, 'action': [0.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 79.99514551684746, 'action': [0.0, 0.0]}, {'num_count': 363, 'sum_payoffs': 88.9562800488526, 'action': [1.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 93.62703405383228, 'action': [1.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 90.02381967481641, 'action': [1.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 76.20740448005706, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15801999048072346, 0.1603998096144693, 0.17277486910994763, 0.1789623988576868, 0.17420276059019515, 0.15516420752022847]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.019184589385986 s
