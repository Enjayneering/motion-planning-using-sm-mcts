Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 223, 'sum_payoffs': 56.56777778716751, 'action': [0.0, 1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 59.923631414247, 'action': [0.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 65.00003787036388, 'action': [2.0, -1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 60.30971325941032, 'action': [2.0, 1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 57.62116466641863, 'action': [0.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 58.8611169054151, 'action': [1.0, 1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 60.66343664949335, 'action': [1.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 63.05032554354101, 'action': [2.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 62.33644785490781, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10613993336506425, 0.11042360780580676, 0.11661113755354593, 0.11089957163255593, 0.10756782484531176, 0.10899571632555925, 0.11137553545930509, 0.1142313184198001, 0.11327939076630177]
Actions to choose Agent 1: dict_values([{'num_count': 336, 'sum_payoffs': 79.5962368773167, 'action': [0.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 93.4498521800726, 'action': [1.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 81.73456610642796, 'action': [0.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 85.80376821802109, 'action': [1.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 89.99429036922112, 'action': [1.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 76.15452227957184, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15992384578772012, 0.1789623988576868, 0.16277962874821514, 0.16849119466920515, 0.17420276059019515, 0.15516420752022847]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.08554983139038 s
