Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 223, 'sum_payoffs': 56.04339365191758, 'action': [0.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 65.26984887531171, 'action': [2.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 56.69828714743889, 'action': [0.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 60.650021818314336, 'action': [1.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 63.70876868604321, 'action': [2.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 58.98904310385789, 'action': [1.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 55.617629882394866, 'action': [1.0, 0.0]}, {'num_count': 232, 'sum_payoffs': 59.46765700171648, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 63.382387104838095, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10613993336506425, 0.11756306520704426, 0.10709186101856259, 0.11185149928605426, 0.11565920990004759, 0.1099476439790576, 0.10566396953831508, 0.11042360780580676, 0.11518324607329843]
Actions to choose Agent 1: dict_values([{'num_count': 344, 'sum_payoffs': 82.45221542359184, 'action': [0.0, -1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 76.89673090995343, 'action': [0.0, 1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 88.0153441525258, 'action': [1.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 81.44879982999969, 'action': [0.0, 0.0]}, {'num_count': 369, 'sum_payoffs': 91.12032664700497, 'action': [1.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 87.30960032054645, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16373155640171347, 0.1561161351737268, 0.17134697762970014, 0.16230366492146597, 0.17563065207044265, 0.1703950499762018]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.870044469833374 s
