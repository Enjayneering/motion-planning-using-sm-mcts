Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 234, 'sum_payoffs': 60.54886553080391, 'action': [1.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 62.5139604123488, 'action': [2.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 62.52782847048926, 'action': [2.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 58.648302665212526, 'action': [0.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 58.98272962983885, 'action': [1.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 58.573382964226376, 'action': [0.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 58.22070954961746, 'action': [0.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 62.46411563077208, 'action': [2.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 60.16241523322507, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11137553545930509, 0.11375535459305093, 0.11375535459305093, 0.10899571632555925, 0.10947168015230842, 0.10899571632555925, 0.10851975249881009, 0.11375535459305093, 0.11089957163255593]
Actions to choose Agent 1: dict_values([{'num_count': 336, 'sum_payoffs': 79.0460701933954, 'action': [0.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 80.86008359829137, 'action': [0.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 86.04988288993803, 'action': [1.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 79.4828087248438, 'action': [0.0, 1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 87.34986417098459, 'action': [1.0, -1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 90.84037765660744, 'action': [1.0, 0.0]}])
Weights num count: [0.15992384578772012, 0.16230366492146597, 0.16944312232270348, 0.1603998096144693, 0.17134697762970014, 0.17610661589719181]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.340092182159424 s
