Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 223, 'sum_payoffs': 56.42639589267082, 'action': [0.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 58.37401452339724, 'action': [1.0, 0.0]}, {'num_count': 221, 'sum_payoffs': 55.808123453240576, 'action': [0.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 64.62563619514552, 'action': [2.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 58.85311276732856, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 61.16244949708869, 'action': [1.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 61.47302966068076, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 62.72197131626186, 'action': [2.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 65.04989571148296, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10613993336506425, 0.10851975249881009, 0.10518800571156592, 0.11613517372679677, 0.10899571632555925, 0.11185149928605426, 0.11232746311280342, 0.11375535459305093, 0.11661113755354593]
Actions to choose Agent 1: dict_values([{'num_count': 331, 'sum_payoffs': 78.1739148064397, 'action': [0.0, 1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 89.25234667985494, 'action': [1.0, -1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 91.34122350942287, 'action': [1.0, 0.0]}, {'num_count': 334, 'sum_payoffs': 79.20008109925266, 'action': [0.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 92.79149945590522, 'action': [1.0, 1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 77.75935273121142, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1575440266539743, 0.17277486910994763, 0.17563065207044265, 0.1589719181342218, 0.1775345073774393, 0.15706806282722513]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.09296751022339 s
