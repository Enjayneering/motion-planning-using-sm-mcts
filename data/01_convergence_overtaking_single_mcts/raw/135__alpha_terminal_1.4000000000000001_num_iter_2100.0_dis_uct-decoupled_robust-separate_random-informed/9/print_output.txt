Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 244, 'sum_payoffs': 64.39992826675436, 'action': [2.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.061003772087616, 'action': [0.0, -1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 60.597196155873185, 'action': [1.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 61.381769728288084, 'action': [1.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 63.39308230130097, 'action': [2.0, -1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.04975772475898, 'action': [1.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 61.766211387420356, 'action': [2.0, 0.0]}, {'num_count': 226, 'sum_payoffs': 57.60238247107878, 'action': [0.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 55.96325143963208, 'action': [0.0, 0.0]}])
Weights num count: [0.11613517372679677, 0.10947168015230842, 0.11137553545930509, 0.11232746311280342, 0.11470728224654926, 0.10947168015230842, 0.1128034269395526, 0.10756782484531176, 0.10566396953831508]
Actions to choose Agent 1: dict_values([{'num_count': 360, 'sum_payoffs': 87.32490450633028, 'action': [1.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 77.40168621842582, 'action': [0.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 88.12361962625555, 'action': [1.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 78.0209404750503, 'action': [0.0, 1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 92.87739876188004, 'action': [1.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 79.75919495021859, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17134697762970014, 0.1575440266539743, 0.17229890528319847, 0.15849595430747263, 0.1789623988576868, 0.16087577344121848]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.04767346382141 s
