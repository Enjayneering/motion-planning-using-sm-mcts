Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 278, 'sum_payoffs': 70.43130068437627, 'action': [0.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 74.20698830916531, 'action': [1.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 73.71222461310323, 'action': [1.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 67.84968336592581, 'action': [0.0, 1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 81.75540269413573, 'action': [2.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.08694930209631, 'action': [1.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 80.20598268890666, 'action': [2.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 74.57753999683058, 'action': [2.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 74.92206430872882, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10688196847366398, 0.11072664359861592, 0.11034217608612072, 0.10419069588619762, 0.1184159938485198, 0.10957324106113034, 0.11687812379853903, 0.1111111111111111, 0.1114955786236063]
Actions to choose Agent 1: dict_values([{'num_count': 451, 'sum_payoffs': 110.91374541301091, 'action': [1.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 99.69317448782903, 'action': [0.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 111.93947589024344, 'action': [1.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 102.10656170556395, 'action': [0.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 90.39536038846526, 'action': [0.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 114.58681738946312, 'action': [1.0, 0.0]}])
Weights num count: [0.17339484813533257, 0.16070742022299117, 0.17454825067281815, 0.16339869281045752, 0.14994232987312572, 0.1776239907727797]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.8289589881897 s
