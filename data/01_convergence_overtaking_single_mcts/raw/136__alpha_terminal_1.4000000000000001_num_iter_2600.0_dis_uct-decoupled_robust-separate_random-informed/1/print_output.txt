Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 274, 'sum_payoffs': 69.06657493797054, 'action': [0.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 75.46374361229567, 'action': [1.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 77.98029962966413, 'action': [1.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 78.05693550074706, 'action': [2.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 78.84277692389826, 'action': [2.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 79.21261498117822, 'action': [2.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 71.70483569092701, 'action': [0.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 68.7207665998261, 'action': [0.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 72.82949667138904, 'action': [1.0, 0.0]}])
Weights num count: [0.1053440984236832, 0.1118800461361015, 0.11457131872356786, 0.11457131872356786, 0.11534025374855825, 0.11572472126105345, 0.10803537101114956, 0.104959630911188, 0.10918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 445, 'sum_payoffs': 107.98341266178109, 'action': [1.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 102.66987971471418, 'action': [0.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 102.95932560443136, 'action': [0.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 107.299122165562, 'action': [1.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 95.91914322589204, 'action': [0.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 107.6934719858686, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1710880430603614, 0.16493656286043828, 0.1653210303729335, 0.170319108035371, 0.1572472126105344, 0.1707035755478662]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.98179006576538 s
