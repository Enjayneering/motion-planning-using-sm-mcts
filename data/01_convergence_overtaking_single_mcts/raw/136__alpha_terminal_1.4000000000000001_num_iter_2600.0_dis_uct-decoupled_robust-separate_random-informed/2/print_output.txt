Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 293, 'sum_payoffs': 75.8785188276446, 'action': [1.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 70.76975375368957, 'action': [0.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 73.74922145359164, 'action': [1.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 71.10048333217614, 'action': [0.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 74.15909579732612, 'action': [2.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 78.28056092875212, 'action': [2.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 73.2840465297876, 'action': [1.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 73.00913778733612, 'action': [0.0, -1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 79.77885152728484, 'action': [2.0, 0.0]}])
Weights num count: [0.11264898116109189, 0.10726643598615918, 0.11034217608612072, 0.10765090349865436, 0.11072664359861592, 0.11495578623606305, 0.10995770857362552, 0.10957324106113034, 0.11649365628604383]
Actions to choose Agent 1: dict_values([{'num_count': 412, 'sum_payoffs': 97.14322460281055, 'action': [0.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 109.23288024221291, 'action': [1.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 113.16048385439125, 'action': [1.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 109.21584558084234, 'action': [1.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 96.02840329029765, 'action': [0.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 100.73999835845116, 'action': [0.0, 0.0]}])
Weights num count: [0.15840061514801998, 0.172241445597847, 0.1768550557477893, 0.172241445597847, 0.1572472126105344, 0.16262975778546712]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.165597438812256 s
