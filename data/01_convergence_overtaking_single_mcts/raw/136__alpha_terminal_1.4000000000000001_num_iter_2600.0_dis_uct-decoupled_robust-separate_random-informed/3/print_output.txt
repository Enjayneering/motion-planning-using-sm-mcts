Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 291, 'sum_payoffs': 75.18304308286693, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 72.53066613089281, 'action': [1.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 77.83899945939551, 'action': [2.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 77.47052317392205, 'action': [2.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 73.37054230877061, 'action': [1.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 69.99722796589722, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 74.32497890682059, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 70.3762047691365, 'action': [0.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 78.48986318995718, 'action': [2.0, 0.0]}])
Weights num count: [0.1118800461361015, 0.10918877354863514, 0.11457131872356786, 0.11418685121107267, 0.10995770857362552, 0.10649750096116878, 0.1111111111111111, 0.10688196847366398, 0.11534025374855825]
Actions to choose Agent 1: dict_values([{'num_count': 414, 'sum_payoffs': 97.8857841951098, 'action': [0.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 110.66333971446163, 'action': [1.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 98.2111319361569, 'action': [0.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 111.6566216880563, 'action': [1.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 97.1964807908001, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 110.68794969924251, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15916955017301038, 0.17377931564782775, 0.15955401768550556, 0.17493271818531334, 0.15840061514801998, 0.17377931564782775]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.618064165115356 s
