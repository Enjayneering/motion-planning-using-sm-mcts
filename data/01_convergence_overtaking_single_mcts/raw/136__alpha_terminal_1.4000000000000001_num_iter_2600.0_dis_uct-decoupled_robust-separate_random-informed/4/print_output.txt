Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 307, 'sum_payoffs': 81.58878212473817, 'action': [2.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 72.85043508005877, 'action': [1.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 69.17550748455191, 'action': [0.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 76.25787300968832, 'action': [0.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 71.37867602197312, 'action': [1.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 76.59031733677321, 'action': [2.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 76.63921212296263, 'action': [1.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 75.31389189921435, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 72.53910568433167, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11803152633602461, 0.10918877354863514, 0.1053440984236832, 0.11264898116109189, 0.10765090349865436, 0.11303344867358708, 0.11303344867358708, 0.1118800461361015, 0.10880430603613994]
Actions to choose Agent 1: dict_values([{'num_count': 464, 'sum_payoffs': 114.3533758902651, 'action': [1.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 110.24162811380252, 'action': [1.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 95.88528916013858, 'action': [0.0, 0.0]}, {'num_count': 432, 'sum_payoffs': 103.56855031791625, 'action': [1.0, 1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 103.11622196572723, 'action': [0.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 96.88405879604332, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17839292579777008, 0.17377931564782775, 0.1572472126105344, 0.16608996539792387, 0.16570549788542868, 0.15840061514801998]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.96768236160278 s
