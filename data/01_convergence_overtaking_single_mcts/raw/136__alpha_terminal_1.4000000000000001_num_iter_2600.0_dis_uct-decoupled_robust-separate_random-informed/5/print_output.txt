Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 304, 'sum_payoffs': 79.99705187800006, 'action': [2.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 76.92799415036178, 'action': [1.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 68.04653865464707, 'action': [0.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 76.2237119557104, 'action': [2.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 73.30183601603002, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 70.21457127475516, 'action': [0.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 74.7441809041368, 'action': [1.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 73.97758614192841, 'action': [1.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 75.43648625432274, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11687812379853903, 0.11380238369857747, 0.10457516339869281, 0.11303344867358708, 0.10995770857362552, 0.10688196847366398, 0.1114955786236063, 0.11072664359861592, 0.1122645136485967]
Actions to choose Agent 1: dict_values([{'num_count': 441, 'sum_payoffs': 107.28049555657698, 'action': [1.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 105.56338624174607, 'action': [1.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 113.34807432155127, 'action': [1.0, 0.0]}, {'num_count': 430, 'sum_payoffs': 103.55334034205198, 'action': [0.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 98.52177065943191, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 99.83805257644627, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1695501730103806, 0.16762783544790466, 0.17647058823529413, 0.1653210303729335, 0.15955401768550556, 0.16109188773548636]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.83159685134888 s
