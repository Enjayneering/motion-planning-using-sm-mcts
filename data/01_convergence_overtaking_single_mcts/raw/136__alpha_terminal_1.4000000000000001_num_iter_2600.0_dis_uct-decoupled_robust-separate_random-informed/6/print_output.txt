Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 72.80543850780438, 'action': [1.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 71.61373480480769, 'action': [0.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 71.91674865156921, 'action': [1.0, 1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 80.35082902649283, 'action': [2.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 72.70082032670756, 'action': [0.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 76.80083451573323, 'action': [2.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 75.8133161616608, 'action': [2.0, 1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 77.6301292865833, 'action': [1.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 71.71052092541494, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10918877354863514, 0.10803537101114956, 0.10841983852364476, 0.11687812379853903, 0.10918877354863514, 0.11341791618608228, 0.1122645136485967, 0.11418685121107267, 0.10803537101114956]
Actions to choose Agent 1: dict_values([{'num_count': 412, 'sum_payoffs': 96.93854264105047, 'action': [0.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 117.37393585143565, 'action': [1.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 94.28492937348858, 'action': [0.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 98.8087451044978, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 109.17590782217677, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 107.59025230450038, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15840061514801998, 0.18185313341022682, 0.15532487504805845, 0.16070742022299117, 0.17262591311034217, 0.1707035755478662]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.76654386520386 s
