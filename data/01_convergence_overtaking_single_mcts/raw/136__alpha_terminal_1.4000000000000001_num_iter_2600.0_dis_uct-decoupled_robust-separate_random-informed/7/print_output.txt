Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 286, 'sum_payoffs': 73.05861601620919, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 70.50484642886022, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 77.96380925273496, 'action': [2.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 69.9865161228681, 'action': [0.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 75.70401643643531, 'action': [1.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 71.26648715973509, 'action': [0.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 74.87421308207392, 'action': [1.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 76.4398047028536, 'action': [2.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 77.5286022993667, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10995770857362552, 0.10726643598615918, 0.11495578623606305, 0.10688196847366398, 0.11264898116109189, 0.10803537101114956, 0.1118800461361015, 0.11341791618608228, 0.11457131872356786]
Actions to choose Agent 1: dict_values([{'num_count': 415, 'sum_payoffs': 98.10011169426103, 'action': [0.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 114.57201924649142, 'action': [1.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 111.56764421432118, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 97.1517588161586, 'action': [0.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 107.55924370650625, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 96.7770786223321, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15955401768550556, 0.17839292579777008, 0.17493271818531334, 0.15840061514801998, 0.170319108035371, 0.1580161476355248]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.01203107833862 s
