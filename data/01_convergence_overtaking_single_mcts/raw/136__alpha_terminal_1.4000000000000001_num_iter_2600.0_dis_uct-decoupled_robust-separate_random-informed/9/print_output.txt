Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 293, 'sum_payoffs': 76.38287808655505, 'action': [1.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 74.92536621269798, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 74.14251499274559, 'action': [1.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 79.42961002446081, 'action': [2.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 76.85773319848684, 'action': [2.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 79.77712672098097, 'action': [2.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 68.19233440418128, 'action': [0.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 69.73261780484653, 'action': [0.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 74.44049701259746, 'action': [1.0, 0.0]}])
Weights num count: [0.11264898116109189, 0.1111111111111111, 0.11034217608612072, 0.11572472126105345, 0.11303344867358708, 0.11610918877354863, 0.10419069588619762, 0.1057285659361784, 0.11072664359861592]
Actions to choose Agent 1: dict_values([{'num_count': 459, 'sum_payoffs': 112.35247706678662, 'action': [1.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 95.90921547631253, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 108.00789767964172, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 100.67425586532758, 'action': [0.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.39143268515546, 'action': [1.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 100.28764261247818, 'action': [0.0, 0.0]}])
Weights num count: [0.17647058823529413, 0.1576316801230296, 0.1714725105728566, 0.16301422529796233, 0.16839677047289503, 0.16262975778546712]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.99576139450073 s
