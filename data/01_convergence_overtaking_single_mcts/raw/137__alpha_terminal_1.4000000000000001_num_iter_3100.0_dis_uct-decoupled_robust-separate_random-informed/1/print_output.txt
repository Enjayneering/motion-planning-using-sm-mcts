Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 343, 'sum_payoffs': 88.06826425393069, 'action': [1.0, 1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 90.65336610906063, 'action': [1.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 95.8455621550172, 'action': [2.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 83.777760977542, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 87.4684951145323, 'action': [1.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 95.06563519999443, 'action': [2.0, -1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 81.55336852848463, 'action': [0.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 89.97702066829373, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 85.56759477562521, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11060948081264109, 0.11286681715575621, 0.11738148984198646, 0.10673976136730087, 0.10996452757175104, 0.11673653660109642, 0.10480490164463076, 0.11222186391486617, 0.10835214446952596]
Actions to choose Agent 1: dict_values([{'num_count': 482, 'sum_payoffs': 111.99493058341157, 'action': [0.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 131.87051488865688, 'action': [1.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 130.92309851179198, 'action': [1.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 118.55777337461488, 'action': [0.0, 0.0]}, {'num_count': 541, 'sum_payoffs': 131.30836381154802, 'action': [1.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 115.20737753949183, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15543373105449854, 0.17510480490164462, 0.17413737504030957, 0.1618832634633989, 0.1744598516607546, 0.15865849725894873]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.50237464904785 s
