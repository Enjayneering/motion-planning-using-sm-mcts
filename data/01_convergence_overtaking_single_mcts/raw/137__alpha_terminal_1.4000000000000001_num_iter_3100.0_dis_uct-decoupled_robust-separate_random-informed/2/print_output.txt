Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 351, 'sum_payoffs': 91.17231457849533, 'action': [2.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 88.20794916081249, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 85.99120884302184, 'action': [0.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 91.15403810205787, 'action': [2.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 85.29608266085681, 'action': [0.0, 1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 82.35627797121829, 'action': [0.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 90.42193888311945, 'action': [1.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 93.38497006611169, 'action': [2.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 90.68817378079702, 'action': [1.0, 0.0]}])
Weights num count: [0.11318929377620122, 0.11060948081264109, 0.10867462108997097, 0.11318929377620122, 0.10802966784908094, 0.1054498548855208, 0.11254434053531119, 0.11512415349887133, 0.11286681715575621]
Actions to choose Agent 1: dict_values([{'num_count': 544, 'sum_payoffs': 131.95875204449118, 'action': [1.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 127.68031608813165, 'action': [1.0, 1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 120.25300358831885, 'action': [0.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 114.37921248220592, 'action': [0.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 126.70937624454582, 'action': [1.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 117.30052925972542, 'action': [0.0, 0.0]}])
Weights num count: [0.17542728152208964, 0.17123508545630442, 0.163818123186069, 0.1580135440180587, 0.17026765559496937, 0.16091583360206385]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.56782269477844 s
