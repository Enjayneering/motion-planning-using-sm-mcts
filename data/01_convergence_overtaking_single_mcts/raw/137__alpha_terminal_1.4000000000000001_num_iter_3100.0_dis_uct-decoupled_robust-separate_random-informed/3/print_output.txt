Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 352, 'sum_payoffs': 91.50338155052883, 'action': [1.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 79.07868203579055, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 93.66668135766716, 'action': [2.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 84.1726944040036, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 94.43486147242781, 'action': [2.0, -1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 88.1388035055943, 'action': [1.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 88.88094423695061, 'action': [1.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 86.65715625327596, 'action': [0.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 91.7641565383246, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11351177039664624, 0.10254756530151564, 0.11544663011931634, 0.10706223798774589, 0.11609158336020639, 0.11060948081264109, 0.11125443405353112, 0.10931957433086101, 0.11383424701709126]
Actions to choose Agent 1: dict_values([{'num_count': 497, 'sum_payoffs': 116.68289736267069, 'action': [0.0, 1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 130.07777275338245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 128.69263365498253, 'action': [1.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 118.18180905675209, 'action': [0.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 129.71863135925835, 'action': [1.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 115.02515764002366, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16027088036117382, 0.17349242179941954, 0.17220251531763947, 0.1618832634633989, 0.17316994517897452, 0.15865849725894873]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.56054878234863 s
