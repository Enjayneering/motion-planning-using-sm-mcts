Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 341, 'sum_payoffs': 87.28404729660264, 'action': [1.0, 0.0]}, {'num_count': 348, 'sum_payoffs': 89.95187459509765, 'action': [1.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 93.88180004021096, 'action': [2.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 90.28551333184164, 'action': [1.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 86.94403773469607, 'action': [0.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 81.77433774435013, 'action': [0.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 83.6903385781811, 'action': [0.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 90.89615921159883, 'action': [2.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 92.47246749566861, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10996452757175104, 0.11222186391486617, 0.11576910673976137, 0.11254434053531119, 0.10964205095130602, 0.10512737826507579, 0.10673976136730087, 0.11318929377620122, 0.11447920025798129]
Actions to choose Agent 1: dict_values([{'num_count': 551, 'sum_payoffs': 135.0495639528863, 'action': [1.0, 0.0]}, {'num_count': 545, 'sum_payoffs': 132.9911479877157, 'action': [1.0, -1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 112.05106847007853, 'action': [0.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 118.95369108827295, 'action': [0.0, 0.0]}, {'num_count': 518, 'sum_payoffs': 124.16661532552234, 'action': [1.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 119.22182722449986, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17768461786520479, 0.17574975814253466, 0.15511125443405352, 0.1618832634633989, 0.1670428893905192, 0.16220574008384392]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.59173035621643 s
