Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 354, 'sum_payoffs': 91.7560898992777, 'action': [2.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 94.69527751089977, 'action': [2.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 92.13488678321912, 'action': [2.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 87.80830282211532, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 85.94336831536236, 'action': [0.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 91.48910458793671, 'action': [1.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 80.87661411838715, 'action': [0.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 86.01251398304282, 'action': [1.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 84.09620364372631, 'action': [0.0, 0.0]}])
Weights num count: [0.11415672363753628, 0.11673653660109642, 0.11447920025798129, 0.11060948081264109, 0.10899709771041599, 0.11383424701709126, 0.10448242502418574, 0.10899709771041599, 0.1073847146081909]
Actions to choose Agent 1: dict_values([{'num_count': 522, 'sum_payoffs': 125.35408336915106, 'action': [1.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 117.87254564350367, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 117.79216238684087, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 118.82533886162649, 'action': [0.0, 1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 135.1655364042996, 'action': [1.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 126.71497280693502, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16833279587229927, 0.16091583360206385, 0.16091583360206385, 0.1618832634633989, 0.1780070944856498, 0.16962270235407934]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.75301718711853 s
