Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 347, 'sum_payoffs': 89.67740950117202, 'action': [1.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 91.52441215595599, 'action': [1.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 86.80080654747542, 'action': [0.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 83.82041790423361, 'action': [0.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 91.26072068065695, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 87.91221906535284, 'action': [1.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 85.38347539395586, 'action': [0.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 93.34687140349733, 'action': [2.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 89.38471306973176, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11189938729442116, 0.11351177039664624, 0.10931957433086101, 0.10673976136730087, 0.11318929377620122, 0.11028700419219607, 0.10802966784908094, 0.11512415349887133, 0.11157691067397614]
Actions to choose Agent 1: dict_values([{'num_count': 510, 'sum_payoffs': 121.40191883164468, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 118.82232993822085, 'action': [0.0, 1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 128.03200252604483, 'action': [1.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 132.29474508296002, 'action': [1.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 117.18942717661145, 'action': [0.0, -1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 124.05694264003942, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16446307642695904, 0.1618832634633989, 0.1709126088358594, 0.17510480490164462, 0.16027088036117382, 0.1670428893905192]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.547499656677246 s
