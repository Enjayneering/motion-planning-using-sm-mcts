Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 344, 'sum_payoffs': 88.44285336816051, 'action': [1.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 87.93594458873609, 'action': [0.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 92.82108223139419, 'action': [1.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 82.25679556254849, 'action': [0.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 81.85398552599395, 'action': [0.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 89.07622191778644, 'action': [1.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 89.7444859879403, 'action': [2.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 96.43252932583286, 'action': [2.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 88.31937221247794, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1109319574330861, 0.11060948081264109, 0.11480167687842631, 0.1054498548855208, 0.10512737826507579, 0.11157691067397614, 0.11222186391486617, 0.11802644308287649, 0.1109319574330861]
Actions to choose Agent 1: dict_values([{'num_count': 528, 'sum_payoffs': 127.59159076635967, 'action': [1.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 123.91926223899485, 'action': [1.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 117.0948968797947, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 118.36834418181755, 'action': [0.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 132.47886865024432, 'action': [1.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 123.57748170380823, 'action': [0.0, 0.0]}])
Weights num count: [0.17026765559496937, 0.16672041277007418, 0.1599484037407288, 0.16123831022250887, 0.17510480490164462, 0.16639793614962914]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.86085748672485 s
