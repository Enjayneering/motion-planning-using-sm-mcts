Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 326, 'sum_payoffs': 81.67925474455961, 'action': [0.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 83.73338077121932, 'action': [0.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 90.30513223426136, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 96.58048910256545, 'action': [2.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 95.85048726614622, 'action': [2.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 87.07091694063014, 'action': [1.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 91.42072732003443, 'action': [2.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 79.83610656317383, 'action': [0.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 88.45237649334013, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10512737826507579, 0.10706223798774589, 0.11286681715575621, 0.11834891970332151, 0.11770396646243148, 0.10996452757175104, 0.11383424701709126, 0.10351499516285069, 0.11125443405353112]
Actions to choose Agent 1: dict_values([{'num_count': 538, 'sum_payoffs': 130.80249775304986, 'action': [1.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 114.73513562307915, 'action': [0.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 119.39958146392974, 'action': [0.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 126.90302391032004, 'action': [1.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 115.69344525549809, 'action': [0.0, 1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 135.44195454447447, 'action': [1.0, 0.0]}])
Weights num count: [0.17349242179941954, 0.15769106739761368, 0.16220574008384392, 0.16962270235407934, 0.15865849725894873, 0.1780070944856498]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.15166354179382 s
