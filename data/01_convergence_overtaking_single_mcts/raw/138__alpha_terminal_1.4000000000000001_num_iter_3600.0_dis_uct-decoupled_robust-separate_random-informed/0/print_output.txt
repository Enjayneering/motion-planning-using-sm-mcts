Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 375, 'sum_payoffs': 92.92069676674605, 'action': [0.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 99.9482517357075, 'action': [0.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 104.2454758723429, 'action': [1.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 111.36591204851818, 'action': [2.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 95.64792421924693, 'action': [0.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 100.31676521491029, 'action': [1.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 102.7105984277874, 'action': [2.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 101.71444992331944, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 106.69348855061641, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10413773951680089, 0.10969175229103027, 0.1130241599555679, 0.11857817272979727, 0.10635934462649264, 0.10996945292974174, 0.11191335740072202, 0.11108025548458761, 0.11496806442654818]
Actions to choose Agent 1: dict_values([{'num_count': 577, 'sum_payoffs': 134.77038595846028, 'action': [0.0, 0.0]}, {'num_count': 635, 'sum_payoffs': 153.3162384757168, 'action': [1.0, 0.0]}, {'num_count': 571, 'sum_payoffs': 132.84746841998447, 'action': [0.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 134.13937687689264, 'action': [0.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 148.85988624224893, 'action': [1.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 148.82713893277847, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16023326853651762, 0.17633990558178284, 0.15856706470424883, 0.1596778672590947, 0.17245209663982228, 0.17245209663982228]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.34982490539551 s
