Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 392, 'sum_payoffs': 99.01437359115451, 'action': [0.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 103.59455302472016, 'action': [2.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 101.41164441289061, 'action': [1.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 101.84734583084413, 'action': [1.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 106.09608034822324, 'action': [2.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 100.03719484818392, 'action': [0.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 96.87605468651213, 'action': [0.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 101.03041547095836, 'action': [1.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 106.36047958923318, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10885865037489587, 0.11246875867814496, 0.11080255484587614, 0.11108025548458761, 0.11441266314912524, 0.10969175229103027, 0.10719244654262705, 0.11052485420716468, 0.11469036378783672]
Actions to choose Agent 1: dict_values([{'num_count': 581, 'sum_payoffs': 136.1697838237305, 'action': [0.0, 1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 151.58790841943926, 'action': [1.0, -1.5707963267948966]}, {'num_count': 604, 'sum_payoffs': 143.44817271380032, 'action': [1.0, 1.5707963267948966]}, {'num_count': 592, 'sum_payoffs': 139.60424523636235, 'action': [0.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 150.58003886695263, 'action': [1.0, 0.0]}, {'num_count': 568, 'sum_payoffs': 132.09976612110813, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1613440710913635, 0.17467370174951402, 0.1677311857817273, 0.16439877811718967, 0.17384059983337963, 0.1577339627881144]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.95945596694946 s
