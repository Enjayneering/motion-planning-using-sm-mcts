Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 400, 'sum_payoffs': 101.84137509443262, 'action': [1.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 100.34392933100182, 'action': [1.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 104.99509538815111, 'action': [2.0, 1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 96.51621268602398, 'action': [0.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 104.3069408185507, 'action': [1.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 95.4551058035674, 'action': [0.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 112.24931970186077, 'action': [2.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 103.99650360041869, 'action': [2.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 96.87855765602295, 'action': [0.0, 0.0]}])
Weights num count: [0.11108025548458761, 0.10996945292974174, 0.11357956123299083, 0.10691474590391557, 0.1130241599555679, 0.10608164398778117, 0.11913357400722022, 0.11274645931685642, 0.10719244654262705]
Actions to choose Agent 1: dict_values([{'num_count': 641, 'sum_payoffs': 155.60704030183803, 'action': [1.0, 0.0]}, {'num_count': 604, 'sum_payoffs': 143.77625653108936, 'action': [1.0, -1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 128.1275991034893, 'action': [0.0, 1.5707963267948966]}, {'num_count': 639, 'sum_payoffs': 155.01743204913979, 'action': [1.0, 1.5707963267948966]}, {'num_count': 589, 'sum_payoffs': 138.95190454837726, 'action': [0.0, 0.0]}, {'num_count': 572, 'sum_payoffs': 133.51515175612258, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17800610941405165, 0.1677311857817273, 0.15412385448486532, 0.17745070813662872, 0.16356567620105525, 0.1588447653429603]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.17051386833191 s
