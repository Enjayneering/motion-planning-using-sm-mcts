Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 384, 'sum_payoffs': 95.78518335360856, 'action': [0.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 101.53179340722272, 'action': [1.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 107.20101327950175, 'action': [2.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 92.67827773135183, 'action': [0.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 104.26171655860453, 'action': [1.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 109.01989843577358, 'action': [2.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 106.44045153302966, 'action': [2.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 99.70296010654255, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 96.90813688015858, 'action': [0.0, 0.0]}])
Weights num count: [0.10663704526520411, 0.11108025548458761, 0.11552346570397112, 0.10413773951680089, 0.11330186059427937, 0.11691196889752846, 0.11496806442654818, 0.10969175229103027, 0.10747014718133852]
Actions to choose Agent 1: dict_values([{'num_count': 638, 'sum_payoffs': 155.055792280596, 'action': [1.0, -1.5707963267948966]}, {'num_count': 565, 'sum_payoffs': 131.62557479134503, 'action': [0.0, -1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 151.1165664541532, 'action': [1.0, 0.0]}, {'num_count': 589, 'sum_payoffs': 139.24386750346463, 'action': [0.0, 0.0]}, {'num_count': 601, 'sum_payoffs': 143.03206890011876, 'action': [1.0, 1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 136.65909649302242, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17717300749791726, 0.15690086087198002, 0.17384059983337963, 0.16356567620105525, 0.16689808386559288, 0.1613440710913635]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.6997332572937 s
