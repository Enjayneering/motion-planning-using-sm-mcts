Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 99.64981555039769, 'action': [0.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 105.32000642691186, 'action': [1.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 95.45467972934694, 'action': [1.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 109.98769772577528, 'action': [2.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 108.16408747804527, 'action': [2.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 99.3342905605622, 'action': [1.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 93.99045036160396, 'action': [0.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 107.15391660374081, 'action': [2.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 97.18633760376733, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1094140516523188, 0.11385726187170231, 0.10608164398778117, 0.1174673701749514, 0.11607886698139405, 0.10913635101360733, 0.1049708414329353, 0.11524576506525964, 0.10747014718133852]
Actions to choose Agent 1: dict_values([{'num_count': 573, 'sum_payoffs': 134.16369045095337, 'action': [0.0, 1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 153.6624740745814, 'action': [1.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 133.1157135538647, 'action': [0.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 149.48991692843543, 'action': [1.0, 0.0]}, {'num_count': 583, 'sum_payoffs': 137.30953513345847, 'action': [0.0, 0.0]}, {'num_count': 619, 'sum_payoffs': 148.7759771729919, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15912246598167176, 0.17606220494307137, 0.15828936406553734, 0.17245209663982228, 0.16189947236878643, 0.17189669536239932]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.31253147125244 s
