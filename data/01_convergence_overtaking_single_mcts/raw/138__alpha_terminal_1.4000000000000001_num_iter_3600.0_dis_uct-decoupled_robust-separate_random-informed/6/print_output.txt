Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 386, 'sum_payoffs': 96.51921704738388, 'action': [0.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 100.78653457463416, 'action': [1.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 103.98127177470563, 'action': [1.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 94.71571979682767, 'action': [0.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 97.57979149554396, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 96.90999323688405, 'action': [0.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 103.57463104553851, 'action': [2.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 115.0936608184884, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 104.28599060975482, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10719244654262705, 0.11052485420716468, 0.1130241599555679, 0.1058039433490697, 0.10802554845876146, 0.10747014718133852, 0.11274645931685642, 0.12163287975562344, 0.11330186059427937]
Actions to choose Agent 1: dict_values([{'num_count': 575, 'sum_payoffs': 133.82459202695068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 132.33546369661613, 'action': [0.0, -1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 135.78091441650318, 'action': [0.0, 0.0]}, {'num_count': 607, 'sum_payoffs': 144.02774971062064, 'action': [1.0, 1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 152.72706564115126, 'action': [1.0, 0.0]}, {'num_count': 633, 'sum_payoffs': 152.4367308847054, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1596778672590947, 0.15828936406553734, 0.1613440710913635, 0.1685642876978617, 0.17606220494307137, 0.1757845043043599]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.17210698127747 s
