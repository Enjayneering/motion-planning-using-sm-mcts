Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 393, 'sum_payoffs': 99.41149679352333, 'action': [0.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 106.84325671441283, 'action': [2.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 96.18314724485785, 'action': [0.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 109.99355650078286, 'action': [2.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 106.48832637513084, 'action': [2.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 98.24723191747057, 'action': [0.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 98.31064658672152, 'action': [1.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 102.27308874171851, 'action': [1.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 99.05847747557462, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10913635101360733, 0.11496806442654818, 0.10663704526520411, 0.1174673701749514, 0.11469036378783672, 0.10830324909747292, 0.10830324909747292, 0.11135795612329909, 0.10885865037489587]
Actions to choose Agent 1: dict_values([{'num_count': 624, 'sum_payoffs': 149.84204876945165, 'action': [1.0, 0.0]}, {'num_count': 611, 'sum_payoffs': 145.69486294363105, 'action': [1.0, -1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 138.33588091852909, 'action': [0.0, -1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 135.47770930755036, 'action': [0.0, 1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 146.57321953222277, 'action': [1.0, 1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 137.04839025705803, 'action': [0.0, 0.0]}])
Weights num count: [0.17328519855595667, 0.16967509025270758, 0.16328797556234378, 0.16078866981394058, 0.170508192168842, 0.16217717300749793]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.24921250343323 s
