Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 99.68223471296847, 'action': [1.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 99.74135321767957, 'action': [1.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 97.20975373504473, 'action': [0.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 107.50804858891459, 'action': [2.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 107.13471605745296, 'action': [2.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 115.4182524673446, 'action': [2.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 94.42168020933207, 'action': [0.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 96.14340504795442, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 99.31302765906467, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1094140516523188, 0.1094140516523188, 0.10747014718133852, 0.11552346570397112, 0.11524576506525964, 0.12163287975562344, 0.10524854207164676, 0.10663704526520411, 0.10913635101360733]
Actions to choose Agent 1: dict_values([{'num_count': 570, 'sum_payoffs': 132.98668563030247, 'action': [0.0, 1.5707963267948966]}, {'num_count': 589, 'sum_payoffs': 139.13365765425462, 'action': [0.0, 0.0]}, {'num_count': 573, 'sum_payoffs': 133.96389680833946, 'action': [0.0, -1.5707963267948966]}, {'num_count': 619, 'sum_payoffs': 148.7306334602571, 'action': [1.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 149.3721629284887, 'action': [1.0, 1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 151.61473316791768, 'action': [1.0, 0.0]}])
Weights num count: [0.15828936406553734, 0.16356567620105525, 0.15912246598167176, 0.17189669536239932, 0.17245209663982228, 0.17439600111080256]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.08851075172424 s
