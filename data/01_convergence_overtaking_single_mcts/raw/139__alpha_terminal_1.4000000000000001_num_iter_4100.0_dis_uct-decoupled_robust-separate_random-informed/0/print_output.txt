Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 483, 'sum_payoffs': 124.22392749264188, 'action': [2.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 109.3627946601079, 'action': [0.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 113.48644366657898, 'action': [1.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 119.7642693598865, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 106.57112028787809, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 113.49137760730466, 'action': [1.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 116.27137872267144, 'action': [1.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 111.06778767803065, 'action': [0.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 118.28335495164151, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11777615215801024, 0.10729090465740064, 0.11021702023896611, 0.11460619361131431, 0.10534016093635698, 0.11021702023896611, 0.11216776396000976, 0.10851011948305292, 0.11363082175079249]
Actions to choose Agent 1: dict_values([{'num_count': 713, 'sum_payoffs': 170.64921573652362, 'action': [1.0, 1.5707963267948966]}, {'num_count': 693, 'sum_payoffs': 164.3712809727486, 'action': [1.0, -1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 174.7322889379955, 'action': [1.0, 0.0]}, {'num_count': 677, 'sum_payoffs': 159.34880579307085, 'action': [0.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 149.6046820130478, 'action': [0.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 149.20796559044214, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17386003413801512, 0.168983174835406, 0.17702999268471104, 0.16508168739331872, 0.15752255547427457, 0.1572787125091441]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.46992945671082 s
