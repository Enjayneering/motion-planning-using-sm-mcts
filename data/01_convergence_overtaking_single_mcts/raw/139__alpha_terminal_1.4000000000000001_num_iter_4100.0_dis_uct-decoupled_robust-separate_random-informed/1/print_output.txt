Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 480, 'sum_payoffs': 123.3513013587597, 'action': [2.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 107.30135135457739, 'action': [0.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 117.4136202816156, 'action': [2.0, 1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 106.8843892424759, 'action': [0.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 130.65091836310754, 'action': [2.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 108.02334864706518, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 113.90817433948762, 'action': [1.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 115.63719561539284, 'action': [1.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 110.02711366646305, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11704462326261887, 0.10582784686661789, 0.11289929285540112, 0.10558400390148744, 0.12216532553035844, 0.1063155327968788, 0.11046086320409657, 0.11168007802974884, 0.10777859058766155]
Actions to choose Agent 1: dict_values([{'num_count': 665, 'sum_payoffs': 155.0365742231158, 'action': [0.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 176.33265392183358, 'action': [1.0, 0.0]}, {'num_count': 722, 'sum_payoffs': 172.87498212039614, 'action': [1.0, 1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 166.32223331460503, 'action': [1.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 147.45753502643404, 'action': [0.0, -1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 146.55195625042398, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16215557181175322, 0.17873689344062424, 0.1760546208241892, 0.17093391855644965, 0.15630334064862228, 0.15557181175323093]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.3137321472168 s
