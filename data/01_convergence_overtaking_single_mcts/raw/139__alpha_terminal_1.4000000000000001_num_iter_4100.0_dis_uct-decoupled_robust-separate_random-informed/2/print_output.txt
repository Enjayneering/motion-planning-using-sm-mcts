Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 438, 'sum_payoffs': 109.0281262574246, 'action': [1.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 116.19191874009395, 'action': [1.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 108.61745692012482, 'action': [0.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 109.60447850404631, 'action': [0.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 119.44358663805122, 'action': [2.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 118.39215924142516, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 118.3935611681919, 'action': [2.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 128.46390958422393, 'action': [2.0, 0.0]}, {'num_count': 434, 'sum_payoffs': 107.61334161820415, 'action': [0.0, 0.0]}])
Weights num count: [0.10680321872713973, 0.1119239209948793, 0.10655937576200927, 0.10729090465740064, 0.1141185076810534, 0.11338697878566203, 0.11338697878566203, 0.12045842477444525, 0.10582784686661789]
Actions to choose Agent 1: dict_values([{'num_count': 663, 'sum_payoffs': 153.7372534799955, 'action': [0.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 148.4520870574889, 'action': [0.0, 1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 164.68808611973856, 'action': [1.0, 1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 151.85307339788196, 'action': [0.0, -1.5707963267948966]}, {'num_count': 693, 'sum_payoffs': 163.02439730218725, 'action': [1.0, -1.5707963267948966]}, {'num_count': 743, 'sum_payoffs': 178.8005228303117, 'action': [1.0, 0.0]}])
Weights num count: [0.1616678858814923, 0.15752255547427457, 0.17020238966105827, 0.16020482809070957, 0.168983174835406, 0.1811753230919288]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.45199751853943 s
