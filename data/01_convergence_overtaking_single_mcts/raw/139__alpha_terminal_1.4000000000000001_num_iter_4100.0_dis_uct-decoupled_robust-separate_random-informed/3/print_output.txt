Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 468, 'sum_payoffs': 119.2909460095655, 'action': [2.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 116.5266352045439, 'action': [2.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 116.83784632349439, 'action': [1.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 124.93153329712415, 'action': [2.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 112.29204395256092, 'action': [1.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 114.07699116500102, 'action': [1.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 109.55327931841974, 'action': [0.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 110.49797557482543, 'action': [0.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 110.58571872741952, 'action': [0.0, 0.0]}])
Weights num count: [0.1141185076810534, 0.11216776396000976, 0.11241160692514021, 0.1180199951231407, 0.10924164837844429, 0.11046086320409657, 0.10729090465740064, 0.10802243355279201, 0.10802243355279201]
Actions to choose Agent 1: dict_values([{'num_count': 642, 'sum_payoffs': 148.51561715701513, 'action': [0.0, -1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 169.23494473469228, 'action': [1.0, -1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 155.65854997973818, 'action': [0.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 153.514920926981, 'action': [0.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 172.61706483830238, 'action': [1.0, 0.0]}, {'num_count': 708, 'sum_payoffs': 169.2498534374651, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15654718361375275, 0.17264081931236283, 0.16215557181175322, 0.16044867105584004, 0.17532309192879786, 0.17264081931236283]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.45272302627563 s
