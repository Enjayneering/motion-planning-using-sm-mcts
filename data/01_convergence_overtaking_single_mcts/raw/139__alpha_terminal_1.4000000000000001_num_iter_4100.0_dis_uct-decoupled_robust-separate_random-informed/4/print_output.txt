Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 453, 'sum_payoffs': 114.06988243994442, 'action': [1.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 116.9519161839875, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 114.12835196453705, 'action': [1.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 108.47543082588977, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 128.20385590613375, 'action': [2.0, 0.0]}, {'num_count': 434, 'sum_payoffs': 107.58836077210621, 'action': [0.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 119.39172134918759, 'action': [2.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 119.66396612823569, 'action': [2.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 106.85898254210221, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11046086320409657, 0.11241160692514021, 0.11046086320409657, 0.10655937576200927, 0.1202145818093148, 0.10582784686661789, 0.1141185076810534, 0.11436235064618386, 0.10534016093635698]
Actions to choose Agent 1: dict_values([{'num_count': 657, 'sum_payoffs': 152.9292293136429, 'action': [0.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 153.77648604096225, 'action': [0.0, 0.0]}, {'num_count': 649, 'sum_payoffs': 150.39966864542362, 'action': [0.0, -1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 164.53475944408999, 'action': [1.0, 1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 176.15254709347386, 'action': [1.0, 0.0]}, {'num_count': 709, 'sum_payoffs': 169.26342020627362, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16020482809070957, 0.16093635698610095, 0.15825408436966593, 0.16922701780053645, 0.17824920751036333, 0.1728846622774933]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.93015646934509 s
