Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 474, 'sum_payoffs': 121.31681056914408, 'action': [2.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 121.32215805104912, 'action': [2.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 106.3106640628197, 'action': [0.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 117.85291323268974, 'action': [2.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 111.22728400410483, 'action': [1.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 112.93896225058094, 'action': [0.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 112.22062892531369, 'action': [0.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 114.41138199093236, 'action': [1.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 116.40690534505075, 'action': [1.0, 0.0]}])
Weights num count: [0.11558156547183614, 0.11558156547183614, 0.10509631797122652, 0.11314313582053158, 0.10851011948305292, 0.1097293343087052, 0.10924164837844429, 0.11070470616922702, 0.11216776396000976]
Actions to choose Agent 1: dict_values([{'num_count': 660, 'sum_payoffs': 154.05723127112987, 'action': [0.0, -1.5707963267948966]}, {'num_count': 642, 'sum_payoffs': 148.36719548854265, 'action': [0.0, 1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 171.62156839208626, 'action': [1.0, 1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 166.01195169181543, 'action': [1.0, -1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 169.7492285973906, 'action': [1.0, 0.0]}, {'num_count': 674, 'sum_payoffs': 158.37459267270458, 'action': [0.0, 0.0]}])
Weights num count: [0.16093635698610095, 0.15654718361375275, 0.17459156303340648, 0.17020238966105827, 0.17312850524262374, 0.16435015849792733]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.49390411376953 s
