Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 427, 'sum_payoffs': 104.96090154653476, 'action': [0.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 123.3528527364526, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 111.50166962259891, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 115.98037656546795, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 109.74949428815117, 'action': [0.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 114.53715443076517, 'action': [1.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 121.95210863444551, 'action': [2.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 115.65878127221296, 'action': [2.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 115.66528782508595, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1041209461107047, 0.11704462326261887, 0.10875396244818337, 0.1119239209948793, 0.1075347476225311, 0.11094854913435748, 0.11606925140209705, 0.11168007802974884, 0.11168007802974884]
Actions to choose Agent 1: dict_values([{'num_count': 652, 'sum_payoffs': 151.20544878101194, 'action': [0.0, -1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 167.22295910082556, 'action': [1.0, -1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 169.14426371498456, 'action': [1.0, 1.5707963267948966]}, {'num_count': 651, 'sum_payoffs': 150.83511614397358, 'action': [0.0, 1.5707963267948966]}, {'num_count': 723, 'sum_payoffs': 173.61127386579102, 'action': [1.0, 0.0]}, {'num_count': 662, 'sum_payoffs': 154.37279989081108, 'action': [0.0, 0.0]}])
Weights num count: [0.1589856132650573, 0.17142160448671057, 0.1728846622774933, 0.15874177029992684, 0.17629846378931968, 0.16142404291636187]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.90443110466003 s
