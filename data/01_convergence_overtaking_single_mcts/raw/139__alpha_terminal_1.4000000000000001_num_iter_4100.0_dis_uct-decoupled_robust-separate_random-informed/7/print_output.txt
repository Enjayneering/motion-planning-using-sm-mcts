Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 429, 'sum_payoffs': 106.03956926244656, 'action': [0.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 121.02758093984794, 'action': [2.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 110.2138211465225, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 112.22395124804062, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 114.61142189534107, 'action': [1.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 119.26867156353566, 'action': [2.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 109.15279290574432, 'action': [0.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 117.81860271565684, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 126.90410581825554, 'action': [2.0, 0.0]}])
Weights num count: [0.10460863204096561, 0.11509387954157523, 0.1075347476225311, 0.10899780541331383, 0.11070470616922702, 0.11387466471592295, 0.10680321872713973, 0.11289929285540112, 0.11923920994879297]
Actions to choose Agent 1: dict_values([{'num_count': 699, 'sum_payoffs': 165.26103252011745, 'action': [1.0, -1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 172.20364278697963, 'action': [1.0, 0.0]}, {'num_count': 661, 'sum_payoffs': 153.38390022296747, 'action': [0.0, -1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 150.89954783579674, 'action': [0.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 171.57934764734654, 'action': [1.0, 1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 149.0151605334377, 'action': [0.0, 0.0]}])
Weights num count: [0.17044623262618874, 0.17581077785905877, 0.1611801999512314, 0.15922945623018775, 0.17532309192879786, 0.15776639843940501]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.43400001525879 s
