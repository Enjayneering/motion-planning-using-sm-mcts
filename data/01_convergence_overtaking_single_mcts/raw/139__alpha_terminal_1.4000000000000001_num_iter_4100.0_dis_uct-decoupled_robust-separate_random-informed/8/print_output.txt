Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 455, 'sum_payoffs': 114.94354656968146, 'action': [0.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 106.64782423698125, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 110.34800829614817, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 130.75305506782712, 'action': [2.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 116.97906175572385, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 114.24461293645494, 'action': [2.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 111.07155998176243, 'action': [1.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 112.54006455890304, 'action': [1.0, 1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 118.8159218617077, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11094854913435748, 0.10509631797122652, 0.10777859058766155, 0.12192148256522799, 0.11241160692514021, 0.11046086320409657, 0.10826627651792246, 0.10924164837844429, 0.11363082175079249]
Actions to choose Agent 1: dict_values([{'num_count': 705, 'sum_payoffs': 167.582048324384, 'action': [1.0, -1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 153.5730518088322, 'action': [0.0, 0.0]}, {'num_count': 717, 'sum_payoffs': 171.44974676449038, 'action': [1.0, 0.0]}, {'num_count': 705, 'sum_payoffs': 167.63216857339745, 'action': [1.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 153.56194643248708, 'action': [0.0, -1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 151.23770911741715, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17190929041697148, 0.16093635698610095, 0.17483540599853695, 0.17190929041697148, 0.16093635698610095, 0.15922945623018775]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 75.67550611495972 s
