Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 461, 'sum_payoffs': 116.67866884494185, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 112.49702503629481, 'action': [2.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 108.99910373984729, 'action': [0.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 108.99764250488677, 'action': [0.0, 1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 122.88821167862253, 'action': [2.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 110.68233952923006, 'action': [0.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 121.18250219828951, 'action': [2.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 116.25815477355594, 'action': [1.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 114.61756737263451, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11241160692514021, 0.10948549134357474, 0.10704706169227018, 0.10704706169227018, 0.11680078029748842, 0.10826627651792246, 0.11558156547183614, 0.11216776396000976, 0.11094854913435748]
Actions to choose Agent 1: dict_values([{'num_count': 650, 'sum_payoffs': 150.898891831189, 'action': [0.0, 1.5707963267948966]}, {'num_count': 723, 'sum_payoffs': 173.81271914002065, 'action': [1.0, 0.0]}, {'num_count': 672, 'sum_payoffs': 157.78440932977284, 'action': [0.0, 0.0]}, {'num_count': 706, 'sum_payoffs': 168.51305544650634, 'action': [1.0, -1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 163.4491459192594, 'action': [1.0, 1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 153.73851403732579, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1584979273347964, 0.17629846378931968, 0.16386247256766642, 0.17215313338210192, 0.16825164594001463, 0.16069251402097048]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.98960590362549 s
