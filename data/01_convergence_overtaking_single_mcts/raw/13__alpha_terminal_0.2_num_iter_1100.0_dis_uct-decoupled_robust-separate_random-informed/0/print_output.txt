Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 127, 'sum_payoffs': 31.031223513539782, 'action': [2.0, -1.5707963267948966]}, {'num_count': 107, 'sum_payoffs': 22.977424327533765, 'action': [0.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 27.314386280616596, 'action': [1.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 28.042645339383686, 'action': [0.0, 0.0]}, {'num_count': 108, 'sum_payoffs': 23.357017139767258, 'action': [0.0, 1.5707963267948966]}, {'num_count': 142, 'sum_payoffs': 37.29765551388602, 'action': [2.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 31.41646567499627, 'action': [2.0, 1.5707963267948966]}, {'num_count': 133, 'sum_payoffs': 33.54717568199376, 'action': [1.0, 0.0]}, {'num_count': 117, 'sum_payoffs': 26.818981809021967, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11534968210717529, 0.0971843778383288, 0.10717529518619437, 0.10899182561307902, 0.09809264305177112, 0.12897366030881016, 0.11625794732061762, 0.12079927338782924, 0.10626702997275204]
Actions to choose Agent 1: dict_values([{'num_count': 189, 'sum_payoffs': 53.90695955476999, 'action': [1.0, -1.5707963267948966]}, {'num_count': 205, 'sum_payoffs': 60.825456826741224, 'action': [1.0, 0.0]}, {'num_count': 164, 'sum_payoffs': 43.555396207713, 'action': [0.0, -1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 44.827368916813015, 'action': [0.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 55.26877864501593, 'action': [1.0, 1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 51.45057904981604, 'action': [0.0, 0.0]}])
Weights num count: [0.17166212534059946, 0.18619436875567666, 0.14895549500454133, 0.1516802906448683, 0.17438692098092642, 0.16621253405994552]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.8818883895874023 s
