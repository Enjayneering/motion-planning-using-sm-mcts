Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 107, 'sum_payoffs': 22.994806940572182, 'action': [0.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.509245372096, 'action': [2.0, -1.5707963267948966]}, {'num_count': 106, 'sum_payoffs': 22.563066289223464, 'action': [0.0, -1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.45123090107307, 'action': [2.0, 1.5707963267948966]}, {'num_count': 142, 'sum_payoffs': 37.264918259339986, 'action': [2.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 27.331768893655, 'action': [1.0, 1.5707963267948966]}, {'num_count': 134, 'sum_payoffs': 33.833083452704436, 'action': [1.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 27.709551017042426, 'action': [0.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 27.389783364677932, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.0971843778383288, 0.11625794732061762, 0.09627611262488647, 0.11625794732061762, 0.12897366030881016, 0.10717529518619437, 0.12170753860127158, 0.1080835603996367, 0.10717529518619437]
Actions to choose Agent 1: dict_values([{'num_count': 192, 'sum_payoffs': 55.158616334877784, 'action': [1.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 45.55743866442619, 'action': [0.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 48.077917554995416, 'action': [0.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 54.82552201253651, 'action': [1.0, 1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 43.613410678735946, 'action': [0.0, 1.5707963267948966]}, {'num_count': 209, 'sum_payoffs': 62.51562623452848, 'action': [1.0, 0.0]}])
Weights num count: [0.17438692098092642, 0.15349682107175294, 0.1589464123524069, 0.1734786557674841, 0.14895549500454133, 0.18982742960944596]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.9271392822265625 s
