Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 120, 'sum_payoffs': 28.09004917369895, 'action': [0.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 31.329552609804214, 'action': [2.0, -1.5707963267948966]}, {'num_count': 129, 'sum_payoffs': 31.72652803507611, 'action': [2.0, 1.5707963267948966]}, {'num_count': 108, 'sum_payoffs': 23.17732437747546, 'action': [0.0, 1.5707963267948966]}, {'num_count': 107, 'sum_payoffs': 22.89051126234172, 'action': [0.0, -1.5707963267948966]}, {'num_count': 140, 'sum_payoffs': 36.42265823005786, 'action': [2.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 27.618582008788735, 'action': [1.0, -1.5707963267948966]}, {'num_count': 132, 'sum_payoffs': 33.072992483814424, 'action': [1.0, 0.0]}, {'num_count': 117, 'sum_payoffs': 26.88264562926788, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10899182561307902, 0.11625794732061762, 0.11716621253405994, 0.09809264305177112, 0.0971843778383288, 0.1271571298819255, 0.1080835603996367, 0.11989100817438691, 0.10626702997275204]
Actions to choose Agent 1: dict_values([{'num_count': 190, 'sum_payoffs': 54.44299588434917, 'action': [1.0, -1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 45.18957911600809, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 54.51839296841048, 'action': [1.0, 1.5707963267948966]}, {'num_count': 207, 'sum_payoffs': 61.72370335551548, 'action': [1.0, 0.0]}, {'num_count': 167, 'sum_payoffs': 44.87365012402056, 'action': [0.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 49.38081682979535, 'action': [0.0, 0.0]}])
Weights num count: [0.17257039055404177, 0.15258855585831063, 0.17257039055404177, 0.1880108991825613, 0.1516802906448683, 0.16167120799273388]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.9085116386413574 s
