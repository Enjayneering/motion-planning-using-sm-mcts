Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 117, 'sum_payoffs': 27.010190552444453, 'action': [1.0, -1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.34693522284262, 'action': [2.0, 1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 26.86526301622946, 'action': [1.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 28.537144466555276, 'action': [0.0, 0.0]}, {'num_count': 132, 'sum_payoffs': 33.07208713939139, 'action': [1.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 31.381700448919442, 'action': [2.0, -1.5707963267948966]}, {'num_count': 107, 'sum_payoffs': 22.884644630434014, 'action': [0.0, -1.5707963267948966]}, {'num_count': 143, 'sum_payoffs': 37.691697623204014, 'action': [2.0, 0.0]}, {'num_count': 107, 'sum_payoffs': 22.89051126234173, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10626702997275204, 0.11625794732061762, 0.10626702997275204, 0.10990009082652134, 0.11989100817438691, 0.11625794732061762, 0.0971843778383288, 0.1298819255222525, 0.0971843778383288]
Actions to choose Agent 1: dict_values([{'num_count': 192, 'sum_payoffs': 55.35569171020795, 'action': [1.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 55.67140341951083, 'action': [1.0, 1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 43.63665992368205, 'action': [0.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 50.21337156679287, 'action': [0.0, 0.0]}, {'num_count': 205, 'sum_payoffs': 60.85457270359505, 'action': [1.0, 0.0]}, {'num_count': 166, 'sum_payoffs': 44.3841122843336, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17438692098092642, 0.17529518619436876, 0.14895549500454133, 0.16348773841961853, 0.18619436875567666, 0.15077202543142598]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.9342491626739502 s
