Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 106, 'sum_payoffs': 22.528301063146653, 'action': [0.0, 1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 26.43938899678848, 'action': [1.0, 1.5707963267948966]}, {'num_count': 143, 'sum_payoffs': 37.576791308265925, 'action': [2.0, 0.0]}, {'num_count': 134, 'sum_payoffs': 33.868754023204325, 'action': [1.0, 0.0]}, {'num_count': 129, 'sum_payoffs': 31.755426629245243, 'action': [2.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 30.93257718453232, 'action': [2.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 27.64950857448869, 'action': [0.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 27.227473215424546, 'action': [1.0, -1.5707963267948966]}, {'num_count': 108, 'sum_payoffs': 23.333767894821136, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.09627611262488647, 0.10535876475930972, 0.1298819255222525, 0.12170753860127158, 0.11716621253405994, 0.11534968210717529, 0.1080835603996367, 0.10717529518619437, 0.09809264305177112]
Actions to choose Agent 1: dict_values([{'num_count': 165, 'sum_payoffs': 43.842209322846735, 'action': [0.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 48.951104149977425, 'action': [0.0, 0.0]}, {'num_count': 164, 'sum_payoffs': 43.48586575555937, 'action': [0.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 55.59893965140329, 'action': [1.0, -1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 54.70384372126762, 'action': [1.0, 1.5707963267948966]}, {'num_count': 210, 'sum_payoffs': 62.79679000043921, 'action': [1.0, 0.0]}])
Weights num count: [0.14986376021798364, 0.16076294277929154, 0.14895549500454133, 0.17529518619436876, 0.1734786557674841, 0.1907356948228883]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.9491634368896484 s
