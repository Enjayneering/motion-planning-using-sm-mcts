Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 503, 'sum_payoffs': 125.45908201956678, 'action': [1.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 127.08807222730945, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 124.72422520189575, 'action': [1.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 135.22839242222554, 'action': [2.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 125.07820808420165, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 122.02455358078481, 'action': [0.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 118.29400974746031, 'action': [0.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 133.9330529588666, 'action': [2.0, 1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 141.8893733013978, 'action': [2.0, 0.0]}])
Weights num count: [0.10932405998695936, 0.11041078026515974, 0.1088893718756792, 0.11562703760052162, 0.10910671593131928, 0.10715061943055858, 0.10475983481851771, 0.11475766137796131, 0.11975657465768311]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 168.4536957169268, 'action': [0.0, 0.0]}, {'num_count': 777, 'sum_payoffs': 181.9468037394542, 'action': [1.0, -1.5707963267948966]}, {'num_count': 805, 'sum_payoffs': 190.59334931978702, 'action': [1.0, 1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 163.209812048756, 'action': [0.0, 1.5707963267948966]}, {'num_count': 825, 'sum_payoffs': 196.72116952854978, 'action': [1.0, 0.0]}, {'num_count': 744, 'sum_payoffs': 171.77679816315825, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15931319278417735, 0.1688763312323408, 0.174961964790263, 0.15561834383829604, 0.17930884590306456, 0.16170397739621822]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 83.37446880340576 s
