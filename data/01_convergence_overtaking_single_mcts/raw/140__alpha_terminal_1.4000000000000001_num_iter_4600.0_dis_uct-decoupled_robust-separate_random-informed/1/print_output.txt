Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 521, 'sum_payoffs': 131.41828469462595, 'action': [1.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 122.23398826781857, 'action': [0.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 132.82124318742817, 'action': [2.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 124.96746728225378, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 126.30685091096022, 'action': [1.0, 1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 115.49799137371319, 'action': [0.0, 1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 142.69165570225212, 'action': [2.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 132.79800196103244, 'action': [2.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 123.97122322826193, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11323625298848077, 0.10736796348619865, 0.11410562921104107, 0.10910671593131928, 0.10997609215387959, 0.10302108237339709, 0.12040860682460335, 0.11410562921104107, 0.10845468376439904]
Actions to choose Agent 1: dict_values([{'num_count': 757, 'sum_payoffs': 176.18956888661825, 'action': [0.0, 0.0]}, {'num_count': 794, 'sum_payoffs': 187.59679713706856, 'action': [1.0, 1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 161.2615417298561, 'action': [0.0, 1.5707963267948966]}, {'num_count': 783, 'sum_payoffs': 184.26146999763264, 'action': [1.0, -1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 172.46820353351976, 'action': [0.0, -1.5707963267948966]}, {'num_count': 813, 'sum_payoffs': 193.5554346840575, 'action': [1.0, 0.0]}])
Weights num count: [0.16452945011953923, 0.17257118017822212, 0.1538795913931754, 0.17018039556618125, 0.16192132145185828, 0.1767007172353836]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 81.63127660751343 s
