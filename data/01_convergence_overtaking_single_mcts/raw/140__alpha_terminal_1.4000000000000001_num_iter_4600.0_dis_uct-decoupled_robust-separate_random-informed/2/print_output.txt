Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 548, 'sum_payoffs': 140.26361586831797, 'action': [2.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 119.82906252145685, 'action': [0.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 122.09072891677651, 'action': [0.0, 1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 127.83872914084625, 'action': [1.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 129.63104907049282, 'action': [1.0, 0.0]}, {'num_count': 518, 'sum_payoffs': 129.97140935856618, 'action': [2.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 121.86479161626906, 'action': [0.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 123.55031241468627, 'action': [1.0, 1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 133.6666294212268, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11910454249076288, 0.10606389915235818, 0.10758530754183873, 0.11128015648772006, 0.11236687676592046, 0.11258422082156053, 0.10736796348619865, 0.10845468376439904, 0.1149750054336014]
Actions to choose Agent 1: dict_values([{'num_count': 777, 'sum_payoffs': 182.74963358933016, 'action': [1.0, -1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 195.4315123806261, 'action': [1.0, 0.0]}, {'num_count': 747, 'sum_payoffs': 173.51477940337722, 'action': [0.0, 0.0]}, {'num_count': 802, 'sum_payoffs': 190.49076942750284, 'action': [1.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 170.09482094289928, 'action': [0.0, 1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 165.15898669389588, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1688763312323408, 0.177787437513584, 0.16235600956313845, 0.17430993262334274, 0.15996522495109758, 0.15648772006085634]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.8355085849762 s
