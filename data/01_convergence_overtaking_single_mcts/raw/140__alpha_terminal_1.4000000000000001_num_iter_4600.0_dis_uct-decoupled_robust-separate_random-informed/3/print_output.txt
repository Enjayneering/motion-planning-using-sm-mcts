Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 511, 'sum_payoffs': 127.81937401520197, 'action': [2.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 136.68165719606517, 'action': [2.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 137.0242881019004, 'action': [2.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 132.55181837610172, 'action': [1.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 115.51719953654232, 'action': [0.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 121.01008193950031, 'action': [0.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 131.87021831861904, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 126.12146571053967, 'action': [1.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 121.98506228463049, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11106281243207998, 0.11671375787872201, 0.1169311019343621, 0.11410562921104107, 0.10323842642903716, 0.10671593131927842, 0.11367094109976092, 0.10997609215387959, 0.10736796348619865]
Actions to choose Agent 1: dict_values([{'num_count': 780, 'sum_payoffs': 184.49444650339453, 'action': [1.0, -1.5707963267948966]}, {'num_count': 755, 'sum_payoffs': 176.84090214647944, 'action': [0.0, 0.0]}, {'num_count': 793, 'sum_payoffs': 188.5806176469985, 'action': [1.0, 1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 164.19297543944828, 'action': [0.0, 1.5707963267948966]}, {'num_count': 825, 'sum_payoffs': 198.5222912041842, 'action': [1.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 170.01119611012894, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16952836339926103, 0.16409476200825907, 0.17235383612258204, 0.15518365572701587, 0.17930884590306456, 0.15931319278417735]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.22232627868652 s
