Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 479, 'sum_payoffs': 117.1680531228055, 'action': [0.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 132.40853652412193, 'action': [2.0, 1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 141.0461668569181, 'action': [2.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 120.45546906779748, 'action': [0.0, 0.0]}, {'num_count': 536, 'sum_payoffs': 136.57214448666537, 'action': [2.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 124.64517461271049, 'action': [0.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 126.00818058134435, 'action': [1.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 124.98065190150454, 'action': [1.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 129.39087381940084, 'action': [1.0, 0.0]}])
Weights num count: [0.10410780265159748, 0.113888285155401, 0.11932188654640295, 0.10628124320799825, 0.11649641382308194, 0.1088893718756792, 0.10975874809823952, 0.10910671593131928, 0.1119321886546403]
Actions to choose Agent 1: dict_values([{'num_count': 735, 'sum_payoffs': 170.01518570276775, 'action': [0.0, -1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 173.95624918723774, 'action': [0.0, 0.0]}, {'num_count': 779, 'sum_payoffs': 183.5687842763072, 'action': [1.0, -1.5707963267948966]}, {'num_count': 791, 'sum_payoffs': 187.2619069501099, 'action': [1.0, 1.5707963267948966]}, {'num_count': 725, 'sum_payoffs': 166.93873189588115, 'action': [0.0, 1.5707963267948966]}, {'num_count': 822, 'sum_payoffs': 196.8892221949094, 'action': [1.0, 0.0]}])
Weights num count: [0.15974788089545752, 0.16257335361877853, 0.16931101934362094, 0.1719191480113019, 0.15757444033905674, 0.1786568137361443]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 84.80434846878052 s
