Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 532, 'sum_payoffs': 135.47021778357242, 'action': [2.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 129.31774525532825, 'action': [1.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 129.98597158557985, 'action': [1.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 120.45361534249007, 'action': [0.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 129.90111935152368, 'action': [2.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 135.1419816840958, 'action': [2.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 122.37810811260591, 'action': [0.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 119.3617691817169, 'action': [0.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 132.67460638104384, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11562703760052162, 0.11171484459900022, 0.11214953271028037, 0.10606389915235818, 0.11214953271028037, 0.11540969354488155, 0.10736796348619865, 0.10541186698543795, 0.113888285155401]
Actions to choose Agent 1: dict_values([{'num_count': 707, 'sum_payoffs': 160.84177097773326, 'action': [0.0, 1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 187.87418362090625, 'action': [1.0, -1.5707963267948966]}, {'num_count': 740, 'sum_payoffs': 170.95496538687104, 'action': [0.0, 0.0]}, {'num_count': 838, 'sum_payoffs': 201.12540256521558, 'action': [1.0, 0.0]}, {'num_count': 734, 'sum_payoffs': 169.02140226329053, 'action': [0.0, -1.5707963267948966]}, {'num_count': 786, 'sum_payoffs': 185.079733246681, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1536622473375353, 0.1727885242338622, 0.1608346011736579, 0.18213431862638557, 0.15953053683981744, 0.1708324277331015]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.61126160621643 s
