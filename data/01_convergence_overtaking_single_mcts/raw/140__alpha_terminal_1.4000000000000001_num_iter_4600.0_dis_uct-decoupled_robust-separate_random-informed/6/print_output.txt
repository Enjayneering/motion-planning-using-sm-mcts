Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 527, 'sum_payoffs': 133.79715707251344, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 120.71797725121303, 'action': [0.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 126.1912156780906, 'action': [1.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 119.11455065412892, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 124.5299701019691, 'action': [1.0, 1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 134.44163378726373, 'action': [2.0, 1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 134.82981587813813, 'action': [2.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 139.2741696425209, 'action': [2.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 122.20721045438451, 'action': [0.0, 0.0]}])
Weights num count: [0.11454031732232123, 0.10628124320799825, 0.10975874809823952, 0.10519452292979788, 0.10867202782003912, 0.1149750054336014, 0.11519234948924147, 0.11801782221256249, 0.10715061943055858]
Actions to choose Agent 1: dict_values([{'num_count': 796, 'sum_payoffs': 188.01727724236974, 'action': [1.0, 1.5707963267948966]}, {'num_count': 752, 'sum_payoffs': 174.50488865595676, 'action': [0.0, 0.0]}, {'num_count': 726, 'sum_payoffs': 166.5079266307092, 'action': [0.0, -1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 164.40140201196272, 'action': [0.0, 1.5707963267948966]}, {'num_count': 786, 'sum_payoffs': 184.9712709875351, 'action': [1.0, -1.5707963267948966]}, {'num_count': 821, 'sum_payoffs': 195.70954513498924, 'action': [1.0, 0.0]}])
Weights num count: [0.1730058682895023, 0.16344272984133884, 0.1577917843946968, 0.15627037600521626, 0.1708324277331015, 0.17843946968050423]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 84.15824460983276 s
