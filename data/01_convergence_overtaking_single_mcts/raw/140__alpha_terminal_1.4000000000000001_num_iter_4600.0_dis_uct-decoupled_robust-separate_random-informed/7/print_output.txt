Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 534, 'sum_payoffs': 136.10294160840468, 'action': [2.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 126.48529443954233, 'action': [1.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 129.80725028668405, 'action': [2.0, 1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 141.20034157368735, 'action': [2.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 118.30802088903425, 'action': [0.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 120.39452718186969, 'action': [0.0, 1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 130.8709006879077, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 122.69963939500634, 'action': [0.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 128.21682813195704, 'action': [1.0, 0.0]}])
Weights num count: [0.11606172571180179, 0.10997609215387959, 0.11214953271028037, 0.11932188654640295, 0.10475983481851771, 0.10606389915235818, 0.11280156487720061, 0.10758530754183873, 0.11106281243207998]
Actions to choose Agent 1: dict_values([{'num_count': 822, 'sum_payoffs': 196.33714423048343, 'action': [1.0, 0.0]}, {'num_count': 793, 'sum_payoffs': 187.34259670522363, 'action': [1.0, -1.5707963267948966]}, {'num_count': 707, 'sum_payoffs': 160.93040105038995, 'action': [0.0, -1.5707963267948966]}, {'num_count': 753, 'sum_payoffs': 175.02287462807615, 'action': [0.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 167.6592191884134, 'action': [0.0, 1.5707963267948966]}, {'num_count': 796, 'sum_payoffs': 188.21664780476578, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1786568137361443, 0.17235383612258204, 0.1536622473375353, 0.16366007389697892, 0.15844381656161705, 0.1730058682895023]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 81.17616581916809 s
