Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 535, 'sum_payoffs': 135.9645346066141, 'action': [2.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 118.92934440110514, 'action': [0.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 129.49269664926513, 'action': [1.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 121.56823416258592, 'action': [0.0, -1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 136.3065403784023, 'action': [2.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 124.34798947353877, 'action': [1.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 130.7794062604587, 'action': [1.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 121.94149874444236, 'action': [0.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 130.79200073245215, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11627906976744186, 0.10541186698543795, 0.11214953271028037, 0.10715061943055858, 0.11649641382308194, 0.1088893718756792, 0.11301890893284068, 0.10736796348619865, 0.11301890893284068]
Actions to choose Agent 1: dict_values([{'num_count': 801, 'sum_payoffs': 190.00547815459856, 'action': [1.0, -1.5707963267948966]}, {'num_count': 743, 'sum_payoffs': 172.11435130413994, 'action': [0.0, 0.0]}, {'num_count': 819, 'sum_payoffs': 195.5729669945449, 'action': [1.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 167.8512197013442, 'action': [0.0, 1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 167.51701185137122, 'action': [0.0, -1.5707963267948966]}, {'num_count': 780, 'sum_payoffs': 183.57954607079745, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17409258856770268, 0.16148663334057814, 0.17800478156922409, 0.15844381656161705, 0.15822647250597696, 0.16952836339926103]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 83.08146667480469 s
