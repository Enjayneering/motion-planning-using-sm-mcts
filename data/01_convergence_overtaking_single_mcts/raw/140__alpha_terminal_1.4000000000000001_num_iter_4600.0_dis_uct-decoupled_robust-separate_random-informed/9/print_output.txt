Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 501, 'sum_payoffs': 124.76316517280058, 'action': [0.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 119.58554175749467, 'action': [0.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 126.83737320083937, 'action': [1.0, 1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 137.3706899207488, 'action': [2.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 115.88849582291124, 'action': [0.0, 1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 131.89505358810737, 'action': [2.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 135.34893560914756, 'action': [2.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 129.87935821209396, 'action': [1.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 132.3145956655268, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1088893718756792, 0.10562921104107803, 0.11019343620951967, 0.1169311019343621, 0.10323842642903716, 0.11345359704412085, 0.11562703760052162, 0.11214953271028037, 0.11367094109976092]
Actions to choose Agent 1: dict_values([{'num_count': 780, 'sum_payoffs': 182.89946562399285, 'action': [1.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 164.1581423512284, 'action': [0.0, 1.5707963267948966]}, {'num_count': 752, 'sum_payoffs': 174.3057636478613, 'action': [0.0, 0.0]}, {'num_count': 835, 'sum_payoffs': 199.9651990584594, 'action': [1.0, 0.0]}, {'num_count': 713, 'sum_payoffs': 162.42672957817268, 'action': [0.0, -1.5707963267948966]}, {'num_count': 801, 'sum_payoffs': 189.36485358199246, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16952836339926103, 0.15627037600521626, 0.16344272984133884, 0.18148228645946535, 0.15496631167137578, 0.17409258856770268]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.35415005683899 s
