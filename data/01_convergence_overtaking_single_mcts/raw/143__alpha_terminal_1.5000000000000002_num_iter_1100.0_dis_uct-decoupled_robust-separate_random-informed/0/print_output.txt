Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 123, 'sum_payoffs': 31.925436271184317, 'action': [2.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 31.438792036442955, 'action': [0.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 31.483222782578544, 'action': [2.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 31.103611148599555, 'action': [0.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 32.38640885411357, 'action': [1.0, 1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 31.823576666867755, 'action': [1.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 31.013978820042933, 'action': [0.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 31.92387739414074, 'action': [1.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 31.097761047302075, 'action': [2.0, 0.0]}])
Weights num count: [0.11171662125340599, 0.11080835603996367, 0.11080835603996367, 0.10990009082652134, 0.11262488646684832, 0.11171662125340599, 0.10990009082652134, 0.11171662125340599, 0.10990009082652134]
Actions to choose Agent 1: dict_values([{'num_count': 178, 'sum_payoffs': 41.89917883764873, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 43.4540157028149, 'action': [1.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 42.66937913168183, 'action': [0.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 43.0965413625687, 'action': [0.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 46.90596586679241, 'action': [1.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 45.594352311167285, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16167120799273388, 0.16530426884650318, 0.16348773841961853, 0.16439600363306087, 0.1734786557674841, 0.17075386012715713]
Selected final action: [1.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 20.836618900299072 s
