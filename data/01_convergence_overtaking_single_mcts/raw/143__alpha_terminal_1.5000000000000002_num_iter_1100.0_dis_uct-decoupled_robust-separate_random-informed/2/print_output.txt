Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 130, 'sum_payoffs': 35.02265210881966, 'action': [2.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 31.500931442198066, 'action': [0.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 30.71593395815663, 'action': [1.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 30.29442181901305, 'action': [1.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 30.776337854215075, 'action': [0.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 30.72835418819857, 'action': [0.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 32.826988419026186, 'action': [2.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 30.750033252951702, 'action': [1.0, -1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 32.413454850764644, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11807447774750227, 0.11080835603996367, 0.10899182561307902, 0.1080835603996367, 0.10899182561307902, 0.10899182561307902, 0.11353315168029064, 0.10899182561307902, 0.11262488646684832]
Actions to choose Agent 1: dict_values([{'num_count': 191, 'sum_payoffs': 47.11716602763227, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 41.50784472619009, 'action': [0.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 42.278362686330155, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.06799557758059, 'action': [0.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 44.85507381044648, 'action': [1.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 46.66580473409335, 'action': [1.0, 0.0]}])
Weights num count: [0.1734786557674841, 0.15985467756584923, 0.16167120799273388, 0.16348773841961853, 0.16802906448683017, 0.17257039055404177]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.755146026611328 s
