Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 184, 'sum_payoffs': 48.78337421296432, 'action': [2.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 48.68572780944411, 'action': [2.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 42.66351699556859, 'action': [0.0, 1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 48.76739728949796, 'action': [1.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 44.70304841489865, 'action': [1.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 44.35643994429442, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 47.9842297901327, 'action': [2.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 46.375969698871856, 'action': [1.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 43.90332253828251, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11492816989381636, 0.11492816989381636, 0.10555902560899438, 0.11492816989381636, 0.10868207370393504, 0.1080574640849469, 0.1136789506558401, 0.11118051217988757, 0.10743285446595878]
Actions to choose Agent 1: dict_values([{'num_count': 270, 'sum_payoffs': 64.97913518269569, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 67.78948579678612, 'action': [1.0, -1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 64.21426937981786, 'action': [1.0, 0.0]}, {'num_count': 268, 'sum_payoffs': 64.17491068680636, 'action': [1.0, 1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 60.68107500799526, 'action': [0.0, 0.0]}, {'num_count': 258, 'sum_payoffs': 60.616803078278515, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16864459712679575, 0.1736414740787008, 0.16739537788881947, 0.16739537788881947, 0.16114928169893816, 0.16114928169893816]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 30.555625677108765 s
