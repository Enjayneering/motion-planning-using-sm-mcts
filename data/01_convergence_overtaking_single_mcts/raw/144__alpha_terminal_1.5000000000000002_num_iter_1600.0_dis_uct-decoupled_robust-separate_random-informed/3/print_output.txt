Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 42.89341873533757, 'action': [0.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 43.30130516499321, 'action': [1.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 45.575572378553346, 'action': [1.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 47.312149125379726, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 45.30558261794193, 'action': [0.0, 1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 48.510867988609554, 'action': [2.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 44.386875079429196, 'action': [0.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 48.90106852043962, 'action': [2.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 47.66966005328773, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1061836352279825, 0.10680824484697064, 0.11055590256089944, 0.11305434103685197, 0.1099312929419113, 0.11492816989381636, 0.10868207370393504, 0.1155527795128045, 0.1136789506558401]
Actions to choose Agent 1: dict_values([{'num_count': 281, 'sum_payoffs': 69.7137223371002, 'action': [1.0, 0.0]}, {'num_count': 262, 'sum_payoffs': 62.850372320471436, 'action': [0.0, 1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 65.28541014535026, 'action': [1.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 58.1736298034347, 'action': [0.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 67.87589545627996, 'action': [1.0, 1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 63.223745359403836, 'action': [0.0, 0.0]}])
Weights num count: [0.1755153029356652, 0.16364772017489068, 0.1680199875078076, 0.15552779512804496, 0.17239225484072454, 0.16427232979387882]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 30.381546020507812 s
