Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 178, 'sum_payoffs': 46.03945741577715, 'action': [1.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 46.355385145655745, 'action': [2.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 46.03387497327656, 'action': [1.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 43.62622378926589, 'action': [0.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 44.44260442666544, 'action': [0.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 46.829677252186045, 'action': [2.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 47.61130363064574, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 45.24575389017933, 'action': [0.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 47.028713084658825, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11118051217988757, 0.11180512179887571, 0.11118051217988757, 0.10743285446595878, 0.10868207370393504, 0.11242973141786383, 0.1136789506558401, 0.1099312929419113, 0.11305434103685197]
Actions to choose Agent 1: dict_values([{'num_count': 271, 'sum_payoffs': 65.89225556216198, 'action': [1.0, 0.0]}, {'num_count': 268, 'sum_payoffs': 64.82760393540293, 'action': [1.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 58.71385264752516, 'action': [0.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 58.67688485709, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 71.50462954382452, 'action': [1.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 66.24915117497208, 'action': [0.0, 0.0]}])
Weights num count: [0.16926920674578388, 0.16739537788881947, 0.15677701436602123, 0.15677701436602123, 0.17926296064959402, 0.16989381636477202]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 30.448136568069458 s
