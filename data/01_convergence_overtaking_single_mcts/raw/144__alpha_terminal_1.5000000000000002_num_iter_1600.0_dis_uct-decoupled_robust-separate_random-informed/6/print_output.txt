Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 169, 'sum_payoffs': 42.25115273030669, 'action': [0.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 45.551459151006135, 'action': [1.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 46.3492211772158, 'action': [2.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 48.771468231793115, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 43.12801075545739, 'action': [0.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 45.877002997525764, 'action': [2.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 46.689166119679314, 'action': [1.0, 0.0]}, {'num_count': 186, 'sum_payoffs': 49.15465776784893, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 44.68299046888779, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10555902560899438, 0.11055590256089944, 0.11180512179887571, 0.1155527795128045, 0.10680824484697064, 0.11118051217988757, 0.11242973141786383, 0.11617738913179262, 0.10930668332292318]
Actions to choose Agent 1: dict_values([{'num_count': 257, 'sum_payoffs': 61.00970008515838, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 68.51942369419973, 'action': [1.0, -1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 62.15656431621273, 'action': [0.0, 1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 59.653439265725645, 'action': [0.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 69.76734839193797, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 66.12135817092597, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16052467207995003, 0.1736414740787008, 0.16239850093691444, 0.1580262336039975, 0.1755153029356652, 0.16926920674578388]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 30.412946701049805 s
