Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 246, 'sum_payoffs': 65.195998329427, 'action': [2.0, -1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 65.95167119194605, 'action': [2.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 58.96047126735407, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 62.46403531535737, 'action': [1.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 55.55314595889651, 'action': [0.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.02984001512221, 'action': [1.0, 1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 60.55144248725972, 'action': [2.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 58.262969237265104, 'action': [0.0, 0.0]}, {'num_count': 224, 'sum_payoffs': 56.76603087052712, 'action': [1.0, 0.0]}])
Weights num count: [0.1170871013802951, 0.11803902903379343, 0.10947168015230842, 0.11375535459305093, 0.10518800571156592, 0.10947168015230842, 0.11137553545930509, 0.10851975249881009, 0.10661589719181343]
Actions to choose Agent 1: dict_values([{'num_count': 358, 'sum_payoffs': 87.37236913745058, 'action': [1.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 90.85940649984548, 'action': [1.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 82.04387434950173, 'action': [0.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 80.70629994316322, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 87.3232720916091, 'action': [1.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 79.0146315630293, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1703950499762018, 0.17515468824369348, 0.1632555925749643, 0.16135173726796764, 0.1703950499762018, 0.1589719181342218]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.84797811508179 s
