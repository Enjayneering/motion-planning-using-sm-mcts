Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 244, 'sum_payoffs': 64.59129231959363, 'action': [2.0, 0.0]}, {'num_count': 221, 'sum_payoffs': 55.68102057603337, 'action': [0.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 66.14061710874024, 'action': [2.0, -1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 56.06501812899164, 'action': [0.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 62.58115743959109, 'action': [2.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 57.89614316882573, 'action': [1.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 58.289291648902555, 'action': [0.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 61.47701531814917, 'action': [1.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 61.06609405046153, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11613517372679677, 0.10518800571156592, 0.11803902903379343, 0.10566396953831508, 0.11375535459305093, 0.10804378867206092, 0.10851975249881009, 0.11232746311280342, 0.11185149928605426]
Actions to choose Agent 1: dict_values([{'num_count': 360, 'sum_payoffs': 87.36569569835842, 'action': [1.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 88.06168006498528, 'action': [1.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 90.81645867415745, 'action': [1.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 76.68994040261869, 'action': [0.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 78.8015738179938, 'action': [0.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 81.9030679068827, 'action': [0.0, 0.0]}])
Weights num count: [0.17134697762970014, 0.17229890528319847, 0.17610661589719181, 0.15659209900047596, 0.15944788196097096, 0.16373155640171347]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.11963391304016 s
