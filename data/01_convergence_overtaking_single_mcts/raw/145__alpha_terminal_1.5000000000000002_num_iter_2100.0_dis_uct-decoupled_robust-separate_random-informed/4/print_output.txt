Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 232, 'sum_payoffs': 59.833819724220945, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 62.464046465683815, 'action': [1.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 64.08566496819728, 'action': [2.0, 1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 56.3775429323706, 'action': [0.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 55.95146339368263, 'action': [0.0, 0.0]}, {'num_count': 227, 'sum_payoffs': 57.9016576816533, 'action': [0.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 64.45845060002036, 'action': [2.0, -1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 60.579757050311095, 'action': [1.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 61.328994605122915, 'action': [2.0, 0.0]}])
Weights num count: [0.11042360780580676, 0.11375535459305093, 0.11565920990004759, 0.10613993336506425, 0.10566396953831508, 0.10804378867206092, 0.11613517372679677, 0.11137553545930509, 0.11232746311280342]
Actions to choose Agent 1: dict_values([{'num_count': 335, 'sum_payoffs': 79.36391720880044, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 87.99652787754472, 'action': [1.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 88.3087961104118, 'action': [1.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 80.72270836326356, 'action': [0.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 89.64156669132473, 'action': [1.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 81.09521423921255, 'action': [0.0, 0.0]}])
Weights num count: [0.15944788196097096, 0.17134697762970014, 0.1718229414564493, 0.16135173726796764, 0.173726796763446, 0.1618277010947168]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.24563646316528 s
