Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 251, 'sum_payoffs': 66.8909589007697, 'action': [2.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 58.41398964002074, 'action': [1.0, 1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 64.94860690998063, 'action': [2.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 64.54669176097403, 'action': [1.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 58.416748267090725, 'action': [1.0, 0.0]}, {'num_count': 227, 'sum_payoffs': 57.62727166337981, 'action': [0.0, -1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 53.43251211274879, 'action': [0.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 56.47347841392127, 'action': [0.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 59.88022552260598, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11946692051404094, 0.10899571632555925, 0.1170871013802951, 0.11661113755354593, 0.10899571632555925, 0.10804378867206092, 0.10280818657782008, 0.10661589719181343, 0.11089957163255593]
Actions to choose Agent 1: dict_values([{'num_count': 332, 'sum_payoffs': 78.41466240690292, 'action': [0.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 80.88733213297398, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 91.5832301245256, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 88.07613143400064, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 80.18848295876751, 'action': [0.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 88.75879886295641, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15801999048072346, 0.16135173726796764, 0.17610661589719181, 0.17134697762970014, 0.1603998096144693, 0.17229890528319847]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.93994426727295 s
