Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 246, 'sum_payoffs': 65.3556178833426, 'action': [2.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 61.06647798994996, 'action': [2.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 59.49139462740109, 'action': [1.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 56.86144270077099, 'action': [0.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 60.21257232069436, 'action': [1.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 56.43868970771696, 'action': [0.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 58.75961994931056, 'action': [1.0, 0.0]}, {'num_count': 244, 'sum_payoffs': 64.5635345335458, 'action': [2.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 61.122262742233836, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1170871013802951, 0.11185149928605426, 0.1099476439790576, 0.10661589719181343, 0.11089957163255593, 0.10613993336506425, 0.10899571632555925, 0.11613517372679677, 0.11185149928605426]
Actions to choose Agent 1: dict_values([{'num_count': 333, 'sum_payoffs': 78.6639652930873, 'action': [0.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 79.38363673619142, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 91.51295426609433, 'action': [1.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 85.24591104117245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 78.36212272098207, 'action': [0.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 94.32694934589709, 'action': [1.0, 0.0]}])
Weights num count: [0.15849595430747263, 0.15944788196097096, 0.17610661589719181, 0.16753926701570682, 0.15801999048072346, 0.17991432651118516]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.018675088882446 s
