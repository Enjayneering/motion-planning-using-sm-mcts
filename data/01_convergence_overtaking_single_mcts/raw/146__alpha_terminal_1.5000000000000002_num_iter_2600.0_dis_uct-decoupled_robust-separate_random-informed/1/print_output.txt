Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 303, 'sum_payoffs': 80.06829331791981, 'action': [1.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 73.63384951840531, 'action': [1.0, 0.0]}, {'num_count': 305, 'sum_payoffs': 80.76709168893179, 'action': [2.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 69.8547130186224, 'action': [0.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 75.8525997336962, 'action': [0.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 70.19445364932201, 'action': [0.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 75.13916469223295, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.27017689598024, 'action': [1.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 73.63300920251308, 'action': [2.0, 0.0]}])
Weights num count: [0.11649365628604383, 0.10995770857362552, 0.11726259131103421, 0.1061130334486736, 0.1122645136485967, 0.10649750096116878, 0.1114955786236063, 0.10957324106113034, 0.10995770857362552]
Actions to choose Agent 1: dict_values([{'num_count': 451, 'sum_payoffs': 110.04280461828908, 'action': [1.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 114.50670766241392, 'action': [1.0, -1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 99.69705532829977, 'action': [0.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 108.4176507701934, 'action': [1.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 96.98199715912632, 'action': [0.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 95.38809209583937, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17339484813533257, 0.17839292579777008, 0.16147635524798154, 0.1714725105728566, 0.15840061514801998, 0.15647827758554403]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.45534038543701 s
