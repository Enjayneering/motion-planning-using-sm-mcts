Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 296, 'sum_payoffs': 77.56325563230926, 'action': [2.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 75.98588754056873, 'action': [1.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 71.43001926019316, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 78.59237620538387, 'action': [2.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 74.87531984950624, 'action': [1.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 79.81698914351216, 'action': [2.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 71.09039433944095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 74.82818029774326, 'action': [1.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 69.20029507672054, 'action': [0.0, 0.0]}])
Weights num count: [0.11380238369857747, 0.1122645136485967, 0.10765090349865436, 0.11495578623606305, 0.1111111111111111, 0.11610918877354863, 0.10726643598615918, 0.1111111111111111, 0.1053440984236832]
Actions to choose Agent 1: dict_values([{'num_count': 402, 'sum_payoffs': 93.7931283882068, 'action': [0.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 111.30666036355676, 'action': [1.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 99.12127359211338, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 108.5023578996925, 'action': [1.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 113.97057974131457, 'action': [1.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 99.17621250077381, 'action': [0.0, 0.0]}])
Weights num count: [0.15455594002306805, 0.17454825067281815, 0.16070742022299117, 0.1714725105728566, 0.1776239907727797, 0.16070742022299117]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.58827304840088 s
