Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 275, 'sum_payoffs': 69.29149695431818, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 73.32615471697069, 'action': [1.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 83.53503540055378, 'action': [2.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 75.97135082411465, 'action': [2.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 68.50486568318709, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 74.51662937369017, 'action': [0.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 77.08569668141006, 'action': [2.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 74.90980568162286, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.04689991440557, 'action': [1.0, 0.0]}])
Weights num count: [0.1057285659361784, 0.10995770857362552, 0.12033833141099577, 0.11264898116109189, 0.104959630911188, 0.1111111111111111, 0.11380238369857747, 0.1114955786236063, 0.10957324106113034]
Actions to choose Agent 1: dict_values([{'num_count': 447, 'sum_payoffs': 108.87905245195954, 'action': [1.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 103.91036838442122, 'action': [0.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 107.17675874400202, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 101.05453519141955, 'action': [0.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 111.29713123802439, 'action': [1.0, 1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 93.49339392180909, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17185697808535177, 0.16608996539792387, 0.16993464052287582, 0.16301422529796233, 0.17454825067281815, 0.15417147251057287]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.78239917755127 s
