Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 297, 'sum_payoffs': 77.48194710919189, 'action': [2.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 72.93212320292122, 'action': [0.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 75.6342468100549, 'action': [2.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 70.66878366170633, 'action': [0.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 75.952778093148, 'action': [1.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 70.35446346924435, 'action': [0.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 75.81600919926977, 'action': [2.0, 1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 77.52686080903447, 'action': [1.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 73.32155497891475, 'action': [1.0, 0.0]}])
Weights num count: [0.11418685121107267, 0.10957324106113034, 0.1122645136485967, 0.10726643598615918, 0.11264898116109189, 0.10688196847366398, 0.11264898116109189, 0.11418685121107267, 0.10995770857362552]
Actions to choose Agent 1: dict_values([{'num_count': 462, 'sum_payoffs': 113.81149593053814, 'action': [1.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 96.99770188393596, 'action': [0.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 109.76619082362947, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 96.74262819033999, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 100.02524819373204, 'action': [0.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 107.76960620278017, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1776239907727797, 0.15840061514801998, 0.17301038062283736, 0.1580161476355248, 0.16186082276047675, 0.1707035755478662]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.77173447608948 s
