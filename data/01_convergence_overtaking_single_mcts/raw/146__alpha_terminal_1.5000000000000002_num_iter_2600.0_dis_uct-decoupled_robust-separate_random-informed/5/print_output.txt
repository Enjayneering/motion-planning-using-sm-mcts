Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 283, 'sum_payoffs': 72.32454192799906, 'action': [0.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 76.82103633777885, 'action': [2.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 76.09039766147588, 'action': [0.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 74.5784052752659, 'action': [1.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 75.37870859840866, 'action': [1.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 68.98510025061846, 'action': [0.0, 0.0]}, {'num_count': 299, 'sum_payoffs': 78.32646321097485, 'action': [2.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 77.25940513105985, 'action': [2.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 71.13284907025337, 'action': [1.0, 0.0]}])
Weights num count: [0.10880430603613994, 0.11341791618608228, 0.11264898116109189, 0.1111111111111111, 0.1118800461361015, 0.1053440984236832, 0.11495578623606305, 0.11380238369857747, 0.10765090349865436]
Actions to choose Agent 1: dict_values([{'num_count': 406, 'sum_payoffs': 94.85356274701233, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 111.88395115221732, 'action': [1.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 98.43661982231741, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 111.51008066377118, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 107.51646152144161, 'action': [1.0, 1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 99.49267687424849, 'action': [0.0, 0.0]}])
Weights num count: [0.15609381007304882, 0.17570165321030373, 0.16032295271049596, 0.17531718569780855, 0.1707035755478662, 0.16147635524798154]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.88588809967041 s
