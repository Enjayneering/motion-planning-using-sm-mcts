Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 280, 'sum_payoffs': 71.10893922120933, 'action': [0.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 71.1557688504997, 'action': [0.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 69.62988848901185, 'action': [1.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 77.09553444325093, 'action': [2.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 79.32830059391104, 'action': [2.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 74.53375751532442, 'action': [1.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 78.68359807431148, 'action': [2.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 71.54501750408743, 'action': [0.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 77.1095581801517, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10765090349865436, 0.10765090349865436, 0.1061130334486736, 0.11380238369857747, 0.11610918877354863, 0.1111111111111111, 0.11534025374855825, 0.10803537101114956, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 456, 'sum_payoffs': 112.1565255641925, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 98.32598943766111, 'action': [0.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 95.69497828229068, 'action': [0.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 113.84409808315759, 'action': [1.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 97.69975192531645, 'action': [0.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 109.49928053613168, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17531718569780855, 0.15955401768550556, 0.15647827758554403, 0.1772395232602845, 0.1587850826605152, 0.172241445597847]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.945212841033936 s
