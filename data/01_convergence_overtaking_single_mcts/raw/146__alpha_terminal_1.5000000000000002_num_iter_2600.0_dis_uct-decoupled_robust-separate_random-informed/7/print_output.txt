Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 282, 'sum_payoffs': 71.61735353031548, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 73.11897722702568, 'action': [1.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 72.37399707570385, 'action': [0.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 74.54228656789218, 'action': [2.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 74.1102119238702, 'action': [1.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 75.73321990998834, 'action': [1.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 77.56443741082516, 'action': [2.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 69.3843837674697, 'action': [0.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 79.06252839896254, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10841983852364476, 0.10995770857362552, 0.10918877354863514, 0.1114955786236063, 0.1111111111111111, 0.11264898116109189, 0.11457131872356786, 0.1061130334486736, 0.11610918877354863]
Actions to choose Agent 1: dict_values([{'num_count': 454, 'sum_payoffs': 111.5982865608356, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 112.36966314993523, 'action': [1.0, -1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 98.5012361740293, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 97.22018030362126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 101.41323149390085, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 106.85382514869853, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17454825067281815, 0.17531718569780855, 0.15955401768550556, 0.1580161476355248, 0.16301422529796233, 0.16916570549788543]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.61711263656616 s
