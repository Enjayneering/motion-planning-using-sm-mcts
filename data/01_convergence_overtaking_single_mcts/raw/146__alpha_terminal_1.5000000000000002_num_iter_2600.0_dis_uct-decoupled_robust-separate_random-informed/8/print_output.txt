Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 276, 'sum_payoffs': 69.8354476158894, 'action': [0.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 74.19187561865188, 'action': [1.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 75.04565032659174, 'action': [1.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 75.79903994249075, 'action': [2.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 78.7939922022382, 'action': [2.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.18373359095631, 'action': [0.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 69.79385573578027, 'action': [0.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 80.28946140259428, 'action': [2.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 74.62715066882636, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1061130334486736, 0.11072664359861592, 0.1114955786236063, 0.1122645136485967, 0.11534025374855825, 0.10957324106113034, 0.1061130334486736, 0.11687812379853903, 0.1111111111111111]
Actions to choose Agent 1: dict_values([{'num_count': 415, 'sum_payoffs': 97.80795104336634, 'action': [0.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.51253969943575, 'action': [1.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 111.44350854804229, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 94.39915707087107, 'action': [0.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 112.53253554813742, 'action': [1.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 101.83711242659359, 'action': [0.0, 0.0]}])
Weights num count: [0.15955401768550556, 0.16839677047289503, 0.17531718569780855, 0.15570934256055363, 0.17647058823529413, 0.16416762783544792]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.935651540756226 s
