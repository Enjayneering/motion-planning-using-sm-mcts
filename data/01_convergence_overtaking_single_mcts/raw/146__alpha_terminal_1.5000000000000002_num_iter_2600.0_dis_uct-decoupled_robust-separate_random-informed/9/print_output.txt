Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 281, 'sum_payoffs': 71.68909813126568, 'action': [0.0, -1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 76.8452415013252, 'action': [1.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 78.02851211246742, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 72.41249706665664, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.11228584742142, 'action': [0.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 72.04046178808879, 'action': [0.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 78.75616449595886, 'action': [2.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 76.52119624057234, 'action': [2.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 72.06125172569759, 'action': [1.0, 0.0]}])
Weights num count: [0.10803537101114956, 0.11341791618608228, 0.11457131872356786, 0.10880430603613994, 0.10957324106113034, 0.10841983852364476, 0.11534025374855825, 0.11303344867358708, 0.10841983852364476]
Actions to choose Agent 1: dict_values([{'num_count': 417, 'sum_payoffs': 98.60957620072433, 'action': [0.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 107.69992857853515, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 107.70762188908587, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 115.79861695284004, 'action': [1.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 95.70425154938562, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 99.31041654275633, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16032295271049596, 0.1707035755478662, 0.1707035755478662, 0.17993079584775087, 0.1568627450980392, 0.16109188773548636]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.959033727645874 s
