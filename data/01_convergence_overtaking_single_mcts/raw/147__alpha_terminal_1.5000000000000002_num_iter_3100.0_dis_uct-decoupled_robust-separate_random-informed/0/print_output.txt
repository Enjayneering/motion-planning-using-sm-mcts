Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 347, 'sum_payoffs': 89.19274661022916, 'action': [2.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 86.18287061904014, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 86.9924009102414, 'action': [1.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 94.6076680805787, 'action': [2.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 82.23827026792074, 'action': [0.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 86.29118368891777, 'action': [1.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 88.82174828135855, 'action': [2.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 85.90175936687709, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 93.84625886498304, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11189938729442116, 0.10931957433086101, 0.10996452757175104, 0.11673653660109642, 0.10577233150596582, 0.10931957433086101, 0.11157691067397614, 0.10899709771041599, 0.11609158336020639]
Actions to choose Agent 1: dict_values([{'num_count': 548, 'sum_payoffs': 134.60064870973432, 'action': [1.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 129.2924073584242, 'action': [1.0, 1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 131.25115562863692, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 115.42721012998074, 'action': [0.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 115.15344968556325, 'action': [0.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 119.73542968769763, 'action': [0.0, 0.0]}])
Weights num count: [0.1767171880038697, 0.17155756207674944, 0.17349242179941954, 0.1580135440180587, 0.15769106739761368, 0.16220574008384392]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.2611129283905 s
