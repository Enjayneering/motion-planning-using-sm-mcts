Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 347, 'sum_payoffs': 89.39163822595596, 'action': [1.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 85.30982953517187, 'action': [0.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 83.53142972395543, 'action': [0.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 90.77533928249956, 'action': [1.0, 0.0]}, {'num_count': 324, 'sum_payoffs': 81.04618097056814, 'action': [0.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 91.62126664487973, 'action': [2.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 94.83777277621553, 'action': [2.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 93.0564393739363, 'action': [2.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 86.41465447692812, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11189938729442116, 0.10835214446952596, 0.10673976136730087, 0.11318929377620122, 0.10448242502418574, 0.11383424701709126, 0.11673653660109642, 0.11512415349887133, 0.10931957433086101]
Actions to choose Agent 1: dict_values([{'num_count': 500, 'sum_payoffs': 118.17059317985364, 'action': [0.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 128.3030250613255, 'action': [1.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 126.92703335089645, 'action': [1.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 115.90832970583868, 'action': [0.0, 1.5707963267948966]}, {'num_count': 557, 'sum_payoffs': 136.80793798777063, 'action': [1.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 115.56784562664178, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16123831022250887, 0.17123508545630442, 0.16994517897452435, 0.15898097387939375, 0.1796194775878749, 0.15865849725894873]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.91655468940735 s
