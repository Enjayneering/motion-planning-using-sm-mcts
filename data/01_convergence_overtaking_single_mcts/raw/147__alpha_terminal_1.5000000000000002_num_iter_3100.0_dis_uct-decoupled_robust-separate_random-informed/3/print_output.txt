Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 341, 'sum_payoffs': 87.31067729518526, 'action': [0.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 94.62958431084786, 'action': [2.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 85.8712081981244, 'action': [1.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 92.40686633540503, 'action': [1.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 91.63180716135189, 'action': [2.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 97.19074305072948, 'action': [2.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 79.33807260588503, 'action': [0.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 81.87073556069518, 'action': [0.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 86.83403175432684, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10996452757175104, 0.11641405998065141, 0.10867462108997097, 0.11447920025798129, 0.11383424701709126, 0.11867139632376653, 0.10287004192196066, 0.10512737826507579, 0.10964205095130602]
Actions to choose Agent 1: dict_values([{'num_count': 480, 'sum_payoffs': 111.19412772237204, 'action': [0.0, -1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 132.67004509576438, 'action': [1.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 119.28878325178471, 'action': [0.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 131.39421414512285, 'action': [1.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 126.7434643367443, 'action': [1.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 117.30273726965373, 'action': [0.0, 0.0]}])
Weights num count: [0.1547887778136085, 0.17607223476297967, 0.16285069332473395, 0.1747823282811996, 0.17026765559496937, 0.16091583360206385]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.57932710647583 s
