Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 340, 'sum_payoffs': 86.95027019408093, 'action': [1.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 82.97527433853058, 'action': [0.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 93.56601393254347, 'action': [2.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 92.03842604288837, 'action': [2.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 84.87111117570545, 'action': [0.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 90.31660636440003, 'action': [1.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 88.50705819794132, 'action': [1.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 84.14658780780398, 'action': [0.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 94.39300209248063, 'action': [2.0, 0.0]}])
Weights num count: [0.10964205095130602, 0.10609480812641084, 0.11544663011931634, 0.11415672363753628, 0.10770719122863592, 0.11254434053531119, 0.1109319574330861, 0.10706223798774589, 0.11609158336020639]
Actions to choose Agent 1: dict_values([{'num_count': 530, 'sum_payoffs': 128.16241686724072, 'action': [1.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 131.4083464088826, 'action': [1.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 123.54923717757109, 'action': [0.0, 0.0]}, {'num_count': 535, 'sum_payoffs': 129.81393477735043, 'action': [1.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 114.416395187442, 'action': [0.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 115.3928727713299, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1709126088358594, 0.17413737504030957, 0.16639793614962914, 0.1725249919380845, 0.15736859077716867, 0.15833602063850372]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.30773591995239 s
