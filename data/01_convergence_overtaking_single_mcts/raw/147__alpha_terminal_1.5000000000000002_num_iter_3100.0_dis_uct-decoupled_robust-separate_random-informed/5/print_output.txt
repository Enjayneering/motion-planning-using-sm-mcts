Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 374, 'sum_payoffs': 99.10856486730809, 'action': [2.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 84.14496928525637, 'action': [0.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 83.62557753581198, 'action': [0.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 88.40401559640485, 'action': [1.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 85.87154693228746, 'action': [1.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 84.82027286088984, 'action': [1.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 93.17938072097357, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 84.84946211272478, 'action': [0.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 90.33866012491087, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12060625604643663, 0.1073847146081909, 0.10706223798774589, 0.11125443405353112, 0.10899709771041599, 0.10802966784908094, 0.11544663011931634, 0.10802966784908094, 0.11286681715575621]
Actions to choose Agent 1: dict_values([{'num_count': 502, 'sum_payoffs': 119.07692391285042, 'action': [0.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 131.53419891637913, 'action': [1.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 114.44953460612112, 'action': [0.0, -1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 128.9592112542854, 'action': [1.0, -1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 129.84271687689926, 'action': [1.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 119.38130990224755, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1618832634633989, 0.17413737504030957, 0.15736859077716867, 0.17155756207674944, 0.1725249919380845, 0.16220574008384392]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.75481152534485 s
