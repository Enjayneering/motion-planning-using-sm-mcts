Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 324, 'sum_payoffs': 80.86114557017001, 'action': [0.0, 1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 84.11958309120493, 'action': [1.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 96.11629369357105, 'action': [2.0, 0.0]}, {'num_count': 363, 'sum_payoffs': 95.08122502613719, 'action': [2.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 84.11999073242991, 'action': [0.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 88.89227072217352, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 88.50233637157112, 'action': [1.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 84.88115873387125, 'action': [0.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 92.06117329960097, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10448242502418574, 0.1073847146081909, 0.11802644308287649, 0.11705901322154144, 0.1073847146081909, 0.11157691067397614, 0.11125443405353112, 0.10802966784908094, 0.11447920025798129]
Actions to choose Agent 1: dict_values([{'num_count': 497, 'sum_payoffs': 117.13401384045478, 'action': [0.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 130.60012687090403, 'action': [1.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 131.9058636060016, 'action': [1.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 130.54909216403536, 'action': [1.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 114.47470786759834, 'action': [0.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 116.73925654414903, 'action': [0.0, 0.0]}])
Weights num count: [0.16027088036117382, 0.17349242179941954, 0.1747823282811996, 0.17349242179941954, 0.15769106739761368, 0.1599484037407288]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.63421130180359 s
