Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 357, 'sum_payoffs': 92.7300644509861, 'action': [2.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 86.92717270793189, 'action': [0.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 83.51594777612775, 'action': [0.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 88.28388203618141, 'action': [1.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 92.24990681278815, 'action': [2.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 88.39615406404413, 'action': [2.0, 1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 81.0747730585807, 'action': [0.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 88.39900406552111, 'action': [1.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 91.66318127080419, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11512415349887133, 0.10996452757175104, 0.10706223798774589, 0.11125443405353112, 0.11480167687842631, 0.11125443405353112, 0.10480490164463076, 0.11125443405353112, 0.11415672363753628]
Actions to choose Agent 1: dict_values([{'num_count': 555, 'sum_payoffs': 136.37430702886726, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 119.0139075388302, 'action': [0.0, 0.0]}, {'num_count': 536, 'sum_payoffs': 130.08037845325867, 'action': [1.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 111.23733531981385, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 129.7878561474776, 'action': [1.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 116.36872738798459, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17897452434698485, 0.1618832634633989, 0.1728474685585295, 0.15414382457271847, 0.1725249919380845, 0.15930345049983877]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.729828119277954 s
