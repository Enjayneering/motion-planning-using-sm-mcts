Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 333, 'sum_payoffs': 84.73362997485985, 'action': [1.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 92.74419194195909, 'action': [2.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 94.96540522609749, 'action': [2.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 86.8242700737742, 'action': [0.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 90.42605824636902, 'action': [1.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 91.2760152660868, 'action': [2.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 84.29090419610974, 'action': [0.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 87.9143543710742, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 86.47105602724483, 'action': [0.0, 0.0]}])
Weights num count: [0.1073847146081909, 0.11447920025798129, 0.11641405998065141, 0.10931957433086101, 0.11254434053531119, 0.11318929377620122, 0.10706223798774589, 0.11028700419219607, 0.10899709771041599]
Actions to choose Agent 1: dict_values([{'num_count': 546, 'sum_payoffs': 132.73592358147573, 'action': [1.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 113.08369619965146, 'action': [0.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 119.54616486935778, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 117.66011877017277, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 126.77941450102746, 'action': [1.0, -1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 128.78180181019133, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17607223476297967, 0.15672363753627863, 0.16317316994517897, 0.16123831022250887, 0.17026765559496937, 0.17220251531763947]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.937586069107056 s
