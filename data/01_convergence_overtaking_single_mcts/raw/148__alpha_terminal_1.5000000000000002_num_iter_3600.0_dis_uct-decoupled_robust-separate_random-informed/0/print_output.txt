Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 396, 'sum_payoffs': 100.24792461034302, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 110.32429280655343, 'action': [2.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 100.97504839330433, 'action': [1.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 104.22613681523585, 'action': [2.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 100.6896182633549, 'action': [1.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 92.48896554092354, 'action': [0.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 109.98131623033036, 'action': [2.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 101.0244473738416, 'action': [0.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 95.62449532282814, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10996945292974174, 0.11774507081366287, 0.11052485420716468, 0.1130241599555679, 0.1102471535684532, 0.10386003887808942, 0.1174673701749514, 0.11052485420716468, 0.10635934462649264]
Actions to choose Agent 1: dict_values([{'num_count': 611, 'sum_payoffs': 146.25059287439933, 'action': [1.0, 1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 149.47957155485588, 'action': [1.0, 0.0]}, {'num_count': 572, 'sum_payoffs': 133.81690640993952, 'action': [0.0, -1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 152.01605935923843, 'action': [1.0, -1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 135.04961522012644, 'action': [0.0, 1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 139.84254357897865, 'action': [0.0, 0.0]}])
Weights num count: [0.16967509025270758, 0.17245209663982228, 0.1588447653429603, 0.17467370174951402, 0.15995556789780616, 0.1641210774784782]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.16813206672668 s
