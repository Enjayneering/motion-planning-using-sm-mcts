Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 387, 'sum_payoffs': 97.3392727321399, 'action': [0.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 102.29264944908094, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 107.0323027072867, 'action': [2.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 107.02872031837654, 'action': [2.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 100.18786076419939, 'action': [0.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 97.41968938558463, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 100.9368719905787, 'action': [1.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 102.0071680200657, 'action': [2.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 103.79118220956603, 'action': [1.0, 0.0]}])
Weights num count: [0.10747014718133852, 0.11135795612329909, 0.11496806442654818, 0.11496806442654818, 0.10969175229103027, 0.10747014718133852, 0.1102471535684532, 0.11108025548458761, 0.11246875867814496]
Actions to choose Agent 1: dict_values([{'num_count': 571, 'sum_payoffs': 132.81382951213612, 'action': [0.0, -1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 152.68757627167366, 'action': [1.0, 0.0]}, {'num_count': 574, 'sum_payoffs': 133.77350291393503, 'action': [0.0, 1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 147.44934657261007, 'action': [1.0, -1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 136.61948831540587, 'action': [0.0, 0.0]}, {'num_count': 622, 'sum_payoffs': 149.09587478859842, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15856706470424883, 0.1757845043043599, 0.15940016662038323, 0.1713412940849764, 0.16189947236878643, 0.17272979727853374]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.64678478240967 s
