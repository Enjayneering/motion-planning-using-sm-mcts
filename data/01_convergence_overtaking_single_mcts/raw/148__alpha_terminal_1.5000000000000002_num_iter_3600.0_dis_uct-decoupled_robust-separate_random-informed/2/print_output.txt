Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 392, 'sum_payoffs': 99.12369947382138, 'action': [1.0, 0.0]}, {'num_count': 388, 'sum_payoffs': 97.61460360575688, 'action': [0.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 98.36877129578902, 'action': [0.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 111.28932885020997, 'action': [2.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 107.69521851477863, 'action': [2.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 101.89887416317733, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 104.09388040403995, 'action': [1.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 95.61094619433229, 'action': [0.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 101.92694958805623, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10885865037489587, 0.10774784782004998, 0.10830324909747292, 0.11830047209108581, 0.11552346570397112, 0.11108025548458761, 0.11274645931685642, 0.10608164398778117, 0.11108025548458761]
Actions to choose Agent 1: dict_values([{'num_count': 580, 'sum_payoffs': 136.3276851825934, 'action': [0.0, 0.0]}, {'num_count': 613, 'sum_payoffs': 146.85193414475097, 'action': [1.0, -1.5707963267948966]}, {'num_count': 574, 'sum_payoffs': 134.30324660949086, 'action': [0.0, 1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 150.1063647412666, 'action': [1.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 134.65916624023106, 'action': [0.0, -1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 153.9231454009792, 'action': [1.0, 0.0]}])
Weights num count: [0.16106637045265204, 0.1702304915301305, 0.15940016662038323, 0.1730074979172452, 0.1596778672590947, 0.17633990558178284]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.67598462104797 s
