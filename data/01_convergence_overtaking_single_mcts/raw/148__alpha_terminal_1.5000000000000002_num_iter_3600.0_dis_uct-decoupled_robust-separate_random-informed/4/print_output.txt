Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 420, 'sum_payoffs': 108.89278510294145, 'action': [2.0, 0.0]}, {'num_count': 381, 'sum_payoffs': 95.03831746513228, 'action': [0.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 97.83974497627187, 'action': [0.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 108.55930617048277, 'action': [2.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 105.3576086283667, 'action': [2.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 96.04175107266798, 'action': [0.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 103.17358473524267, 'action': [1.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 101.36970753916619, 'action': [1.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 99.62933063693418, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.116634268258817, 0.1058039433490697, 0.10802554845876146, 0.11635656762010553, 0.11385726187170231, 0.10663704526520411, 0.1121910580394335, 0.11080255484587614, 0.1094140516523188]
Actions to choose Agent 1: dict_values([{'num_count': 608, 'sum_payoffs': 144.78345157339092, 'action': [1.0, -1.5707963267948966]}, {'num_count': 589, 'sum_payoffs': 138.82730669241872, 'action': [0.0, 0.0]}, {'num_count': 561, 'sum_payoffs': 129.91661557996514, 'action': [0.0, 1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 154.49355717384907, 'action': [1.0, 0.0]}, {'num_count': 564, 'sum_payoffs': 130.7755788060978, 'action': [0.0, -1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 155.14667781827814, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16884198833657318, 0.16356567620105525, 0.15579005831713413, 0.17717300749791726, 0.15662316023326853, 0.17772840877534019]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.05013871192932 s
