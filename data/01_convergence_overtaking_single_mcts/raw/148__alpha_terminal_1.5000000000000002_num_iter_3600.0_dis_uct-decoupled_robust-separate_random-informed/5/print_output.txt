Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 412, 'sum_payoffs': 106.29430178418373, 'action': [2.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 96.9890674520375, 'action': [0.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 100.92441180829765, 'action': [1.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 104.94387561164007, 'action': [2.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 109.58931975655801, 'action': [2.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 104.48732344682944, 'action': [1.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 103.04586759359968, 'action': [1.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 94.62525633341956, 'action': [0.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 97.3647793163069, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11441266314912524, 0.10719244654262705, 0.1102471535684532, 0.11330186059427937, 0.11691196889752846, 0.1130241599555679, 0.11191335740072202, 0.10524854207164676, 0.10747014718133852]
Actions to choose Agent 1: dict_values([{'num_count': 569, 'sum_payoffs': 132.06099756096432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 642, 'sum_payoffs': 155.3632220974346, 'action': [1.0, 0.0]}, {'num_count': 579, 'sum_payoffs': 135.23020674302595, 'action': [0.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 150.26330833174606, 'action': [1.0, 1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 135.2716933715356, 'action': [0.0, -1.5707963267948966]}, {'num_count': 605, 'sum_payoffs': 143.5538892965283, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15801166342682588, 0.17828381005276311, 0.16078866981394058, 0.17384059983337963, 0.16078866981394058, 0.16800888642043876]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.85555148124695 s
