Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 409, 'sum_payoffs': 105.0040602442035, 'action': [2.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 100.47621735857933, 'action': [1.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 106.44451098764716, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 94.72980413633934, 'action': [0.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 111.57651548268937, 'action': [2.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 105.7891307982769, 'action': [2.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 94.70812400743183, 'action': [0.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 100.141608871551, 'action': [1.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 97.97068715103545, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11357956123299083, 0.10996945292974174, 0.11469036378783672, 0.10552624271035824, 0.11857817272979727, 0.11413496251041377, 0.10552624271035824, 0.10969175229103027, 0.10802554845876146]
Actions to choose Agent 1: dict_values([{'num_count': 579, 'sum_payoffs': 135.49871283403067, 'action': [0.0, 0.0]}, {'num_count': 633, 'sum_payoffs': 152.80446543620107, 'action': [1.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 134.58422601399067, 'action': [0.0, 1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 145.65147144001347, 'action': [1.0, -1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 148.02594118856567, 'action': [1.0, 1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 136.83154664923873, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16078866981394058, 0.1757845043043599, 0.15995556789780616, 0.16967509025270758, 0.17161899472368786, 0.16189947236878643]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.09385442733765 s
