Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 391, 'sum_payoffs': 98.6971417892304, 'action': [0.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 104.30719250299181, 'action': [2.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 102.82827803971814, 'action': [1.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 100.0275540085752, 'action': [1.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 95.44620010121582, 'action': [0.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 95.45145602846033, 'action': [0.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 104.68062890494453, 'action': [2.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 105.03361606452357, 'action': [1.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 109.98058382379492, 'action': [2.0, 0.0]}])
Weights num count: [0.10858094973618439, 0.1130241599555679, 0.11191335740072202, 0.10969175229103027, 0.10608164398778117, 0.10608164398778117, 0.11330186059427937, 0.11357956123299083, 0.1174673701749514]
Actions to choose Agent 1: dict_values([{'num_count': 570, 'sum_payoffs': 132.52745191653563, 'action': [0.0, -1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 152.26627525001555, 'action': [1.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 155.16859015731734, 'action': [1.0, 0.0]}, {'num_count': 578, 'sum_payoffs': 135.06286081870263, 'action': [0.0, 0.0]}, {'num_count': 610, 'sum_payoffs': 145.20497642199499, 'action': [1.0, 1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 132.2084403491828, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15828936406553734, 0.17550680366564844, 0.17800610941405165, 0.1605109691752291, 0.1693973896139961, 0.15801166342682588]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.07727980613708 s
