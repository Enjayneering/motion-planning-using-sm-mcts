Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 384, 'sum_payoffs': 96.06946236402513, 'action': [0.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 105.98279536096263, 'action': [2.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 104.20328257383711, 'action': [1.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 108.45955856661637, 'action': [2.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 107.44801717339246, 'action': [2.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 97.05722587945758, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 100.70079696075622, 'action': [1.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 97.855484060985, 'action': [1.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 97.74257557821166, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10663704526520411, 0.11441266314912524, 0.1130241599555679, 0.11635656762010553, 0.11552346570397112, 0.10747014718133852, 0.1102471535684532, 0.10802554845876146, 0.10802554845876146]
Actions to choose Agent 1: dict_values([{'num_count': 592, 'sum_payoffs': 140.0359089971724, 'action': [0.0, 0.0]}, {'num_count': 620, 'sum_payoffs': 149.06781014754333, 'action': [1.0, 1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 131.77368726313915, 'action': [0.0, -1.5707963267948966]}, {'num_count': 651, 'sum_payoffs': 158.98032441155155, 'action': [1.0, 0.0]}, {'num_count': 623, 'sum_payoffs': 149.94230529644875, 'action': [1.0, -1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 126.01723782702055, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16439877811718967, 0.1721743960011108, 0.15717856151069148, 0.18078311580116635, 0.1730074979172452, 0.15217995001388504]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.58654308319092 s
