Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 421, 'sum_payoffs': 108.97781370144642, 'action': [2.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 105.01344355411905, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 99.69847260154397, 'action': [0.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 99.67550186655785, 'action': [1.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 99.2714438935413, 'action': [1.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 95.06436883001334, 'action': [0.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 106.08594946932749, 'action': [2.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 94.76452837693768, 'action': [0.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 104.64821431079423, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11691196889752846, 0.11385726187170231, 0.10969175229103027, 0.10969175229103027, 0.1094140516523188, 0.10608164398778117, 0.11469036378783672, 0.1058039433490697, 0.11357956123299083]
Actions to choose Agent 1: dict_values([{'num_count': 584, 'sum_payoffs': 137.560120998246, 'action': [0.0, 0.0]}, {'num_count': 635, 'sum_payoffs': 153.97403605206793, 'action': [1.0, 0.0]}, {'num_count': 617, 'sum_payoffs': 148.1086707124323, 'action': [1.0, -1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 134.7328293430044, 'action': [0.0, 1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 131.8872499193943, 'action': [0.0, -1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 150.05395586269793, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16217717300749793, 0.17633990558178284, 0.1713412940849764, 0.1596778672590947, 0.15717856151069148, 0.1730074979172452]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.89165449142456 s
