Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 479, 'sum_payoffs': 123.11474954858403, 'action': [2.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 113.70912683627397, 'action': [1.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 109.14711376452343, 'action': [0.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 115.41123529821768, 'action': [1.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 116.15137554305586, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 125.85858075825826, 'action': [2.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 113.66111673600338, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 109.19311376875142, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 108.09292248503479, 'action': [0.0, 0.0]}])
Weights num count: [0.11680078029748842, 0.11021702023896611, 0.10704706169227018, 0.11143623506461839, 0.1119239209948793, 0.11875152401853206, 0.11021702023896611, 0.10704706169227018, 0.1063155327968788]
Actions to choose Agent 1: dict_values([{'num_count': 714, 'sum_payoffs': 170.88507989937085, 'action': [1.0, 0.0]}, {'num_count': 648, 'sum_payoffs': 150.153809675453, 'action': [0.0, -1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 151.3122090328411, 'action': [0.0, 1.5707963267948966]}, {'num_count': 704, 'sum_payoffs': 167.77678850281598, 'action': [1.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 158.01153566433808, 'action': [0.0, 0.0]}, {'num_count': 709, 'sum_payoffs': 169.3229294582346, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17410387710314557, 0.15801024140453548, 0.1589856132650573, 0.171665447451841, 0.1641063155327969, 0.1728846622774933]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.74085807800293 s
