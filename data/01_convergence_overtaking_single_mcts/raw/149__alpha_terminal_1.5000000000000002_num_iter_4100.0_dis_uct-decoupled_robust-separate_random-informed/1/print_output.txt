Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 430, 'sum_payoffs': 106.17095236812763, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 115.14836823170009, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 126.9701016235414, 'action': [2.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 122.10000589918297, 'action': [2.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 109.82409895246838, 'action': [0.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 109.88366053746957, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 116.110858975121, 'action': [1.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 114.11795671404425, 'action': [2.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 114.4490192205519, 'action': [1.0, 0.0]}])
Weights num count: [0.10485247500609607, 0.11119239209948793, 0.11948305291392343, 0.11606925140209705, 0.1075347476225311, 0.1075347476225311, 0.1119239209948793, 0.11046086320409657, 0.11070470616922702]
Actions to choose Agent 1: dict_values([{'num_count': 666, 'sum_payoffs': 155.38332497039983, 'action': [0.0, 0.0]}, {'num_count': 657, 'sum_payoffs': 152.61542134210225, 'action': [0.0, 1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 149.77167909164794, 'action': [0.0, -1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 168.57064549606423, 'action': [1.0, -1.5707963267948966]}, {'num_count': 729, 'sum_payoffs': 175.19266566355026, 'action': [1.0, 0.0]}, {'num_count': 692, 'sum_payoffs': 163.515402072854, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1623994147768837, 0.16020482809070957, 0.15801024140453548, 0.17264081931236283, 0.17776152158010242, 0.16873933187027554]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.21455264091492 s
