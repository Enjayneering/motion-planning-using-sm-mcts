Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 465, 'sum_payoffs': 118.18007166087975, 'action': [2.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 112.89736708783877, 'action': [1.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 127.29715098534102, 'action': [2.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 111.9299160823761, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 114.92551285512761, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 110.87004248642957, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 114.29998663444727, 'action': [1.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 118.53647767200117, 'action': [2.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 104.91925356483652, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11338697878566203, 0.1097293343087052, 0.11972689587905389, 0.10899780541331383, 0.11119239209948793, 0.10826627651792246, 0.11070470616922702, 0.11363082175079249, 0.1041209461107047]
Actions to choose Agent 1: dict_values([{'num_count': 650, 'sum_payoffs': 150.70013306224433, 'action': [0.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 172.359713642511, 'action': [1.0, 1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 155.38851055552564, 'action': [0.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 149.45238182267778, 'action': [0.0, -1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 172.66609820043482, 'action': [1.0, 0.0]}, {'num_count': 700, 'sum_payoffs': 166.40773694581767, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1584979273347964, 0.17532309192879786, 0.16215557181175322, 0.15752255547427457, 0.1755669348939283, 0.17069007559131918]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.51138758659363 s
