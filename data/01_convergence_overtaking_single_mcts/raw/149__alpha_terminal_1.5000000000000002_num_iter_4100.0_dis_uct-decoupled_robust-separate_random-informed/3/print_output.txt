Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 437, 'sum_payoffs': 108.13610157927799, 'action': [0.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 110.92291823065344, 'action': [0.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 116.47130192957503, 'action': [2.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 117.51015331971549, 'action': [1.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 114.0422268716408, 'action': [1.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 107.84941655805551, 'action': [0.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 117.83452577977252, 'action': [2.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 125.57742569078881, 'action': [2.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 112.98130663121418, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10655937576200927, 0.10851011948305292, 0.11241160692514021, 0.11314313582053158, 0.11070470616922702, 0.1063155327968788, 0.11338697878566203, 0.11875152401853206, 0.10997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 666, 'sum_payoffs': 155.85145998981542, 'action': [0.0, -1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 172.8403106434489, 'action': [1.0, 0.0]}, {'num_count': 705, 'sum_payoffs': 168.16158240224792, 'action': [1.0, -1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 169.0311643885807, 'action': [1.0, 1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 149.84414692640115, 'action': [0.0, 0.0]}, {'num_count': 654, 'sum_payoffs': 152.0171578855132, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1623994147768837, 0.1755669348939283, 0.17190929041697148, 0.17264081931236283, 0.15776639843940501, 0.15947329919531822]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.28905725479126 s
