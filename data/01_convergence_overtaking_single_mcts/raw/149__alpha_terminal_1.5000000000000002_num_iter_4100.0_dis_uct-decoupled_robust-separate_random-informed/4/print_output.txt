Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 111.13972781879694, 'action': [0.0, -1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 105.32994027817789, 'action': [0.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 126.24217934104388, 'action': [2.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 116.4234745713678, 'action': [1.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 124.07318160134623, 'action': [2.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 108.33160161063032, 'action': [0.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 113.64117016658571, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 111.880761529912, 'action': [1.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 116.791695537713, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10851011948305292, 0.10436478907583516, 0.11899536698366252, 0.11216776396000976, 0.11753230919287978, 0.10655937576200927, 0.11021702023896611, 0.10899780541331383, 0.11241160692514021]
Actions to choose Agent 1: dict_values([{'num_count': 731, 'sum_payoffs': 176.11699650857068, 'action': [1.0, 0.0]}, {'num_count': 645, 'sum_payoffs': 149.11066758689094, 'action': [0.0, -1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 153.79200205201082, 'action': [0.0, 0.0]}, {'num_count': 698, 'sum_payoffs': 165.7236791420649, 'action': [1.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 153.7003218978256, 'action': [0.0, 1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 168.22738433879354, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17824920751036333, 0.1572787125091441, 0.16093635698610095, 0.17020238966105827, 0.16093635698610095, 0.17215313338210192]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.2518093585968 s
