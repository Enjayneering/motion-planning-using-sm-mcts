Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 454, 'sum_payoffs': 114.35984968265447, 'action': [1.0, 1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 106.52369386001554, 'action': [0.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 114.84530523057538, 'action': [2.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 119.25537292762945, 'action': [2.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 107.178566639276, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 115.16464070956458, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 115.07838847838315, 'action': [1.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 130.53691247414775, 'action': [2.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 112.06840357907451, 'action': [0.0, 0.0]}])
Weights num count: [0.11070470616922702, 0.10509631797122652, 0.11094854913435748, 0.1141185076810534, 0.10558400390148744, 0.11119239209948793, 0.11119239209948793, 0.12192148256522799, 0.10899780541331383]
Actions to choose Agent 1: dict_values([{'num_count': 654, 'sum_payoffs': 152.36518596151802, 'action': [0.0, -1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 150.13778882796953, 'action': [0.0, 1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 166.49146798347994, 'action': [1.0, -1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 181.04888524574588, 'action': [1.0, 0.0]}, {'num_count': 700, 'sum_payoffs': 166.813179857478, 'action': [1.0, 1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 152.66985667874766, 'action': [0.0, 0.0]}])
Weights num count: [0.15947329919531822, 0.15776639843940501, 0.17044623262618874, 0.1816630090221897, 0.17069007559131918, 0.15971714216044866]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.23089218139648 s
