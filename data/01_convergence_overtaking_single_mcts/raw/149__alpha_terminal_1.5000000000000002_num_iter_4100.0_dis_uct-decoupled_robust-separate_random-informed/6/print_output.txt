Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 468, 'sum_payoffs': 118.75039240834332, 'action': [1.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 118.38424374280095, 'action': [2.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 109.82409156993926, 'action': [1.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 125.42303196902668, 'action': [2.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 113.27168426698955, 'action': [0.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 109.82337840321185, 'action': [0.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 106.60662073704951, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 111.77225136053082, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 116.28125658366554, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1141185076810534, 0.11387466471592295, 0.10777859058766155, 0.11875152401853206, 0.11021702023896611, 0.10777859058766155, 0.10558400390148744, 0.10924164837844429, 0.11241160692514021]
Actions to choose Agent 1: dict_values([{'num_count': 710, 'sum_payoffs': 169.75394792433167, 'action': [1.0, 1.5707963267948966]}, {'num_count': 676, 'sum_payoffs': 158.93338568665592, 'action': [0.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 175.7324804047682, 'action': [1.0, 0.0]}, {'num_count': 647, 'sum_payoffs': 149.9799031502037, 'action': [0.0, 1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 163.44874642376607, 'action': [1.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 150.30054115409453, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17312850524262374, 0.16483784442818825, 0.17776152158010242, 0.15776639843940501, 0.16825164594001463, 0.15801024140453548]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.14986801147461 s
