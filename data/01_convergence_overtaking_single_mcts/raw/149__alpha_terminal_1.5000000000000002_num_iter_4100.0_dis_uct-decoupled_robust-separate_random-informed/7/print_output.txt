Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 465, 'sum_payoffs': 118.43634216341177, 'action': [1.0, -1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 122.28914040163673, 'action': [2.0, 1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 123.37999521788554, 'action': [2.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 110.08456910704132, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 119.78778787363947, 'action': [2.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 107.00557924674402, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 111.12057669716323, 'action': [1.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 110.85111341562684, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 113.58206204184359, 'action': [1.0, 0.0]}])
Weights num count: [0.11338697878566203, 0.11606925140209705, 0.11680078029748842, 0.1075347476225311, 0.11436235064618386, 0.10534016093635698, 0.10826627651792246, 0.10802243355279201, 0.10997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 661, 'sum_payoffs': 153.7895469319795, 'action': [0.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 165.74578731599564, 'action': [1.0, 1.5707963267948966]}, {'num_count': 735, 'sum_payoffs': 177.0642076308528, 'action': [1.0, 0.0]}, {'num_count': 667, 'sum_payoffs': 155.6604983846365, 'action': [0.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 150.2928150802614, 'action': [0.0, 1.5707963267948966]}, {'num_count': 688, 'sum_payoffs': 162.28319895211968, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1611801999512314, 0.17044623262618874, 0.17922457937088515, 0.16264325774201413, 0.1584979273347964, 0.16776396000975372]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.16191911697388 s
