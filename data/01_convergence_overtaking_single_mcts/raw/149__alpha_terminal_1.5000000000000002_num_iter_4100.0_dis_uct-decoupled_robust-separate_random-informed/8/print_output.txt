Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 468, 'sum_payoffs': 119.72148913615285, 'action': [2.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 118.2408400055227, 'action': [2.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 113.08312550037766, 'action': [1.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 117.57629125132767, 'action': [1.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 109.87586656885728, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 110.53301914059621, 'action': [0.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 123.58319500186882, 'action': [2.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 112.6362914907932, 'action': [1.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 112.71634195354396, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1141185076810534, 0.11314313582053158, 0.10948549134357474, 0.11265544989027067, 0.10729090465740064, 0.10777859058766155, 0.11680078029748842, 0.10924164837844429, 0.10924164837844429]
Actions to choose Agent 1: dict_values([{'num_count': 739, 'sum_payoffs': 178.00092200785528, 'action': [1.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 167.96903568085898, 'action': [1.0, 1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 149.16482289019203, 'action': [0.0, 0.0]}, {'num_count': 647, 'sum_payoffs': 149.20598723951443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 700, 'sum_payoffs': 165.75925125548858, 'action': [1.0, -1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 153.22811006103018, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18019995123140697, 0.1723969763472324, 0.15776639843940501, 0.15776639843940501, 0.17069007559131918, 0.16093635698610095]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 73.90902590751648 s
