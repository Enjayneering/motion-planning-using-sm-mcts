Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 456, 'sum_payoffs': 114.83269743678028, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 108.93010521801675, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 113.10904186004396, 'action': [1.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 125.6186576086044, 'action': [2.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 109.61007946045461, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 109.63152031104126, 'action': [0.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 123.90642853497633, 'action': [2.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 117.19068321128402, 'action': [2.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 109.25751851018019, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11119239209948793, 0.10704706169227018, 0.10997317727383565, 0.11875152401853206, 0.1075347476225311, 0.1075347476225311, 0.11753230919287978, 0.11289929285540112, 0.10729090465740064]
Actions to choose Agent 1: dict_values([{'num_count': 660, 'sum_payoffs': 154.26552422218086, 'action': [0.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 169.5883584738923, 'action': [1.0, -1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 166.19062256770763, 'action': [1.0, 1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 153.5896844868374, 'action': [0.0, 0.0]}, {'num_count': 736, 'sum_payoffs': 178.1752543842126, 'action': [1.0, 0.0]}, {'num_count': 639, 'sum_payoffs': 147.68565708382576, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16093635698610095, 0.1728846622774933, 0.17020238966105827, 0.16044867105584004, 0.1794684223360156, 0.15581565471836137]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.69554424285889 s
