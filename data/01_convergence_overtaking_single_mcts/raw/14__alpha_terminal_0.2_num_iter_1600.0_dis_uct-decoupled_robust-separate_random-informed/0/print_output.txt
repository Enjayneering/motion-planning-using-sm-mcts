Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 189, 'sum_payoffs': 46.1134649989704, 'action': [2.0, -1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 57.352446955409306, 'action': [2.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 38.161136816583365, 'action': [1.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 46.1714794699933, 'action': [2.0, 1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 32.54372813056606, 'action': [0.0, -1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.123503460348054, 'action': [0.0, 1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 47.94573726836116, 'action': [1.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 38.503031086786194, 'action': [0.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 38.103122345560436, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11805121798875702, 0.13616489693941286, 0.10493441599000625, 0.11805121798875702, 0.09556527170518427, 0.09494066208619613, 0.12117426608369769, 0.10555902560899438, 0.10493441599000625]
Actions to choose Agent 1: dict_values([{'num_count': 238, 'sum_payoffs': 62.937009745538276, 'action': [0.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 71.26472994207089, 'action': [0.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 80.40110378260073, 'action': [1.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 89.15042477282778, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 79.2392933836388, 'action': [1.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 63.04717205567639, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1486570893191755, 0.1617738913179263, 0.17613991255465333, 0.18988132417239226, 0.17426608369768895, 0.1486570893191755]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.8395941257476807 s
