Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 190, 'sum_payoffs': 46.336614293858204, 'action': [2.0, -1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 31.984442556040772, 'action': [0.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 47.55169515904312, 'action': [1.0, 0.0]}, {'num_count': 152, 'sum_payoffs': 32.09460486617894, 'action': [0.0, 1.5707963267948966]}, {'num_count': 214, 'sum_payoffs': 55.63631226940677, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 39.179142306438045, 'action': [0.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 45.864241784524964, 'action': [2.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 38.38993546069416, 'action': [1.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.71738043381242, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11867582760774516, 0.09494066208619613, 0.12054965646470955, 0.09494066208619613, 0.13366645846346034, 0.10680824484697064, 0.11805121798875702, 0.10555902560899438, 0.1061836352279825]
Actions to choose Agent 1: dict_values([{'num_count': 311, 'sum_payoffs': 92.25546645439923, 'action': [1.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 77.83988439270458, 'action': [1.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 80.27073418481261, 'action': [1.0, -1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 64.82899853345569, 'action': [0.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 70.73151828710319, 'action': [0.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 61.639180399565376, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1942535915053092, 0.1717676452217364, 0.1755153029356652, 0.15115552779512806, 0.16052467207995003, 0.146158650843223]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.6327269077301025 s
