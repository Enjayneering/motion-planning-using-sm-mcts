Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 189, 'sum_payoffs': 45.991786707701515, 'action': [2.0, -1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 46.821625411429956, 'action': [1.0, 0.0]}, {'num_count': 214, 'sum_payoffs': 55.67401081143743, 'action': [2.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 39.898601417343535, 'action': [0.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 38.83905872508128, 'action': [1.0, 1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 32.39293396244337, 'action': [0.0, 1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 32.450948433466316, 'action': [0.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 38.389935460694154, 'action': [1.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 45.68759097952938, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11805121798875702, 0.1193004372267333, 0.13366645846346034, 0.1080574640849469, 0.1061836352279825, 0.09556527170518427, 0.09556527170518427, 0.10555902560899438, 0.1174266083697689]
Actions to choose Agent 1: dict_values([{'num_count': 237, 'sum_payoffs': 62.74590964345815, 'action': [0.0, -1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 73.32456958712231, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 78.5351889142409, 'action': [1.0, 1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 89.00288984468602, 'action': [1.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 63.89305346265078, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 79.43636875896905, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14803247970018737, 0.16489693941286696, 0.17301686445971268, 0.18925671455340412, 0.14990630855715179, 0.17426608369768895]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.7132816314697266 s
