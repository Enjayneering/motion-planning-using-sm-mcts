Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 172, 'sum_payoffs': 39.52871389739467, 'action': [0.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 45.991786707701536, 'action': [2.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.83905872508127, 'action': [1.0, -1.5707963267948966]}, {'num_count': 151, 'sum_payoffs': 31.784542506099076, 'action': [0.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 45.99178670770153, 'action': [2.0, -1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.07135562123282, 'action': [0.0, -1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 56.43794044070434, 'action': [2.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 38.41318470564028, 'action': [1.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 47.17594100718652, 'action': [1.0, 0.0]}])
Weights num count: [0.10743285446595878, 0.11805121798875702, 0.1061836352279825, 0.09431605246720799, 0.11805121798875702, 0.09494066208619613, 0.13491567770143661, 0.10555902560899438, 0.11992504684572143]
Actions to choose Agent 1: dict_values([{'num_count': 263, 'sum_payoffs': 73.03007915123476, 'action': [0.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 91.95329869775787, 'action': [1.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 62.47647914136284, 'action': [0.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 62.08243703204479, 'action': [0.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 78.33822218025307, 'action': [1.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 79.79857895816413, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16427232979387882, 0.19362898188632105, 0.14740787008119924, 0.14678326046221113, 0.17239225484072454, 0.1748906933166771]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.782982110977173 s
