Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 189, 'sum_payoffs': 46.13671424391651, 'action': [2.0, -1.5707963267948966]}, {'num_count': 151, 'sum_payoffs': 31.819307732175897, 'action': [0.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 45.63544314041416, 'action': [2.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.9153611535657, 'action': [0.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 39.12587184021501, 'action': [1.0, 1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 56.14819400961678, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 39.16063706629182, 'action': [1.0, -1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 46.82253075585301, 'action': [1.0, 0.0]}, {'num_count': 154, 'sum_payoffs': 32.830541245699784, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11805121798875702, 0.09431605246720799, 0.1174266083697689, 0.1061836352279825, 0.10680824484697064, 0.13429106808244848, 0.10680824484697064, 0.1193004372267333, 0.09618988132417239]
Actions to choose Agent 1: dict_values([{'num_count': 260, 'sum_payoffs': 71.87413538418062, 'action': [0.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 79.05090931482783, 'action': [1.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 79.37270493872309, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 63.4904286881559, 'action': [0.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 63.99756642356594, 'action': [0.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 89.42876386412708, 'action': [1.0, 0.0]}])
Weights num count: [0.16239850093691444, 0.1736414740787008, 0.17426608369768895, 0.14928169893816365, 0.14990630855715179, 0.18988132417239226]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.809180736541748 s
