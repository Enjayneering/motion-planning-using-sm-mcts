Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 38.78691088596605, 'action': [1.0, -1.5707963267948966]}, {'num_count': 214, 'sum_payoffs': 55.68078278776817, 'action': [2.0, 0.0]}, {'num_count': 153, 'sum_payoffs': 32.53786149865835, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 46.38017946779664, 'action': [1.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 45.67020836649097, 'action': [2.0, 1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 32.39880059435108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 39.88889612505891, 'action': [0.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 46.423527359050226, 'action': [2.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 38.534862996909126, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1061836352279825, 0.13366645846346034, 0.09556527170518427, 0.11867582760774516, 0.1174266083697689, 0.09556527170518427, 0.1080574640849469, 0.11867582760774516, 0.10555902560899438]
Actions to choose Agent 1: dict_values([{'num_count': 264, 'sum_payoffs': 73.48666245369098, 'action': [0.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 63.24131411505274, 'action': [0.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 80.6677096100845, 'action': [1.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 78.17612931368446, 'action': [1.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 89.50416094818839, 'action': [1.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 62.473545825408934, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16489693941286696, 0.1486570893191755, 0.17613991255465333, 0.17239225484072454, 0.18988132417239226, 0.14740787008119924]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.8255090713500977 s
