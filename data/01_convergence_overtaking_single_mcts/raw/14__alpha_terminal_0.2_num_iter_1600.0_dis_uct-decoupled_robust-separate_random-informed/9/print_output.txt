Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 155, 'sum_payoffs': 33.29118049121761, 'action': [0.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 45.68759097952938, 'action': [2.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 38.500097770832326, 'action': [1.0, 1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.08873823427123, 'action': [0.0, 1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 46.77422157711465, 'action': [1.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 37.99882666732998, 'action': [1.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 45.65282575345256, 'action': [2.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 40.35246868653058, 'action': [0.0, 0.0]}, {'num_count': 215, 'sum_payoffs': 56.164671278232134, 'action': [2.0, 0.0]}])
Weights num count: [0.09681449094316052, 0.1174266083697689, 0.10555902560899438, 0.09494066208619613, 0.1193004372267333, 0.10493441599000625, 0.1174266083697689, 0.10868207370393504, 0.13429106808244848]
Actions to choose Agent 1: dict_values([{'num_count': 261, 'sum_payoffs': 72.10858337751439, 'action': [0.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 64.18573320969219, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 78.47445840994887, 'action': [1.0, 1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 88.85209567656331, 'action': [1.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 78.06010037163858, 'action': [1.0, -1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 64.63778979003314, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16302311055590257, 0.15053091817613992, 0.17301686445971268, 0.18925671455340412, 0.17239225484072454, 0.15115552779512806]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.7624258995056152 s
