Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 516, 'sum_payoffs': 129.3111896556796, 'action': [1.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 120.73619884593793, 'action': [0.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 135.1575821340182, 'action': [2.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 122.08544419808152, 'action': [0.0, -1.5707963267948966]}, {'num_count': 547, 'sum_payoffs': 139.80887561671057, 'action': [2.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 122.88348282382846, 'action': [0.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 128.92082751901228, 'action': [2.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 129.63393515365723, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 120.16397785073053, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11214953271028037, 0.10671593131927842, 0.1158443816561617, 0.10758530754183873, 0.1188871984351228, 0.10801999565311889, 0.1119321886546403, 0.11236687676592046, 0.10628124320799825]
Actions to choose Agent 1: dict_values([{'num_count': 806, 'sum_payoffs': 192.48949625099894, 'action': [1.0, 1.5707963267948966]}, {'num_count': 815, 'sum_payoffs': 195.3456991813957, 'action': [1.0, 0.0]}, {'num_count': 776, 'sum_payoffs': 183.2289608668595, 'action': [1.0, -1.5707963267948966]}, {'num_count': 725, 'sum_payoffs': 167.4903571386786, 'action': [0.0, -1.5707963267948966]}, {'num_count': 730, 'sum_payoffs': 169.03441813396, 'action': [0.0, 1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 174.52227790574207, 'action': [0.0, 0.0]}])
Weights num count: [0.17517930884590308, 0.17713540534666378, 0.16865898717670072, 0.15757444033905674, 0.15866116061725713, 0.16257335361877853]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 86.8570168018341 s
