Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 509, 'sum_payoffs': 126.862955290354, 'action': [1.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 128.19416165794206, 'action': [2.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 127.19516808567164, 'action': [1.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 117.94702980682293, 'action': [0.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 130.52677583498098, 'action': [1.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 121.76195594542473, 'action': [0.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 122.0794169532722, 'action': [0.0, 0.0]}, {'num_count': 548, 'sum_payoffs': 140.14956410731804, 'action': [2.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 133.28410001809078, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11062812432079983, 0.11149750054336013, 0.11084546837643991, 0.10497717887415779, 0.11301890893284068, 0.10736796348619865, 0.10758530754183873, 0.11910454249076288, 0.11475766137796131]
Actions to choose Agent 1: dict_values([{'num_count': 790, 'sum_payoffs': 187.19508977577394, 'action': [1.0, -1.5707963267948966]}, {'num_count': 831, 'sum_payoffs': 199.86307179158345, 'action': [1.0, 0.0]}, {'num_count': 732, 'sum_payoffs': 169.28406217319846, 'action': [0.0, 1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 166.21909257042952, 'action': [0.0, -1.5707963267948966]}, {'num_count': 732, 'sum_payoffs': 169.32374072536038, 'action': [0.0, 0.0]}, {'num_count': 793, 'sum_payoffs': 188.10934112448817, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1717018039556618, 0.180612910236905, 0.15909584872853727, 0.15692240817213648, 0.15909584872853727, 0.17235383612258204]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 85.11824011802673 s
