Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 524, 'sum_payoffs': 131.65827433720582, 'action': [1.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 124.46424086992589, 'action': [1.0, 1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 140.2635388987849, 'action': [2.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 115.40959240480777, 'action': [0.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 131.6723404865034, 'action': [2.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 122.06042246279232, 'action': [0.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 124.54872629284998, 'action': [1.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 125.91527307264951, 'action': [0.0, 0.0]}, {'num_count': 518, 'sum_payoffs': 129.55744915297788, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.113888285155401, 0.10932405998695936, 0.11932188654640295, 0.10345577048467725, 0.113888285155401, 0.10780265159747882, 0.10932405998695936, 0.11019343620951967, 0.11258422082156053]
Actions to choose Agent 1: dict_values([{'num_count': 782, 'sum_payoffs': 184.6343359841205, 'action': [1.0, 1.5707963267948966]}, {'num_count': 842, 'sum_payoffs': 203.28010535257496, 'action': [1.0, 0.0]}, {'num_count': 761, 'sum_payoffs': 178.14904199108358, 'action': [0.0, 0.0]}, {'num_count': 778, 'sum_payoffs': 183.37302989832077, 'action': [1.0, -1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 164.01645141281827, 'action': [0.0, 1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 166.0546877770966, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1699630515105412, 0.18300369484894588, 0.16539882634209954, 0.16909367528798086, 0.15540099978265595, 0.15692240817213648]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 93.39691519737244 s
