Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 519, 'sum_payoffs': 130.96490758051056, 'action': [2.0, -1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 132.3305263803677, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 122.76239925230276, 'action': [0.0, 0.0]}, {'num_count': 544, 'sum_payoffs': 139.57993504804435, 'action': [2.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 130.97007602103727, 'action': [2.0, 1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 128.27751276247693, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 123.76341719983701, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 119.3472155579199, 'action': [0.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 126.44980649736608, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11280156487720061, 0.11367094109976092, 0.10758530754183873, 0.11823516626820256, 0.11280156487720061, 0.11106281243207998, 0.10823733970875897, 0.10541186698543795, 0.10997609215387959]
Actions to choose Agent 1: dict_values([{'num_count': 781, 'sum_payoffs': 183.76579968197296, 'action': [1.0, -1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 163.54199781265976, 'action': [0.0, 1.5707963267948966]}, {'num_count': 835, 'sum_payoffs': 200.555489892631, 'action': [1.0, 0.0]}, {'num_count': 738, 'sum_payoffs': 170.5195533473689, 'action': [0.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 167.80681854306926, 'action': [0.0, -1.5707963267948966]}, {'num_count': 802, 'sum_payoffs': 190.30053611298646, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1697457074549011, 0.15540099978265595, 0.18148228645946535, 0.16039991306237775, 0.15844381656161705, 0.17430993262334274]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.74919891357422 s
