Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 497, 'sum_payoffs': 123.22848029901594, 'action': [0.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 134.79125786881343, 'action': [2.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 119.8269234663214, 'action': [0.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 129.0184497303138, 'action': [1.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 125.1975080037868, 'action': [1.0, 1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 128.36909800269638, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 123.4557934822551, 'action': [0.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 138.9364173642506, 'action': [2.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 129.28688933594913, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10801999565311889, 0.11540969354488155, 0.1058465550967181, 0.11171484459900022, 0.10932405998695936, 0.11128015648772006, 0.10823733970875897, 0.11801782221256249, 0.1119321886546403]
Actions to choose Agent 1: dict_values([{'num_count': 725, 'sum_payoffs': 166.3557951261781, 'action': [0.0, 1.5707963267948966]}, {'num_count': 816, 'sum_payoffs': 194.24206655963482, 'action': [1.0, 1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 167.24473646181494, 'action': [0.0, -1.5707963267948966]}, {'num_count': 725, 'sum_payoffs': 166.28486981738632, 'action': [0.0, 0.0]}, {'num_count': 795, 'sum_payoffs': 187.82310437430655, 'action': [1.0, -1.5707963267948966]}, {'num_count': 811, 'sum_payoffs': 192.85433456113628, 'action': [1.0, 0.0]}])
Weights num count: [0.15757444033905674, 0.17735274940230383, 0.15822647250597696, 0.15757444033905674, 0.1727885242338622, 0.17626602912410347]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.59307193756104 s
