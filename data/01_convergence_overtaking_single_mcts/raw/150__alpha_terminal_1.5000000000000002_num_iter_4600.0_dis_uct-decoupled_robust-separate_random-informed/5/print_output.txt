Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 489, 'sum_payoffs': 120.20187921024389, 'action': [0.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 120.90946554898272, 'action': [0.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 137.6386872518997, 'action': [2.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 118.46107424929211, 'action': [0.0, 1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 130.4396468268556, 'action': [1.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 124.58387670523881, 'action': [1.0, 0.0]}, {'num_count': 536, 'sum_payoffs': 136.2839327023194, 'action': [2.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 134.2067857060942, 'action': [2.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 127.0433392376425, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10628124320799825, 0.10671593131927842, 0.11736579004564225, 0.10519452292979788, 0.11280156487720061, 0.10910671593131928, 0.11649641382308194, 0.11519234948924147, 0.11062812432079983]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 169.17789768619585, 'action': [0.0, 0.0]}, {'num_count': 808, 'sum_payoffs': 192.35159813431213, 'action': [1.0, -1.5707963267948966]}, {'num_count': 834, 'sum_payoffs': 200.45918127841213, 'action': [1.0, 0.0]}, {'num_count': 717, 'sum_payoffs': 164.33667595337602, 'action': [0.0, 1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 169.54787117521678, 'action': [0.0, -1.5707963267948966]}, {'num_count': 774, 'sum_payoffs': 181.75341770803465, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15931319278417735, 0.17561399695718322, 0.18126494240382526, 0.1558356878939361, 0.15953053683981744, 0.16822429906542055]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.6683099269867 s
