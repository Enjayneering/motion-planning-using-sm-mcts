Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 529, 'sum_payoffs': 134.09581965716396, 'action': [2.0, -1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 135.39999216719184, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 119.80023390508549, 'action': [0.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 127.54953875631956, 'action': [1.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 137.9135594557248, 'action': [2.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 121.7850020848444, 'action': [0.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 121.82219925385402, 'action': [0.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 127.95814655423207, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 125.52228812888146, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1149750054336014, 0.1158443816561617, 0.1058465550967181, 0.11084546837643991, 0.11736579004564225, 0.10715061943055858, 0.10715061943055858, 0.11106281243207998, 0.10954140404259943]
Actions to choose Agent 1: dict_values([{'num_count': 707, 'sum_payoffs': 161.8898630441382, 'action': [0.0, 1.5707963267948966]}, {'num_count': 825, 'sum_payoffs': 198.35142129712239, 'action': [1.0, 0.0]}, {'num_count': 731, 'sum_payoffs': 169.19989861711568, 'action': [0.0, -1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 188.35819359027067, 'action': [1.0, 1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 173.5368569096219, 'action': [0.0, 0.0]}, {'num_count': 799, 'sum_payoffs': 190.22795760589392, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1536622473375353, 0.17930884590306456, 0.1588785046728972, 0.17235383612258204, 0.16192132145185828, 0.17365790045642251]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.54093146324158 s
