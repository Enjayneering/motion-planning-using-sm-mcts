Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 516, 'sum_payoffs': 129.66260331688625, 'action': [1.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 124.20605476108611, 'action': [1.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 131.95949820087756, 'action': [2.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 142.28143718619788, 'action': [2.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 116.36424532118986, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 123.8174774731997, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 126.80959135505637, 'action': [1.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 120.08440293994911, 'action': [0.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 136.35993722355963, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11214953271028037, 0.10867202782003912, 0.11367094109976092, 0.12019126276896328, 0.10367311454031732, 0.10845468376439904, 0.11041078026515974, 0.10606389915235818, 0.11649641382308194]
Actions to choose Agent 1: dict_values([{'num_count': 814, 'sum_payoffs': 194.39105415062826, 'action': [1.0, 0.0]}, {'num_count': 712, 'sum_payoffs': 162.8799488834929, 'action': [0.0, -1.5707963267948966]}, {'num_count': 810, 'sum_payoffs': 193.1431204618699, 'action': [1.0, 1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 167.7871198325754, 'action': [0.0, 0.0]}, {'num_count': 720, 'sum_payoffs': 165.40049252686464, 'action': [0.0, 1.5707963267948966]}, {'num_count': 816, 'sum_payoffs': 194.9719190030405, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1769180612910237, 0.1547489676157357, 0.17604868506846338, 0.15822647250597696, 0.15648772006085634, 0.17735274940230383]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.09374332427979 s
