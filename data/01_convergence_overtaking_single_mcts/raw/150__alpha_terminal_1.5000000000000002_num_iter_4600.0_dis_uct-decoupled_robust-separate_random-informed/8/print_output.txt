Searching game tree in timestep 0...
Max timehorizon: 10
Actions to choose Agent 0: dict_values([{'num_count': 510, 'sum_payoffs': 127.12672555014906, 'action': [2.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 122.42262068615508, 'action': [0.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 123.44607196437998, 'action': [1.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 125.70707832598372, 'action': [0.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 126.82279295953164, 'action': [1.0, 0.0]}, {'num_count': 546, 'sum_payoffs': 139.4953637626438, 'action': [2.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 118.36513265234389, 'action': [0.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 136.31199529435767, 'action': [2.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 128.19320447332998, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11084546837643991, 0.10780265159747882, 0.10845468376439904, 0.10997609215387959, 0.11062812432079983, 0.11866985437948271, 0.10519452292979788, 0.11671375787872201, 0.11149750054336013]
Actions to choose Agent 1: dict_values([{'num_count': 722, 'sum_payoffs': 166.04728487601386, 'action': [0.0, 1.5707963267948966]}, {'num_count': 814, 'sum_payoffs': 194.40897260326437, 'action': [1.0, 0.0]}, {'num_count': 798, 'sum_payoffs': 189.48830137922457, 'action': [1.0, 1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 173.9638759388888, 'action': [0.0, 0.0]}, {'num_count': 780, 'sum_payoffs': 183.83677753496568, 'action': [1.0, -1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 170.86558598438836, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15692240817213648, 0.1769180612910237, 0.17344055640078243, 0.16257335361877853, 0.16952836339926103, 0.16039991306237775]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.79410815238953 s
