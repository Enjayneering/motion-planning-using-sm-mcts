Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 11, 'sum_payoffs': 2.7822736950596116, 'action': [1.0, 0.0]}, {'num_count': 11, 'sum_payoffs': 2.851103746403728, 'action': [0.0, -1.5707963267948966]}, {'num_count': 11, 'sum_payoffs': 2.674949957646393, 'action': [2.0, 1.5707963267948966]}, {'num_count': 11, 'sum_payoffs': 2.7368373984475456, 'action': [0.0, 1.5707963267948966]}, {'num_count': 11, 'sum_payoffs': 2.5731856536550697, 'action': [0.0, 0.0]}, {'num_count': 11, 'sum_payoffs': 2.9099694407144, 'action': [2.0, -1.5707963267948966]}, {'num_count': 11, 'sum_payoffs': 2.7776466213751787, 'action': [1.0, -1.5707963267948966]}, {'num_count': 11, 'sum_payoffs': 2.8113205197022437, 'action': [2.0, 0.0]}, {'num_count': 12, 'sum_payoffs': 3.2610116882762976, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10891089108910891, 0.10891089108910891, 0.10891089108910891, 0.10891089108910891, 0.10891089108910891, 0.10891089108910891, 0.10891089108910891, 0.10891089108910891, 0.1188118811881188]
Actions to choose Agent 1: dict_values([{'num_count': 16, 'sum_payoffs': 3.585594539741847, 'action': [0.0, -1.5707963267948966]}, {'num_count': 17, 'sum_payoffs': 3.925250446981215, 'action': [1.0, -1.5707963267948966]}, {'num_count': 17, 'sum_payoffs': 4.069879280271067, 'action': [0.0, 0.0]}, {'num_count': 17, 'sum_payoffs': 4.088067698401904, 'action': [1.0, 0.0]}, {'num_count': 16, 'sum_payoffs': 3.4736614350141055, 'action': [0.0, 1.5707963267948966]}, {'num_count': 17, 'sum_payoffs': 4.167272082263192, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15841584158415842, 0.16831683168316833, 0.16831683168316833, 0.16831683168316833, 0.15841584158415842, 0.16831683168316833]
Selected final action: [1.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 2.6936333179473877 s
