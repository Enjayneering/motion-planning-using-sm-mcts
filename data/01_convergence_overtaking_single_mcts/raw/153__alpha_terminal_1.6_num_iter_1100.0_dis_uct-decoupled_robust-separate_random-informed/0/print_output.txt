Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 125, 'sum_payoffs': 31.81830371216646, 'action': [2.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 30.89570491681024, 'action': [0.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 30.443081651669274, 'action': [0.0, 1.5707963267948966]}, {'num_count': 114, 'sum_payoffs': 27.211603240535954, 'action': [1.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 30.95786754408118, 'action': [2.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 30.030395639417094, 'action': [0.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 30.835119978642368, 'action': [2.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 30.959858454441974, 'action': [1.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 32.20003696553107, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11353315168029064, 0.11171662125340599, 0.11080835603996367, 0.10354223433242507, 0.11171662125340599, 0.10990009082652134, 0.11171662125340599, 0.11171662125340599, 0.11444141689373297]
Actions to choose Agent 1: dict_values([{'num_count': 189, 'sum_payoffs': 44.08727520129163, 'action': [1.0, -1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 43.33883099128827, 'action': [1.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 40.4240432530662, 'action': [0.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.07105960685563, 'action': [0.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 40.149910469284976, 'action': [0.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 44.15669546932028, 'action': [1.0, 0.0]}])
Weights num count: [0.17166212534059946, 0.16984559491371481, 0.16257947320617622, 0.16167120799273388, 0.16167120799273388, 0.17166212534059946]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 26.285544633865356 s
