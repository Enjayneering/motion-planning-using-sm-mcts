Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 43.49432339976792, 'action': [0.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 46.59709455209048, 'action': [1.0, -1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 46.20624948603106, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 43.46066766853805, 'action': [2.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 43.53027451993967, 'action': [0.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 43.82624392280336, 'action': [1.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 45.47063765561715, 'action': [2.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 41.938281575658785, 'action': [1.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 43.0528550380308, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1099312929419113, 0.11492816989381636, 0.11430356027482823, 0.1099312929419113, 0.1099312929419113, 0.11055590256089944, 0.11305434103685197, 0.10743285446595878, 0.10930668332292318]
Actions to choose Agent 1: dict_values([{'num_count': 260, 'sum_payoffs': 58.9094336411209, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 65.47214557088886, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 63.50782312378521, 'action': [1.0, 1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 58.898809957719955, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 62.68992420167172, 'action': [1.0, 0.0]}, {'num_count': 257, 'sum_payoffs': 57.8611178656866, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16239850093691444, 0.17426608369768895, 0.17051842598376016, 0.16239850093691444, 0.16926920674578388, 0.16052467207995003]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 38.351983070373535 s
