Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 43.38515395999823, 'action': [1.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 42.88363164032912, 'action': [0.0, -1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 46.21594673919486, 'action': [2.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 45.71127020270225, 'action': [2.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 43.35291609314866, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 45.78950736010739, 'action': [1.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 42.207804601129965, 'action': [0.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 43.472880608229566, 'action': [1.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 43.7112954251344, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1099312929419113, 0.10930668332292318, 0.11430356027482823, 0.1136789506558401, 0.1099312929419113, 0.1136789506558401, 0.1080574640849469, 0.1099312929419113, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 258, 'sum_payoffs': 58.94574623323378, 'action': [0.0, 0.0]}, {'num_count': 268, 'sum_payoffs': 62.3399683072021, 'action': [1.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 65.16360500462741, 'action': [1.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 59.17275342853987, 'action': [0.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 67.32433921514199, 'action': [1.0, 0.0]}, {'num_count': 257, 'sum_payoffs': 58.600852257280124, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16114928169893816, 0.16739537788881947, 0.17239225484072454, 0.1617738913179263, 0.17613991255465333, 0.16052467207995003]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.15930151939392 s
