Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 43.59788620449425, 'action': [0.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 46.04895662750437, 'action': [2.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 45.72867626828459, 'action': [2.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 41.75190653948931, 'action': [0.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 41.791259910458, 'action': [1.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 41.69526759676793, 'action': [0.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 45.63571040764401, 'action': [2.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 45.26618302424457, 'action': [1.0, -1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 47.964390415829776, 'action': [1.0, 0.0]}])
Weights num count: [0.1099312929419113, 0.1136789506558401, 0.11305434103685197, 0.10680824484697064, 0.10680824484697064, 0.10680824484697064, 0.11305434103685197, 0.11242973141786383, 0.11680199875078076]
Actions to choose Agent 1: dict_values([{'num_count': 269, 'sum_payoffs': 62.00517231683999, 'action': [1.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 62.35632483529937, 'action': [0.0, 0.0]}, {'num_count': 267, 'sum_payoffs': 61.34184560718456, 'action': [1.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 65.87399319556798, 'action': [1.0, 0.0]}, {'num_count': 254, 'sum_payoffs': 56.76204843729937, 'action': [0.0, -1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 58.86396029128793, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1680199875078076, 0.16864459712679575, 0.16677076826983137, 0.1748906933166771, 0.15865084322298564, 0.16239850093691444]
Selected final action: [1.0, 0.0, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 38.144869565963745 s
