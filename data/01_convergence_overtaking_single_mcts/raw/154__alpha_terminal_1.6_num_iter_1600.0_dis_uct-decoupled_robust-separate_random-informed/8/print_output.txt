Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 43.55690391200271, 'action': [1.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 46.71679574837988, 'action': [2.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 45.593253148304896, 'action': [2.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 42.85187868549666, 'action': [1.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 45.9684002687743, 'action': [2.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 43.19569694452581, 'action': [0.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 45.6410444746022, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 43.6789445928742, 'action': [0.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 41.69029441692114, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1099312929419113, 0.11492816989381636, 0.11305434103685197, 0.10868207370393504, 0.1136789506558401, 0.10930668332292318, 0.11305434103685197, 0.1099312929419113, 0.10680824484697064]
Actions to choose Agent 1: dict_values([{'num_count': 274, 'sum_payoffs': 63.83141145205303, 'action': [1.0, -1.5707963267948966]}, {'num_count': 262, 'sum_payoffs': 59.497489437394506, 'action': [0.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 66.15726692137008, 'action': [1.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 63.128514376623606, 'action': [1.0, 1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 59.29082750645468, 'action': [0.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 55.458235004998976, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1711430356027483, 0.16364772017489068, 0.1755153029356652, 0.16989381636477202, 0.16302311055590257, 0.1561524047470331]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 37.8806266784668 s
