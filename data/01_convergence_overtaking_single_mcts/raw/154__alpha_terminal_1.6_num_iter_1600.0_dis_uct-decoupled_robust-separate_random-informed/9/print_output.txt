Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 182, 'sum_payoffs': 45.86052093720711, 'action': [1.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 42.80743682908009, 'action': [0.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 43.96030558793593, 'action': [2.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 43.51700115938764, 'action': [1.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 44.02830369527147, 'action': [0.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 45.43860292027985, 'action': [2.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 42.754354676752484, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 45.82658107234688, 'action': [2.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 43.961882935837075, 'action': [1.0, 0.0]}])
Weights num count: [0.1136789506558401, 0.10868207370393504, 0.11055590256089944, 0.1099312929419113, 0.11055590256089944, 0.11305434103685197, 0.10868207370393504, 0.1136789506558401, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 280, 'sum_payoffs': 65.94863049030494, 'action': [1.0, -1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 57.26241784557161, 'action': [0.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 63.790547108435035, 'action': [1.0, 0.0]}, {'num_count': 260, 'sum_payoffs': 59.05139868665879, 'action': [0.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 62.44578967655038, 'action': [1.0, 1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 59.37792423791388, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1748906933166771, 0.15927545284197375, 0.1711430356027483, 0.16239850093691444, 0.16864459712679575, 0.16302311055590257]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 37.73445153236389 s
