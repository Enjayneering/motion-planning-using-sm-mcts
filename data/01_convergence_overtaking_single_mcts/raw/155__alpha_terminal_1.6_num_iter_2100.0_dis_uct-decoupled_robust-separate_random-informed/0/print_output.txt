Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 243, 'sum_payoffs': 62.10080242606552, 'action': [2.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 55.19876806694273, 'action': [1.0, 0.0]}, {'num_count': 222, 'sum_payoffs': 54.13031739761416, 'action': [0.0, 1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 58.59733747141187, 'action': [1.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 60.853434511682956, 'action': [2.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 61.68935407006805, 'action': [1.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 60.14325438141533, 'action': [2.0, 1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 55.26734537769816, 'action': [0.0, -1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 57.444549936616, 'action': [0.0, 0.0]}])
Weights num count: [0.11565920990004759, 0.10709186101856259, 0.10566396953831508, 0.11137553545930509, 0.1142313184198001, 0.11518324607329843, 0.11327939076630177, 0.10709186101856259, 0.1099476439790576]
Actions to choose Agent 1: dict_values([{'num_count': 337, 'sum_payoffs': 76.13098496548396, 'action': [0.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 87.16020308764162, 'action': [1.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 81.47142579160503, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 76.05376994410501, 'action': [0.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 75.73301993054113, 'action': [0.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 86.18748472930054, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1603998096144693, 0.17610661589719181, 0.16801523084245598, 0.1603998096144693, 0.15992384578772012, 0.17467872441694432]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.57539772987366 s
