Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 234, 'sum_payoffs': 58.47013159299387, 'action': [0.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 58.811137402149, 'action': [1.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 61.88305791105535, 'action': [2.0, -1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 57.37689128197282, 'action': [1.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 58.57833383367571, 'action': [1.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 58.89299999240477, 'action': [2.0, 1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 55.36170168855698, 'action': [0.0, 1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 54.25969780780359, 'action': [0.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 60.46942254146551, 'action': [2.0, 0.0]}])
Weights num count: [0.11137553545930509, 0.11185149928605426, 0.11565920990004759, 0.1099476439790576, 0.11137553545930509, 0.11185149928605426, 0.10756782484531176, 0.10613993336506425, 0.11375535459305093]
Actions to choose Agent 1: dict_values([{'num_count': 326, 'sum_payoffs': 72.53546232586477, 'action': [0.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 86.14655522935858, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 83.86403860746759, 'action': [1.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 80.0790016727126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 77.14745597666575, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 83.20889813253203, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15516420752022847, 0.17467872441694432, 0.17134697762970014, 0.1661113755354593, 0.1618277010947168, 0.1703950499762018]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.74108028411865 s
