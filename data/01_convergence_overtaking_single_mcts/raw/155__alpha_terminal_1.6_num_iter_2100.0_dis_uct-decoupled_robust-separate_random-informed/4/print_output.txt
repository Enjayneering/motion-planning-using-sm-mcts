Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 225, 'sum_payoffs': 54.90376392085705, 'action': [0.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 56.7424913285108, 'action': [1.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 58.9603445966195, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 60.12203449530142, 'action': [2.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 57.09922797288192, 'action': [1.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 60.197731645288705, 'action': [1.0, -1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 57.848438770703744, 'action': [2.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 60.126594063395004, 'action': [2.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 56.05100264180867, 'action': [0.0, 0.0]}])
Weights num count: [0.10709186101856259, 0.10947168015230842, 0.11232746311280342, 0.11375535459305093, 0.1099476439790576, 0.11375535459305093, 0.11089957163255593, 0.11375535459305093, 0.10851975249881009]
Actions to choose Agent 1: dict_values([{'num_count': 342, 'sum_payoffs': 77.46473205293525, 'action': [0.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 83.85349645825757, 'action': [1.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 83.06790017915095, 'action': [1.0, -1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 87.52078016774769, 'action': [1.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 73.80594866962122, 'action': [0.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 75.17968393347766, 'action': [0.0, 0.0]}])
Weights num count: [0.16277962874821514, 0.1718229414564493, 0.17087101380295097, 0.17705854355069015, 0.1575440266539743, 0.15944788196097096]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.00674104690552 s
