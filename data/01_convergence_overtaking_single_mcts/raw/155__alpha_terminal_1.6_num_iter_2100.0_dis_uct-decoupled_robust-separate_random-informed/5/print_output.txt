Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 238, 'sum_payoffs': 60.00299869489947, 'action': [2.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 55.744758010073625, 'action': [0.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 56.57692300378932, 'action': [0.0, -1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 55.431678480297286, 'action': [1.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 59.978598032818006, 'action': [2.0, -1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 56.943548197235664, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 58.85482308679129, 'action': [1.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 59.82407962402501, 'action': [2.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 60.34633649307462, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11327939076630177, 0.10804378867206092, 0.10899571632555925, 0.10756782484531176, 0.11327939076630177, 0.10947168015230842, 0.11185149928605426, 0.11327939076630177, 0.11375535459305093]
Actions to choose Agent 1: dict_values([{'num_count': 343, 'sum_payoffs': 78.35221257263164, 'action': [0.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 88.8623946928142, 'action': [1.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 77.08989732533308, 'action': [0.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 74.78343272802286, 'action': [0.0, -1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 85.18857297762145, 'action': [1.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 80.42658708271155, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1632555925749643, 0.17801047120418848, 0.16135173726796764, 0.15801999048072346, 0.17277486910994763, 0.1661113755354593]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 51.61361527442932 s
