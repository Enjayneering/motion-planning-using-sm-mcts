Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 243, 'sum_payoffs': 62.05342360161634, 'action': [2.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 56.65920535933437, 'action': [0.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 55.99182094305989, 'action': [1.0, 0.0]}, {'num_count': 222, 'sum_payoffs': 54.15010576923642, 'action': [0.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 54.06038933401409, 'action': [0.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 58.643220096008115, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 60.5292433877068, 'action': [2.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 61.25280180641086, 'action': [1.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 61.997699650780284, 'action': [2.0, 0.0]}])
Weights num count: [0.11565920990004759, 0.10899571632555925, 0.10804378867206092, 0.10566396953831508, 0.10566396953831508, 0.11137553545930509, 0.11375535459305093, 0.11470728224654926, 0.11565920990004759]
Actions to choose Agent 1: dict_values([{'num_count': 343, 'sum_payoffs': 78.2794367099786, 'action': [0.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 84.63147398549864, 'action': [1.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 85.35960652390426, 'action': [1.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 74.60680836167705, 'action': [0.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 74.97161228360055, 'action': [0.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 86.04984317385525, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1632555925749643, 0.17229890528319847, 0.17325083293669682, 0.15801999048072346, 0.15849595430747263, 0.17420276059019515]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 52.109323263168335 s
