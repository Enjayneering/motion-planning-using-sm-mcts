Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 281, 'sum_payoffs': 68.84444299779243, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 70.37279187228972, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 67.37751633513801, 'action': [0.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 69.67887413386929, 'action': [1.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 76.18987722770862, 'action': [2.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 73.32445856493587, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 70.37809918227411, 'action': [2.0, 1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 77.25221957949928, 'action': [2.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 72.6012517110572, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10803537101114956, 0.10957324106113034, 0.10649750096116878, 0.10880430603613994, 0.11572472126105345, 0.11264898116109189, 0.10957324106113034, 0.11687812379853903, 0.1118800461361015]
Actions to choose Agent 1: dict_values([{'num_count': 444, 'sum_payoffs': 103.69316881577733, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 95.21694100470214, 'action': [0.0, -1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 96.46630857324695, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 110.26568796663281, 'action': [1.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 93.21394036585443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 102.35520901624814, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1707035755478662, 0.16070742022299117, 0.16224529027297194, 0.17839292579777008, 0.15840061514801998, 0.16916570549788543]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 56.64551401138306 s
