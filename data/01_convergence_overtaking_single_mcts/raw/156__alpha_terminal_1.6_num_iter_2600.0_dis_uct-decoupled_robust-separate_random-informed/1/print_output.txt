Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 291, 'sum_payoffs': 72.96036207594341, 'action': [1.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 76.70639014577927, 'action': [2.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 68.25002515973426, 'action': [0.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 73.39527896049435, 'action': [1.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 69.66497610704599, 'action': [0.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 70.74231299602782, 'action': [1.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 71.87816063948635, 'action': [2.0, 1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 77.387842448943, 'action': [2.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 68.93788866823198, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1118800461361015, 0.11572472126105345, 0.10688196847366398, 0.1122645136485967, 0.10841983852364476, 0.10957324106113034, 0.11072664359861592, 0.11649365628604383, 0.10765090349865436]
Actions to choose Agent 1: dict_values([{'num_count': 413, 'sum_payoffs': 92.84332568827953, 'action': [0.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 108.64850564384207, 'action': [1.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 91.52009841227814, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 105.1221029612408, 'action': [1.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 103.18832567790847, 'action': [1.0, -1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 94.98426287790524, 'action': [0.0, 0.0]}])
Weights num count: [0.1587850826605152, 0.1776239907727797, 0.1572472126105344, 0.17339484813533257, 0.1710880430603614, 0.16147635524798154]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 56.73392295837402 s
