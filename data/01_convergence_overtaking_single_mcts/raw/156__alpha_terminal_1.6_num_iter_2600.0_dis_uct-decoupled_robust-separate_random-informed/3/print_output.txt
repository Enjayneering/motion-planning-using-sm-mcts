Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 287, 'sum_payoffs': 71.35487945936218, 'action': [1.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 68.71698979247542, 'action': [1.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 73.92619322757169, 'action': [2.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 68.41164963662494, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 71.23166374593812, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 70.26554208127352, 'action': [0.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 74.94910714379299, 'action': [2.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 74.568467592081, 'action': [2.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 74.59499189426802, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11034217608612072, 0.10765090349865436, 0.11303344867358708, 0.10726643598615918, 0.11034217608612072, 0.10918877354863514, 0.11418685121107267, 0.11380238369857747, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 456, 'sum_payoffs': 106.90953499592821, 'action': [1.0, -1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 89.76069009985845, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 94.88767126257235, 'action': [0.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 104.95626736046582, 'action': [1.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 104.25560561325385, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 96.45013823413504, 'action': [0.0, 0.0]}])
Weights num count: [0.17531718569780855, 0.15494040753556323, 0.16109188773548636, 0.17301038062283736, 0.172241445597847, 0.16301422529796233]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 56.92245388031006 s
