Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 286, 'sum_payoffs': 70.9206720224705, 'action': [1.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 72.31075488461845, 'action': [1.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 74.55069395592996, 'action': [2.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 68.71746921509072, 'action': [0.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 68.59489803074271, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 69.79359571017584, 'action': [0.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 74.52057960228792, 'action': [2.0, 0.0]}, {'num_count': 303, 'sum_payoffs': 77.12450229795849, 'action': [2.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 70.81323288705033, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10995770857362552, 0.1114955786236063, 0.11380238369857747, 0.10765090349865436, 0.10765090349865436, 0.10880430603613994, 0.11380238369857747, 0.11649365628604383, 0.10995770857362552]
Actions to choose Agent 1: dict_values([{'num_count': 429, 'sum_payoffs': 98.22003928615501, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 106.38569405845122, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 102.14314274789339, 'action': [1.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 104.5066254424265, 'action': [1.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 91.13810370098795, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 95.62527533569815, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16493656286043828, 0.17454825067281815, 0.1695501730103806, 0.172241445597847, 0.15647827758554403, 0.16186082276047675]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.71029877662659 s
