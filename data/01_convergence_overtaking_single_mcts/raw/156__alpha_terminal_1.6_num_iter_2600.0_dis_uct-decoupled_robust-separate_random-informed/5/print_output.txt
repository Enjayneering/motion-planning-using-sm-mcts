Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 295, 'sum_payoffs': 74.48908247004107, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 70.73558063559166, 'action': [1.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 71.74439662375757, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 70.77968170335349, 'action': [0.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 79.22165144360983, 'action': [2.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 67.46493964488432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 70.06818165586373, 'action': [1.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 74.12887315959966, 'action': [2.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 71.11884278285562, 'action': [0.0, 0.0]}])
Weights num count: [0.11341791618608228, 0.10957324106113034, 0.11072664359861592, 0.10957324106113034, 0.1184159938485198, 0.1061130334486736, 0.10880430603613994, 0.11303344867358708, 0.10995770857362552]
Actions to choose Agent 1: dict_values([{'num_count': 446, 'sum_payoffs': 103.11349768541194, 'action': [1.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 89.54643379677157, 'action': [0.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 107.01103599499703, 'action': [1.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 92.51359278079502, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 107.0931902817742, 'action': [1.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 94.98357230355766, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1714725105728566, 0.15532487504805845, 0.17608612072279892, 0.1587850826605152, 0.17608612072279892, 0.16186082276047675]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.378594398498535 s
