Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 298, 'sum_payoffs': 75.0252931291038, 'action': [2.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 72.46260668542446, 'action': [1.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 68.84074037116677, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 69.87167580049966, 'action': [1.0, 0.0]}, {'num_count': 299, 'sum_payoffs': 75.36380946259412, 'action': [2.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 70.95880358129644, 'action': [1.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 67.30370295363588, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 70.60497052000319, 'action': [0.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 74.67335030959512, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11457131872356786, 0.1118800461361015, 0.10803537101114956, 0.10918877354863514, 0.11495578623606305, 0.11034217608612072, 0.10649750096116878, 0.10995770857362552, 0.11418685121107267]
Actions to choose Agent 1: dict_values([{'num_count': 412, 'sum_payoffs': 92.6918477767077, 'action': [0.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 104.66506231344894, 'action': [1.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 91.95977006892838, 'action': [0.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 106.67355607781947, 'action': [1.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 108.19180282327154, 'action': [1.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 93.29156695030744, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15840061514801998, 0.17262591311034217, 0.1576316801230296, 0.17493271818531334, 0.1768550557477893, 0.15916955017301038]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 58.21501445770264 s
