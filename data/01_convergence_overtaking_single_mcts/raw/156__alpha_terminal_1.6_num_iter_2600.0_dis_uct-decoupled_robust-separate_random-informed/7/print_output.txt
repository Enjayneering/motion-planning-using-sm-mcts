Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 285, 'sum_payoffs': 70.08947585514255, 'action': [0.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 69.37988822355234, 'action': [1.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 68.21372908324271, 'action': [0.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 71.66264354330406, 'action': [1.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 72.3248162927745, 'action': [1.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 71.58903330309687, 'action': [0.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 74.09018861396557, 'action': [2.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 73.38372450086507, 'action': [2.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 73.06515511914235, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10957324106113034, 0.10880430603613994, 0.10765090349865436, 0.1111111111111111, 0.1118800461361015, 0.1111111111111111, 0.11380238369857747, 0.11303344867358708, 0.11264898116109189]
Actions to choose Agent 1: dict_values([{'num_count': 449, 'sum_payoffs': 104.35237382127487, 'action': [1.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 91.74660079423951, 'action': [0.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 94.34379569718249, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 94.63351762370293, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 105.02718257023697, 'action': [1.0, 0.0]}, {'num_count': 453, 'sum_payoffs': 105.62973813335164, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17262591311034217, 0.1576316801230296, 0.16070742022299117, 0.16109188773548636, 0.17339484813533257, 0.17416378316032297]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.32404541969299 s
