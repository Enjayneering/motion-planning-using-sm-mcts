Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 281, 'sum_payoffs': 68.94426671697073, 'action': [0.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 71.85401853498433, 'action': [1.0, 0.0]}, {'num_count': 307, 'sum_payoffs': 78.3826801452358, 'action': [2.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 66.39898578265766, 'action': [0.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 70.75518728699232, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 70.32814462210885, 'action': [1.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 72.95744354052243, 'action': [1.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 75.01331389378704, 'action': [2.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 71.4465155700231, 'action': [0.0, 0.0]}])
Weights num count: [0.10803537101114956, 0.1111111111111111, 0.11803152633602461, 0.1053440984236832, 0.10995770857362552, 0.10957324106113034, 0.1122645136485967, 0.11457131872356786, 0.11072664359861592]
Actions to choose Agent 1: dict_values([{'num_count': 421, 'sum_payoffs': 95.63573286263058, 'action': [0.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 103.99731506052325, 'action': [1.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 107.95302650240562, 'action': [1.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 91.38455886647071, 'action': [0.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 92.95048677586412, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 105.65170405385658, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16186082276047675, 0.17185697808535177, 0.17647058823529413, 0.1568627450980392, 0.1587850826605152, 0.17377931564782775]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.38345909118652 s
