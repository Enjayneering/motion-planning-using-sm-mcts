Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 278, 'sum_payoffs': 67.93831831115766, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 70.16144853841105, 'action': [0.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 71.93894886325187, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 68.33973935705042, 'action': [0.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 71.58538358774084, 'action': [1.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 77.74314671481369, 'action': [2.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 71.96171998865573, 'action': [2.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 70.07721984909281, 'action': [1.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 77.43607663928178, 'action': [2.0, 0.0]}])
Weights num count: [0.10688196847366398, 0.10918877354863514, 0.1111111111111111, 0.10726643598615918, 0.11072664359861592, 0.11726259131103421, 0.1111111111111111, 0.10918877354863514, 0.11687812379853903]
Actions to choose Agent 1: dict_values([{'num_count': 410, 'sum_payoffs': 92.34649456534552, 'action': [0.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 102.32361917502364, 'action': [1.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 94.78441304961434, 'action': [0.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 96.2522626348182, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 105.97028992337263, 'action': [1.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 107.65687928584043, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1576316801230296, 0.1695501730103806, 0.16070742022299117, 0.16224529027297194, 0.17377931564782775, 0.17570165321030373]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.84579586982727 s
