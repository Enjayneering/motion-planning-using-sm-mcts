Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 339, 'sum_payoffs': 83.1861556724064, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 84.19693286468024, 'action': [1.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 86.73084969216879, 'action': [2.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 89.15485957741218, 'action': [2.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 85.26578919608232, 'action': [1.0, 1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 80.99430636166137, 'action': [0.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 89.21292475791905, 'action': [2.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 83.19527400303596, 'action': [0.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 83.81089523142954, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10931957433086101, 0.11028700419219607, 0.11254434053531119, 0.11480167687842631, 0.11125443405353112, 0.1073847146081909, 0.11480167687842631, 0.10931957433086101, 0.10996452757175104]
Actions to choose Agent 1: dict_values([{'num_count': 506, 'sum_payoffs': 114.72088244409866, 'action': [0.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 121.66841894240675, 'action': [1.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 112.18529743875031, 'action': [0.0, -1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 123.64976590647983, 'action': [1.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 113.50741301604845, 'action': [0.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 123.03699906096718, 'action': [1.0, 0.0]}])
Weights num count: [0.16317316994517897, 0.17026765559496937, 0.16059335698161883, 0.17220251531763947, 0.1618832634633989, 0.17155756207674944]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.6191155910492 s
