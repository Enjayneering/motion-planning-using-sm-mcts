Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 372, 'sum_payoffs': 95.19436251207455, 'action': [2.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 89.50577516415537, 'action': [2.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 79.78204795730963, 'action': [0.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 84.37036345013861, 'action': [2.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 80.9549561901669, 'action': [1.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 81.98459982930862, 'action': [0.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 86.51840606490273, 'action': [1.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 83.01191379479937, 'action': [0.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 86.6489901036075, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1199613028055466, 0.11480167687842631, 0.10609480812641084, 0.11028700419219607, 0.10706223798774589, 0.10802966784908094, 0.11222186391486617, 0.10899709771041599, 0.11222186391486617]
Actions to choose Agent 1: dict_values([{'num_count': 497, 'sum_payoffs': 111.16333568965574, 'action': [0.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 112.54931682903579, 'action': [0.0, 0.0]}, {'num_count': 544, 'sum_payoffs': 126.1430839066491, 'action': [1.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 121.9337993958681, 'action': [1.0, -1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 122.29878370640023, 'action': [1.0, 1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 110.6482749725931, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16027088036117382, 0.16156078684295389, 0.17542728152208964, 0.17123508545630442, 0.17155756207674944, 0.15962592712028378]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.11176681518555 s
