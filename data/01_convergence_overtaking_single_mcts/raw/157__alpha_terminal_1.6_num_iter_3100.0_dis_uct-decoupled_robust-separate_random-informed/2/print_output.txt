Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 354, 'sum_payoffs': 88.50090892911356, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 81.72070792164641, 'action': [1.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 84.6010870875867, 'action': [1.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 83.87181551840436, 'action': [0.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 87.367907877577, 'action': [1.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 88.1392490600176, 'action': [2.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 88.16104417349796, 'action': [2.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 82.7664366632188, 'action': [0.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 80.72296840258909, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11415672363753628, 0.10802966784908094, 0.11060948081264109, 0.10996452757175104, 0.11318929377620122, 0.11383424701709126, 0.11383424701709126, 0.10899709771041599, 0.10706223798774589]
Actions to choose Agent 1: dict_values([{'num_count': 508, 'sum_payoffs': 114.80298204701428, 'action': [0.0, 1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 122.70918992763396, 'action': [1.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 120.849618539081, 'action': [1.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 108.51531829144375, 'action': [0.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 111.04205489357946, 'action': [0.0, 0.0]}, {'num_count': 548, 'sum_payoffs': 127.52231360126429, 'action': [1.0, 0.0]}])
Weights num count: [0.163818123186069, 0.17188003869719445, 0.16994517897452435, 0.15736859077716867, 0.1599484037407288, 0.1767171880038697]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.62548565864563 s
