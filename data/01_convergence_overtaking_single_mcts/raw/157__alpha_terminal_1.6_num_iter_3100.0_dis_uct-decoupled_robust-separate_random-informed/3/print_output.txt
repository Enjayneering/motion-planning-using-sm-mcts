Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 344, 'sum_payoffs': 84.893333526129, 'action': [1.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 79.5469651899676, 'action': [0.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 88.02969157355804, 'action': [2.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 83.53199104144356, 'action': [1.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 83.18167553023315, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 82.83586143973312, 'action': [0.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 85.87552322609244, 'action': [2.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 85.2073144096692, 'action': [1.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 92.39702992559435, 'action': [2.0, 0.0]}])
Weights num count: [0.1109319574330861, 0.10609480812641084, 0.11383424701709126, 0.10964205095130602, 0.10931957433086101, 0.10899709771041599, 0.11189938729442116, 0.11125443405353112, 0.11770396646243148]
Actions to choose Agent 1: dict_values([{'num_count': 527, 'sum_payoffs': 121.37430157478747, 'action': [1.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 111.92626789414716, 'action': [0.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 130.27227812467473, 'action': [1.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 122.33158960856278, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 111.23143150293353, 'action': [0.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 111.56276394932833, 'action': [0.0, 0.0]}])
Weights num count: [0.16994517897452435, 0.16027088036117382, 0.17897452434698485, 0.1709126088358594, 0.15962592712028378, 0.1599484037407288]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 69.71326947212219 s
