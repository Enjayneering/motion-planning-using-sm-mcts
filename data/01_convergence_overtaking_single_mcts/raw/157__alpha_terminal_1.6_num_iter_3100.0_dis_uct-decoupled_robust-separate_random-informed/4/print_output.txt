Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 360, 'sum_payoffs': 90.90485529392953, 'action': [2.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 83.94074737733678, 'action': [1.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 80.87158569381715, 'action': [0.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 86.2578767804891, 'action': [1.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 88.30933662514124, 'action': [2.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 85.51144797023709, 'action': [1.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 81.22730269260896, 'action': [0.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 79.87667580603198, 'action': [0.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 90.92325523459716, 'action': [2.0, 0.0]}])
Weights num count: [0.11609158336020639, 0.10996452757175104, 0.10706223798774589, 0.11189938729442116, 0.11383424701709126, 0.11125443405353112, 0.1073847146081909, 0.10609480812641084, 0.11609158336020639]
Actions to choose Agent 1: dict_values([{'num_count': 528, 'sum_payoffs': 121.12442568346955, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 111.61444351319082, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 111.8605658848601, 'action': [0.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 122.35927986160965, 'action': [1.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 112.44735628100244, 'action': [0.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 125.54669360959845, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17026765559496937, 0.16059335698161883, 0.16091583360206385, 0.17155756207674944, 0.16156078684295389, 0.1747823282811996]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 69.37402296066284 s
