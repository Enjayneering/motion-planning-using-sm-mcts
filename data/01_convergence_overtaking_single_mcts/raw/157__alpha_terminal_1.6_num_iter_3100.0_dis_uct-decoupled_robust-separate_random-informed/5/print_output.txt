Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 348, 'sum_payoffs': 86.39602069969075, 'action': [2.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 90.55931709102863, 'action': [2.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 87.08949531256931, 'action': [1.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 83.93139353010353, 'action': [0.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 82.47076002804535, 'action': [0.0, 0.0]}, {'num_count': 330, 'sum_payoffs': 79.8840623250407, 'action': [1.0, 0.0]}, {'num_count': 347, 'sum_payoffs': 86.0351915146862, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 82.03113048746158, 'action': [0.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 87.46653262182836, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11222186391486617, 0.11609158336020639, 0.11286681715575621, 0.10996452757175104, 0.10867462108997097, 0.10641728474685586, 0.11189938729442116, 0.10835214446952596, 0.11318929377620122]
Actions to choose Agent 1: dict_values([{'num_count': 501, 'sum_payoffs': 113.73368101033418, 'action': [0.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 111.81048384027017, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 113.38786865285549, 'action': [0.0, -1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 129.9197034829822, 'action': [1.0, 0.0]}, {'num_count': 521, 'sum_payoffs': 120.07391644748068, 'action': [1.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 123.19003929479152, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16156078684295389, 0.15962592712028378, 0.16123831022250887, 0.1780070944856498, 0.16801031925185425, 0.17123508545630442]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 104.30703496932983 s
