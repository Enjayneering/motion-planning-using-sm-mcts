Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 354, 'sum_payoffs': 88.81199224705831, 'action': [2.0, -1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 79.64516156464994, 'action': [0.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 89.5648993114831, 'action': [2.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 82.77965144102626, 'action': [1.0, 0.0]}, {'num_count': 327, 'sum_payoffs': 79.29442001164925, 'action': [0.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 85.33082033397052, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 90.22516304647334, 'action': [2.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 87.7149186985485, 'action': [1.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 85.68339139224325, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11415672363753628, 0.10577233150596582, 0.11480167687842631, 0.10867462108997097, 0.1054498548855208, 0.1109319574330861, 0.11544663011931634, 0.11318929377620122, 0.11125443405353112]
Actions to choose Agent 1: dict_values([{'num_count': 543, 'sum_payoffs': 125.67273235952726, 'action': [1.0, 0.0]}, {'num_count': 546, 'sum_payoffs': 126.54923614510622, 'action': [1.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 113.60985002801134, 'action': [0.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 109.54740143075645, 'action': [0.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 106.66438224443216, 'action': [0.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 121.87243796928638, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17510480490164462, 0.17607223476297967, 0.16285069332473395, 0.15865849725894873, 0.15575620767494355, 0.17123508545630442]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 161.24388146400452 s
