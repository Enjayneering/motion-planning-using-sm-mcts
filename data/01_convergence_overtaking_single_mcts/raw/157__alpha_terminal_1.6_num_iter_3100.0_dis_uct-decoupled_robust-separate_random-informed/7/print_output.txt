Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 353, 'sum_payoffs': 87.83054806277966, 'action': [2.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 88.58690975018962, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 83.89218805884062, 'action': [1.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 86.4186429637424, 'action': [2.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 79.07624519098302, 'action': [0.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 84.69309648289202, 'action': [0.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 85.78569691281365, 'action': [1.0, 1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 86.47716801289876, 'action': [1.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 80.84835836084979, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11383424701709126, 0.11447920025798129, 0.11028700419219607, 0.11254434053531119, 0.10577233150596582, 0.1109319574330861, 0.11189938729442116, 0.11254434053531119, 0.1073847146081909]
Actions to choose Agent 1: dict_values([{'num_count': 495, 'sum_payoffs': 111.35974643736769, 'action': [0.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 126.28974872773524, 'action': [1.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 119.29309009996895, 'action': [1.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 112.97174360748744, 'action': [0.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 126.2812325118704, 'action': [1.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 113.16579578505413, 'action': [0.0, 0.0]}])
Weights num count: [0.15962592712028378, 0.1747823282811996, 0.16768784263140923, 0.16123831022250887, 0.1747823282811996, 0.16156078684295389]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 193.159814119339 s
