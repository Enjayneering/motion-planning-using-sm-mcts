Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 328, 'sum_payoffs': 79.37472950317226, 'action': [0.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 90.14205228150017, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 84.44064883200485, 'action': [1.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 82.29407304448543, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 85.5143349624417, 'action': [2.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 88.31469964148414, 'action': [2.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 84.04511563383531, 'action': [0.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 82.23805344333493, 'action': [0.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 91.05903399621786, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10577233150596582, 0.11544663011931634, 0.11028700419219607, 0.10835214446952596, 0.11125443405353112, 0.11383424701709126, 0.10996452757175104, 0.10835214446952596, 0.11641405998065141]
Actions to choose Agent 1: dict_values([{'num_count': 524, 'sum_payoffs': 120.07968383297742, 'action': [1.0, 1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 127.19605545792555, 'action': [1.0, -1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 127.19021717560946, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 112.0237786908873, 'action': [0.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 107.20918827166115, 'action': [0.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 113.54826380612812, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1689777491131893, 0.17607223476297967, 0.17607223476297967, 0.16059335698161883, 0.15575620767494355, 0.16220574008384392]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 184.3418209552765 s
