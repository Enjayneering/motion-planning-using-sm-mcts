Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 340, 'sum_payoffs': 83.52443733251795, 'action': [1.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 81.14534684349616, 'action': [0.0, 0.0]}, {'num_count': 334, 'sum_payoffs': 81.37687390259516, 'action': [0.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 84.68100724399227, 'action': [2.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 80.43109386017909, 'action': [0.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 86.76875283916698, 'action': [1.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 90.3221360083837, 'action': [2.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 85.0426114030858, 'action': [1.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 93.21197354637556, 'action': [2.0, 0.0]}])
Weights num count: [0.10964205095130602, 0.1073847146081909, 0.10770719122863592, 0.11060948081264109, 0.10673976136730087, 0.11254434053531119, 0.11576910673976137, 0.1109319574330861, 0.11834891970332151]
Actions to choose Agent 1: dict_values([{'num_count': 495, 'sum_payoffs': 111.3872592195048, 'action': [0.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 114.51920658650603, 'action': [0.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 124.65671020261016, 'action': [1.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 121.21943459461251, 'action': [1.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 111.01408390652563, 'action': [0.0, 0.0]}, {'num_count': 543, 'sum_payoffs': 126.5751936107264, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15962592712028378, 0.16285069332473395, 0.17316994517897452, 0.16962270235407934, 0.15930345049983877, 0.17510480490164462]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 185.6733648777008 s
