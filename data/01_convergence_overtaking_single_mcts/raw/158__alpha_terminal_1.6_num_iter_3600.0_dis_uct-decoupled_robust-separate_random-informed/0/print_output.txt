Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 425, 'sum_payoffs': 106.64393629488168, 'action': [2.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 102.8673422128799, 'action': [2.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 101.47928853659651, 'action': [2.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 100.8574777354093, 'action': [1.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 97.68138956274869, 'action': [1.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 93.92561722950443, 'action': [0.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 95.23874917349883, 'action': [1.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 90.42025749233127, 'action': [0.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 93.13445535528972, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11802277145237434, 0.11496806442654818, 0.11385726187170231, 0.11330186059427937, 0.11080255484587614, 0.10774784782004998, 0.10885865037489587, 0.1049708414329353, 0.10719244654262705]
Actions to choose Agent 1: dict_values([{'num_count': 579, 'sum_payoffs': 128.9965448901764, 'action': [0.0, -1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 141.0358074509434, 'action': [1.0, 0.0]}, {'num_count': 591, 'sum_payoffs': 132.71949350335942, 'action': [0.0, 0.0]}, {'num_count': 580, 'sum_payoffs': 129.36109408460536, 'action': [0.0, 1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 141.0294856362986, 'action': [1.0, -1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 139.81517044090668, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16078866981394058, 0.17161899472368786, 0.1641210774784782, 0.16106637045265204, 0.17161899472368786, 0.170508192168842]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 228.03767085075378 s
