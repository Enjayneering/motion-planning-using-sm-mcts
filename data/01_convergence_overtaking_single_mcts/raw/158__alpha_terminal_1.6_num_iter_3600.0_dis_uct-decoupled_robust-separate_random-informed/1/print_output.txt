Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 406, 'sum_payoffs': 100.30514225213741, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 102.01384468556891, 'action': [2.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 89.55396972920164, 'action': [0.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 91.98487435858316, 'action': [0.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 104.75603421991705, 'action': [2.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 105.189338834611, 'action': [2.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 97.12367711286919, 'action': [1.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 95.4991556112023, 'action': [0.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 97.5152321939901, 'action': [1.0, 0.0]}])
Weights num count: [0.11274645931685642, 0.11413496251041377, 0.10413773951680089, 0.10608164398778117, 0.11635656762010553, 0.116634268258817, 0.1102471535684532, 0.10885865037489587, 0.11052485420716468]
Actions to choose Agent 1: dict_values([{'num_count': 615, 'sum_payoffs': 139.83988061383772, 'action': [1.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 127.49630484673654, 'action': [0.0, 0.0]}, {'num_count': 630, 'sum_payoffs': 144.3813755333763, 'action': [1.0, -1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 124.16395601783277, 'action': [0.0, -1.5707963267948966]}, {'num_count': 630, 'sum_payoffs': 144.37320521703808, 'action': [1.0, 0.0]}, {'num_count': 586, 'sum_payoffs': 130.89462668328682, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17078589280755346, 0.1596778672590947, 0.17495140238822549, 0.15662316023326853, 0.17495140238822549, 0.16273257428492086]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 226.99495315551758 s
