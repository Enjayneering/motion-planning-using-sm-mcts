Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 95.97273824619568, 'action': [0.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 105.67778968268863, 'action': [2.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 101.82760750327873, 'action': [2.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 94.50640522679163, 'action': [1.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 94.89252047624792, 'action': [1.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 100.35334388053067, 'action': [2.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 95.28156645527106, 'action': [0.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 93.20495139899161, 'action': [0.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 100.41282879579236, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1094140516523188, 0.11718966953623994, 0.11413496251041377, 0.10830324909747292, 0.10858094973618439, 0.1130241599555679, 0.10885865037489587, 0.10719244654262705, 0.1130241599555679]
Actions to choose Agent 1: dict_values([{'num_count': 560, 'sum_payoffs': 123.25076377041012, 'action': [0.0, -1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 150.69409461279108, 'action': [1.0, -1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 139.261495963863, 'action': [1.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 145.73470216994306, 'action': [1.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 128.10441125520796, 'action': [0.0, 0.0]}, {'num_count': 570, 'sum_payoffs': 126.26164788229086, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15551235767842267, 0.1802277145237434, 0.16995279089141904, 0.1757845043043599, 0.15995556789780616, 0.15828936406553734]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 209.5949625968933 s
