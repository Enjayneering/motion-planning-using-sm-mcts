Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 401, 'sum_payoffs': 98.39770942813337, 'action': [1.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 101.06456303413651, 'action': [1.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 98.08399404954531, 'action': [1.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 92.45148679326257, 'action': [0.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 101.5185822621431, 'action': [2.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 91.0840346178195, 'action': [0.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 100.41800754169904, 'action': [2.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 106.07241511969315, 'action': [2.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 93.25652074515573, 'action': [0.0, 0.0]}])
Weights num count: [0.11135795612329909, 0.11357956123299083, 0.11108025548458761, 0.10663704526520411, 0.11385726187170231, 0.10552624271035824, 0.1130241599555679, 0.1174673701749514, 0.10719244654262705]
Actions to choose Agent 1: dict_values([{'num_count': 593, 'sum_payoffs': 133.6686120969646, 'action': [0.0, -1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 140.62483426239007, 'action': [1.0, -1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 141.00644604946885, 'action': [1.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 128.1061184266452, 'action': [0.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 128.4077064364747, 'action': [0.0, 1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 142.82937049078896, 'action': [1.0, 0.0]}])
Weights num count: [0.16467647875590113, 0.17106359344626493, 0.1713412940849764, 0.1596778672590947, 0.15995556789780616, 0.1730074979172452]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 210.94932317733765 s
