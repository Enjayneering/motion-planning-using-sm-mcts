Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 388, 'sum_payoffs': 93.82994642690925, 'action': [0.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 107.03073205234469, 'action': [2.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 98.2826761639812, 'action': [2.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 100.00272429293925, 'action': [2.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 97.99645733693069, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 100.08622361732559, 'action': [1.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 95.23349935028433, 'action': [1.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 94.38968903142667, 'action': [0.0, 0.0]}, {'num_count': 391, 'sum_payoffs': 94.85838810896567, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10774784782004998, 0.11830047209108581, 0.11135795612329909, 0.11274645931685642, 0.11108025548458761, 0.11274645931685642, 0.10885865037489587, 0.10830324909747292, 0.10858094973618439]
Actions to choose Agent 1: dict_values([{'num_count': 622, 'sum_payoffs': 142.84648962551285, 'action': [1.0, -1.5707963267948966]}, {'num_count': 607, 'sum_payoffs': 138.2132386449842, 'action': [1.0, 1.5707963267948966]}, {'num_count': 595, 'sum_payoffs': 134.52002854245663, 'action': [0.0, 0.0]}, {'num_count': 577, 'sum_payoffs': 128.9171759326436, 'action': [0.0, -1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 129.8113656272656, 'action': [0.0, 1.5707963267948966]}, {'num_count': 619, 'sum_payoffs': 141.92141848373797, 'action': [1.0, 0.0]}])
Weights num count: [0.17272979727853374, 0.1685642876978617, 0.16523188003332406, 0.16023326853651762, 0.16106637045265204, 0.17189669536239932]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 219.35488057136536 s
