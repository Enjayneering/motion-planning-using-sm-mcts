Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 395, 'sum_payoffs': 96.50664557810849, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 106.55149585376543, 'action': [2.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 90.5762660736854, 'action': [0.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 92.29332957838149, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 102.71848846229095, 'action': [1.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 104.78723336378496, 'action': [2.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 98.50089520135268, 'action': [1.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 97.14846634477384, 'action': [2.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 94.77358114230651, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10969175229103027, 0.11774507081366287, 0.1049708414329353, 0.10635934462649264, 0.11469036378783672, 0.11635656762010553, 0.11135795612329909, 0.1102471535684532, 0.10830324909747292]
Actions to choose Agent 1: dict_values([{'num_count': 637, 'sum_payoffs': 147.13634814995766, 'action': [1.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 126.17810626469242, 'action': [0.0, -1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 129.31702879649893, 'action': [0.0, 0.0]}, {'num_count': 625, 'sum_payoffs': 143.55847998512894, 'action': [1.0, -1.5707963267948966]}, {'num_count': 610, 'sum_payoffs': 138.78161714843478, 'action': [1.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 129.62512010610726, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17689530685920576, 0.15801166342682588, 0.16078866981394058, 0.17356289919466814, 0.1693973896139961, 0.16106637045265204]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 208.06635808944702 s
