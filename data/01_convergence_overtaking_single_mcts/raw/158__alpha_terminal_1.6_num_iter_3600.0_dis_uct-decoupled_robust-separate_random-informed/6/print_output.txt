Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 410, 'sum_payoffs': 101.14952564121408, 'action': [2.0, -1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 98.28256436367083, 'action': [1.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 94.15626310253101, 'action': [0.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 95.93382292437617, 'action': [1.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 96.98974485365002, 'action': [1.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 92.2007367617962, 'action': [0.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 101.07998162270489, 'action': [2.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 93.23045116506799, 'action': [0.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 105.94647001935266, 'action': [2.0, 0.0]}])
Weights num count: [0.11385726187170231, 0.11163565676201055, 0.10830324909747292, 0.10969175229103027, 0.11052485420716468, 0.10663704526520411, 0.11385726187170231, 0.10747014718133852, 0.11774507081366287]
Actions to choose Agent 1: dict_values([{'num_count': 573, 'sum_payoffs': 127.39783408349315, 'action': [0.0, -1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 126.87417964209352, 'action': [0.0, 1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 142.89205003712334, 'action': [1.0, -1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 128.72291119332132, 'action': [0.0, 0.0]}, {'num_count': 640, 'sum_payoffs': 148.1864560685529, 'action': [1.0, 0.0]}, {'num_count': 616, 'sum_payoffs': 140.7066227228358, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15912246598167176, 0.15856706470424883, 0.1730074979172452, 0.16023326853651762, 0.17772840877534019, 0.17106359344626493]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 208.627539396286 s
