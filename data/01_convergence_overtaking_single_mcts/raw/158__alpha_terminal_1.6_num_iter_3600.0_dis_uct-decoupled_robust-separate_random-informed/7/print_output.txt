Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 399, 'sum_payoffs': 98.22826900502251, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 94.05315718708147, 'action': [0.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 106.22433313414058, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 101.38987967598051, 'action': [2.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 98.98626500726512, 'action': [2.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 100.01244892003923, 'action': [1.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 95.09144936623214, 'action': [0.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 92.41042491917167, 'action': [0.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 101.09753842587267, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11080255484587614, 0.10747014718133852, 0.11718966953623994, 0.11330186059427937, 0.11135795612329909, 0.1121910580394335, 0.10830324909747292, 0.10608164398778117, 0.1130241599555679]
Actions to choose Agent 1: dict_values([{'num_count': 569, 'sum_payoffs': 125.56887300821684, 'action': [0.0, 1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 139.7665915618617, 'action': [1.0, 1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 143.49549773120623, 'action': [1.0, -1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 146.86426879773913, 'action': [1.0, 0.0]}, {'num_count': 572, 'sum_payoffs': 126.49797014954542, 'action': [0.0, -1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 128.63542050816625, 'action': [0.0, 0.0]}])
Weights num count: [0.15801166342682588, 0.17078589280755346, 0.1741183004720911, 0.17717300749791726, 0.1588447653429603, 0.16078866981394058]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 213.0644052028656 s
