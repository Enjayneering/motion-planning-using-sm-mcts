Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 408, 'sum_payoffs': 100.45865741912372, 'action': [1.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 100.4686335774285, 'action': [2.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 92.81978821409264, 'action': [0.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 101.07824222123466, 'action': [2.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 99.00054267953921, 'action': [1.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 105.95445974438633, 'action': [2.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 89.71640741015926, 'action': [0.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 94.91654153318488, 'action': [1.0, 0.0]}, {'num_count': 391, 'sum_payoffs': 94.55983546511854, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11330186059427937, 0.11330186059427937, 0.10719244654262705, 0.11385726187170231, 0.1121910580394335, 0.11774507081366287, 0.10469314079422383, 0.10885865037489587, 0.10858094973618439]
Actions to choose Agent 1: dict_values([{'num_count': 571, 'sum_payoffs': 126.69817300745663, 'action': [0.0, 1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 144.29871925901057, 'action': [1.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 143.66337064323835, 'action': [1.0, 1.5707963267948966]}, {'num_count': 568, 'sum_payoffs': 125.74329028588119, 'action': [0.0, -1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 130.93312783076232, 'action': [0.0, 0.0]}, {'num_count': 622, 'sum_payoffs': 142.3429441542282, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15856706470424883, 0.17439600111080256, 0.17384059983337963, 0.1577339627881144, 0.1624548736462094, 0.17272979727853374]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 211.24711346626282 s
