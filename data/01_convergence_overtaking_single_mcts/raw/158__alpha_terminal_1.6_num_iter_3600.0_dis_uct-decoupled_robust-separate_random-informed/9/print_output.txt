Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 385, 'sum_payoffs': 92.78726408534348, 'action': [0.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 92.39214873679043, 'action': [0.0, 1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 92.68413980316649, 'action': [0.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 107.66210291922616, 'action': [2.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 98.23162711170372, 'action': [1.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 100.72818719878539, 'action': [2.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 100.65868898204705, 'action': [2.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 100.43035342652253, 'action': [1.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 95.86670358244496, 'action': [1.0, 0.0]}])
Weights num count: [0.10691474590391557, 0.10663704526520411, 0.10691474590391557, 0.11885587336850875, 0.11135795612329909, 0.11330186059427937, 0.11330186059427937, 0.1130241599555679, 0.1094140516523188]
Actions to choose Agent 1: dict_values([{'num_count': 579, 'sum_payoffs': 128.94181401692353, 'action': [0.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 131.0434969363275, 'action': [0.0, 0.0]}, {'num_count': 638, 'sum_payoffs': 147.18687514747728, 'action': [1.0, 0.0]}, {'num_count': 605, 'sum_payoffs': 136.88859719311935, 'action': [1.0, -1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 128.34443073310476, 'action': [0.0, -1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 140.0458574319067, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16078866981394058, 0.16273257428492086, 0.17717300749791726, 0.16800888642043876, 0.16023326853651762, 0.17078589280755346]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 211.15630674362183 s
