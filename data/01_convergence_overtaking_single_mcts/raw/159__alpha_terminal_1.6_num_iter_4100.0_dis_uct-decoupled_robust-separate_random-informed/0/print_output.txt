Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 443, 'sum_payoffs': 106.32779048842166, 'action': [1.0, 1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 105.70600711933146, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 113.5080775788053, 'action': [2.0, 1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 123.97488052828234, 'action': [2.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 110.06806348896335, 'action': [0.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 112.11600098706822, 'action': [1.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 110.12600716242461, 'action': [1.0, 0.0]}, {'num_count': 432, 'sum_payoffs': 102.61340652729729, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 111.15060367934915, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10802243355279201, 0.1075347476225311, 0.11314313582053158, 0.12070226773957571, 0.11070470616922702, 0.11216776396000976, 0.11070470616922702, 0.10534016093635698, 0.11143623506461839]
Actions to choose Agent 1: dict_values([{'num_count': 707, 'sum_payoffs': 160.06594136760376, 'action': [1.0, 1.5707963267948966]}, {'num_count': 707, 'sum_payoffs': 160.08572702186922, 'action': [1.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 144.98712002505886, 'action': [0.0, 1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 142.8451683768976, 'action': [0.0, -1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 160.73698343345075, 'action': [1.0, 0.0]}, {'num_count': 670, 'sum_payoffs': 148.85907045312808, 'action': [0.0, 0.0]}])
Weights num count: [0.1723969763472324, 0.1723969763472324, 0.16020482809070957, 0.1584979273347964, 0.1728846622774933, 0.1633747866374055]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 217.98159503936768 s
