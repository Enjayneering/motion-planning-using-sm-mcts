Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 107.1975934393798, 'action': [0.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 111.9851490685517, 'action': [2.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 113.61272329705388, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 124.30296292759778, 'action': [2.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 105.16382095735064, 'action': [1.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 113.06935246535485, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 112.67248317896534, 'action': [2.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 105.24440117426822, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 104.25330360447094, 'action': [0.0, 0.0]}])
Weights num count: [0.10851011948305292, 0.1119239209948793, 0.11314313582053158, 0.12070226773957571, 0.10704706169227018, 0.11265544989027067, 0.11241160692514021, 0.10704706169227018, 0.1063155327968788]
Actions to choose Agent 1: dict_values([{'num_count': 653, 'sum_payoffs': 143.66912655240137, 'action': [0.0, 1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 165.75583654108482, 'action': [1.0, 0.0]}, {'num_count': 702, 'sum_payoffs': 158.47090863623492, 'action': [1.0, 1.5707963267948966]}, {'num_count': 651, 'sum_payoffs': 143.096876361555, 'action': [0.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 149.72045898513258, 'action': [0.0, 0.0]}, {'num_count': 695, 'sum_payoffs': 156.37181902413786, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15922945623018775, 0.17702999268471104, 0.1711777615215801, 0.15874177029992684, 0.1641063155327969, 0.16947086076566692]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 198.71149325370789 s
