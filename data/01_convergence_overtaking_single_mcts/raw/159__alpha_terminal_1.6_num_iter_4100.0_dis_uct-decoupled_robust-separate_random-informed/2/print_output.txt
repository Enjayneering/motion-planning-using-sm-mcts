Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 437, 'sum_payoffs': 104.34192330402979, 'action': [0.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 113.4807646786709, 'action': [2.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 112.49039306345695, 'action': [1.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 110.17006107885837, 'action': [0.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 109.51465394474212, 'action': [1.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 109.82601221748043, 'action': [1.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 115.25065392644859, 'action': [2.0, -1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 115.95491635479152, 'action': [2.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 105.116303371585, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10655937576200927, 0.11314313582053158, 0.11241160692514021, 0.11070470616922702, 0.11021702023896611, 0.11046086320409657, 0.11436235064618386, 0.11485003657644477, 0.10704706169227018]
Actions to choose Agent 1: dict_values([{'num_count': 646, 'sum_payoffs': 141.7573782734092, 'action': [0.0, 1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 159.88591641991607, 'action': [1.0, 1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 159.87825473504654, 'action': [1.0, -1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 146.2556047891364, 'action': [0.0, -1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 161.37934231566317, 'action': [1.0, 0.0]}, {'num_count': 670, 'sum_payoffs': 149.00055007584012, 'action': [0.0, 0.0]}])
Weights num count: [0.15752255547427457, 0.17215313338210192, 0.17215313338210192, 0.1611801999512314, 0.1733723482077542, 0.1633747866374055]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 95.42490029335022 s
