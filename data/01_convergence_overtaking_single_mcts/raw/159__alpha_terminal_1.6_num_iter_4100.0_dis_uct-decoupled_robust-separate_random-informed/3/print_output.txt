Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 440, 'sum_payoffs': 105.35967985410191, 'action': [0.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 113.37112736957025, 'action': [2.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 110.09813280648724, 'action': [0.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 119.93595447737616, 'action': [2.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 109.31909003256703, 'action': [1.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 108.05921066836567, 'action': [1.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 107.0194484749749, 'action': [0.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 107.99595108830738, 'action': [1.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 114.14428812346841, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10729090465740064, 0.11314313582053158, 0.11070470616922702, 0.11777615215801024, 0.11021702023896611, 0.10924164837844429, 0.10851011948305292, 0.10924164837844429, 0.11363082175079249]
Actions to choose Agent 1: dict_values([{'num_count': 646, 'sum_payoffs': 142.28766568200837, 'action': [0.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 160.6985226043579, 'action': [1.0, -1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 145.90849671308942, 'action': [0.0, -1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 159.58667702445365, 'action': [1.0, 1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 166.86249804077457, 'action': [1.0, 0.0]}, {'num_count': 659, 'sum_payoffs': 146.15205583520128, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15752255547427457, 0.1723969763472324, 0.16044867105584004, 0.17142160448671057, 0.1772738356498415, 0.16069251402097048]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 96.60224771499634 s
