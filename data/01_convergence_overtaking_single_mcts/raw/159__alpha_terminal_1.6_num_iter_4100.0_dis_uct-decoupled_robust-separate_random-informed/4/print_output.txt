Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 446, 'sum_payoffs': 107.46234763308713, 'action': [1.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 106.01457936481786, 'action': [0.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 107.69407368856228, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 110.7701358362113, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 104.94911389329789, 'action': [0.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 106.46237594324657, 'action': [1.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 115.24359997537007, 'action': [2.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 121.96700346871357, 'action': [2.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 115.21628910934295, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10875396244818337, 0.10777859058766155, 0.10899780541331383, 0.11119239209948793, 0.10704706169227018, 0.10802243355279201, 0.11436235064618386, 0.11923920994879297, 0.11436235064618386]
Actions to choose Agent 1: dict_values([{'num_count': 657, 'sum_payoffs': 145.41936623021243, 'action': [0.0, -1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 147.28410439424715, 'action': [0.0, 0.0]}, {'num_count': 721, 'sum_payoffs': 164.77436024272927, 'action': [1.0, 0.0]}, {'num_count': 704, 'sum_payoffs': 159.5936288263312, 'action': [1.0, 1.5707963267948966]}, {'num_count': 646, 'sum_payoffs': 142.16387259315894, 'action': [0.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 161.1907221420075, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16020482809070957, 0.1616678858814923, 0.17581077785905877, 0.171665447451841, 0.15752255547427457, 0.1728846622774933]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 95.11014199256897 s
