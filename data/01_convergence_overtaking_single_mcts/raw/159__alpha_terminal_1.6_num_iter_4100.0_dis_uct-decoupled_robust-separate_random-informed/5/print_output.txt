Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 467, 'sum_payoffs': 114.09737076045037, 'action': [2.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 107.24912340386038, 'action': [1.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 104.36715685158063, 'action': [0.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 116.10498509111335, 'action': [2.0, -1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 102.00083095367887, 'action': [0.0, 1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 113.74564377097064, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 106.3472189196071, 'action': [0.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 111.0664044060283, 'action': [1.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 117.19145182964301, 'action': [2.0, 0.0]}])
Weights num count: [0.11387466471592295, 0.10899780541331383, 0.10680321872713973, 0.11533772250670568, 0.10509631797122652, 0.11363082175079249, 0.10826627651792246, 0.11168007802974884, 0.11606925140209705]
Actions to choose Agent 1: dict_values([{'num_count': 723, 'sum_payoffs': 165.61967788254074, 'action': [1.0, 1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 142.88312548781215, 'action': [0.0, -1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 141.3832807837859, 'action': [0.0, 1.5707963267948966]}, {'num_count': 696, 'sum_payoffs': 157.3265105357572, 'action': [1.0, -1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 148.34462572117124, 'action': [0.0, 0.0]}, {'num_count': 724, 'sum_payoffs': 165.8973576612257, 'action': [1.0, 0.0]}])
Weights num count: [0.17629846378931968, 0.15801024140453548, 0.1567910265788832, 0.16971470373079736, 0.1623994147768837, 0.17654230675445012]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 95.88309526443481 s
