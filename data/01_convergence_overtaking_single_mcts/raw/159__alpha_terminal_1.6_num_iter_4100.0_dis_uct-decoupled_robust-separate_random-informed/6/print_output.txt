Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 457, 'sum_payoffs': 110.92665461377291, 'action': [1.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 112.08637178762604, 'action': [1.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 104.30279238955109, 'action': [0.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 114.4019053153893, 'action': [2.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 103.94344690442112, 'action': [0.0, 1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 105.57858556344146, 'action': [0.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 111.06255175888404, 'action': [1.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 111.74995814243088, 'action': [2.0, 1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 120.84091278451251, 'action': [2.0, 0.0]}])
Weights num count: [0.11143623506461839, 0.11216776396000976, 0.10655937576200927, 0.11387466471592295, 0.1063155327968788, 0.1075347476225311, 0.11143623506461839, 0.1119239209948793, 0.1185076810534016]
Actions to choose Agent 1: dict_values([{'num_count': 649, 'sum_payoffs': 143.21783815810508, 'action': [0.0, -1.5707963267948966]}, {'num_count': 712, 'sum_payoffs': 162.24840303786578, 'action': [1.0, -1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 164.09407695301286, 'action': [1.0, 0.0]}, {'num_count': 697, 'sum_payoffs': 157.70725223648543, 'action': [1.0, 1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 148.03576654286343, 'action': [0.0, 0.0]}, {'num_count': 659, 'sum_payoffs': 146.1152188517095, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15825408436966593, 0.17361619117288465, 0.1750792489636674, 0.16995854669592783, 0.16215557181175322, 0.16069251402097048]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 96.39079666137695 s
