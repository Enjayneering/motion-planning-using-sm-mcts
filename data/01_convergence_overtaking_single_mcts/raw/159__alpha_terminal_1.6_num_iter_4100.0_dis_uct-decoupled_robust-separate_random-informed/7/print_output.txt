Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 480, 'sum_payoffs': 118.71604532099754, 'action': [2.0, 0.0]}, {'num_count': 432, 'sum_payoffs': 102.51124315627033, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 105.9356721003596, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 113.33071621675656, 'action': [2.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 118.04743375736568, 'action': [2.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 108.24343933283019, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 110.67010461863273, 'action': [1.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 110.99815152479609, 'action': [1.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 105.9217179920837, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11704462326261887, 0.10534016093635698, 0.10777859058766155, 0.11314313582053158, 0.11655693733235796, 0.10948549134357474, 0.11119239209948793, 0.11143623506461839, 0.10777859058766155]
Actions to choose Agent 1: dict_values([{'num_count': 664, 'sum_payoffs': 147.4521745107225, 'action': [0.0, -1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 144.20362026243546, 'action': [0.0, 1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 165.06382016409447, 'action': [1.0, 0.0]}, {'num_count': 701, 'sum_payoffs': 158.6248841051519, 'action': [1.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 147.47196406239541, 'action': [0.0, 0.0]}, {'num_count': 696, 'sum_payoffs': 157.19656939250515, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16191172884662278, 0.15922945623018775, 0.1760546208241892, 0.17093391855644965, 0.16191172884662278, 0.16971470373079736]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 93.15895462036133 s
