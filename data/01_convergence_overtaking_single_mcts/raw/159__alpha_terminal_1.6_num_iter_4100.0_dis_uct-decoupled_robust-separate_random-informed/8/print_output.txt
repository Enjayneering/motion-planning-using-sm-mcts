Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 460, 'sum_payoffs': 112.0536254209723, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 114.64796545490908, 'action': [2.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 106.64310082869142, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 109.2791419899833, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 109.03848238140638, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 107.59944920667157, 'action': [0.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 112.39180432165793, 'action': [2.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 114.7807320563037, 'action': [2.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 108.31525576046481, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11216776396000976, 0.1141185076810534, 0.10826627651792246, 0.11021702023896611, 0.10997317727383565, 0.10899780541331383, 0.11241160692514021, 0.1141185076810534, 0.10948549134357474]
Actions to choose Agent 1: dict_values([{'num_count': 693, 'sum_payoffs': 156.34732806896832, 'action': [1.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 146.33873504114612, 'action': [0.0, 0.0]}, {'num_count': 652, 'sum_payoffs': 143.99511341566787, 'action': [0.0, 1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 143.3643645225476, 'action': [0.0, -1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 168.7704638490511, 'action': [1.0, 0.0]}, {'num_count': 711, 'sum_payoffs': 161.8171331929614, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.168983174835406, 0.16093635698610095, 0.1589856132650573, 0.1584979273347964, 0.17898073640575468, 0.1733723482077542]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 95.27540683746338 s
