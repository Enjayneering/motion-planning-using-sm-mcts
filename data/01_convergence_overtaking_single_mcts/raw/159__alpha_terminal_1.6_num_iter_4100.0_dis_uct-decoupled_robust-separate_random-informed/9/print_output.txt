Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 473, 'sum_payoffs': 116.55085311702724, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 122.42782610680746, 'action': [2.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 102.42624189725139, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 106.7755613891493, 'action': [1.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 106.34744397189117, 'action': [0.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 108.46818454460319, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 110.7573810657833, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 106.73659975878823, 'action': [0.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 115.53905574355304, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11533772250670568, 0.11948305291392343, 0.10509631797122652, 0.10826627651792246, 0.10802243355279201, 0.10948549134357474, 0.11119239209948793, 0.10826627651792246, 0.11460619361131431]
Actions to choose Agent 1: dict_values([{'num_count': 721, 'sum_payoffs': 165.2187437401239, 'action': [1.0, 0.0]}, {'num_count': 660, 'sum_payoffs': 146.67142202545384, 'action': [0.0, 0.0]}, {'num_count': 706, 'sum_payoffs': 160.69151196714023, 'action': [1.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 143.04117144599172, 'action': [0.0, -1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 148.4491835214521, 'action': [0.0, 1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 158.52464723641432, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17581077785905877, 0.16093635698610095, 0.17215313338210192, 0.15801024140453548, 0.1623994147768837, 0.17044623262618874]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 89.2437207698822 s
