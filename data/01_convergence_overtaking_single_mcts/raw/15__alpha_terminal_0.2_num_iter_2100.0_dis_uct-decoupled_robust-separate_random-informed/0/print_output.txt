Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 221, 'sum_payoffs': 49.93199051819725, 'action': [1.0, -1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 49.52349911179466, 'action': [1.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 60.491710656345475, 'action': [2.0, 1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 41.42037676128503, 'action': [0.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 60.117984476019664, 'action': [2.0, -1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 60.822088945417676, 'action': [1.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 50.614873714179865, 'action': [0.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 75.98276947220624, 'action': [2.0, 0.0]}, {'num_count': 197, 'sum_payoffs': 41.42037676128504, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10518800571156592, 0.10471204188481675, 0.11899095668729176, 0.09376487386958592, 0.1185149928605426, 0.11946692051404094, 0.10613993336506425, 0.13898143741075678, 0.09376487386958592]
Actions to choose Agent 1: dict_values([{'num_count': 369, 'sum_payoffs': 104.63529103275359, 'action': [1.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 80.29354886438927, 'action': [0.0, -1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 80.92518956567967, 'action': [0.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 104.57456052846156, 'action': [1.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 120.11595649459105, 'action': [1.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 93.7925964398484, 'action': [0.0, 0.0]}])
Weights num count: [0.17563065207044265, 0.14516896715849595, 0.14612089481199428, 0.17563065207044265, 0.19466920514040933, 0.16230366492146597]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.496178150177002 s
