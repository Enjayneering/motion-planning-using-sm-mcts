Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 226, 'sum_payoffs': 51.73130825035725, 'action': [0.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.69732524217874, 'action': [1.0, -1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 62.389674717513, 'action': [1.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 59.84855397392435, 'action': [2.0, -1.5707963267948966]}, {'num_count': 195, 'sum_payoffs': 40.86413314405596, 'action': [0.0, 1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 49.984138357312474, 'action': [1.0, 1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 41.431892742415734, 'action': [0.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 60.17013231513491, 'action': [2.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 74.98124125096695, 'action': [2.0, 0.0]}])
Weights num count: [0.10756782484531176, 0.10471204188481675, 0.1213707758210376, 0.11803902903379343, 0.09281294621608757, 0.10518800571156592, 0.09376487386958592, 0.1185149928605426, 0.13755354593050928]
Actions to choose Agent 1: dict_values([{'num_count': 405, 'sum_payoffs': 118.41427110567312, 'action': [1.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 82.33904785369846, 'action': [0.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 93.36085578849968, 'action': [0.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 81.65232599733896, 'action': [0.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 103.29965450340768, 'action': [1.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 104.44994892123879, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.19276534983341265, 0.14802475011899097, 0.1618277010947168, 0.14707282246549264, 0.17420276059019515, 0.17563065207044265]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.6511054039001465 s
