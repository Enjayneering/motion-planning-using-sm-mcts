Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 221, 'sum_payoffs': 49.931990518197246, 'action': [1.0, -1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 61.2217804039586, 'action': [2.0, 1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 62.21494324270583, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.52349911179466, 'action': [1.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 60.830671610594415, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 72.70299631582766, 'action': [2.0, 0.0]}, {'num_count': 196, 'sum_payoffs': 41.07554917512836, 'action': [0.0, 1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 51.748690863395666, 'action': [0.0, 0.0]}, {'num_count': 196, 'sum_payoffs': 41.11618103311289, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10518800571156592, 0.1199428843407901, 0.1213707758210376, 0.10471204188481675, 0.11946692051404094, 0.1346977629700143, 0.09328891004283675, 0.10756782484531176, 0.09328891004283675]
Actions to choose Agent 1: dict_values([{'num_count': 365, 'sum_payoffs': 103.19253415056572, 'action': [1.0, -1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 82.57382177105929, 'action': [0.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 103.93433716199432, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 119.13767751829789, 'action': [1.0, 0.0]}, {'num_count': 308, 'sum_payoffs': 81.4727418763895, 'action': [0.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 94.68294836518409, 'action': [0.0, 0.0]}])
Weights num count: [0.173726796763446, 0.14802475011899097, 0.17467872441694432, 0.19324131366016184, 0.14659685863874344, 0.1632555925749643]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.6925365924835205 s
