Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 220, 'sum_payoffs': 49.73209046825556, 'action': [1.0, 1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 41.197227466397216, 'action': [0.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 50.78192786823321, 'action': [0.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 50.760706594817854, 'action': [1.0, -1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 59.80227276671682, 'action': [2.0, 1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 62.39554134942069, 'action': [1.0, 0.0]}, {'num_count': 197, 'sum_payoffs': 41.62896811774591, 'action': [0.0, 1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 61.33194271409676, 'action': [2.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 73.96819704859698, 'action': [2.0, 0.0]}])
Weights num count: [0.10471204188481675, 0.09328891004283675, 0.10613993336506425, 0.10613993336506425, 0.11803902903379343, 0.1213707758210376, 0.09376487386958592, 0.1199428843407901, 0.13612565445026178]
Actions to choose Agent 1: dict_values([{'num_count': 312, 'sum_payoffs': 82.75612192527787, 'action': [0.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 118.42962574718072, 'action': [1.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 82.79382046730862, 'action': [0.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 102.5668687225254, 'action': [1.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 93.01805617387373, 'action': [0.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 104.03852419888237, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14850071394574013, 0.19276534983341265, 0.14850071394574013, 0.17325083293669682, 0.16135173726796764, 0.17515468824369348]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.634765386581421 s
