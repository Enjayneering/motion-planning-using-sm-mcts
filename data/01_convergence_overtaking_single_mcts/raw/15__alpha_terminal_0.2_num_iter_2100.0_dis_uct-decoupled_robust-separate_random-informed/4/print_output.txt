Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 221, 'sum_payoffs': 50.15434311000442, 'action': [0.0, 0.0]}, {'num_count': 222, 'sum_payoffs': 50.39849639562278, 'action': [1.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 76.2387646629824, 'action': [2.0, 0.0]}, {'num_count': 252, 'sum_payoffs': 61.38409055321197, 'action': [2.0, 1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 41.27262455045856, 'action': [0.0, -1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 50.46802684777639, 'action': [1.0, -1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 40.484540331822465, 'action': [0.0, 1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 60.2222801542501, 'action': [2.0, -1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 61.276173497289435, 'action': [1.0, 0.0]}])
Weights num count: [0.10518800571156592, 0.10566396953831508, 0.13898143741075678, 0.1199428843407901, 0.09328891004283675, 0.10566396953831508, 0.0923369823893384, 0.1185149928605426, 0.1199428843407901]
Actions to choose Agent 1: dict_values([{'num_count': 345, 'sum_payoffs': 95.22564802975162, 'action': [0.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 101.76433520680476, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 120.63540691421254, 'action': [1.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 103.99495902494398, 'action': [1.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 80.81502725554147, 'action': [0.0, -1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 80.79764464250309, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16420752022846263, 0.17229890528319847, 0.19562113279390766, 0.17515468824369348, 0.14612089481199428, 0.14612089481199428]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.6893980503082275 s
