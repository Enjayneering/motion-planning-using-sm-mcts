Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 256, 'sum_payoffs': 62.892973792546144, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.69732524217875, 'action': [1.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 50.41587900866118, 'action': [1.0, -1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 59.81378874784756, 'action': [2.0, 1.5707963267948966]}, {'num_count': 195, 'sum_payoffs': 40.81198530494074, 'action': [0.0, -1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 41.23785932438174, 'action': [0.0, 1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 61.40733979815809, 'action': [2.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 50.8891568624175, 'action': [0.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 74.76689190394069, 'action': [2.0, 0.0]}])
Weights num count: [0.12184673964778676, 0.10471204188481675, 0.10566396953831508, 0.11803902903379343, 0.09281294621608757, 0.09328891004283675, 0.1199428843407901, 0.10613993336506425, 0.13707758210376011]
Actions to choose Agent 1: dict_values([{'num_count': 369, 'sum_payoffs': 104.4036677140313, 'action': [1.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 104.02114158584399, 'action': [1.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 118.24993298488894, 'action': [1.0, 0.0]}, {'num_count': 306, 'sum_payoffs': 80.45868368825406, 'action': [0.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 94.81512865181826, 'action': [0.0, 0.0]}, {'num_count': 308, 'sum_payoffs': 81.22351866194408, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17563065207044265, 0.17515468824369348, 0.19276534983341265, 0.1456449309852451, 0.16373155640171347, 0.14659685863874344]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.726808786392212 s
