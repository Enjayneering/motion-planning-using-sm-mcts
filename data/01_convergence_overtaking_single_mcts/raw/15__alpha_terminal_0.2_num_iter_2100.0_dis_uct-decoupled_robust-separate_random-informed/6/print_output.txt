Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 222, 'sum_payoffs': 50.43912825360731, 'action': [1.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 40.128196764535105, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 73.2004287589531, 'action': [2.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 59.90070181303957, 'action': [2.0, -1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 61.29717748801991, 'action': [2.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 52.5397083979856, 'action': [0.0, 0.0]}, {'num_count': 253, 'sum_payoffs': 61.73681274280726, 'action': [1.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 50.83023704697149, 'action': [1.0, 1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 41.48990721343867, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10566396953831508, 0.09186101856258924, 0.13517372679676345, 0.11803902903379343, 0.1199428843407901, 0.10851975249881009, 0.12041884816753927, 0.10613993336506425, 0.09376487386958592]
Actions to choose Agent 1: dict_values([{'num_count': 369, 'sum_payoffs': 104.35423590818525, 'action': [1.0, -1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 81.57127956405459, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 94.02320577280523, 'action': [0.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 116.7588669239627, 'action': [1.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 82.64031026591668, 'action': [0.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 103.655998070695, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17563065207044265, 0.14707282246549264, 0.16277962874821514, 0.190861494526416, 0.14850071394574013, 0.17467872441694432]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.6258323192596436 s
