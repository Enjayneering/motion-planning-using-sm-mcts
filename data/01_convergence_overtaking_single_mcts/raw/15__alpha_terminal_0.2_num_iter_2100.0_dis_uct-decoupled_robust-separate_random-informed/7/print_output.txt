Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 248, 'sum_payoffs': 59.84855397392435, 'action': [2.0, 1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 41.1335636461513, 'action': [0.0, 1.5707963267948966]}, {'num_count': 219, 'sum_payoffs': 49.39312951400659, 'action': [1.0, -1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 61.26827889385083, 'action': [2.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 75.78580273821841, 'action': [2.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.749473081293964, 'action': [1.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 51.162426024889726, 'action': [0.0, 0.0]}, {'num_count': 256, 'sum_payoffs': 62.7536956055542, 'action': [1.0, 0.0]}, {'num_count': 194, 'sum_payoffs': 40.55407078397612, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11803902903379343, 0.09328891004283675, 0.10423607805806759, 0.1199428843407901, 0.1385054735840076, 0.10471204188481675, 0.10661589719181343, 0.12184673964778676, 0.0923369823893384]
Actions to choose Agent 1: dict_values([{'num_count': 366, 'sum_payoffs': 103.36060229038445, 'action': [1.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 93.81584568479454, 'action': [0.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 105.30169696012084, 'action': [1.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 118.48380155782674, 'action': [1.0, 0.0]}, {'num_count': 308, 'sum_payoffs': 81.22938529385176, 'action': [0.0, -1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 81.6378767002544, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17420276059019515, 0.16230366492146597, 0.17658257972394098, 0.19276534983341265, 0.14659685863874344, 0.14707282246549264]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.5926625728607178 s
