Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 516, 'sum_payoffs': 124.53109901501523, 'action': [1.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 125.90031787669727, 'action': [2.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 117.26398335765428, 'action': [0.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 126.84473220375081, 'action': [1.0, -1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 112.89981357544312, 'action': [0.0, -1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 132.86926159881958, 'action': [2.0, 0.0]}, {'num_count': 544, 'sum_payoffs': 133.8542421856458, 'action': [2.0, -1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 112.99868754135217, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 119.28171096346111, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11214953271028037, 0.11301890893284068, 0.10736796348619865, 0.11367094109976092, 0.10454249076287764, 0.11758313410128234, 0.11823516626820256, 0.10454249076287764, 0.10867202782003912]
Actions to choose Agent 1: dict_values([{'num_count': 743, 'sum_payoffs': 164.32359301534635, 'action': [0.0, 0.0]}, {'num_count': 735, 'sum_payoffs': 161.89938753388367, 'action': [0.0, -1.5707963267948966]}, {'num_count': 791, 'sum_payoffs': 178.60470999500458, 'action': [1.0, 1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 159.81679782247647, 'action': [0.0, 1.5707963267948966]}, {'num_count': 811, 'sum_payoffs': 184.60345970028024, 'action': [1.0, 0.0]}, {'num_count': 792, 'sum_payoffs': 178.87305235384244, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16148663334057814, 0.15974788089545752, 0.1719191480113019, 0.15822647250597696, 0.17626602912410347, 0.17213649206694198]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 107.35303258895874 s
