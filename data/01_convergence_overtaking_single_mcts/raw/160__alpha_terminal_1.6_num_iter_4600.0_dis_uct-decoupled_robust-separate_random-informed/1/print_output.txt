Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 496, 'sum_payoffs': 117.99384235270938, 'action': [0.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 122.33935562266424, 'action': [1.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 118.30174436345985, 'action': [0.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 112.78222145303934, 'action': [0.0, 1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 134.33037578684167, 'action': [2.0, 0.0]}, {'num_count': 518, 'sum_payoffs': 125.34161844815435, 'action': [1.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 125.55819821013864, 'action': [2.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 118.38902322191926, 'action': [1.0, 0.0]}, {'num_count': 539, 'sum_payoffs': 132.34814045515992, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10780265159747882, 0.11062812432079983, 0.10801999565311889, 0.10432514670723755, 0.11845251032384264, 0.11258422082156053, 0.11280156487720061, 0.10801999565311889, 0.11714844599000217]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 160.87323749123013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 740, 'sum_payoffs': 162.93276712701245, 'action': [0.0, 1.5707963267948966]}, {'num_count': 805, 'sum_payoffs': 182.2560463727064, 'action': [1.0, 0.0]}, {'num_count': 738, 'sum_payoffs': 162.33957688428055, 'action': [0.0, 0.0]}, {'num_count': 804, 'sum_payoffs': 181.97249154414516, 'action': [1.0, -1.5707963267948966]}, {'num_count': 780, 'sum_payoffs': 174.86361743080548, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15931319278417735, 0.1608346011736579, 0.174961964790263, 0.16039991306237775, 0.1747446207346229, 0.16952836339926103]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 103.80780506134033 s
