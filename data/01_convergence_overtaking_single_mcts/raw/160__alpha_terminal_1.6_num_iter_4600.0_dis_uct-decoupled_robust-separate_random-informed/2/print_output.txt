Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 490, 'sum_payoffs': 116.18093984641023, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 131.17961960485684, 'action': [1.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 118.21603704541566, 'action': [1.0, 1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 135.60719208870432, 'action': [2.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 124.15558748646743, 'action': [1.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 116.91705558672423, 'action': [0.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 126.23573502447903, 'action': [2.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 125.17335969373785, 'action': [2.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 115.54225815877632, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10649858726363834, 0.11627906976744186, 0.10780265159747882, 0.11910454249076288, 0.11171484459900022, 0.1069332753749185, 0.11301890893284068, 0.11236687676592046, 0.10606389915235818]
Actions to choose Agent 1: dict_values([{'num_count': 728, 'sum_payoffs': 159.18091307047484, 'action': [0.0, 1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 158.95161252012286, 'action': [0.0, -1.5707963267948966]}, {'num_count': 804, 'sum_payoffs': 181.74811135847668, 'action': [1.0, -1.5707963267948966]}, {'num_count': 796, 'sum_payoffs': 179.4029628257148, 'action': [1.0, 1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 164.2192887953356, 'action': [0.0, 0.0]}, {'num_count': 800, 'sum_payoffs': 180.61723068820734, 'action': [1.0, 0.0]}])
Weights num count: [0.15822647250597696, 0.15800912845033688, 0.1747446207346229, 0.1730058682895023, 0.16192132145185828, 0.1738752445120626]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.37773704528809 s
