Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 488, 'sum_payoffs': 115.59965982260675, 'action': [0.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 132.65137550657874, 'action': [2.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 117.64343528883614, 'action': [0.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 122.63607514427008, 'action': [1.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 126.25026280881923, 'action': [2.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 122.6167687382583, 'action': [1.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 119.65436584554816, 'action': [0.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 124.23641592901679, 'action': [1.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 128.63290320137946, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10606389915235818, 0.11714844599000217, 0.10736796348619865, 0.11062812432079983, 0.11301890893284068, 0.11062812432079983, 0.10867202782003912, 0.11171484459900022, 0.11454031732232123]
Actions to choose Agent 1: dict_values([{'num_count': 744, 'sum_payoffs': 163.96823892148117, 'action': [0.0, 1.5707963267948966]}, {'num_count': 804, 'sum_payoffs': 181.8028262232611, 'action': [1.0, 1.5707963267948966]}, {'num_count': 805, 'sum_payoffs': 182.12174047596977, 'action': [1.0, 0.0]}, {'num_count': 734, 'sum_payoffs': 161.04857579932377, 'action': [0.0, -1.5707963267948966]}, {'num_count': 779, 'sum_payoffs': 174.30635423998962, 'action': [1.0, -1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 160.99143192379773, 'action': [0.0, 0.0]}])
Weights num count: [0.16170397739621822, 0.1747446207346229, 0.174961964790263, 0.15953053683981744, 0.16931101934362094, 0.15953053683981744]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.8400011062622 s
