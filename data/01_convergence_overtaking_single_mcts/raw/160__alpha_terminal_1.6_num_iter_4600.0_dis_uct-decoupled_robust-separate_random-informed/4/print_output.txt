Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 510, 'sum_payoffs': 122.83081895321664, 'action': [1.0, 1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 125.53854681017286, 'action': [1.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 121.90641869626283, 'action': [1.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 130.55343234404765, 'action': [2.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 133.92241546172994, 'action': [2.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 120.77580448272649, 'action': [2.0, 1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 121.90537861355685, 'action': [0.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 115.59966474418299, 'action': [0.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 116.19318434982289, 'action': [0.0, 0.0]}])
Weights num count: [0.11084546837643991, 0.11258422082156053, 0.11019343620951967, 0.1158443816561617, 0.11801782221256249, 0.10954140404259943, 0.11019343620951967, 0.10606389915235818, 0.10649858726363834]
Actions to choose Agent 1: dict_values([{'num_count': 739, 'sum_payoffs': 162.4978825998802, 'action': [0.0, 1.5707963267948966]}, {'num_count': 821, 'sum_payoffs': 186.84682535800945, 'action': [1.0, 0.0]}, {'num_count': 724, 'sum_payoffs': 158.0804734808817, 'action': [0.0, -1.5707963267948966]}, {'num_count': 780, 'sum_payoffs': 174.6612948908885, 'action': [1.0, 1.5707963267948966]}, {'num_count': 791, 'sum_payoffs': 177.7934809208148, 'action': [1.0, -1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 164.25694442933641, 'action': [0.0, 0.0]}])
Weights num count: [0.16061725711801783, 0.17843946968050423, 0.15735709628341665, 0.16952836339926103, 0.1719191480113019, 0.16192132145185828]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 106.07327008247375 s
