Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 530, 'sum_payoffs': 129.40369618704372, 'action': [2.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 117.36035193394125, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 121.05154274495824, 'action': [0.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 112.17348430982685, 'action': [0.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 132.75797437151883, 'action': [2.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 129.01686809622885, 'action': [2.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 115.8593656315184, 'action': [0.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 126.72500028063091, 'action': [1.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 123.74219108625559, 'action': [1.0, 0.0]}])
Weights num count: [0.11519234948924147, 0.10736796348619865, 0.10975874809823952, 0.1038904585959574, 0.11736579004564225, 0.1149750054336014, 0.10628124320799825, 0.11345359704412085, 0.11149750054336013]
Actions to choose Agent 1: dict_values([{'num_count': 785, 'sum_payoffs': 176.29049371651553, 'action': [1.0, -1.5707963267948966]}, {'num_count': 742, 'sum_payoffs': 163.50992919325782, 'action': [0.0, 0.0]}, {'num_count': 751, 'sum_payoffs': 166.11462690837527, 'action': [0.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 156.64077076130013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 814, 'sum_payoffs': 184.88513887684405, 'action': [1.0, 0.0]}, {'num_count': 789, 'sum_payoffs': 177.41713230346045, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17061508367746142, 0.16126928928493806, 0.16322538578569876, 0.15627037600521626, 0.1769180612910237, 0.17148445990002173]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 105.81769561767578 s
