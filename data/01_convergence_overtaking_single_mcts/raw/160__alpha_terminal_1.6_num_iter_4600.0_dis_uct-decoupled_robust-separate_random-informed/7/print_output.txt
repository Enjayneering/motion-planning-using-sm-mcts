Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 490, 'sum_payoffs': 115.95924147659076, 'action': [1.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 124.82634397706198, 'action': [2.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 132.8564967226046, 'action': [2.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 128.49832636958143, 'action': [1.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 123.83104842089452, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 121.2374281098631, 'action': [0.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 113.92847931493475, 'action': [0.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 116.60810642489608, 'action': [0.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 128.5128954948202, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10649858726363834, 0.11236687676592046, 0.11758313410128234, 0.11475766137796131, 0.11171484459900022, 0.10997609215387959, 0.10519452292979788, 0.1069332753749185, 0.11475766137796131]
Actions to choose Agent 1: dict_values([{'num_count': 725, 'sum_payoffs': 158.5417247761405, 'action': [0.0, 1.5707963267948966]}, {'num_count': 754, 'sum_payoffs': 167.07547129009654, 'action': [0.0, 0.0]}, {'num_count': 814, 'sum_payoffs': 184.96504053311597, 'action': [1.0, 0.0]}, {'num_count': 767, 'sum_payoffs': 170.94663889150158, 'action': [1.0, 1.5707963267948966]}, {'num_count': 735, 'sum_payoffs': 161.5333491002846, 'action': [0.0, -1.5707963267948966]}, {'num_count': 805, 'sum_payoffs': 182.33350128193385, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15757444033905674, 0.163877417952619, 0.1769180612910237, 0.16670289067594002, 0.15974788089545752, 0.174961964790263]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 102.15445899963379 s
