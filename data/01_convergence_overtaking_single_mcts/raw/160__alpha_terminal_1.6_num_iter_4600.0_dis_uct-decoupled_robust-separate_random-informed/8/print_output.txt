Searching game tree in timestep 0...
Max timehorizon: 11
Actions to choose Agent 0: dict_values([{'num_count': 478, 'sum_payoffs': 112.15632500348127, 'action': [0.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 116.38947743455199, 'action': [1.0, 1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 125.26170515386643, 'action': [1.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 126.64906616072378, 'action': [1.0, 0.0]}, {'num_count': 512, 'sum_payoffs': 123.37940730209546, 'action': [2.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 118.3297710633284, 'action': [0.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 117.42579808849881, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 128.61098369763656, 'action': [2.0, -1.5707963267948966]}, {'num_count': 560, 'sum_payoffs': 139.33829522199153, 'action': [2.0, 0.0]}])
Weights num count: [0.1038904585959574, 0.10671593131927842, 0.11258422082156053, 0.11345359704412085, 0.11128015648772006, 0.10801999565311889, 0.10736796348619865, 0.11475766137796131, 0.12171267115844382]
Actions to choose Agent 1: dict_values([{'num_count': 746, 'sum_payoffs': 164.61496088014104, 'action': [0.0, -1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 162.56676423112268, 'action': [0.0, 1.5707963267948966]}, {'num_count': 807, 'sum_payoffs': 182.76550361011638, 'action': [1.0, 0.0]}, {'num_count': 785, 'sum_payoffs': 176.2273294624087, 'action': [1.0, -1.5707963267948966]}, {'num_count': 791, 'sum_payoffs': 177.92244273151792, 'action': [1.0, 1.5707963267948966]}, {'num_count': 732, 'sum_payoffs': 160.51189880379405, 'action': [0.0, 0.0]}])
Weights num count: [0.16213866550749836, 0.16061725711801783, 0.17539665290154313, 0.17061508367746142, 0.1719191480113019, 0.15909584872853727]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.31957054138184 s
