Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 119, 'sum_payoffs': 28.056718839170685, 'action': [0.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 30.12991811193103, 'action': [2.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 29.245066203945413, 'action': [1.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 28.011952898406584, 'action': [0.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 28.484946643352416, 'action': [1.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 29.31324680545943, 'action': [0.0, 1.5707963267948966]}, {'num_count': 129, 'sum_payoffs': 32.136946736225845, 'action': [2.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 29.710678466553603, 'action': [1.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 29.379747315721993, 'action': [2.0, 0.0]}])
Weights num count: [0.1080835603996367, 0.11262488646684832, 0.11080835603996367, 0.1080835603996367, 0.10899182561307902, 0.11080835603996367, 0.11716621253405994, 0.11171662125340599, 0.11080835603996367]
Actions to choose Agent 1: dict_values([{'num_count': 179, 'sum_payoffs': 38.84730279139536, 'action': [0.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 38.851634600799876, 'action': [0.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 43.830510448980675, 'action': [1.0, 1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 40.97758518540629, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 37.714928681571635, 'action': [0.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 41.92214368216328, 'action': [1.0, 0.0]}])
Weights num count: [0.16257947320617622, 0.16257947320617622, 0.17529518619436876, 0.16802906448683017, 0.15985467756584923, 0.17075386012715713]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.201148509979248 s
