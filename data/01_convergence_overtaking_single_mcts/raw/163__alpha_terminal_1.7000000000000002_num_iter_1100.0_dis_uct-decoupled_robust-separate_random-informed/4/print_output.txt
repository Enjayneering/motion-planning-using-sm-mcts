Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 124, 'sum_payoffs': 30.168761522944667, 'action': [1.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 28.098499797975283, 'action': [1.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 30.187071556757832, 'action': [2.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 29.297285556710555, 'action': [0.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 28.5294404593252, 'action': [0.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 29.243803306082537, 'action': [2.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 29.244508479847017, 'action': [1.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 28.1087432030454, 'action': [0.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 31.651473480079343, 'action': [2.0, 0.0]}])
Weights num count: [0.11262488646684832, 0.1080835603996367, 0.11262488646684832, 0.11080835603996367, 0.10899182561307902, 0.11080835603996367, 0.11080835603996367, 0.1080835603996367, 0.11625794732061762]
Actions to choose Agent 1: dict_values([{'num_count': 188, 'sum_payoffs': 42.28244812744271, 'action': [1.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 39.460775893266025, 'action': [0.0, 0.0]}, {'num_count': 186, 'sum_payoffs': 41.563736577180016, 'action': [0.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 41.695245909756004, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 38.01175814799496, 'action': [0.0, 1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 40.87232822499231, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17075386012715713, 0.16348773841961853, 0.16893732970027248, 0.16893732970027248, 0.15985467756584923, 0.16712079927338783]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.82445454597473 s
