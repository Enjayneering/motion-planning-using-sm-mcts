Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 123, 'sum_payoffs': 29.93884189062984, 'action': [1.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 28.25386888238795, 'action': [0.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 29.11096206081115, 'action': [1.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 30.38933242720365, 'action': [2.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 31.117984296460055, 'action': [2.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 31.24640281708654, 'action': [2.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 29.128782887496964, 'action': [0.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 28.257105557247076, 'action': [0.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 29.066689156796272, 'action': [1.0, 0.0]}])
Weights num count: [0.11171662125340599, 0.1080835603996367, 0.10990009082652134, 0.11262488646684832, 0.11444141689373297, 0.11444141689373297, 0.10990009082652134, 0.1080835603996367, 0.10990009082652134]
Actions to choose Agent 1: dict_values([{'num_count': 173, 'sum_payoffs': 36.520195031848324, 'action': [0.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 40.74177681853941, 'action': [1.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 38.60701301470522, 'action': [0.0, 0.0]}, {'num_count': 187, 'sum_payoffs': 41.55632221587315, 'action': [1.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 40.67919607790533, 'action': [0.0, 1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 42.8716933793097, 'action': [1.0, 0.0]}])
Weights num count: [0.15712988192552224, 0.16802906448683017, 0.16257947320617622, 0.16984559491371481, 0.16802906448683017, 0.1734786557674841]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.18924593925476 s
