Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 186, 'sum_payoffs': 45.926157133487195, 'action': [2.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 41.69382292332163, 'action': [2.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 42.358489517145564, 'action': [0.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 42.70418218566604, 'action': [2.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 43.894767040494784, 'action': [1.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 41.27328327508284, 'action': [1.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 39.79074443906952, 'action': [0.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 43.24583961785607, 'action': [0.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 43.5849277476074, 'action': [1.0, 0.0]}])
Weights num count: [0.11617738913179262, 0.10930668332292318, 0.11055590256089944, 0.11118051217988757, 0.11305434103685197, 0.10868207370393504, 0.1061836352279825, 0.11180512179887571, 0.11242973141786383]
Actions to choose Agent 1: dict_values([{'num_count': 256, 'sum_payoffs': 55.45725134294062, 'action': [0.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 61.19928482207081, 'action': [1.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 61.50621479078196, 'action': [1.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 60.51726982190116, 'action': [1.0, 1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 59.72710148118713, 'action': [0.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 55.69200626758792, 'action': [0.0, 0.0]}])
Weights num count: [0.1599000624609619, 0.17051842598376016, 0.1711430356027483, 0.16926920674578388, 0.1680199875078076, 0.16052467207995003]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 41.327043533325195 s
