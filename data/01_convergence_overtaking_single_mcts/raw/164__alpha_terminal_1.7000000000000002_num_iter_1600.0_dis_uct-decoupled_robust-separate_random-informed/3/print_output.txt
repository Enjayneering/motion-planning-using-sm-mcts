Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 175, 'sum_payoffs': 41.70948340227547, 'action': [0.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 40.92660095429093, 'action': [1.0, -1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 46.957448650986706, 'action': [2.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 45.490183257533744, 'action': [2.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.53884458998014, 'action': [2.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 39.35051770338216, 'action': [0.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 41.64516450576298, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.55570455244411, 'action': [1.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 41.212977728970905, 'action': [1.0, 0.0]}])
Weights num count: [0.10930668332292318, 0.1080574640849469, 0.11805121798875702, 0.1155527795128045, 0.11242973141786383, 0.10555902560899438, 0.10930668332292318, 0.11242973141786383, 0.10868207370393504]
Actions to choose Agent 1: dict_values([{'num_count': 268, 'sum_payoffs': 59.59395921855951, 'action': [1.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 55.537862318326106, 'action': [0.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 62.98974185387533, 'action': [1.0, 1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 56.81317546782085, 'action': [0.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 62.8965310873614, 'action': [1.0, 0.0]}, {'num_count': 260, 'sum_payoffs': 56.81687830499181, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16739537788881947, 0.1599000624609619, 0.1736414740787008, 0.16239850093691444, 0.1736414740787008, 0.16239850093691444]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.61862301826477 s
