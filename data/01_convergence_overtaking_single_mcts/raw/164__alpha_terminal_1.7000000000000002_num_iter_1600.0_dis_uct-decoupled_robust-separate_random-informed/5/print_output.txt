Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 173, 'sum_payoffs': 40.64431998803343, 'action': [1.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 40.33913511926178, 'action': [0.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 47.19951538404658, 'action': [2.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 42.54216844810708, 'action': [1.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 41.10811882844132, 'action': [0.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 41.085032282452595, 'action': [0.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 42.601880625555225, 'action': [1.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.27473363518112, 'action': [2.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 43.67644830453761, 'action': [2.0, 0.0]}])
Weights num count: [0.1080574640849469, 0.10743285446595878, 0.11867582760774516, 0.11118051217988757, 0.10868207370393504, 0.10868207370393504, 0.11118051217988757, 0.11242973141786383, 0.11305434103685197]
Actions to choose Agent 1: dict_values([{'num_count': 256, 'sum_payoffs': 55.114145572334415, 'action': [0.0, 1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 58.826411013488894, 'action': [1.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 55.422464839460055, 'action': [0.0, 0.0]}, {'num_count': 262, 'sum_payoffs': 57.037840999218744, 'action': [0.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 63.177523784255115, 'action': [1.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 62.53308419500718, 'action': [1.0, 0.0]}])
Weights num count: [0.1599000624609619, 0.16677076826983137, 0.16052467207995003, 0.16364772017489068, 0.1748906933166771, 0.1736414740787008]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 41.04187536239624 s
