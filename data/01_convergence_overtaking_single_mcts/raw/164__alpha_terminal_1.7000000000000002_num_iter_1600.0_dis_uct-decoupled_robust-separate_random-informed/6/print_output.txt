Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 42.15713588983768, 'action': [1.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 42.89643175717118, 'action': [1.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 45.2920118933296, 'action': [2.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 42.56180642392194, 'action': [2.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 41.034370164233145, 'action': [0.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 42.87644127854562, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 42.209784768547955, 'action': [1.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 41.336393495918635, 'action': [0.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 45.266402045315346, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1099312929419113, 0.11118051217988757, 0.11492816989381636, 0.11055590256089944, 0.1080574640849469, 0.11118051217988757, 0.1099312929419113, 0.10868207370393504, 0.11492816989381636]
Actions to choose Agent 1: dict_values([{'num_count': 257, 'sum_payoffs': 55.4627229059488, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 60.58173174521427, 'action': [1.0, 1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 56.89345471847777, 'action': [0.0, -1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 55.85234549260929, 'action': [0.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 63.2949478162697, 'action': [1.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 60.64825380604844, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16052467207995003, 0.16989381636477202, 0.16302311055590257, 0.16114928169893816, 0.1748906933166771, 0.16989381636477202]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.72194170951843 s
