Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 172, 'sum_payoffs': 40.42020845436478, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 41.37165336883732, 'action': [0.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 43.286110411772974, 'action': [1.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 43.1862002172313, 'action': [0.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 42.42993216334653, 'action': [1.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.58472334440514, 'action': [1.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 43.52084468923094, 'action': [2.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 43.28386470417975, 'action': [2.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 43.579784195967726, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10743285446595878, 0.10868207370393504, 0.11180512179887571, 0.11180512179887571, 0.11055590256089944, 0.11242973141786383, 0.11242973141786383, 0.11180512179887571, 0.11242973141786383]
Actions to choose Agent 1: dict_values([{'num_count': 264, 'sum_payoffs': 57.93289364922609, 'action': [0.0, 0.0]}, {'num_count': 260, 'sum_payoffs': 56.62199667526238, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 60.74401647697693, 'action': [1.0, 1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 56.01115342852534, 'action': [0.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 59.696199810725545, 'action': [1.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 62.34425931343554, 'action': [1.0, 0.0]}])
Weights num count: [0.16489693941286696, 0.16239850093691444, 0.16989381636477202, 0.16114928169893816, 0.1680199875078076, 0.17301686445971268]
Selected final action: [1.0, 0.0, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 41.5929012298584 s
