Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 230, 'sum_payoffs': 54.88875457926943, 'action': [0.0, 1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 53.1440046397213, 'action': [1.0, 0.0]}, {'num_count': 244, 'sum_payoffs': 60.108680185857445, 'action': [2.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 54.16585104752224, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 59.329414642377145, 'action': [1.0, -1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 55.29546106401621, 'action': [0.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 57.785255210868876, 'action': [2.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 55.606861426747095, 'action': [1.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 54.93635954522113, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10947168015230842, 0.10709186101856259, 0.11613517372679677, 0.10851975249881009, 0.11518324607329843, 0.1099476439790576, 0.11327939076630177, 0.11042360780580676, 0.10947168015230842]
Actions to choose Agent 1: dict_values([{'num_count': 338, 'sum_payoffs': 73.35009053183747, 'action': [0.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 82.73679552205596, 'action': [1.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 73.0075210297195, 'action': [0.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 79.2243278220416, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 73.27364994082036, 'action': [0.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 81.86588888452286, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16087577344121848, 0.17467872441694432, 0.1603998096144693, 0.16944312232270348, 0.16087577344121848, 0.17325083293669682]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.735990047454834 s
