Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 234, 'sum_payoffs': 56.31745531129901, 'action': [2.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 57.07340096352934, 'action': [1.0, 1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 53.06191065425479, 'action': [0.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 57.853331373212235, 'action': [1.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 54.49301159351926, 'action': [1.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 61.68260341664685, 'action': [2.0, 0.0]}, {'num_count': 227, 'sum_payoffs': 53.910048832711084, 'action': [0.0, -1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 51.285012090943034, 'action': [0.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 59.81787345563255, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11137553545930509, 0.11232746311280342, 0.10709186101856259, 0.11327939076630177, 0.10899571632555925, 0.11803902903379343, 0.10804378867206092, 0.10471204188481675, 0.11565920990004759]
Actions to choose Agent 1: dict_values([{'num_count': 338, 'sum_payoffs': 73.05185749718822, 'action': [0.0, 0.0]}, {'num_count': 369, 'sum_payoffs': 83.17493369821305, 'action': [1.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 73.78569465606377, 'action': [0.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 78.02267968598481, 'action': [1.0, 1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 81.17396724386559, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 72.80177698949555, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16087577344121848, 0.17563065207044265, 0.1618277010947168, 0.16801523084245598, 0.17277486910994763, 0.1603998096144693]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 52.731221437454224 s
