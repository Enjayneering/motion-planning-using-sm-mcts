Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 230, 'sum_payoffs': 54.87832058254459, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 59.326748691948666, 'action': [2.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 56.77700904489505, 'action': [1.0, 1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 57.45279355682565, 'action': [1.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 58.263179765346315, 'action': [2.0, 0.0]}, {'num_count': 226, 'sum_payoffs': 53.36605483963602, 'action': [0.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 57.1240123265156, 'action': [2.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 52.754871115445546, 'action': [1.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 55.27042837404996, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10947168015230842, 0.11518324607329843, 0.11185149928605426, 0.1128034269395526, 0.11375535459305093, 0.10756782484531176, 0.11232746311280342, 0.10661589719181343, 0.1099476439790576]
Actions to choose Agent 1: dict_values([{'num_count': 360, 'sum_payoffs': 80.29532135146064, 'action': [1.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 79.94340031220908, 'action': [1.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 76.05276827973681, 'action': [1.0, 0.0]}, {'num_count': 347, 'sum_payoffs': 76.0379821305222, 'action': [0.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 73.07184595086862, 'action': [0.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 76.60336262847902, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17134697762970014, 0.17087101380295097, 0.16515944788196096, 0.16515944788196096, 0.16087577344121848, 0.1661113755354593]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.92758870124817 s
