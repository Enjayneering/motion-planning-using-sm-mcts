Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 230, 'sum_payoffs': 54.88015626064556, 'action': [1.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 55.81658407546799, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 56.72024729029996, 'action': [2.0, 1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 55.48346880593131, 'action': [0.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 54.05943419275177, 'action': [0.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 56.708634696310355, 'action': [1.0, 1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 56.25907825150257, 'action': [1.0, -1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 59.20810632277314, 'action': [2.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 55.23067692742888, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10947168015230842, 0.11089957163255593, 0.11185149928605426, 0.11042360780580676, 0.10851975249881009, 0.11185149928605426, 0.11137553545930509, 0.11518324607329843, 0.1099476439790576]
Actions to choose Agent 1: dict_values([{'num_count': 358, 'sum_payoffs': 79.96553301304436, 'action': [1.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 82.60307055287811, 'action': [1.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 70.28642281522033, 'action': [0.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 73.52982945394088, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 83.90344164806092, 'action': [1.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 74.08952871090098, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1703950499762018, 0.17420276059019515, 0.1561161351737268, 0.16087577344121848, 0.17610661589719181, 0.1618277010947168]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.44880676269531 s
