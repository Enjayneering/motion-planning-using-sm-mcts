Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 225, 'sum_payoffs': 53.051119711009946, 'action': [0.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 58.59086850919302, 'action': [2.0, -1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 56.44206482290448, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 58.2360108758316, 'action': [1.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 54.849811381593504, 'action': [0.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 58.58966569659615, 'action': [1.0, -1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 55.27503419122451, 'action': [2.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 54.58222407085802, 'action': [0.0, 0.0]}, {'num_count': 232, 'sum_payoffs': 55.638773504290945, 'action': [2.0, 0.0]}])
Weights num count: [0.10709186101856259, 0.1142313184198001, 0.11137553545930509, 0.11375535459305093, 0.10947168015230842, 0.1142313184198001, 0.1099476439790576, 0.10899571632555925, 0.11042360780580676]
Actions to choose Agent 1: dict_values([{'num_count': 343, 'sum_payoffs': 74.4788456467192, 'action': [0.0, 1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 80.9829716905085, 'action': [1.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 75.47576887445783, 'action': [0.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 78.0089796694147, 'action': [1.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 72.56183279577786, 'action': [0.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 79.04558551135631, 'action': [1.0, 0.0]}])
Weights num count: [0.1632555925749643, 0.17277486910994763, 0.1646834840552118, 0.16849119466920515, 0.1603998096144693, 0.16991908614945264]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.808783292770386 s
