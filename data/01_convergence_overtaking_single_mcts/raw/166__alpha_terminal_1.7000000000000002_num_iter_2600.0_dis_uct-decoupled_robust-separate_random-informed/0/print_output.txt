Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 67.79440073441745, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 65.63460659663227, 'action': [0.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 70.90687801677002, 'action': [1.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 75.16055904543552, 'action': [2.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 69.10001586830462, 'action': [1.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 69.51564853443209, 'action': [2.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 70.63292211735642, 'action': [1.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 71.25591218275345, 'action': [2.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 65.29325341965797, 'action': [0.0, 0.0]}])
Weights num count: [0.10918877354863514, 0.10688196847366398, 0.11264898116109189, 0.11726259131103421, 0.11072664359861592, 0.1111111111111111, 0.1122645136485967, 0.11303344867358708, 0.10649750096116878]
Actions to choose Agent 1: dict_values([{'num_count': 456, 'sum_payoffs': 102.76949863086705, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 92.71045318673868, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 100.291158705522, 'action': [1.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 91.14131219914628, 'action': [0.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 91.47463202398252, 'action': [0.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 95.5304017756632, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17531718569780855, 0.16301422529796233, 0.172241445597847, 0.16109188773548636, 0.16147635524798154, 0.16647443291041908]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.13209104537964 s
