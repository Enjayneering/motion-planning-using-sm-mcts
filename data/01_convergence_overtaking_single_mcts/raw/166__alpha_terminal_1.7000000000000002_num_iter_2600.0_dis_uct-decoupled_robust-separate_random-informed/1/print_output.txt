Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 300, 'sum_payoffs': 73.73559774878514, 'action': [2.0, -1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 71.99419271236663, 'action': [1.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 73.42103990570253, 'action': [2.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.93426565485917, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.62885822037687, 'action': [0.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 71.22236820666012, 'action': [2.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 64.74191315374145, 'action': [0.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 66.63982024247862, 'action': [0.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 70.53258027878607, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11534025374855825, 0.11341791618608228, 0.11495578623606305, 0.10918877354863514, 0.10880430603613994, 0.11264898116109189, 0.1057285659361784, 0.10765090349865436, 0.1118800461361015]
Actions to choose Agent 1: dict_values([{'num_count': 419, 'sum_payoffs': 90.39845457447562, 'action': [0.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 89.77569814890285, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 100.44311942069831, 'action': [1.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 90.10508250852635, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 100.82794666340806, 'action': [1.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 97.90903157615446, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16109188773548636, 0.16032295271049596, 0.17339484813533257, 0.16070742022299117, 0.17377931564782775, 0.170319108035371]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.62924003601074 s
