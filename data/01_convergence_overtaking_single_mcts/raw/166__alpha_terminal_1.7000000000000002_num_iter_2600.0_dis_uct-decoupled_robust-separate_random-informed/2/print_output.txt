Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 285, 'sum_payoffs': 68.19689601951038, 'action': [2.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 69.97123905207218, 'action': [2.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 69.96156696626021, 'action': [1.0, -1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 74.63204632451557, 'action': [1.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.8603970200658, 'action': [0.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.5162479452948, 'action': [0.0, 1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 74.89787958084005, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 66.35437721975224, 'action': [1.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 66.80538526406002, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10957324106113034, 0.1114955786236063, 0.1114955786236063, 0.11649365628604383, 0.10918877354863514, 0.10880430603613994, 0.11687812379853903, 0.10765090349865436, 0.10803537101114956]
Actions to choose Agent 1: dict_values([{'num_count': 417, 'sum_payoffs': 89.50384917812204, 'action': [0.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 88.61812332950066, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 98.6692556511744, 'action': [1.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 103.3549953858289, 'action': [1.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 98.96315134268936, 'action': [1.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 88.96868906883168, 'action': [0.0, 0.0]}])
Weights num count: [0.16032295271049596, 0.15916955017301038, 0.1714725105728566, 0.1772395232602845, 0.17185697808535177, 0.15955401768550556]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.93345284461975 s
