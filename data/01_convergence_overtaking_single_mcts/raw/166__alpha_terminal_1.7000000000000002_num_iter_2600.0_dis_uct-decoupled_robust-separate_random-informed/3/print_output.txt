Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 296, 'sum_payoffs': 72.07074357703779, 'action': [2.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 68.48108239908842, 'action': [0.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 70.6161435673406, 'action': [1.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 72.04173593528343, 'action': [1.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 66.34873222425097, 'action': [1.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 73.89812692301713, 'action': [2.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 64.56887630498358, 'action': [0.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 66.73900207549245, 'action': [0.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 70.97083369687884, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11380238369857747, 0.10995770857362552, 0.1122645136485967, 0.11380238369857747, 0.10765090349865436, 0.11572472126105345, 0.1057285659361784, 0.10803537101114956, 0.11264898116109189]
Actions to choose Agent 1: dict_values([{'num_count': 440, 'sum_payoffs': 97.47760599562694, 'action': [1.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 90.27531681711315, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 101.37193968290308, 'action': [1.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 94.05827943070446, 'action': [0.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 89.08636509570545, 'action': [0.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 100.43685046146481, 'action': [1.0, 0.0]}])
Weights num count: [0.16916570549788543, 0.16032295271049596, 0.17377931564782775, 0.16493656286043828, 0.1587850826605152, 0.17262591311034217]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 66.29016375541687 s
