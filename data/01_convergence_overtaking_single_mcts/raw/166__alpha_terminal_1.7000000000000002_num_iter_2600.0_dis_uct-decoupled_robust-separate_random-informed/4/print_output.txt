Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 295, 'sum_payoffs': 71.72709375270836, 'action': [1.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 72.8084125717474, 'action': [2.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.43415648693087, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 65.66563865680996, 'action': [0.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 69.97081077636877, 'action': [1.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 73.49454902949196, 'action': [2.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 71.80065250272017, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.36548607510339, 'action': [1.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 65.71551707408372, 'action': [0.0, 0.0]}])
Weights num count: [0.11341791618608228, 0.11457131872356786, 0.10880430603613994, 0.10688196847366398, 0.1114955786236063, 0.11534025374855825, 0.11341791618608228, 0.10880430603613994, 0.10688196847366398]
Actions to choose Agent 1: dict_values([{'num_count': 423, 'sum_payoffs': 92.16746541697862, 'action': [0.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 99.97491252647261, 'action': [1.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 96.55176876110497, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 96.16215378039448, 'action': [1.0, 1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 97.77190082045294, 'action': [1.0, -1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 89.62203689538883, 'action': [0.0, 0.0]}])
Weights num count: [0.16262975778546712, 0.172241445597847, 0.16801230296039985, 0.16762783544790466, 0.1695501730103806, 0.15955401768550556]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 63.24192500114441 s
