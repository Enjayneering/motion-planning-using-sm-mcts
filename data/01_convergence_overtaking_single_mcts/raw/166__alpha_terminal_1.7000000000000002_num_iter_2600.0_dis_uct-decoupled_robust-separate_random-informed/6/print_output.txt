Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 289, 'sum_payoffs': 69.59926341340845, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.55626760935831, 'action': [0.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 70.40445744719155, 'action': [2.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 69.95475122613271, 'action': [2.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 71.33796056202314, 'action': [2.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 70.43252792064752, 'action': [1.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 71.0979658510318, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.94517748453723, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 68.14482473271623, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1111111111111111, 0.10880430603613994, 0.1118800461361015, 0.1114955786236063, 0.11303344867358708, 0.1118800461361015, 0.11264898116109189, 0.10918877354863514, 0.10957324106113034]
Actions to choose Agent 1: dict_values([{'num_count': 450, 'sum_payoffs': 100.58676583181055, 'action': [1.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 90.2053369177104, 'action': [0.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 89.55652447015441, 'action': [0.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 89.1931604767795, 'action': [0.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 103.72758364132567, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 98.61765845922115, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17301038062283736, 0.16032295271049596, 0.15955401768550556, 0.15916955017301038, 0.1768550557477893, 0.1707035755478662]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.48497319221497 s
