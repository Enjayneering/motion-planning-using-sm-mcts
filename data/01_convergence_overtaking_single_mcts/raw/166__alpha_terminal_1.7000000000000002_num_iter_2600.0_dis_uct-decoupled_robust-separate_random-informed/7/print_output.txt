Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 283, 'sum_payoffs': 67.50260444878009, 'action': [0.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 65.37656719281948, 'action': [0.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 72.8041002994393, 'action': [2.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 72.53499268540585, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 68.26092763566122, 'action': [1.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 66.39701567604759, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 66.06991177681037, 'action': [0.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 71.75624357940742, 'action': [1.0, -1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 75.76901178638002, 'action': [2.0, 0.0]}])
Weights num count: [0.10880430603613994, 0.10649750096116878, 0.11457131872356786, 0.11418685121107267, 0.10957324106113034, 0.10765090349865436, 0.10726643598615918, 0.11341791618608228, 0.11764705882352941]
Actions to choose Agent 1: dict_values([{'num_count': 420, 'sum_payoffs': 90.59618116058655, 'action': [0.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 96.246137559601, 'action': [1.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 100.6167387823833, 'action': [1.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 101.85121615293377, 'action': [1.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 89.92317934080896, 'action': [0.0, 1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 89.37423496447033, 'action': [0.0, 0.0]}])
Weights num count: [0.16147635524798154, 0.16839677047289503, 0.17377931564782775, 0.17531718569780855, 0.16070742022299117, 0.15993848519800077]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.8137595653534 s
