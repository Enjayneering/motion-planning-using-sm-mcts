Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 290, 'sum_payoffs': 70.15934011652033, 'action': [1.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 68.82731959240309, 'action': [0.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 70.96945536825346, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 68.45200108620988, 'action': [1.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 72.41916246696572, 'action': [2.0, -1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 73.78796534362061, 'action': [2.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 71.21690834931012, 'action': [1.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 65.52548592402576, 'action': [0.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 66.91852664527242, 'action': [0.0, 0.0]}])
Weights num count: [0.1114955786236063, 0.10995770857362552, 0.1122645136485967, 0.10957324106113034, 0.11380238369857747, 0.11534025374855825, 0.11264898116109189, 0.10649750096116878, 0.10803537101114956]
Actions to choose Agent 1: dict_values([{'num_count': 419, 'sum_payoffs': 90.59969534032992, 'action': [0.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 89.67479633671202, 'action': [0.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 89.28256528369259, 'action': [0.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 98.15143395555566, 'action': [1.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 96.56730658529945, 'action': [1.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 106.40339573434073, 'action': [1.0, 0.0]}])
Weights num count: [0.16109188773548636, 0.15993848519800077, 0.15955401768550556, 0.170319108035371, 0.16839677047289503, 0.18031526336024606]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 67.89051675796509 s
