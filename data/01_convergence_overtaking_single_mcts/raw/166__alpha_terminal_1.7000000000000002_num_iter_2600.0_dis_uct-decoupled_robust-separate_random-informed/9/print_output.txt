Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 291, 'sum_payoffs': 70.05834175909345, 'action': [2.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.25187414963276, 'action': [0.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 71.90036384185065, 'action': [1.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 70.53097308862942, 'action': [1.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 70.4167632693478, 'action': [2.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 71.92879319052219, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 66.20901585368394, 'action': [1.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 68.72267693517665, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.33104106883455, 'action': [0.0, 0.0]}])
Weights num count: [0.1118800461361015, 0.10880430603613994, 0.11380238369857747, 0.1122645136485967, 0.1122645136485967, 0.11380238369857747, 0.10765090349865436, 0.11034217608612072, 0.10880430603613994]
Actions to choose Agent 1: dict_values([{'num_count': 415, 'sum_payoffs': 89.88958427801587, 'action': [0.0, -1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 94.83102665289, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 102.54030358894401, 'action': [1.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 99.33763475569698, 'action': [1.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 90.46551663682935, 'action': [0.0, 1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 96.75630675047036, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15955401768550556, 0.16570549788542868, 0.17493271818531334, 0.1710880430603614, 0.16032295271049596, 0.16801230296039985]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 67.02237248420715 s
