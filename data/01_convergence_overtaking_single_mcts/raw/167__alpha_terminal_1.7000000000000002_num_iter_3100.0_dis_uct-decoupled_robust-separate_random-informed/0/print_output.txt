Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 340, 'sum_payoffs': 81.0831086249655, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 88.09709114829008, 'action': [2.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 78.33904346860483, 'action': [0.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 82.131807902604, 'action': [1.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 83.76413379239004, 'action': [2.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 82.15965742940331, 'action': [1.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 86.35636371262824, 'action': [2.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 81.35971087162379, 'action': [1.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 80.43170978351043, 'action': [0.0, 0.0]}])
Weights num count: [0.10964205095130602, 0.11609158336020639, 0.10706223798774589, 0.11060948081264109, 0.11222186391486617, 0.11060948081264109, 0.11447920025798129, 0.10996452757175104, 0.10899709771041599]
Actions to choose Agent 1: dict_values([{'num_count': 533, 'sum_payoffs': 117.99128382231392, 'action': [1.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 109.96562424401974, 'action': [0.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 117.98407707964829, 'action': [1.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 104.78138772533003, 'action': [0.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 116.77352391793691, 'action': [1.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 110.36118246249842, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17188003869719445, 0.163495646565624, 0.17188003869719445, 0.1580135440180587, 0.1705901322154144, 0.163818123186069]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 84.51532697677612 s
