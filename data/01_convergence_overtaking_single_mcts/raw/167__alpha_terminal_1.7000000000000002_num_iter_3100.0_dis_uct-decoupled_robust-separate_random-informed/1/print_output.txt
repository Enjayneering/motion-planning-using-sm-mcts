Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 333, 'sum_payoffs': 77.9350232948224, 'action': [0.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 85.52013805359638, 'action': [2.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 81.72969797980146, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 80.89089942403032, 'action': [0.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 85.81313479570443, 'action': [2.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 80.33303147277348, 'action': [1.0, -1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 81.3324582653688, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 82.02371244207743, 'action': [1.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 80.99873707130651, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1073847146081909, 0.11447920025798129, 0.1109319574330861, 0.11028700419219607, 0.11480167687842631, 0.10964205095130602, 0.11060948081264109, 0.11125443405353112, 0.11028700419219607]
Actions to choose Agent 1: dict_values([{'num_count': 542, 'sum_payoffs': 121.1890959854119, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 105.12708745915366, 'action': [0.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 108.48293425112644, 'action': [0.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 119.58401477360617, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 107.63418638692146, 'action': [0.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 118.09042240856094, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1747823282811996, 0.1580135440180587, 0.16156078684295389, 0.17316994517897452, 0.16059335698161883, 0.17155756207674944]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 76.64815950393677 s
