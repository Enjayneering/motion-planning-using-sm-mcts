Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 324, 'sum_payoffs': 74.97669100307454, 'action': [0.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 80.11731537236638, 'action': [1.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 87.65182171476303, 'action': [2.0, 0.0]}, {'num_count': 330, 'sum_payoffs': 77.00915100263192, 'action': [1.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 84.96172864291742, 'action': [2.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 80.46162622521115, 'action': [0.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 87.05135110493792, 'action': [2.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 81.81565687766856, 'action': [0.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 83.85290353542635, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10448242502418574, 0.10931957433086101, 0.11641405998065141, 0.10641728474685586, 0.11383424701709126, 0.10964205095130602, 0.11576910673976137, 0.1109319574330861, 0.11286681715575621]
Actions to choose Agent 1: dict_values([{'num_count': 552, 'sum_payoffs': 123.98580280353278, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 108.57645125519446, 'action': [0.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 108.28649663731251, 'action': [0.0, 0.0]}, {'num_count': 517, 'sum_payoffs': 113.1852462978993, 'action': [1.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 117.14798275850285, 'action': [1.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 107.34111939227508, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1780070944856498, 0.1618832634633989, 0.16156078684295389, 0.16672041277007418, 0.1709126088358594, 0.16059335698161883]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 78.44055938720703 s
