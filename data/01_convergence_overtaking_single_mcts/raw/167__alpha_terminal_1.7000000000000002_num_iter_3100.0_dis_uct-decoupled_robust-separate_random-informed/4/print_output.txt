Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 82.67768025920692, 'action': [0.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 82.99016373704771, 'action': [1.0, 1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 84.12965887060777, 'action': [2.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 85.82892561128128, 'action': [2.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 80.98001951866031, 'action': [2.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 82.05395251776943, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 82.38154543738823, 'action': [1.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 77.45242550654973, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 81.00333194391771, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11157691067397614, 0.11189938729442116, 0.11286681715575621, 0.11447920025798129, 0.10996452757175104, 0.1109319574330861, 0.11125443405353112, 0.10673976136730087, 0.10996452757175104]
Actions to choose Agent 1: dict_values([{'num_count': 544, 'sum_payoffs': 120.9041729295019, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 107.99862603052769, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 118.17763231521612, 'action': [1.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 114.11881317590235, 'action': [1.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 104.09235996629188, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 109.8428713411181, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17542728152208964, 0.1618832634633989, 0.1725249919380845, 0.16833279587229927, 0.15769106739761368, 0.163818123186069]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 77.7418441772461 s
