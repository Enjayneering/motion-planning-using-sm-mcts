Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 361, 'sum_payoffs': 87.87048059101022, 'action': [2.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 77.8725751358855, 'action': [0.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 79.60571661071857, 'action': [1.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 85.8719443372919, 'action': [1.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 80.65919006656209, 'action': [1.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 79.22682571344629, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 79.53620485857677, 'action': [0.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 85.86431966949014, 'action': [2.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 83.07179127471788, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11641405998065141, 0.10706223798774589, 0.10867462108997097, 0.11447920025798129, 0.10964205095130602, 0.10835214446952596, 0.10867462108997097, 0.11447920025798129, 0.11189938729442116]
Actions to choose Agent 1: dict_values([{'num_count': 542, 'sum_payoffs': 121.04255303379439, 'action': [1.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 105.6452389817803, 'action': [0.0, -1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 119.09574400413024, 'action': [1.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 117.56893061538943, 'action': [1.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 106.54250912072251, 'action': [0.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 109.30388354763016, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1747823282811996, 0.15865849725894873, 0.1728474685585295, 0.17123508545630442, 0.15962592712028378, 0.16252821670428894]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 76.41359281539917 s
