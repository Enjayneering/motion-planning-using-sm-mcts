Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 336, 'sum_payoffs': 79.33990227263259, 'action': [0.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 80.03523959083715, 'action': [1.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 84.50813709489417, 'action': [2.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 80.04367446380883, 'action': [0.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 84.19194082405528, 'action': [1.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 82.53619882256177, 'action': [1.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 76.68586286283944, 'action': [0.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 83.5474063995875, 'action': [2.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 89.78543690088136, 'action': [2.0, 0.0]}])
Weights num count: [0.10835214446952596, 0.10899709771041599, 0.11318929377620122, 0.10899709771041599, 0.11286681715575621, 0.11125443405353112, 0.10577233150596582, 0.11222186391486617, 0.11802644308287649]
Actions to choose Agent 1: dict_values([{'num_count': 497, 'sum_payoffs': 106.79470278927464, 'action': [0.0, -1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 113.1618797728367, 'action': [1.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 111.62426670783368, 'action': [0.0, 1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 116.83994010128501, 'action': [1.0, 1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 119.3488273699676, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 108.93957763650755, 'action': [0.0, 0.0]}])
Weights num count: [0.16027088036117382, 0.1670428893905192, 0.1654305062882941, 0.1709126088358594, 0.17349242179941954, 0.16252821670428894]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 76.52861475944519 s
