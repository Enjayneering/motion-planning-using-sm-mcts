Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 341, 'sum_payoffs': 81.28344278773139, 'action': [0.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 78.82186644511455, 'action': [1.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 81.96297804978295, 'action': [1.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 80.25745787805086, 'action': [0.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 80.96057580065803, 'action': [0.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 85.51395377699205, 'action': [2.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 84.33603375154063, 'action': [2.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 81.85960552772247, 'action': [1.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 87.14412098257613, 'action': [2.0, 0.0]}])
Weights num count: [0.10996452757175104, 0.10770719122863592, 0.11060948081264109, 0.10899709771041599, 0.10964205095130602, 0.11383424701709126, 0.11286681715575621, 0.11060948081264109, 0.11544663011931634]
Actions to choose Agent 1: dict_values([{'num_count': 544, 'sum_payoffs': 121.12943310337812, 'action': [1.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 116.85340012234008, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 102.44876177135292, 'action': [0.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 109.11895692735283, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 118.38843598948235, 'action': [1.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 108.56270766359498, 'action': [0.0, 0.0]}])
Weights num count: [0.17542728152208964, 0.1709126088358594, 0.15575620767494355, 0.16285069332473395, 0.1725249919380845, 0.16220574008384392]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 77.9005811214447 s
