Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 340, 'sum_payoffs': 80.77680593694014, 'action': [0.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 83.12520235170726, 'action': [1.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 82.16450823168823, 'action': [1.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 84.3223003525656, 'action': [2.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 86.75160795062347, 'action': [2.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 80.02725424500923, 'action': [0.0, 1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 76.22252449780912, 'action': [0.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 80.68847602072263, 'action': [1.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 86.64011369203378, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10964205095130602, 0.11189938729442116, 0.1109319574330861, 0.11286681715575621, 0.11512415349887133, 0.10899709771041599, 0.1054498548855208, 0.10964205095130602, 0.11512415349887133]
Actions to choose Agent 1: dict_values([{'num_count': 506, 'sum_payoffs': 109.71818429575528, 'action': [0.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 107.78884674015714, 'action': [0.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 120.78859068242816, 'action': [1.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 117.06244243304212, 'action': [1.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 105.05847621679585, 'action': [0.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 117.32141254004516, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16317316994517897, 0.16123831022250887, 0.1747823282811996, 0.1709126088358594, 0.15833602063850372, 0.17123508545630442]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 77.35604214668274 s
