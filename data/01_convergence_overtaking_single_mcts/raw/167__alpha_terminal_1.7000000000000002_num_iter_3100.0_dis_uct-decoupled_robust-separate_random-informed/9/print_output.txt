Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 82.94336163200806, 'action': [0.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 82.59795079993322, 'action': [1.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 84.41568623375002, 'action': [2.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 81.64597275304097, 'action': [0.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 88.21379771557051, 'action': [2.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 79.86773339267081, 'action': [1.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 81.61901768181133, 'action': [1.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 81.9008679800441, 'action': [2.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 78.8304010406871, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11157691067397614, 0.11125443405353112, 0.11286681715575621, 0.11028700419219607, 0.11641405998065141, 0.10867462108997097, 0.11028700419219607, 0.11060948081264109, 0.10770719122863592]
Actions to choose Agent 1: dict_values([{'num_count': 503, 'sum_payoffs': 109.49235597255479, 'action': [0.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 105.85270711627325, 'action': [0.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 117.10766987080204, 'action': [1.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 120.81583421762552, 'action': [1.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 117.49811851699245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 111.29491412420826, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16220574008384392, 0.15833602063850372, 0.17026765559496937, 0.17413737504030957, 0.1705901322154144, 0.16414059980651402]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 76.7580189704895 s
