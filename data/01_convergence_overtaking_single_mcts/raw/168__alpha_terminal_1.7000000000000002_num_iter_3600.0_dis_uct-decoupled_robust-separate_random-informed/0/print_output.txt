Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 400, 'sum_payoffs': 94.59756266478152, 'action': [1.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 91.24497715242921, 'action': [1.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 90.94246806736254, 'action': [0.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 97.75641175002472, 'action': [2.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 97.71661402669606, 'action': [2.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 91.52840219620285, 'action': [0.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 97.42026411800555, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 98.38695228717373, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 92.33701527500264, 'action': [0.0, 0.0]}])
Weights num count: [0.11108025548458761, 0.10830324909747292, 0.10802554845876146, 0.11357956123299083, 0.11357956123299083, 0.10858094973618439, 0.11330186059427937, 0.11413496251041377, 0.10913635101360733]
Actions to choose Agent 1: dict_values([{'num_count': 624, 'sum_payoffs': 137.06129246533553, 'action': [1.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 122.68512763312305, 'action': [0.0, 0.0]}, {'num_count': 622, 'sum_payoffs': 136.42562839477748, 'action': [1.0, 1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 123.58081049765109, 'action': [0.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 122.29081289055347, 'action': [0.0, -1.5707963267948966]}, {'num_count': 624, 'sum_payoffs': 136.961750097594, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17328519855595667, 0.15995556789780616, 0.17272979727853374, 0.16078866981394058, 0.1596778672590947, 0.17328519855595667]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.91692590713501 s
