Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 404, 'sum_payoffs': 96.13773818036046, 'action': [1.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 98.82068844232519, 'action': [2.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 101.92862476316495, 'action': [2.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 93.40539772845257, 'action': [1.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 93.85801018430725, 'action': [0.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 89.45175476595433, 'action': [0.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 90.11338224954135, 'action': [0.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.86044777908495, 'action': [2.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 92.75189338543423, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1121910580394335, 0.11441266314912524, 0.11691196889752846, 0.10996945292974174, 0.1102471535684532, 0.10663704526520411, 0.10719244654262705, 0.11274645931685642, 0.1094140516523188]
Actions to choose Agent 1: dict_values([{'num_count': 581, 'sum_payoffs': 124.22971531064694, 'action': [0.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 122.78544219819798, 'action': [0.0, 1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 137.44704431465252, 'action': [1.0, 0.0]}, {'num_count': 613, 'sum_payoffs': 133.84703847515462, 'action': [1.0, -1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 122.69947136300743, 'action': [0.0, -1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 138.55499571387534, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1613440710913635, 0.15995556789780616, 0.17356289919466814, 0.1702304915301305, 0.15995556789780616, 0.17467370174951402]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 89.46367478370667 s
