Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 397, 'sum_payoffs': 93.59723150037436, 'action': [1.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 93.1849200036225, 'action': [0.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 94.66490682375999, 'action': [2.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 97.60347224834753, 'action': [2.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 91.52700637985036, 'action': [0.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 92.62060295280956, 'action': [1.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 99.30510450138785, 'action': [1.0, -1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 86.78241119734754, 'action': [0.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 101.98571343006178, 'action': [2.0, 0.0]}])
Weights num count: [0.1102471535684532, 0.10996945292974174, 0.11108025548458761, 0.11357956123299083, 0.10858094973618439, 0.1094140516523188, 0.11496806442654818, 0.10469314079422383, 0.11718966953623994]
Actions to choose Agent 1: dict_values([{'num_count': 616, 'sum_payoffs': 134.8876221089955, 'action': [1.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 125.9093575740703, 'action': [0.0, 0.0]}, {'num_count': 613, 'sum_payoffs': 133.9719855958765, 'action': [1.0, 0.0]}, {'num_count': 579, 'sum_payoffs': 123.89822589058859, 'action': [0.0, 1.5707963267948966]}, {'num_count': 619, 'sum_payoffs': 135.81045903622507, 'action': [1.0, -1.5707963267948966]}, {'num_count': 587, 'sum_payoffs': 126.25451774596937, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17106359344626493, 0.16273257428492086, 0.1702304915301305, 0.16078866981394058, 0.17189669536239932, 0.16301027492363232]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 89.59946894645691 s
