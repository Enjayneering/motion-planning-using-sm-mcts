Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 416, 'sum_payoffs': 99.94901426362425, 'action': [2.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 97.55564296179323, 'action': [2.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 94.18442304128827, 'action': [1.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 94.4957956510657, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 90.11737621006043, 'action': [0.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 90.04444291557762, 'action': [0.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 94.1643492546078, 'action': [1.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 96.89660362551844, 'action': [2.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 93.07762033202731, 'action': [0.0, 0.0]}])
Weights num count: [0.11552346570397112, 0.11357956123299083, 0.11080255484587614, 0.11108025548458761, 0.10747014718133852, 0.10747014718133852, 0.11080255484587614, 0.1130241599555679, 0.10996945292974174]
Actions to choose Agent 1: dict_values([{'num_count': 575, 'sum_payoffs': 122.7585119057925, 'action': [0.0, 0.0]}, {'num_count': 583, 'sum_payoffs': 125.18084540193729, 'action': [0.0, -1.5707963267948966]}, {'num_count': 587, 'sum_payoffs': 126.29592295678209, 'action': [0.0, 1.5707963267948966]}, {'num_count': 631, 'sum_payoffs': 139.55896543235727, 'action': [1.0, 0.0]}, {'num_count': 610, 'sum_payoffs': 133.1920518499583, 'action': [1.0, 1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 134.41729977272814, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1596778672590947, 0.16189947236878643, 0.16301027492363232, 0.17522910302693695, 0.1693973896139961, 0.170508192168842]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.95772004127502 s
