Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 387, 'sum_payoffs': 90.35313843204709, 'action': [0.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 96.11231671073834, 'action': [1.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 104.66107548544535, 'action': [2.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 97.08755507538022, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 98.50762983048365, 'action': [2.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 94.04423904760458, 'action': [2.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 91.39967781475339, 'action': [0.0, -1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 87.06532789451319, 'action': [0.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 93.79249245997907, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10747014718133852, 0.1121910580394335, 0.11913357400722022, 0.1130241599555679, 0.11413496251041377, 0.11052485420716468, 0.10830324909747292, 0.10469314079422383, 0.1102471535684532]
Actions to choose Agent 1: dict_values([{'num_count': 608, 'sum_payoffs': 132.76282121644175, 'action': [1.0, 1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 125.2260750729912, 'action': [0.0, 1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 121.92659555891342, 'action': [0.0, 0.0]}, {'num_count': 630, 'sum_payoffs': 139.36945182122238, 'action': [1.0, 0.0]}, {'num_count': 628, 'sum_payoffs': 138.75818503903022, 'action': [1.0, -1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 124.00578170875092, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16884198833657318, 0.16189947236878643, 0.1588447653429603, 0.17495140238822549, 0.17439600111080256, 0.16078866981394058]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.31489086151123 s
