Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 385, 'sum_payoffs': 89.9465065663108, 'action': [0.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 98.0639357963024, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 98.76333350483942, 'action': [2.0, -1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 91.36560170447841, 'action': [0.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 97.44817749470177, 'action': [1.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 95.75064140523031, 'action': [1.0, 1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 86.88401773307137, 'action': [0.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 97.44641982403508, 'action': [2.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 99.7656737977223, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10691474590391557, 0.11357956123299083, 0.11413496251041377, 0.10802554845876146, 0.1130241599555679, 0.11163565676201055, 0.10441544015551235, 0.1130241599555679, 0.11496806442654818]
Actions to choose Agent 1: dict_values([{'num_count': 591, 'sum_payoffs': 127.34220342393792, 'action': [0.0, 0.0]}, {'num_count': 609, 'sum_payoffs': 132.78608901026394, 'action': [1.0, 1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 124.71527627000289, 'action': [0.0, 1.5707963267948966]}, {'num_count': 590, 'sum_payoffs': 127.08961382318141, 'action': [0.0, -1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 135.19299995512262, 'action': [1.0, -1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 133.40727820152392, 'action': [1.0, 0.0]}])
Weights num count: [0.1641210774784782, 0.16911968897528465, 0.16162177173007497, 0.16384337683976674, 0.1713412940849764, 0.16967509025270758]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 87.66471219062805 s
