Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 404, 'sum_payoffs': 96.30440450238055, 'action': [2.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 103.13429736399924, 'action': [2.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 97.28627872519084, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 89.46274471725626, 'action': [0.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 93.60049557137701, 'action': [2.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 94.89181914829175, 'action': [1.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 92.13453332717486, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 95.52171301414909, 'action': [1.0, 0.0]}, {'num_count': 391, 'sum_payoffs': 91.80042674620421, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1121910580394335, 0.11774507081366287, 0.1130241599555679, 0.10663704526520411, 0.10996945292974174, 0.11108025548458761, 0.10885865037489587, 0.11163565676201055, 0.10858094973618439]
Actions to choose Agent 1: dict_values([{'num_count': 617, 'sum_payoffs': 134.94717639971154, 'action': [1.0, 1.5707963267948966]}, {'num_count': 578, 'sum_payoffs': 123.28668203921676, 'action': [0.0, 1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 135.24013388695204, 'action': [1.0, -1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 127.17811444441163, 'action': [0.0, 0.0]}, {'num_count': 625, 'sum_payoffs': 137.30944509235454, 'action': [1.0, 0.0]}, {'num_count': 571, 'sum_payoffs': 121.22031836983089, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1713412940849764, 0.1605109691752291, 0.17161899472368786, 0.1641210774784782, 0.17356289919466814, 0.15856706470424883]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.04452610015869 s
