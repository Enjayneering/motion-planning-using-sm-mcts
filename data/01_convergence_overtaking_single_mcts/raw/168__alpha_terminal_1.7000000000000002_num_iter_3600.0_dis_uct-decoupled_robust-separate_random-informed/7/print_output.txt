Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 418, 'sum_payoffs': 100.37494545581221, 'action': [2.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 89.28758508462155, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 91.92716607852999, 'action': [0.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 90.65198886182084, 'action': [1.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 97.33074369338199, 'action': [1.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 88.63984039841137, 'action': [0.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 94.35259258030278, 'action': [1.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 99.98432465833999, 'action': [2.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.32384881357295, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11607886698139405, 0.10691474590391557, 0.10913635101360733, 0.10802554845876146, 0.11357956123299083, 0.10635934462649264, 0.11108025548458761, 0.11580116634268259, 0.11274645931685642]
Actions to choose Agent 1: dict_values([{'num_count': 577, 'sum_payoffs': 122.95514258299688, 'action': [0.0, -1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 141.09026831825543, 'action': [1.0, 0.0]}, {'num_count': 586, 'sum_payoffs': 125.58053459020283, 'action': [0.0, 1.5707963267948966]}, {'num_count': 590, 'sum_payoffs': 126.83821111128279, 'action': [1.0, 1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 136.68430628072525, 'action': [1.0, -1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 125.63610076040416, 'action': [0.0, 0.0]}])
Weights num count: [0.16023326853651762, 0.17717300749791726, 0.16273257428492086, 0.16384337683976674, 0.1730074979172452, 0.16273257428492086]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.7135362625122 s
