Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 414, 'sum_payoffs': 99.51670525303292, 'action': [2.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 90.32450386634451, 'action': [0.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 94.45380277484709, 'action': [1.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 94.7984408241708, 'action': [1.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 96.81136248251686, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 97.50136144920822, 'action': [2.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 96.4030781575415, 'action': [1.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 92.45309096127454, 'action': [0.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 90.64591942058007, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11496806442654818, 0.10747014718133852, 0.11080255484587614, 0.11108025548458761, 0.11274645931685642, 0.11330186059427937, 0.11246875867814496, 0.10913635101360733, 0.10774784782004998]
Actions to choose Agent 1: dict_values([{'num_count': 623, 'sum_payoffs': 137.1383287937148, 'action': [1.0, 1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 125.45886203865777, 'action': [0.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 126.07837666211398, 'action': [0.0, 0.0]}, {'num_count': 633, 'sum_payoffs': 140.27296567805482, 'action': [1.0, 0.0]}, {'num_count': 573, 'sum_payoffs': 122.17807798017284, 'action': [0.0, -1.5707963267948966]}, {'num_count': 601, 'sum_payoffs': 130.57318032224444, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1730074979172452, 0.16217717300749793, 0.16273257428492086, 0.1757845043043599, 0.15912246598167176, 0.16689808386559288]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 87.11981010437012 s
