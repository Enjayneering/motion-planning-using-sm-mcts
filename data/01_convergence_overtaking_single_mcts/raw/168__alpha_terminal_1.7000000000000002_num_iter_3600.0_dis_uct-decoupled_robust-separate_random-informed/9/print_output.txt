Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 397, 'sum_payoffs': 93.86619029756092, 'action': [1.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 94.16576279620764, 'action': [1.0, 1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 95.24062792652586, 'action': [0.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 98.52377148660403, 'action': [2.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 96.5536708718348, 'action': [1.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 92.47180933753606, 'action': [2.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 91.19308978058989, 'action': [0.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 100.578536577453, 'action': [2.0, -1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 91.035687138684, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1102471535684532, 0.11052485420716468, 0.11135795612329909, 0.11413496251041377, 0.11246875867814496, 0.10913635101360733, 0.10802554845876146, 0.11580116634268259, 0.10802554845876146]
Actions to choose Agent 1: dict_values([{'num_count': 618, 'sum_payoffs': 135.67910960206274, 'action': [1.0, 0.0]}, {'num_count': 617, 'sum_payoffs': 135.33083149049583, 'action': [1.0, 1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 134.51544353416617, 'action': [1.0, -1.5707963267948966]}, {'num_count': 590, 'sum_payoffs': 127.24139259804976, 'action': [0.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 125.4402009650196, 'action': [0.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 123.30928767512026, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17161899472368786, 0.1713412940849764, 0.170508192168842, 0.16384337683976674, 0.16217717300749793, 0.16023326853651762]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 86.74416875839233 s
