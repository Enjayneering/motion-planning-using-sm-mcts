Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 455, 'sum_payoffs': 106.31062881875691, 'action': [1.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 99.15519746065709, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 119.00146657213273, 'action': [2.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 101.70463142492085, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 104.38824667621564, 'action': [1.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 110.93749465385606, 'action': [2.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 100.70801643722208, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 106.63913485070927, 'action': [1.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 109.91557757202854, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11094854913435748, 0.10558400390148744, 0.1202145818093148, 0.1075347476225311, 0.10948549134357474, 0.11436235064618386, 0.10680321872713973, 0.11119239209948793, 0.11363082175079249]
Actions to choose Agent 1: dict_values([{'num_count': 679, 'sum_payoffs': 146.0323834833541, 'action': [0.0, 0.0]}, {'num_count': 692, 'sum_payoffs': 149.86592673650756, 'action': [1.0, 0.0]}, {'num_count': 698, 'sum_payoffs': 151.57317630276754, 'action': [1.0, 1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 154.55762567618365, 'action': [1.0, -1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 140.42077126061463, 'action': [0.0, -1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 141.29336765918856, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16556937332357963, 0.16873933187027554, 0.17020238966105827, 0.17264081931236283, 0.16093635698610095, 0.1616678858814923]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 96.91822743415833 s
