Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 449, 'sum_payoffs': 104.45303359136129, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 101.41573804500224, 'action': [0.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 100.46509686789325, 'action': [0.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 106.06274576518626, 'action': [1.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 111.05774325541519, 'action': [2.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 109.38982738000739, 'action': [2.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 105.42857150814166, 'action': [1.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 114.06116444799758, 'action': [2.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 107.11255777343379, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10948549134357474, 0.10729090465740064, 0.10655937576200927, 0.11070470616922702, 0.11436235064618386, 0.11314313582053158, 0.11021702023896611, 0.11655693733235796, 0.11143623506461839]
Actions to choose Agent 1: dict_values([{'num_count': 658, 'sum_payoffs': 139.49871218204623, 'action': [0.0, 1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 136.30135163905499, 'action': [0.0, -1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 152.650129819559, 'action': [1.0, 1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 159.34898917262478, 'action': [1.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 151.408440561892, 'action': [1.0, -1.5707963267948966]}, {'num_count': 667, 'sum_payoffs': 142.1119598846089, 'action': [0.0, 0.0]}])
Weights num count: [0.16044867105584004, 0.15776639843940501, 0.17142160448671057, 0.17702999268471104, 0.17044623262618874, 0.16264325774201413]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 96.24344515800476 s
