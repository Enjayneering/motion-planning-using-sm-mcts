Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 452, 'sum_payoffs': 105.64419813860295, 'action': [1.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 105.29384210380628, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 115.89658997543168, 'action': [2.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 109.18823961554249, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 115.58280317263498, 'action': [2.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 103.32681943777378, 'action': [0.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 99.61515792739858, 'action': [0.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 98.04187201979788, 'action': [0.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 108.53174571369777, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11021702023896611, 0.10997317727383565, 0.11777615215801024, 0.11289929285540112, 0.11753230919287978, 0.10851011948305292, 0.10582784686661789, 0.10460863204096561, 0.11241160692514021]
Actions to choose Agent 1: dict_values([{'num_count': 678, 'sum_payoffs': 145.57964239109762, 'action': [0.0, 0.0]}, {'num_count': 713, 'sum_payoffs': 155.93341789044987, 'action': [1.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 139.40175051342675, 'action': [0.0, -1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 152.39639715325572, 'action': [1.0, 0.0]}, {'num_count': 667, 'sum_payoffs': 142.35917544809334, 'action': [0.0, 1.5707963267948966]}, {'num_count': 684, 'sum_payoffs': 147.3503785734265, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16532553035844916, 0.17386003413801512, 0.16020482809070957, 0.17093391855644965, 0.16264325774201413, 0.1667885881492319]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 95.98587107658386 s
