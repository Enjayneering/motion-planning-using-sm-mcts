Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 465, 'sum_payoffs': 110.07459963916435, 'action': [2.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 106.87738229001519, 'action': [1.0, 1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 112.8347548507482, 'action': [2.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 104.89627752338144, 'action': [1.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 102.50166328705113, 'action': [0.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 103.13756498373283, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 110.76977337634726, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 102.82376512727161, 'action': [0.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 109.16882553611289, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11338697878566203, 0.11094854913435748, 0.11533772250670568, 0.10948549134357474, 0.10777859058766155, 0.10826627651792246, 0.11387466471592295, 0.10802243355279201, 0.11265544989027067]
Actions to choose Agent 1: dict_values([{'num_count': 681, 'sum_payoffs': 145.82640143276487, 'action': [0.0, 0.0]}, {'num_count': 650, 'sum_payoffs': 136.80375044352436, 'action': [0.0, -1.5707963267948966]}, {'num_count': 724, 'sum_payoffs': 158.40537184340522, 'action': [1.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 151.058622778721, 'action': [1.0, 1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 139.12384427070774, 'action': [0.0, 1.5707963267948966]}, {'num_count': 688, 'sum_payoffs': 147.91915661900785, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16605705925384054, 0.1584979273347964, 0.17654230675445012, 0.17044623262618874, 0.16044867105584004, 0.16776396000975372]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.85804057121277 s
