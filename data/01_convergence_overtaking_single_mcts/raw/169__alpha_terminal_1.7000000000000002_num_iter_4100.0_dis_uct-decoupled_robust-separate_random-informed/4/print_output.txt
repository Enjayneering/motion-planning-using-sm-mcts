Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 458, 'sum_payoffs': 107.69694032259778, 'action': [0.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 109.32636603430149, 'action': [1.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 108.73370141360445, 'action': [2.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 116.06586263182248, 'action': [2.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 106.70316385709258, 'action': [1.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 102.4055064871806, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 107.08450066454809, 'action': [2.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 103.38955700793305, 'action': [1.0, 1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 100.85655212867842, 'action': [0.0, 0.0]}])
Weights num count: [0.11168007802974884, 0.11289929285540112, 0.11241160692514021, 0.11777615215801024, 0.11094854913435748, 0.10777859058766155, 0.11119239209948793, 0.10851011948305292, 0.10655937576200927]
Actions to choose Agent 1: dict_values([{'num_count': 704, 'sum_payoffs': 152.8354598828684, 'action': [1.0, -1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 149.8295901914339, 'action': [1.0, 1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 140.5233119059273, 'action': [0.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 139.08650299701355, 'action': [0.0, 0.0]}, {'num_count': 653, 'sum_payoffs': 137.79690862161686, 'action': [0.0, 1.5707963267948966]}, {'num_count': 730, 'sum_payoffs': 160.50388988667575, 'action': [1.0, 0.0]}])
Weights num count: [0.171665447451841, 0.16922701780053645, 0.16142404291636187, 0.16020482809070957, 0.15922945623018775, 0.17800536454523286]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.49755191802979 s
