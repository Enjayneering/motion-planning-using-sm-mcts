Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 447, 'sum_payoffs': 104.18471649366231, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 107.24865867968785, 'action': [2.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 107.80224444906926, 'action': [1.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 101.91728132948877, 'action': [1.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 104.27135955448223, 'action': [0.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 114.50444765305633, 'action': [2.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 111.53021043122176, 'action': [2.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 104.24625919232714, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 107.86452842403665, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10899780541331383, 0.11119239209948793, 0.11168007802974884, 0.10729090465740064, 0.10899780541331383, 0.11655693733235796, 0.11436235064618386, 0.10899780541331383, 0.11168007802974884]
Actions to choose Agent 1: dict_values([{'num_count': 650, 'sum_payoffs': 136.85505249440044, 'action': [0.0, -1.5707963267948966]}, {'num_count': 696, 'sum_payoffs': 150.2845429787995, 'action': [1.0, 1.5707963267948966]}, {'num_count': 680, 'sum_payoffs': 145.59045526258546, 'action': [0.0, 0.0]}, {'num_count': 660, 'sum_payoffs': 139.7791127652942, 'action': [0.0, 1.5707963267948966]}, {'num_count': 695, 'sum_payoffs': 149.99111603252553, 'action': [1.0, -1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 157.1310201963898, 'action': [1.0, 0.0]}])
Weights num count: [0.1584979273347964, 0.16971470373079736, 0.16581321628871007, 0.16093635698610095, 0.16947086076566692, 0.17532309192879786]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 102.65434718132019 s
