Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 449, 'sum_payoffs': 104.40940964474471, 'action': [1.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 107.05128006298925, 'action': [2.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 103.47958565495485, 'action': [0.0, 0.0]}, {'num_count': 453, 'sum_payoffs': 105.70163343249595, 'action': [1.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 112.01922886549973, 'action': [2.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 106.41469521616187, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 104.13475662567397, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 115.68894849311201, 'action': [2.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 100.52231139233683, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10948549134357474, 0.11143623506461839, 0.10875396244818337, 0.11046086320409657, 0.11509387954157523, 0.11094854913435748, 0.10924164837844429, 0.11777615215801024, 0.10655937576200927]
Actions to choose Agent 1: dict_values([{'num_count': 707, 'sum_payoffs': 153.83332091882912, 'action': [1.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 137.24457209579793, 'action': [0.0, -1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 140.3972477264753, 'action': [0.0, 1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 159.77641743708742, 'action': [1.0, 0.0]}, {'num_count': 709, 'sum_payoffs': 154.49892357735607, 'action': [1.0, 1.5707963267948966]}, {'num_count': 646, 'sum_payoffs': 135.98617279292384, 'action': [0.0, 0.0]}])
Weights num count: [0.1723969763472324, 0.1584979273347964, 0.1611801999512314, 0.1772738356498415, 0.1728846622774933, 0.15752255547427457]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.65362858772278 s
