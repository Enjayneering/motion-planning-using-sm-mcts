Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 448, 'sum_payoffs': 104.2840451679163, 'action': [1.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 111.18939227742081, 'action': [2.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 109.06727229394892, 'action': [2.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 106.58889593824844, 'action': [2.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 104.22956533879004, 'action': [0.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 103.53226092821953, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 110.56139116986674, 'action': [1.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 108.88155694115216, 'action': [1.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 102.27442150427743, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10924164837844429, 0.11436235064618386, 0.11289929285540112, 0.11094854913435748, 0.10924164837844429, 0.10875396244818337, 0.11387466471592295, 0.11265544989027067, 0.10777859058766155]
Actions to choose Agent 1: dict_values([{'num_count': 723, 'sum_payoffs': 158.9779036062937, 'action': [1.0, 0.0]}, {'num_count': 675, 'sum_payoffs': 144.7481912172578, 'action': [0.0, -1.5707963267948966]}, {'num_count': 700, 'sum_payoffs': 152.0790859893854, 'action': [1.0, -1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 141.25018889751914, 'action': [0.0, 0.0]}, {'num_count': 684, 'sum_payoffs': 147.49031706405407, 'action': [1.0, 1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 138.97705263864216, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17629846378931968, 0.1645940014630578, 0.17069007559131918, 0.1616678858814923, 0.1667885881492319, 0.15971714216044866]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 98.8496642112732 s
