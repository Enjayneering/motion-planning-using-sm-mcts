Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 468, 'sum_payoffs': 111.04493096337512, 'action': [2.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 103.35270229919716, 'action': [0.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 101.15517627018528, 'action': [0.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 102.11555156418764, 'action': [0.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 109.97440432293462, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 115.7055359020664, 'action': [2.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 107.05204829107385, 'action': [1.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 108.07525397132498, 'action': [2.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 103.68524830585046, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1141185076810534, 0.10851011948305292, 0.10680321872713973, 0.1075347476225311, 0.11338697878566203, 0.11753230919287978, 0.11119239209948793, 0.1119239209948793, 0.10875396244818337]
Actions to choose Agent 1: dict_values([{'num_count': 663, 'sum_payoffs': 140.1099417283444, 'action': [0.0, -1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 150.57343805460647, 'action': [1.0, 1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 151.420631755384, 'action': [1.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 133.73855870071364, 'action': [0.0, 1.5707963267948966]}, {'num_count': 675, 'sum_payoffs': 143.65979702643406, 'action': [0.0, 0.0]}, {'num_count': 720, 'sum_payoffs': 156.8207105432797, 'action': [1.0, 0.0]}])
Weights num count: [0.1616678858814923, 0.17044623262618874, 0.1711777615215801, 0.15630334064862228, 0.1645940014630578, 0.1755669348939283]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 103.44123125076294 s
