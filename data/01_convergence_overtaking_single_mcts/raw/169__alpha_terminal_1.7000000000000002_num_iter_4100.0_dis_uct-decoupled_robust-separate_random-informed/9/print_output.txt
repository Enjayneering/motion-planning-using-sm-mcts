Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 467, 'sum_payoffs': 110.70458012170873, 'action': [2.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 100.73325788967817, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 106.38858136635244, 'action': [1.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 102.37085728937886, 'action': [1.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 103.67979839455022, 'action': [0.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 104.30727387523294, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 107.32187374754574, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 110.02766890683542, 'action': [2.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 116.25407373818584, 'action': [2.0, 0.0]}])
Weights num count: [0.11387466471592295, 0.10655937576200927, 0.11070470616922702, 0.10777859058766155, 0.10875396244818337, 0.10924164837844429, 0.11143623506461839, 0.11338697878566203, 0.1180199951231407]
Actions to choose Agent 1: dict_values([{'num_count': 707, 'sum_payoffs': 153.8862503537318, 'action': [1.0, 1.5707963267948966]}, {'num_count': 688, 'sum_payoffs': 148.32829534076023, 'action': [1.0, -1.5707963267948966]}, {'num_count': 674, 'sum_payoffs': 144.20454116609204, 'action': [0.0, 1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 135.7602273173516, 'action': [0.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 160.34314008866318, 'action': [1.0, 0.0]}, {'num_count': 657, 'sum_payoffs': 139.2367071298314, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1723969763472324, 0.16776396000975372, 0.16435015849792733, 0.1572787125091441, 0.17776152158010242, 0.16020482809070957]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.2059075832367 s
