Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 315, 'sum_payoffs': 76.41135952491622, 'action': [2.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.361058590950684, 'action': [1.0, -1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 75.71605500337986, 'action': [2.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 50.02477021529716, 'action': [0.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 94.32334555791579, 'action': [2.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 60.71790190852959, 'action': [1.0, 1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 77.54539395681684, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 61.04150822127097, 'action': [0.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 49.445277353121995, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12110726643598616, 0.10457516339869281, 0.12033833141099577, 0.0918877354863514, 0.14033064206074586, 0.10380622837370242, 0.12226066897347174, 0.10419069588619762, 0.09111880046136102]
Actions to choose Agent 1: dict_values([{'num_count': 382, 'sum_payoffs': 101.0658800866298, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 128.4784781308847, 'action': [1.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 115.3549312109135, 'action': [0.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 147.69426878310642, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 99.95328421082938, 'action': [0.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 128.1221345635973, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14686658977316416, 0.17531718569780855, 0.16186082276047675, 0.19492502883506344, 0.14571318723567858, 0.17493271818531334]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.391201496124268 s
