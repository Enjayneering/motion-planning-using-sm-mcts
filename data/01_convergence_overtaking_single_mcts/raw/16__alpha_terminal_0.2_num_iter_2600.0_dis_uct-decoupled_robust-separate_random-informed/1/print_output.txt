Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 238, 'sum_payoffs': 49.6799426291405, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.29152813879706, 'action': [1.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 61.01351497152495, 'action': [0.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 76.0317667126827, 'action': [2.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 75.77406947440284, 'action': [2.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 93.97851797175912, 'action': [2.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 61.36105859095067, 'action': [1.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 76.45695267038546, 'action': [1.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 50.66792689771827, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.0915032679738562, 0.10457516339869281, 0.10419069588619762, 0.12072279892349097, 0.12033833141099577, 0.13994617454825067, 0.10457516339869281, 0.12110726643598616, 0.09265667051134178]
Actions to choose Agent 1: dict_values([{'num_count': 454, 'sum_payoffs': 127.73102577023313, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 101.795949834243, 'action': [0.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 99.95915084273713, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 128.85513762716437, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 116.46821514845227, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 145.80441660806238, 'action': [1.0, 0.0]}])
Weights num count: [0.17454825067281815, 0.14763552479815456, 0.14571318723567858, 0.17570165321030373, 0.16301422529796233, 0.19300269127258746]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.481755256652832 s
