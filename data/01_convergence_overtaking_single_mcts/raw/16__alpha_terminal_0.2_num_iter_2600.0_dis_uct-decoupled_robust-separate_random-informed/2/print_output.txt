Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 272, 'sum_payoffs': 61.3206440156509, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 50.134932525435325, 'action': [0.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 49.714707855217306, 'action': [0.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 76.41135952491624, 'action': [2.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 75.71605500337988, 'action': [2.0, -1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 75.74223756427995, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 60.99319904253263, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 61.69143688002284, 'action': [0.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 94.71152103532607, 'action': [2.0, 0.0]}])
Weights num count: [0.10457516339869281, 0.0918877354863514, 0.0915032679738562, 0.12110726643598616, 0.12033833141099577, 0.12033833141099577, 0.10419069588619762, 0.104959630911188, 0.14071510957324107]
Actions to choose Agent 1: dict_values([{'num_count': 377, 'sum_payoffs': 99.15661732701642, 'action': [0.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 113.20694723221987, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 129.90385240003414, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 100.22564802887852, 'action': [0.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 147.30315998974234, 'action': [1.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 130.7063859157548, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14494425221068818, 0.15955401768550556, 0.1768550557477893, 0.14609765474817377, 0.19454056132256825, 0.1776239907727797]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.363423824310303 s
