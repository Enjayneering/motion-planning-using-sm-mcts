Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 238, 'sum_payoffs': 49.6799426291405, 'action': [0.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 77.17257312091412, 'action': [1.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 49.714707855217306, 'action': [0.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 61.74065140318418, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 61.760061987753424, 'action': [0.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 74.69330550872523, 'action': [2.0, -1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 75.7682028424951, 'action': [2.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 95.12768976248248, 'action': [2.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 61.09749472076307, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.0915032679738562, 0.12187620146097655, 0.0915032679738562, 0.104959630911188, 0.104959630911188, 0.11918492887351019, 0.12033833141099577, 0.14109957708573626, 0.10419069588619762]
Actions to choose Agent 1: dict_values([{'num_count': 460, 'sum_payoffs': 129.92710164498033, 'action': [1.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 101.33259455545605, 'action': [0.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 147.27223342404235, 'action': [1.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 113.84242659388725, 'action': [0.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 100.27486255203995, 'action': [0.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 127.58316491806433, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1768550557477893, 0.14725105728565935, 0.19454056132256825, 0.16032295271049596, 0.14609765474817377, 0.17454825067281815]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.218327522277832 s
