Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 311, 'sum_payoffs': 75.14242877311237, 'action': [2.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 49.79010493927865, 'action': [0.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 49.0541685597578, 'action': [0.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 94.84967659521034, 'action': [2.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 78.58936472488683, 'action': [1.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 62.15884810187141, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 61.3958238170275, 'action': [1.0, -1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 74.78608520582502, 'action': [2.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.43645567501202, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11956939638600539, 0.0915032679738562, 0.09073433294886582, 0.14071510957324107, 0.12341407151095732, 0.1053440984236832, 0.10457516339869281, 0.11918492887351019, 0.10457516339869281]
Actions to choose Agent 1: dict_values([{'num_count': 376, 'sum_payoffs': 98.86687089592881, 'action': [0.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 100.28659581585536, 'action': [0.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 130.74408445778542, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 146.93146178094742, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 128.44664622076178, 'action': [1.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 115.39918444659013, 'action': [0.0, 0.0]}])
Weights num count: [0.144559784698193, 0.14609765474817377, 0.1776239907727797, 0.19415609381007304, 0.17531718569780855, 0.16186082276047675]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.233936786651611 s
