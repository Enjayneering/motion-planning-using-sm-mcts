Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 276, 'sum_payoffs': 62.84850327418476, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 61.37844120398909, 'action': [1.0, -1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 76.09677044272125, 'action': [1.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 75.10766354703554, 'action': [2.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 62.13176019654835, 'action': [1.0, 1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 74.72807073480207, 'action': [2.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 94.8419992744566, 'action': [2.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 49.82487016535547, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 50.157964487696724, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1061130334486736, 0.10457516339869281, 0.12072279892349097, 0.11956939638600539, 0.1053440984236832, 0.11918492887351019, 0.14071510957324107, 0.0915032679738562, 0.0918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 424, 'sum_payoffs': 116.51065769528293, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 126.51413421620227, 'action': [1.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 101.0078656156069, 'action': [0.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 145.43069042773652, 'action': [1.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 128.84068833007981, 'action': [1.0, 1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 102.1060121943229, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16301422529796233, 0.17339484813533257, 0.14686658977316416, 0.19261822376009227, 0.17570165321030373, 0.14801999231064975]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.298670768737793 s
