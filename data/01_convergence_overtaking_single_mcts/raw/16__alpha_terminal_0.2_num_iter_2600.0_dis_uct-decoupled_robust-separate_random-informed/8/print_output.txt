Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 236, 'sum_payoffs': 49.11218303078074, 'action': [0.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 49.74947308129412, 'action': [0.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.37279185476611, 'action': [0.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 95.44159078293924, 'action': [2.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 74.96860264272831, 'action': [2.0, 1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 76.12454640978247, 'action': [2.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 61.734784771276445, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 77.1852117291526, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 61.04534688164785, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.09073433294886582, 0.0915032679738562, 0.10457516339869281, 0.14148404459823144, 0.11956939638600539, 0.12072279892349097, 0.104959630911188, 0.12187620146097655, 0.10419069588619762]
Actions to choose Agent 1: dict_values([{'num_count': 379, 'sum_payoffs': 99.95350149351403, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 128.40601436277723, 'action': [1.0, 1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 115.07173947347187, 'action': [0.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 148.48619166211944, 'action': [1.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 99.62018988848814, 'action': [0.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 129.17084933646714, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14571318723567858, 0.17531718569780855, 0.16147635524798154, 0.19569396386005383, 0.1453287197231834, 0.17608612072279892]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.3460304737091064 s
