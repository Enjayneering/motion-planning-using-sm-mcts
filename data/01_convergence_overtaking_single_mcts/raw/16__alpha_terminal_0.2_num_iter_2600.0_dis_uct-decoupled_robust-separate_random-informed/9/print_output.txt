Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 271, 'sum_payoffs': 61.126393314932166, 'action': [1.0, 1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 77.63907899833973, 'action': [1.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 62.213929256940496, 'action': [0.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 94.79843410051811, 'action': [2.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 61.52336874020406, 'action': [1.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 50.49127609272269, 'action': [0.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 75.50463897230749, 'action': [2.0, 1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 74.05014882630415, 'action': [2.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 50.146448506566, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10419069588619762, 0.12226066897347174, 0.1053440984236832, 0.14071510957324107, 0.10457516339869281, 0.0922722029988466, 0.11995386389850057, 0.1184159938485198, 0.0918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 377, 'sum_payoffs': 99.13630139802414, 'action': [0.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 100.58785822807356, 'action': [0.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 127.63553003986429, 'action': [1.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 145.64210645880894, 'action': [1.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 117.57968839714498, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 129.44321315451637, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14494425221068818, 0.14648212226066898, 0.17454825067281815, 0.19300269127258746, 0.16416762783544792, 0.17647058823529413]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.212785482406616 s
