Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 537, 'sum_payoffs': 127.1135166140948, 'action': [2.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 118.04182112248253, 'action': [1.0, -1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 118.30737493068429, 'action': [1.0, 1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 107.74590931699834, 'action': [0.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 113.85297999545281, 'action': [0.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 118.68735322500221, 'action': [1.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 111.90516612086381, 'action': [0.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 122.86911859688898, 'action': [2.0, 1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 130.0030100855078, 'action': [2.0, 0.0]}])
Weights num count: [0.11671375787872201, 0.11062812432079983, 0.11084546837643991, 0.10367311454031732, 0.10780265159747882, 0.11106281243207998, 0.10649858726363834, 0.113888285155401, 0.11866985437948271]
Actions to choose Agent 1: dict_values([{'num_count': 789, 'sum_payoffs': 170.01318615197926, 'action': [1.0, 1.5707963267948966]}, {'num_count': 757, 'sum_payoffs': 160.83550611100725, 'action': [0.0, 0.0]}, {'num_count': 748, 'sum_payoffs': 158.18691873587002, 'action': [0.0, -1.5707963267948966]}, {'num_count': 804, 'sum_payoffs': 174.38411616489782, 'action': [1.0, 0.0]}, {'num_count': 738, 'sum_payoffs': 155.31745027117142, 'action': [0.0, 1.5707963267948966]}, {'num_count': 764, 'sum_payoffs': 162.82093737946812, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17148445990002173, 0.16452945011953923, 0.16257335361877853, 0.1747446207346229, 0.16039991306237775, 0.16605085850901977]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 254.37126111984253 s
