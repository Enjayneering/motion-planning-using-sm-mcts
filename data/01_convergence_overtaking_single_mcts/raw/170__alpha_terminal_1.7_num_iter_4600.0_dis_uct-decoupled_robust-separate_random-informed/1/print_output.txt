Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 550, 'sum_payoffs': 131.4783315579783, 'action': [2.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 119.96160478747386, 'action': [2.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 116.52006903692887, 'action': [0.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 109.41328421719575, 'action': [0.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 122.63067329762974, 'action': [2.0, -1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 117.70776273057828, 'action': [1.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 120.3345977822322, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 117.07911926785029, 'action': [1.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 113.96482883727975, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11953923060204304, 0.1119321886546403, 0.10954140404259943, 0.10475983481851771, 0.11367094109976092, 0.11041078026515974, 0.11214953271028037, 0.10997609215387959, 0.10780265159747882]
Actions to choose Agent 1: dict_values([{'num_count': 788, 'sum_payoffs': 169.80702552892467, 'action': [1.0, -1.5707963267948966]}, {'num_count': 741, 'sum_payoffs': 156.29724485437214, 'action': [0.0, -1.5707963267948966]}, {'num_count': 780, 'sum_payoffs': 167.47377381123326, 'action': [1.0, 1.5707963267948966]}, {'num_count': 805, 'sum_payoffs': 174.76061895785148, 'action': [1.0, 0.0]}, {'num_count': 753, 'sum_payoffs': 159.7014603699847, 'action': [0.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 154.0441428195874, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17126711584438165, 0.16105194522929797, 0.16952836339926103, 0.174961964790263, 0.16366007389697892, 0.15931319278417735]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 269.0418984889984 s
