Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 500, 'sum_payoffs': 115.68593924473319, 'action': [0.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 126.0657834447342, 'action': [2.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 119.91849921999986, 'action': [1.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 113.06769250932012, 'action': [0.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 114.08562392317349, 'action': [1.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 120.19725101086077, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 112.5116710027, 'action': [0.0, -1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 130.32521994240219, 'action': [2.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 121.88975798899364, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10867202782003912, 0.11562703760052162, 0.11149750054336013, 0.1069332753749185, 0.10758530754183873, 0.11171484459900022, 0.10649858726363834, 0.11845251032384264, 0.11280156487720061]
Actions to choose Agent 1: dict_values([{'num_count': 801, 'sum_payoffs': 172.8659157282415, 'action': [1.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 152.24910898637236, 'action': [0.0, 1.5707963267948966]}, {'num_count': 743, 'sum_payoffs': 156.19500917865201, 'action': [0.0, -1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 157.68268680841422, 'action': [0.0, 0.0]}, {'num_count': 778, 'sum_payoffs': 166.2356895187838, 'action': [1.0, 1.5707963267948966]}, {'num_count': 801, 'sum_payoffs': 172.8676616156265, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17409258856770268, 0.15844381656161705, 0.16148663334057814, 0.16257335361877853, 0.16909367528798086, 0.17409258856770268]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 252.57205271720886 s
