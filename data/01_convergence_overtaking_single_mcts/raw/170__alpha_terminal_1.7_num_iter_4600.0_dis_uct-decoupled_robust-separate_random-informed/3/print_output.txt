Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 503, 'sum_payoffs': 116.27205482775747, 'action': [1.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 110.73953241622102, 'action': [0.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 119.7630088779736, 'action': [2.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 124.02868726458188, 'action': [2.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 120.69117659950302, 'action': [1.0, -1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 117.85888138498089, 'action': [0.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 116.54910537211519, 'action': [1.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 126.60256632356734, 'action': [2.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 117.1194845976302, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10932405998695936, 0.10562921104107803, 0.11171484459900022, 0.11454031732232123, 0.11236687676592046, 0.11041078026515974, 0.10954140404259943, 0.11627906976744186, 0.10997609215387959]
Actions to choose Agent 1: dict_values([{'num_count': 782, 'sum_payoffs': 167.56915449238883, 'action': [1.0, 1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 157.8008947686136, 'action': [0.0, -1.5707963267948966]}, {'num_count': 786, 'sum_payoffs': 168.709597443177, 'action': [1.0, -1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 149.56702670318, 'action': [0.0, 1.5707963267948966]}, {'num_count': 821, 'sum_payoffs': 178.88462767142153, 'action': [1.0, 0.0]}, {'num_count': 744, 'sum_payoffs': 156.71747064959328, 'action': [0.0, 0.0]}])
Weights num count: [0.1699630515105412, 0.16257335361877853, 0.1708324277331015, 0.15627037600521626, 0.17843946968050423, 0.16170397739621822]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 133.9151840209961 s
