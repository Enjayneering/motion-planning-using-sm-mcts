Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 518, 'sum_payoffs': 120.9123328139239, 'action': [1.0, 0.0]}, {'num_count': 535, 'sum_payoffs': 126.4561360633204, 'action': [2.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 125.0457257584803, 'action': [2.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 113.78620950725153, 'action': [0.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 123.46858489582029, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 117.04660644793468, 'action': [2.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 114.73580982383508, 'action': [1.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 111.49226496646612, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 115.06309457229445, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11258422082156053, 0.11627906976744186, 0.11540969354488155, 0.10780265159747882, 0.11432297326668116, 0.10997609215387959, 0.10845468376439904, 0.10628124320799825, 0.10867202782003912]
Actions to choose Agent 1: dict_values([{'num_count': 790, 'sum_payoffs': 170.0899436694152, 'action': [1.0, 1.5707963267948966]}, {'num_count': 796, 'sum_payoffs': 171.86655591136994, 'action': [1.0, 0.0]}, {'num_count': 732, 'sum_payoffs': 153.40169264772544, 'action': [0.0, 1.5707963267948966]}, {'num_count': 747, 'sum_payoffs': 157.74472615740677, 'action': [0.0, 0.0]}, {'num_count': 741, 'sum_payoffs': 155.9578683530002, 'action': [0.0, -1.5707963267948966]}, {'num_count': 794, 'sum_payoffs': 171.26195873256316, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1717018039556618, 0.1730058682895023, 0.15909584872853727, 0.16235600956313845, 0.16105194522929797, 0.17257118017822212]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 107.97661876678467 s
