Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 543, 'sum_payoffs': 129.09778481398817, 'action': [2.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 112.84466409837329, 'action': [0.0, 1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 124.5383333917654, 'action': [2.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 118.13240648543088, 'action': [1.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 121.28458361376448, 'action': [2.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 115.86302093538073, 'action': [0.0, -1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 116.51953981995355, 'action': [1.0, 1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 110.69379039779922, 'action': [0.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 119.98290349537751, 'action': [1.0, 0.0]}])
Weights num count: [0.11801782221256249, 0.10715061943055858, 0.1149750054336014, 0.11062812432079983, 0.11280156487720061, 0.10910671593131928, 0.10954140404259943, 0.10562921104107803, 0.1119321886546403]
Actions to choose Agent 1: dict_values([{'num_count': 748, 'sum_payoffs': 158.2029561834413, 'action': [0.0, 0.0]}, {'num_count': 727, 'sum_payoffs': 152.21626446830055, 'action': [0.0, -1.5707963267948966]}, {'num_count': 773, 'sum_payoffs': 165.37770052739927, 'action': [1.0, 1.5707963267948966]}, {'num_count': 779, 'sum_payoffs': 167.1088032414657, 'action': [1.0, -1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 178.42921442749878, 'action': [1.0, 0.0]}, {'num_count': 755, 'sum_payoffs': 160.17803622899712, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16257335361877853, 0.15800912845033688, 0.1680069550097805, 0.16931101934362094, 0.177787437513584, 0.16409476200825907]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 123.0715537071228 s
