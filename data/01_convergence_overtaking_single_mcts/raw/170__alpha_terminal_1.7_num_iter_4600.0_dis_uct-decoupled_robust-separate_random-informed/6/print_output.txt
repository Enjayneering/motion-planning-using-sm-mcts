Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 519, 'sum_payoffs': 121.08074086697052, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 111.70730610842092, 'action': [0.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 113.32078465557004, 'action': [0.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 119.8218410433541, 'action': [1.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 130.89284201388534, 'action': [2.0, 0.0]}, {'num_count': 517, 'sum_payoffs': 120.43264310060853, 'action': [1.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 119.85704021414915, 'action': [2.0, 1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 110.53559386237531, 'action': [0.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 119.54305961499249, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11280156487720061, 0.10649858726363834, 0.10758530754183873, 0.1119321886546403, 0.11932188654640295, 0.11236687676592046, 0.1119321886546403, 0.10562921104107803, 0.11171484459900022]
Actions to choose Agent 1: dict_values([{'num_count': 729, 'sum_payoffs': 153.06203711432872, 'action': [0.0, 1.5707963267948966]}, {'num_count': 735, 'sum_payoffs': 154.76560493385148, 'action': [0.0, -1.5707963267948966]}, {'num_count': 746, 'sum_payoffs': 157.91409653633056, 'action': [0.0, 0.0]}, {'num_count': 780, 'sum_payoffs': 167.77657229702928, 'action': [1.0, -1.5707963267948966]}, {'num_count': 799, 'sum_payoffs': 173.2411582342542, 'action': [1.0, 1.5707963267948966]}, {'num_count': 811, 'sum_payoffs': 176.69346357746346, 'action': [1.0, 0.0]}])
Weights num count: [0.15844381656161705, 0.15974788089545752, 0.16213866550749836, 0.16952836339926103, 0.17365790045642251, 0.17626602912410347]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 109.94894242286682 s
