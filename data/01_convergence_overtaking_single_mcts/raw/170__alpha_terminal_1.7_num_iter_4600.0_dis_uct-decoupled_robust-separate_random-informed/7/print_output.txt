Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 533, 'sum_payoffs': 125.54936865122858, 'action': [2.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 110.98497529201344, 'action': [0.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 106.81262635413746, 'action': [0.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 121.29378886380906, 'action': [1.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 119.09196554874033, 'action': [1.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 114.89278747979975, 'action': [0.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 118.66842403849058, 'action': [1.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 127.75670693613839, 'action': [2.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 120.96148648996758, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1158443816561617, 0.10606389915235818, 0.10323842642903716, 0.11301890893284068, 0.11149750054336013, 0.10867202782003912, 0.11128015648772006, 0.11736579004564225, 0.11280156487720061]
Actions to choose Agent 1: dict_values([{'num_count': 729, 'sum_payoffs': 152.61510840186025, 'action': [0.0, 1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 156.86222070894271, 'action': [0.0, 0.0]}, {'num_count': 790, 'sum_payoffs': 170.10734388446824, 'action': [1.0, 1.5707963267948966]}, {'num_count': 735, 'sum_payoffs': 154.31672738095472, 'action': [0.0, -1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 170.8829140225245, 'action': [1.0, 0.0]}, {'num_count': 809, 'sum_payoffs': 175.57124634417167, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15844381656161705, 0.16170397739621822, 0.1717018039556618, 0.15974788089545752, 0.17235383612258204, 0.1758313410128233]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 109.72395753860474 s
