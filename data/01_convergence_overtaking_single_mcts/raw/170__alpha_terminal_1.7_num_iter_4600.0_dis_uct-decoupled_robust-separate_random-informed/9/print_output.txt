Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 483, 'sum_payoffs': 109.53783601727812, 'action': [0.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 127.8984486687042, 'action': [2.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 116.61689844966843, 'action': [2.0, 1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 128.8758157435536, 'action': [2.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 121.15158571915579, 'action': [1.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 115.35194982060894, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 115.23970784508836, 'action': [0.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 119.821600612892, 'action': [0.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 112.73670129893728, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10497717887415779, 0.11736579004564225, 0.10975874809823952, 0.11801782221256249, 0.11280156487720061, 0.1088893718756792, 0.1088893718756792, 0.1119321886546403, 0.10715061943055858]
Actions to choose Agent 1: dict_values([{'num_count': 784, 'sum_payoffs': 168.8985418399369, 'action': [1.0, 0.0]}, {'num_count': 796, 'sum_payoffs': 172.42717144828234, 'action': [1.0, -1.5707963267948966]}, {'num_count': 789, 'sum_payoffs': 170.37761147980729, 'action': [1.0, 1.5707963267948966]}, {'num_count': 762, 'sum_payoffs': 162.57544576960825, 'action': [0.0, 0.0]}, {'num_count': 738, 'sum_payoffs': 155.73287223752288, 'action': [0.0, -1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 153.68038499665357, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17039773962182134, 0.1730058682895023, 0.17148445990002173, 0.16561617039773963, 0.16039991306237775, 0.1588785046728972]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 109.12224125862122 s
