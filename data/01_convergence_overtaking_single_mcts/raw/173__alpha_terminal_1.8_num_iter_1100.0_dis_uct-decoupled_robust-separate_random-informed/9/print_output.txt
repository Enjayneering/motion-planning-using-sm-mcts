Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 121, 'sum_payoffs': 28.804013934089816, 'action': [0.0, 0.0]}, {'num_count': 127, 'sum_payoffs': 31.240202316242662, 'action': [1.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 29.280659052312306, 'action': [0.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 29.560327391930798, 'action': [1.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 30.45726121873979, 'action': [2.0, 1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 26.82145079808604, 'action': [0.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 28.426259801986248, 'action': [1.0, 0.0]}, {'num_count': 125, 'sum_payoffs': 30.479085223532017, 'action': [2.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 28.729919677298565, 'action': [2.0, 0.0]}])
Weights num count: [0.10990009082652134, 0.11534968210717529, 0.11080835603996367, 0.11171662125340599, 0.11353315168029064, 0.10535876475930972, 0.10899182561307902, 0.11353315168029064, 0.10990009082652134]
Actions to choose Agent 1: dict_values([{'num_count': 177, 'sum_payoffs': 38.46730291787907, 'action': [0.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 41.678081332284584, 'action': [1.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 43.229264787447555, 'action': [1.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 40.60556366176985, 'action': [0.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 42.4605280432509, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 38.18531933506555, 'action': [0.0, 0.0]}])
Weights num count: [0.16076294277929154, 0.16893732970027248, 0.17257039055404177, 0.16621253405994552, 0.17075386012715713, 0.15985467756584923]
Selected final action: [1.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 30.79053497314453 s
