Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 190, 'sum_payoffs': 47.377443261420204, 'action': [2.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 43.52499346964326, 'action': [1.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 41.25814841433498, 'action': [0.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 41.97359730862326, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 42.04425533428638, 'action': [1.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.580918421162, 'action': [2.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 39.751697779636835, 'action': [0.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 44.55985563329773, 'action': [2.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 40.11810711767719, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11867582760774516, 0.11242973141786383, 0.10868207370393504, 0.1099312929419113, 0.1099312929419113, 0.11242973141786383, 0.1061836352279825, 0.11430356027482823, 0.10680824484697064]
Actions to choose Agent 1: dict_values([{'num_count': 278, 'sum_payoffs': 62.72327065748134, 'action': [1.0, -1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 54.98978593305673, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 60.321517277495275, 'action': [1.0, 1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 55.30039563567189, 'action': [0.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 61.73091830320381, 'action': [1.0, 0.0]}, {'num_count': 265, 'sum_payoffs': 58.41089142029513, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1736414740787008, 0.15927545284197375, 0.16926920674578388, 0.1599000624609619, 0.1717676452217364, 0.1655215490318551]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.3119695186615 s
