Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 171, 'sum_payoffs': 40.35783000688379, 'action': [1.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 43.482657359169416, 'action': [1.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 41.23205674817115, 'action': [0.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 44.969096391069726, 'action': [2.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 42.65692640039545, 'action': [0.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.88726294487812, 'action': [1.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 44.67387755429203, 'action': [2.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 41.48774976171335, 'action': [0.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 44.27786619696009, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10680824484697064, 0.11180512179887571, 0.1080574640849469, 0.11430356027482823, 0.11055590256089944, 0.11242973141786383, 0.1136789506558401, 0.10868207370393504, 0.11305434103685197]
Actions to choose Agent 1: dict_values([{'num_count': 269, 'sum_payoffs': 59.68820696231757, 'action': [1.0, 1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 57.9831765991231, 'action': [0.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 60.44909665111105, 'action': [1.0, -1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 57.743272992010574, 'action': [0.0, 0.0]}, {'num_count': 257, 'sum_payoffs': 55.69472131494987, 'action': [0.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 62.056663427265924, 'action': [1.0, 0.0]}])
Weights num count: [0.1680199875078076, 0.16489693941286696, 0.16926920674578388, 0.16427232979387882, 0.16052467207995003, 0.17239225484072454]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.64218807220459 s
