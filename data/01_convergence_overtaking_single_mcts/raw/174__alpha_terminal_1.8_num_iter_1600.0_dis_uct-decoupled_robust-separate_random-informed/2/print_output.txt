Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 39.70188136332155, 'action': [0.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 40.787990984436924, 'action': [2.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 41.980481297065424, 'action': [1.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 46.71471801076631, 'action': [2.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 42.410401704161806, 'action': [1.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 43.89589527004714, 'action': [1.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 40.38636355483108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 42.82722939632187, 'action': [0.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 45.45331231404043, 'action': [2.0, 0.0]}])
Weights num count: [0.1061836352279825, 0.1080574640849469, 0.1099312929419113, 0.1174266083697689, 0.11055590256089944, 0.11305434103685197, 0.10743285446595878, 0.11118051217988757, 0.1155527795128045]
Actions to choose Agent 1: dict_values([{'num_count': 281, 'sum_payoffs': 63.90783084044836, 'action': [1.0, 0.0]}, {'num_count': 257, 'sum_payoffs': 55.739839558232404, 'action': [0.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 61.84746124936367, 'action': [1.0, -1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 59.54735207891932, 'action': [1.0, 1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 57.15937403841642, 'action': [0.0, 1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 56.037738747111355, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1755153029356652, 0.16052467207995003, 0.1717676452217364, 0.16739537788881947, 0.16302311055590257, 0.16114928169893816]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.84513974189758 s
