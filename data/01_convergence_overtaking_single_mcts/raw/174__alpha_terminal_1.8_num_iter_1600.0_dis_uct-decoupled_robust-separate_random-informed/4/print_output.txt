Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 183, 'sum_payoffs': 44.64986838308708, 'action': [1.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.59696325598576, 'action': [2.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 41.623934242216166, 'action': [0.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 42.42469916258016, 'action': [2.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 41.58807771288361, 'action': [1.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 44.201148490074175, 'action': [2.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 42.74607527440398, 'action': [1.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 40.91639180492874, 'action': [0.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 42.331606676205794, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11430356027482823, 0.11242973141786383, 0.10930668332292318, 0.11055590256089944, 0.10930668332292318, 0.1136789506558401, 0.11118051217988757, 0.1080574640849469, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 261, 'sum_payoffs': 56.94241235972103, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 62.4487608511425, 'action': [1.0, 0.0]}, {'num_count': 259, 'sum_payoffs': 56.346043579650164, 'action': [0.0, -1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 56.50785256296821, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 60.40013063980701, 'action': [1.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 60.72819419889808, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16302311055590257, 0.17301686445971268, 0.1617738913179263, 0.16239850093691444, 0.16926920674578388, 0.16989381636477202]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 42.53428077697754 s
