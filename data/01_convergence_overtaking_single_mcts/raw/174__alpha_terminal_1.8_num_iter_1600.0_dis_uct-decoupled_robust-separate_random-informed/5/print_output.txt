Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 179, 'sum_payoffs': 43.229832452431225, 'action': [2.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.612828545326686, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 41.74953788888561, 'action': [0.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 40.96283305733209, 'action': [0.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 43.951313814895535, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 42.048314569873156, 'action': [1.0, 1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 45.02865013125222, 'action': [2.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 40.52552711665035, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.5217045366319, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11180512179887571, 0.11242973141786383, 0.10930668332292318, 0.1080574640849469, 0.11305434103685197, 0.1099312929419113, 0.11492816989381636, 0.10743285446595878, 0.11242973141786383]
Actions to choose Agent 1: dict_values([{'num_count': 275, 'sum_payoffs': 62.11341267313477, 'action': [1.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 62.926094037456735, 'action': [1.0, -1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 56.28207390900298, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 60.72605408249942, 'action': [1.0, 0.0]}, {'num_count': 259, 'sum_payoffs': 56.67363087761722, 'action': [0.0, 1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 56.54762062511554, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1717676452217364, 0.1736414740787008, 0.16114928169893816, 0.16926920674578388, 0.1617738913179263, 0.1617738913179263]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.155322313308716 s
