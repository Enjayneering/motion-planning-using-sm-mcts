Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 42.190486097390945, 'action': [1.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 41.37504843865043, 'action': [0.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 40.16110878176072, 'action': [1.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 40.54359631494741, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 43.664841278439575, 'action': [2.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 43.290098848767144, 'action': [0.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 44.13287999163672, 'action': [2.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 43.99383003579693, 'action': [1.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 45.94448584885072, 'action': [2.0, 0.0]}])
Weights num count: [0.1099312929419113, 0.10868207370393504, 0.10680824484697064, 0.10743285446595878, 0.11242973141786383, 0.11180512179887571, 0.11305434103685197, 0.11305434103685197, 0.11617738913179262]
Actions to choose Agent 1: dict_values([{'num_count': 288, 'sum_payoffs': 65.79115033099592, 'action': [1.0, 0.0]}, {'num_count': 252, 'sum_payoffs': 53.531882443789804, 'action': [0.0, 1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 55.91447304492714, 'action': [0.0, 0.0]}, {'num_count': 257, 'sum_payoffs': 55.3349294589869, 'action': [0.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 60.048870884449904, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 60.67696194040702, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17988757026858213, 0.15740162398500937, 0.1617738913179263, 0.16052467207995003, 0.16926920674578388, 0.17051842598376016]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 55.5899760723114 s
