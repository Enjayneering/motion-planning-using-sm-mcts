Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 175, 'sum_payoffs': 41.676433439756266, 'action': [0.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 39.765765006405125, 'action': [1.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 42.71945590955773, 'action': [0.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 41.23678222372357, 'action': [2.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 41.21900099730348, 'action': [1.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 43.95373613297047, 'action': [1.0, -1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 47.04003636900169, 'action': [2.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 45.49374573052176, 'action': [2.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 41.29498918662647, 'action': [0.0, 0.0]}])
Weights num count: [0.10930668332292318, 0.1061836352279825, 0.11118051217988757, 0.10868207370393504, 0.10868207370393504, 0.11305434103685197, 0.11805121798875702, 0.1155527795128045, 0.10868207370393504]
Actions to choose Agent 1: dict_values([{'num_count': 270, 'sum_payoffs': 59.872138127774186, 'action': [1.0, 0.0]}, {'num_count': 255, 'sum_payoffs': 54.68795054466063, 'action': [0.0, 1.5707963267948966]}, {'num_count': 265, 'sum_payoffs': 58.122361546199556, 'action': [0.0, 0.0]}, {'num_count': 255, 'sum_payoffs': 54.739449473389335, 'action': [0.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 62.18067584208102, 'action': [1.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 62.608839241062356, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16864459712679575, 0.15927545284197375, 0.1655215490318551, 0.15927545284197375, 0.17301686445971268, 0.1736414740787008]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 96.84525084495544 s
