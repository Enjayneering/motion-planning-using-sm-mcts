Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 234, 'sum_payoffs': 56.32240264757484, 'action': [2.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 55.28231400710868, 'action': [0.0, 1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 60.49434217182332, 'action': [2.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 54.937074772798574, 'action': [0.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 58.96534443874222, 'action': [2.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 54.52622489572904, 'action': [1.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 55.73104614197426, 'action': [1.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 54.113669570806294, 'action': [0.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 54.86827366892373, 'action': [1.0, 0.0]}])
Weights num count: [0.11137553545930509, 0.1099476439790576, 0.11661113755354593, 0.10947168015230842, 0.11470728224654926, 0.10899571632555925, 0.11042360780580676, 0.10851975249881009, 0.10947168015230842]
Actions to choose Agent 1: dict_values([{'num_count': 332, 'sum_payoffs': 71.26834295381238, 'action': [0.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 75.48886518034264, 'action': [0.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 78.7157326204167, 'action': [1.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 75.40262159099562, 'action': [0.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 80.35780171917963, 'action': [1.0, 0.0]}, {'num_count': 363, 'sum_payoffs': 81.37869730837622, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15801999048072346, 0.16420752022846263, 0.1689671584959543, 0.16420752022846263, 0.17134697762970014, 0.17277486910994763]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 122.84929895401001 s
