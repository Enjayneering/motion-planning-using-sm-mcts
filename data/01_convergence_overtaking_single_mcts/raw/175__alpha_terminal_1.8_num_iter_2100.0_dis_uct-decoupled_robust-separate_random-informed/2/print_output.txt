Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 242, 'sum_payoffs': 59.45445138788274, 'action': [2.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 55.01071081835797, 'action': [1.0, 1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 56.52017258369986, 'action': [1.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 54.65161123457362, 'action': [1.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 59.07349752426478, 'action': [2.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 52.84070422048924, 'action': [0.0, 0.0]}, {'num_count': 247, 'sum_payoffs': 61.408302182216055, 'action': [2.0, 0.0]}, {'num_count': 225, 'sum_payoffs': 53.20746561682697, 'action': [0.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 54.35919850160941, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11518324607329843, 0.10947168015230842, 0.11137553545930509, 0.10899571632555925, 0.11470728224654926, 0.10661589719181343, 0.11756306520704426, 0.10709186101856259, 0.10851975249881009]
Actions to choose Agent 1: dict_values([{'num_count': 346, 'sum_payoffs': 75.66898699983307, 'action': [0.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 82.77832423786572, 'action': [1.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 78.92087049925034, 'action': [1.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 72.07887322945875, 'action': [0.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 77.23299906756182, 'action': [1.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 75.02340016421924, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1646834840552118, 0.17515468824369348, 0.16944312232270348, 0.15944788196097096, 0.16706330318895765, 0.16373155640171347]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 121.82389068603516 s
