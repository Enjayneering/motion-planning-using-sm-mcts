Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 235, 'sum_payoffs': 56.7866752552301, 'action': [2.0, 0.0]}, {'num_count': 227, 'sum_payoffs': 53.78579905617026, 'action': [0.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 54.95076546324573, 'action': [1.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 58.32810484606986, 'action': [1.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 57.20555401282706, 'action': [2.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 58.94219341941576, 'action': [1.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 56.828860731025955, 'action': [2.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 54.08282267575623, 'action': [0.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 54.51949266632509, 'action': [0.0, 0.0]}])
Weights num count: [0.11185149928605426, 0.10804378867206092, 0.10947168015230842, 0.11375535459305093, 0.11232746311280342, 0.11470728224654926, 0.11185149928605426, 0.10851975249881009, 0.10899571632555925]
Actions to choose Agent 1: dict_values([{'num_count': 365, 'sum_payoffs': 82.08738291162194, 'action': [1.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 74.30317548779, 'action': [0.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 76.95662852190725, 'action': [1.0, 1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 80.47981190882561, 'action': [1.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 75.25830439527108, 'action': [0.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 74.29856755699574, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.173726796763446, 0.16230366492146597, 0.1661113755354593, 0.17134697762970014, 0.16373155640171347, 0.16230366492146597]
Selected final action: [1.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 122.03052926063538 s
