Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 290, 'sum_payoffs': 69.8308658550775, 'action': [0.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 69.918728616133, 'action': [1.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 71.33570448422273, 'action': [2.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 69.61312165476396, 'action': [1.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 69.190587513467, 'action': [1.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 66.7035171448969, 'action': [0.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 72.07555250413064, 'action': [2.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 65.60659938665083, 'action': [0.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 71.26163483026262, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1114955786236063, 0.1114955786236063, 0.11303344867358708, 0.1111111111111111, 0.11072664359861592, 0.10803537101114956, 0.11380238369857747, 0.10688196847366398, 0.11303344867358708]
Actions to choose Agent 1: dict_values([{'num_count': 452, 'sum_payoffs': 101.25923395299786, 'action': [1.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 91.52415695113162, 'action': [0.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 90.23060848378059, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 98.82437063382419, 'action': [1.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 99.08223954031062, 'action': [1.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 91.53333765004668, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17377931564782775, 0.16186082276047675, 0.16032295271049596, 0.1707035755478662, 0.1710880430603614, 0.16186082276047675]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.78412532806396 s
