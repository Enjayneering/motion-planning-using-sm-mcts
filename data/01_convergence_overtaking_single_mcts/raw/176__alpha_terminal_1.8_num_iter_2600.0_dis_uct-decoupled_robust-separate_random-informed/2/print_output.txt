Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 283, 'sum_payoffs': 67.24747967099422, 'action': [0.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 69.00996783622044, 'action': [2.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 69.39793080413709, 'action': [1.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 73.32682628924957, 'action': [2.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 73.31771452365534, 'action': [2.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.63046963816099, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 65.0482306263449, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.20202825194828, 'action': [0.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 71.78758555709811, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10880430603613994, 0.11072664359861592, 0.1111111111111111, 0.11534025374855825, 0.11534025374855825, 0.10918877354863514, 0.10649750096116878, 0.10880430603613994, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 415, 'sum_payoffs': 89.72312632496948, 'action': [0.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 92.47822331267882, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 101.71218380024912, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 101.61349238614405, 'action': [1.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 91.62925902250969, 'action': [0.0, 0.0]}, {'num_count': 434, 'sum_payoffs': 95.69198483077804, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15955401768550556, 0.16301422529796233, 0.17416378316032297, 0.17416378316032297, 0.16186082276047675, 0.16685890042291426]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 71.71286940574646 s
