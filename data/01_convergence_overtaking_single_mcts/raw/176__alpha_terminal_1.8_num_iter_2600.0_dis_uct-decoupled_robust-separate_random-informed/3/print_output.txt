Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 298, 'sum_payoffs': 72.75643071825007, 'action': [2.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 68.83474512859149, 'action': [0.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 67.00475924298128, 'action': [0.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 70.27210968591288, 'action': [1.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 71.23234493511613, 'action': [1.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 70.19360710017861, 'action': [2.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 70.89198271708173, 'action': [2.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.73584052190101, 'action': [0.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 66.3305930065786, 'action': [1.0, 0.0]}])
Weights num count: [0.11457131872356786, 0.11034217608612072, 0.10841983852364476, 0.1118800461361015, 0.11303344867358708, 0.1118800461361015, 0.11264898116109189, 0.10918877354863514, 0.10765090349865436]
Actions to choose Agent 1: dict_values([{'num_count': 457, 'sum_payoffs': 102.78925372359168, 'action': [1.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 100.21059485355441, 'action': [1.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 89.25150933603419, 'action': [0.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 88.66791315054884, 'action': [0.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 98.39399253604059, 'action': [1.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 92.72903854397784, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17570165321030373, 0.17262591311034217, 0.15916955017301038, 0.15840061514801998, 0.170319108035371, 0.16339869281045752]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 70.0937385559082 s
