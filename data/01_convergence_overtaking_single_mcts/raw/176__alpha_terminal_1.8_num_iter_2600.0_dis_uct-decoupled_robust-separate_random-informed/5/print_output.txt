Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 289, 'sum_payoffs': 69.42576997052107, 'action': [1.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 69.1161418345327, 'action': [2.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 65.85208338671002, 'action': [0.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 72.65072318585486, 'action': [2.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 68.99149336968125, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 65.51442299287551, 'action': [1.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 72.33844136682721, 'action': [2.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 69.10447770856602, 'action': [0.0, 0.0]}, {'num_count': 295, 'sum_payoffs': 71.6141252467904, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1111111111111111, 0.11072664359861592, 0.10726643598615918, 0.11457131872356786, 0.11072664359861592, 0.10688196847366398, 0.11418685121107267, 0.11072664359861592, 0.11341791618608228]
Actions to choose Agent 1: dict_values([{'num_count': 424, 'sum_payoffs': 92.49328326349642, 'action': [0.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 92.45324301358289, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 98.84448675088072, 'action': [1.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 90.28719885097011, 'action': [0.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 99.82083851670862, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 98.84026313349457, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16301422529796233, 0.16301422529796233, 0.1707035755478662, 0.16032295271049596, 0.17185697808535177, 0.1707035755478662]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 69.25568962097168 s
