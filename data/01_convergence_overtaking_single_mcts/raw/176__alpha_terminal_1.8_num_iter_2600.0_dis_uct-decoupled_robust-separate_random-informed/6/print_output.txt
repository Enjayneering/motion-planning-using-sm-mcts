Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 67.99058736494656, 'action': [1.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 73.27876337306917, 'action': [2.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.59144272926957, 'action': [0.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 70.01712834373512, 'action': [1.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 72.57790308078624, 'action': [2.0, 0.0]}, {'num_count': 302, 'sum_payoffs': 74.40355909744106, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.62746278145477, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 65.80965596525992, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.90400576181926, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10918877354863514, 0.11495578623606305, 0.10880430603613994, 0.1114955786236063, 0.11418685121107267, 0.11610918877354863, 0.10880430603613994, 0.10688196847366398, 0.10918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 441, 'sum_payoffs': 97.6298375175388, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 97.66205278762875, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 90.40707336704484, 'action': [0.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 94.52783427802852, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 91.34321297027762, 'action': [0.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 99.77288921504778, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1695501730103806, 0.1695501730103806, 0.16070742022299117, 0.16570549788542868, 0.16186082276047675, 0.172241445597847]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.92396426200867 s
