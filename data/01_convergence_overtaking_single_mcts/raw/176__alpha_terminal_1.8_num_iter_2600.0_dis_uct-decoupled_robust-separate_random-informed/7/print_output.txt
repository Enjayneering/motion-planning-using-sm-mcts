Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 283, 'sum_payoffs': 67.37289738727145, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.3850829752344, 'action': [2.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 73.83389913924518, 'action': [2.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 66.36143939350215, 'action': [0.0, 0.0]}, {'num_count': 299, 'sum_payoffs': 73.11704550532698, 'action': [2.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 68.75192520205015, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.41323591211444, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 68.0553880527254, 'action': [0.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 73.03222306396307, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10880430603613994, 0.10880430603613994, 0.11572472126105345, 0.10765090349865436, 0.11495578623606305, 0.11034217608612072, 0.10880430603613994, 0.10957324106113034, 0.11495578623606305]
Actions to choose Agent 1: dict_values([{'num_count': 415, 'sum_payoffs': 89.41449584559786, 'action': [0.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 102.02008962100022, 'action': [1.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 89.05198447078224, 'action': [0.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 100.41867494542801, 'action': [1.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 97.9160726801288, 'action': [1.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 92.11298875349117, 'action': [0.0, 0.0]}])
Weights num count: [0.15955401768550556, 0.17493271818531334, 0.15916955017301038, 0.17301038062283736, 0.16993464052287582, 0.16301422529796233]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.49099254608154 s
