Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 301, 'sum_payoffs': 73.74037899548686, 'action': [2.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 71.89415387595129, 'action': [2.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 72.97784813080725, 'action': [1.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 63.361731659340286, 'action': [0.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 66.18988695522424, 'action': [1.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 68.70009331389885, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.23672260014796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.22397782269012, 'action': [1.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 72.96929278140381, 'action': [2.0, 0.0]}])
Weights num count: [0.11572472126105345, 0.11380238369857747, 0.11495578623606305, 0.10457516339869281, 0.10765090349865436, 0.11034217608612072, 0.10880430603613994, 0.10880430603613994, 0.11495578623606305]
Actions to choose Agent 1: dict_values([{'num_count': 436, 'sum_payoffs': 96.49272358485706, 'action': [1.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 103.08889895007269, 'action': [1.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 90.48569570314532, 'action': [0.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 89.8250003862347, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 101.8317324516259, 'action': [1.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 92.02560805440051, 'action': [0.0, 0.0]}])
Weights num count: [0.16762783544790466, 0.17570165321030373, 0.16032295271049596, 0.15955401768550556, 0.17416378316032297, 0.16224529027297194]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.57165694236755 s
