Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 280, 'sum_payoffs': 66.15821655514796, 'action': [0.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 65.84431794010995, 'action': [0.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 66.0841009426714, 'action': [1.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 73.33997617874041, 'action': [2.0, -1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 75.8634615440576, 'action': [2.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.25886598287173, 'action': [0.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 68.97487962255381, 'action': [1.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 71.21050202388953, 'action': [2.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 69.37063574776063, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10765090349865436, 0.10726643598615918, 0.10765090349865436, 0.11534025374855825, 0.11803152633602461, 0.10880430603613994, 0.11072664359861592, 0.11303344867358708, 0.1111111111111111]
Actions to choose Agent 1: dict_values([{'num_count': 420, 'sum_payoffs': 91.26762199667633, 'action': [0.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 96.94757114551163, 'action': [1.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 104.48348922358568, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 92.53598034638178, 'action': [0.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 90.0278545005571, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 97.41786333711535, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16147635524798154, 0.16839677047289503, 0.1776239907727797, 0.16301422529796233, 0.15993848519800077, 0.16916570549788543]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.58103013038635 s
