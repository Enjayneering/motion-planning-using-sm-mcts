Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 355, 'sum_payoffs': 85.78313624246957, 'action': [2.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 83.65478963176412, 'action': [1.0, -1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 81.51490084920411, 'action': [0.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 83.61041282287596, 'action': [2.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 78.45884302182296, 'action': [0.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 87.11309788344258, 'action': [2.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 79.79093533272963, 'action': [0.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 79.1591559625346, 'action': [1.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 79.53263389382356, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11447920025798129, 0.11254434053531119, 0.11060948081264109, 0.11254434053531119, 0.10770719122863592, 0.11576910673976137, 0.10899709771041599, 0.10835214446952596, 0.10867462108997097]
Actions to choose Agent 1: dict_values([{'num_count': 515, 'sum_payoffs': 112.52560729784967, 'action': [0.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 105.49340079214595, 'action': [0.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 117.55729210872929, 'action': [1.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 115.90422387898288, 'action': [1.0, -1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 119.04837336170871, 'action': [1.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 107.97380105296284, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16607545952918412, 0.15865849725894873, 0.17123508545630442, 0.16962270235407934, 0.1728474685585295, 0.16123831022250887]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 80.32068848609924 s
