Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 352, 'sum_payoffs': 84.93841918220369, 'action': [2.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 78.66766372240144, 'action': [0.0, 0.0]}, {'num_count': 330, 'sum_payoffs': 77.29977289999869, 'action': [0.0, 1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 84.14186322805884, 'action': [1.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 82.07063448112463, 'action': [1.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 88.0236266165723, 'action': [2.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 83.78023304045422, 'action': [2.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 78.95048044232851, 'action': [0.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 82.47709830323565, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11351177039664624, 0.10770719122863592, 0.10641728474685586, 0.11286681715575621, 0.1109319574330861, 0.11641405998065141, 0.11254434053531119, 0.10802966784908094, 0.11125443405353112]
Actions to choose Agent 1: dict_values([{'num_count': 533, 'sum_payoffs': 118.33765566784368, 'action': [1.0, 1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 117.49462874713858, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 108.49735476990476, 'action': [0.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 104.53084304092364, 'action': [0.0, -1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 122.43892673848211, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 108.83799547666388, 'action': [0.0, 0.0]}])
Weights num count: [0.17188003869719445, 0.1709126088358594, 0.16156078684295389, 0.15736859077716867, 0.17607223476297967, 0.1618832634633989]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 80.73795461654663 s
