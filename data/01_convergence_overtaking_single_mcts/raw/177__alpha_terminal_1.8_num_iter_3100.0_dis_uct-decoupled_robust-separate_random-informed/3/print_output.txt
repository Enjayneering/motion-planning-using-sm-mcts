Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 328, 'sum_payoffs': 76.74219017034764, 'action': [0.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 78.09585461653565, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 81.17816310008402, 'action': [1.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 84.2992750240712, 'action': [2.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 83.60839898042447, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 80.0709674763139, 'action': [0.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 85.39424434839121, 'action': [2.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 85.73174738069102, 'action': [2.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 86.46539629206423, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10577233150596582, 0.10706223798774589, 0.10996452757175104, 0.11286681715575621, 0.11222186391486617, 0.10899709771041599, 0.11383424701709126, 0.11415672363753628, 0.11480167687842631]
Actions to choose Agent 1: dict_values([{'num_count': 502, 'sum_payoffs': 108.2837404657905, 'action': [0.0, -1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 114.69987959200024, 'action': [1.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 116.84782670145377, 'action': [1.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 107.28696319163137, 'action': [0.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 119.83739356100779, 'action': [1.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 109.43302501173383, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1618832634633989, 0.16865527249274428, 0.1709126088358594, 0.16091583360206385, 0.17413737504030957, 0.16317316994517897]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 82.19470977783203 s
