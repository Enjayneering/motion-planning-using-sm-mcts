Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 331, 'sum_payoffs': 77.42403886795724, 'action': [1.0, 0.0]}, {'num_count': 347, 'sum_payoffs': 82.95271969463371, 'action': [1.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 83.72472852696585, 'action': [1.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 80.82437039226177, 'action': [0.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 76.80466219494492, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 86.81331800793636, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 81.24132180646762, 'action': [0.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 85.63628942424897, 'action': [2.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 83.25920279571459, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10673976136730087, 0.11189938729442116, 0.11254434053531119, 0.10996452757175104, 0.10609480812641084, 0.11544663011931634, 0.11028700419219607, 0.11447920025798129, 0.11222186391486617]
Actions to choose Agent 1: dict_values([{'num_count': 529, 'sum_payoffs': 117.09292329634806, 'action': [1.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 109.18261102713097, 'action': [0.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 107.9114509525267, 'action': [0.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 119.91075673264211, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 109.46813019328953, 'action': [0.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 116.46499800912942, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1705901322154144, 0.16220574008384392, 0.16091583360206385, 0.17349242179941954, 0.16252821670428894, 0.16994517897452435]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.26008415222168 s
