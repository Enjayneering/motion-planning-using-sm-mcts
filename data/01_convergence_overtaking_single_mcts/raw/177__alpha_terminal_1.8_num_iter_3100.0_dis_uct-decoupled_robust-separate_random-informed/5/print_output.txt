Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 356, 'sum_payoffs': 86.3225366670206, 'action': [2.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 82.78581654360731, 'action': [1.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 80.8332620352966, 'action': [1.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 79.70275996694328, 'action': [0.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 87.35218425422632, 'action': [2.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 80.12245439669266, 'action': [0.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 81.03436700618019, 'action': [1.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 76.94521625200917, 'action': [0.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 85.62654469700846, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11480167687842631, 0.11157691067397614, 0.10964205095130602, 0.10867462108997097, 0.11576910673976137, 0.10899709771041599, 0.10996452757175104, 0.10609480812641084, 0.11415672363753628]
Actions to choose Agent 1: dict_values([{'num_count': 505, 'sum_payoffs': 109.0652273775828, 'action': [0.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 119.83253588524526, 'action': [1.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 113.3641980541553, 'action': [1.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 106.3479506141197, 'action': [0.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 106.39129740671227, 'action': [0.0, 0.0]}, {'num_count': 544, 'sum_payoffs': 121.04658158416841, 'action': [1.0, 0.0]}])
Weights num count: [0.16285069332473395, 0.17413737504030957, 0.16736536601096422, 0.1599484037407288, 0.1599484037407288, 0.17542728152208964]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 77.98779606819153 s
