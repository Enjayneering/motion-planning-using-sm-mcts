Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 360, 'sum_payoffs': 88.15269976107892, 'action': [2.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 79.84081797286377, 'action': [1.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 88.83789480930969, 'action': [2.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 78.02087932730241, 'action': [0.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 88.19727847159461, 'action': [2.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 83.31874358670541, 'action': [1.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 78.38927112124072, 'action': [0.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 77.05054657953669, 'action': [0.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 82.88484497988837, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11609158336020639, 0.10835214446952596, 0.11673653660109642, 0.10673976136730087, 0.11609158336020639, 0.11157691067397614, 0.10706223798774589, 0.10577233150596582, 0.11125443405353112]
Actions to choose Agent 1: dict_values([{'num_count': 502, 'sum_payoffs': 108.13346933040982, 'action': [0.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 106.30797581775187, 'action': [0.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 109.26691673655486, 'action': [0.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 118.80732314146772, 'action': [1.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 113.31595377687691, 'action': [1.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 119.76385226586048, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1618832634633989, 0.1599484037407288, 0.16317316994517897, 0.17316994517897452, 0.16736536601096422, 0.17413737504030957]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 89.92243337631226 s
