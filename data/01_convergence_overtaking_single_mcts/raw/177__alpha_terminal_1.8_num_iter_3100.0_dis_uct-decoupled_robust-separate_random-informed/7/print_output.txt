Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 335, 'sum_payoffs': 79.10459389590646, 'action': [0.0, 1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 84.20327480842268, 'action': [2.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 84.31380589576126, 'action': [1.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 91.67398593003257, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 81.45911447763994, 'action': [1.0, 0.0]}, {'num_count': 348, 'sum_payoffs': 83.58367163616433, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 81.45504274984779, 'action': [1.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 78.05421416569561, 'action': [0.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 77.41317069659527, 'action': [0.0, 0.0]}])
Weights num count: [0.10802966784908094, 0.11286681715575621, 0.11286681715575621, 0.11963882618510158, 0.11028700419219607, 0.11222186391486617, 0.11028700419219607, 0.10706223798774589, 0.10641728474685586]
Actions to choose Agent 1: dict_values([{'num_count': 502, 'sum_payoffs': 108.65929936350412, 'action': [0.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 108.63295129131865, 'action': [0.0, -1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 118.20690713932437, 'action': [1.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 116.69339095912058, 'action': [1.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 119.35321950603633, 'action': [1.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 107.47437232916526, 'action': [0.0, 0.0]}])
Weights num count: [0.1618832634633989, 0.1618832634633989, 0.17188003869719445, 0.17026765559496937, 0.17316994517897452, 0.16059335698161883]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.74118852615356 s
