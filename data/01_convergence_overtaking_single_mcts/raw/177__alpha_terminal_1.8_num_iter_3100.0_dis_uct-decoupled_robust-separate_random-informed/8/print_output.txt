Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 335, 'sum_payoffs': 79.30053464885762, 'action': [0.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 83.13487145663862, 'action': [1.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 84.58488261326664, 'action': [1.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 85.6642419085927, 'action': [2.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 79.02739451948723, 'action': [0.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 85.95645039486885, 'action': [2.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 77.24539690242749, 'action': [0.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 90.5387451004868, 'action': [2.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 78.35331397767357, 'action': [1.0, 0.0]}])
Weights num count: [0.10802966784908094, 0.11157691067397614, 0.11286681715575621, 0.11383424701709126, 0.10770719122863592, 0.11415672363753628, 0.10609480812641084, 0.11834891970332151, 0.10706223798774589]
Actions to choose Agent 1: dict_values([{'num_count': 496, 'sum_payoffs': 106.14355282279108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 111.59029937643734, 'action': [0.0, -1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 114.69370970749785, 'action': [1.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 107.97220570857687, 'action': [0.0, 0.0]}, {'num_count': 524, 'sum_payoffs': 114.69999889398426, 'action': [1.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 119.6572973901362, 'action': [1.0, 0.0]}])
Weights num count: [0.1599484037407288, 0.1657529829087391, 0.1689777491131893, 0.1618832634633989, 0.1689777491131893, 0.17413737504030957]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.73036932945251 s
