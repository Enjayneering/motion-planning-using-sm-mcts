Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 398, 'sum_payoffs': 93.74259221044429, 'action': [1.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 90.33320141159001, 'action': [1.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 94.76476477641614, 'action': [1.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 97.73125756636797, 'action': [2.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 97.0849445461823, 'action': [2.0, 1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 99.77604490412863, 'action': [2.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 92.03865218785913, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 92.06150299607712, 'action': [0.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 92.0703877302522, 'action': [0.0, 0.0]}])
Weights num count: [0.11052485420716468, 0.10774784782004998, 0.11135795612329909, 0.11385726187170231, 0.11330186059427937, 0.11552346570397112, 0.10913635101360733, 0.10913635101360733, 0.10913635101360733]
Actions to choose Agent 1: dict_values([{'num_count': 594, 'sum_payoffs': 128.41128391918983, 'action': [0.0, 0.0]}, {'num_count': 570, 'sum_payoffs': 121.34135309706292, 'action': [0.0, 1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 134.39473582532685, 'action': [1.0, -1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 137.20964429209016, 'action': [1.0, 0.0]}, {'num_count': 615, 'sum_payoffs': 134.8077989701586, 'action': [1.0, 1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 125.50546287266745, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1649541793946126, 0.15828936406553734, 0.170508192168842, 0.1730074979172452, 0.17078589280755346, 0.16217717300749793]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 93.8930037021637 s
