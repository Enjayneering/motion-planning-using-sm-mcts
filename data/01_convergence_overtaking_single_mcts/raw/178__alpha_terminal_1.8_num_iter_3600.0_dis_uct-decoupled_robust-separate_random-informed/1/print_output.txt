Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 403, 'sum_payoffs': 95.58990031287537, 'action': [0.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 92.24321940312117, 'action': [0.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 88.1005486122059, 'action': [0.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 94.62609467640513, 'action': [1.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 96.25867775229152, 'action': [1.0, -1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 102.03550063170832, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 97.24399792595503, 'action': [2.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 88.45159877111215, 'action': [1.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 96.6583058389491, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11191335740072202, 0.10913635101360733, 0.1058039433490697, 0.11108025548458761, 0.11246875867814496, 0.11718966953623994, 0.11330186059427937, 0.10608164398778117, 0.11274645931685642]
Actions to choose Agent 1: dict_values([{'num_count': 618, 'sum_payoffs': 135.39817074200408, 'action': [1.0, 0.0]}, {'num_count': 617, 'sum_payoffs': 135.1081900113854, 'action': [1.0, -1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 125.85808851649904, 'action': [0.0, 1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 135.43772890023845, 'action': [1.0, 1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 124.38446235944211, 'action': [0.0, 0.0]}, {'num_count': 580, 'sum_payoffs': 124.05207913870032, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17161899472368786, 0.1713412940849764, 0.16273257428492086, 0.17161899472368786, 0.1613440710913635, 0.16106637045265204]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 104.36987209320068 s
