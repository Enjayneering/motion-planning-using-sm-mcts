Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 387, 'sum_payoffs': 90.50822986751844, 'action': [0.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 97.62353182398579, 'action': [1.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 92.16665550251048, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 96.9380922378853, 'action': [2.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 90.58022118030605, 'action': [0.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 94.50766561335155, 'action': [1.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 93.59161555141296, 'action': [0.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 102.81667077229721, 'action': [2.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 95.65182246921864, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10747014718133852, 0.11330186059427937, 0.10885865037489587, 0.11274645931685642, 0.10747014718133852, 0.11080255484587614, 0.10996945292974174, 0.1174673701749514, 0.11163565676201055]
Actions to choose Agent 1: dict_values([{'num_count': 613, 'sum_payoffs': 133.44675755336377, 'action': [1.0, -1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 125.89714643437222, 'action': [0.0, 0.0]}, {'num_count': 613, 'sum_payoffs': 133.43247927073185, 'action': [1.0, 1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 121.49952193911786, 'action': [0.0, -1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 140.88971581449468, 'action': [1.0, 0.0]}, {'num_count': 575, 'sum_payoffs': 122.0632114088349, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1702304915301305, 0.16328797556234378, 0.1702304915301305, 0.15912246598167176, 0.17717300749791726, 0.1596778672590947]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 97.62146401405334 s
