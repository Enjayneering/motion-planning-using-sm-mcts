Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 383, 'sum_payoffs': 89.40039509698083, 'action': [0.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 96.5268803365321, 'action': [1.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 98.41991637420116, 'action': [2.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 94.00678629370151, 'action': [2.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 93.03591178815117, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 96.76737165531904, 'action': [1.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 94.82942897871693, 'action': [0.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 90.76274901991754, 'action': [0.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 102.2434622441041, 'action': [2.0, 0.0]}])
Weights num count: [0.10635934462649264, 0.1121910580394335, 0.11385726187170231, 0.1102471535684532, 0.1094140516523188, 0.11246875867814496, 0.11080255484587614, 0.10747014718133852, 0.11691196889752846]
Actions to choose Agent 1: dict_values([{'num_count': 609, 'sum_payoffs': 131.91667086825652, 'action': [1.0, -1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 122.3202357702679, 'action': [0.0, 0.0]}, {'num_count': 622, 'sum_payoffs': 135.74414152236784, 'action': [1.0, 0.0]}, {'num_count': 579, 'sum_payoffs': 122.94667766419634, 'action': [0.0, -1.5707963267948966]}, {'num_count': 595, 'sum_payoffs': 127.65857073633562, 'action': [0.0, 1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 134.54862424007413, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16911968897528465, 0.16023326853651762, 0.17272979727853374, 0.16078866981394058, 0.16523188003332406, 0.17161899472368786]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 90.80027198791504 s
