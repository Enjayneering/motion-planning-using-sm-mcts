Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 385, 'sum_payoffs': 89.32288444316873, 'action': [0.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 91.63909386742442, 'action': [1.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 98.08213593277243, 'action': [2.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 101.4110841786052, 'action': [2.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 96.63131676331997, 'action': [1.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 93.36126182800183, 'action': [1.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 96.00768943017901, 'action': [2.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 91.33000931946951, 'action': [0.0, 0.0]}, {'num_count': 391, 'sum_payoffs': 91.33934164637348, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10691474590391557, 0.10885865037489587, 0.11413496251041377, 0.11691196889752846, 0.1130241599555679, 0.1102471535684532, 0.11246875867814496, 0.10858094973618439, 0.10858094973618439]
Actions to choose Agent 1: dict_values([{'num_count': 582, 'sum_payoffs': 124.26339682405722, 'action': [0.0, 1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 133.8490075681366, 'action': [1.0, 1.5707963267948966]}, {'num_count': 603, 'sum_payoffs': 130.48950860683152, 'action': [1.0, -1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 139.2313114737141, 'action': [1.0, 0.0]}, {'num_count': 592, 'sum_payoffs': 127.22398965724878, 'action': [0.0, 0.0]}, {'num_count': 577, 'sum_payoffs': 122.65365795665225, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16162177173007497, 0.170508192168842, 0.16745348514301583, 0.17550680366564844, 0.16439877811718967, 0.16023326853651762]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 96.6505172252655 s
