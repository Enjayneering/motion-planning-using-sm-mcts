Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 409, 'sum_payoffs': 97.70745855322373, 'action': [2.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 91.40913506919587, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 92.36488906558905, 'action': [0.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 97.10174975396373, 'action': [1.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 97.19490017587812, 'action': [2.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 101.22903364174499, 'action': [2.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 91.02016587602108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 92.42893574439893, 'action': [1.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 92.43198297449945, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11357956123299083, 0.10830324909747292, 0.10913635101360733, 0.1130241599555679, 0.1130241599555679, 0.11635656762010553, 0.10802554845876146, 0.10913635101360733, 0.10913635101360733]
Actions to choose Agent 1: dict_values([{'num_count': 623, 'sum_payoffs': 136.75783121142615, 'action': [1.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 125.02147338408543, 'action': [0.0, 1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 124.73162562624178, 'action': [0.0, -1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 133.51214752433933, 'action': [1.0, -1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 133.13059638486033, 'action': [1.0, 1.5707963267948966]}, {'num_count': 587, 'sum_payoffs': 125.96708076223771, 'action': [0.0, 0.0]}])
Weights num count: [0.1730074979172452, 0.16217717300749793, 0.16189947236878643, 0.16995279089141904, 0.16967509025270758, 0.16301027492363232]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 98.92189598083496 s
