Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 397, 'sum_payoffs': 93.3347807505542, 'action': [1.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 89.66948975900394, 'action': [0.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 91.28839190831685, 'action': [0.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 93.91066843858619, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 99.36359774659923, 'action': [2.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 96.625400552055, 'action': [2.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 93.33114253074103, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 98.38511942988511, 'action': [2.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 92.98141993824937, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1102471535684532, 0.10719244654262705, 0.10858094973618439, 0.11080255484587614, 0.11524576506525964, 0.1130241599555679, 0.1102471535684532, 0.11441266314912524, 0.10996945292974174]
Actions to choose Agent 1: dict_values([{'num_count': 585, 'sum_payoffs': 125.96550199612945, 'action': [0.0, 1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 133.79788625733238, 'action': [1.0, 1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 121.70718021006914, 'action': [0.0, 0.0]}, {'num_count': 638, 'sum_payoffs': 141.9743973544439, 'action': [1.0, 0.0]}, {'num_count': 617, 'sum_payoffs': 135.61944761642368, 'action': [1.0, -1.5707963267948966]}, {'num_count': 578, 'sum_payoffs': 123.91315417992644, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1624548736462094, 0.16967509025270758, 0.15856706470424883, 0.17717300749791726, 0.1713412940849764, 0.1605109691752291]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 102.2728579044342 s
