Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 388, 'sum_payoffs': 90.91530412371837, 'action': [0.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 91.59369465757506, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 92.57743226777711, 'action': [1.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 92.25773474829785, 'action': [0.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 99.31120655539378, 'action': [2.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 96.29210209190953, 'action': [1.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 95.99670243420672, 'action': [1.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 97.99296600193684, 'action': [2.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 97.61532043962092, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10774784782004998, 0.10830324909747292, 0.10913635101360733, 0.10885865037489587, 0.11469036378783672, 0.1121910580394335, 0.11191335740072202, 0.11357956123299083, 0.11330186059427937]
Actions to choose Agent 1: dict_values([{'num_count': 585, 'sum_payoffs': 125.08389694043764, 'action': [0.0, 1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 134.35773116384013, 'action': [1.0, 1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 137.63478638612884, 'action': [1.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 124.79054150186036, 'action': [0.0, 0.0]}, {'num_count': 620, 'sum_payoffs': 135.49201622493337, 'action': [1.0, -1.5707963267948966]}, {'num_count': 568, 'sum_payoffs': 119.96796560943733, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1624548736462094, 0.17106359344626493, 0.1741183004720911, 0.16217717300749793, 0.1721743960011108, 0.1577339627881144]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.05817413330078 s
