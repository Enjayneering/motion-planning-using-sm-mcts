Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 386, 'sum_payoffs': 90.40791072759288, 'action': [0.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 97.15016733211083, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 90.75804122084898, 'action': [0.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 95.49300769013632, 'action': [2.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 100.80399519866366, 'action': [2.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 97.19246376666771, 'action': [1.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 94.04812351032939, 'action': [1.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 101.61680689779719, 'action': [2.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 88.68452276721364, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10719244654262705, 0.11274645931685642, 0.10747014718133852, 0.11135795612329909, 0.11580116634268259, 0.11274645931685642, 0.1102471535684532, 0.11635656762010553, 0.1058039433490697]
Actions to choose Agent 1: dict_values([{'num_count': 586, 'sum_payoffs': 125.8940615410864, 'action': [0.0, 1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 125.63016866857934, 'action': [0.0, 0.0]}, {'num_count': 582, 'sum_payoffs': 124.72175426096815, 'action': [0.0, -1.5707963267948966]}, {'num_count': 605, 'sum_payoffs': 131.5347487816837, 'action': [1.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 137.545441595689, 'action': [1.0, 0.0]}, {'num_count': 617, 'sum_payoffs': 135.12682997576434, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16273257428492086, 0.1624548736462094, 0.16162177173007497, 0.16800888642043876, 0.17356289919466814, 0.1713412940849764]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.12467336654663 s
