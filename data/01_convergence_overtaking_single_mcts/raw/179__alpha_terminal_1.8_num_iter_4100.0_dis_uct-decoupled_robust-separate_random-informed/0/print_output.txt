Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 443, 'sum_payoffs': 102.67020415819468, 'action': [0.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 102.76501493893393, 'action': [0.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 114.93385908349997, 'action': [2.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 113.24890873074368, 'action': [2.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 106.39714393491988, 'action': [2.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 103.69150187093572, 'action': [1.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 109.70549871646455, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 102.75542645575845, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 105.71988485956, 'action': [1.0, 0.0]}])
Weights num count: [0.10802243355279201, 0.10802243355279201, 0.11704462326261887, 0.11582540843696659, 0.11070470616922702, 0.10875396244818337, 0.11314313582053158, 0.10802243355279201, 0.11021702023896611]
Actions to choose Agent 1: dict_values([{'num_count': 656, 'sum_payoffs': 139.05298770753802, 'action': [0.0, 1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 137.07110305019353, 'action': [0.0, -1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 142.03247629488374, 'action': [0.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 154.02906946040932, 'action': [1.0, -1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 157.32811185011252, 'action': [1.0, 0.0]}, {'num_count': 704, 'sum_payoffs': 153.1920596629809, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15996098512557913, 0.15825408436966593, 0.1623994147768837, 0.1723969763472324, 0.1750792489636674, 0.171665447451841]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 110.21431231498718 s
