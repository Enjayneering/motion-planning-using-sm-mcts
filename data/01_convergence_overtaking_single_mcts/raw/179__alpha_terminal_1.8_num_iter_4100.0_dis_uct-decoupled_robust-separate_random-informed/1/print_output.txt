Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 455, 'sum_payoffs': 106.68832820443228, 'action': [2.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 107.34642810425588, 'action': [1.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 104.75552633512908, 'action': [0.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 104.09010922215788, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 107.08690191800774, 'action': [1.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 98.16438503912946, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 110.68284197737613, 'action': [2.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 118.03073045884814, 'action': [2.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 105.41185047376534, 'action': [0.0, 0.0]}])
Weights num count: [0.11094854913435748, 0.11143623506461839, 0.10948549134357474, 0.10899780541331383, 0.11119239209948793, 0.10460863204096561, 0.11387466471592295, 0.11923920994879297, 0.10997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 654, 'sum_payoffs': 138.00205394919973, 'action': [0.0, -1.5707963267948966]}, {'num_count': 696, 'sum_payoffs': 150.24972279710533, 'action': [1.0, -1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 139.76435030941735, 'action': [0.0, 1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 140.6689556058683, 'action': [0.0, 0.0]}, {'num_count': 721, 'sum_payoffs': 157.65142653823358, 'action': [1.0, 0.0]}, {'num_count': 706, 'sum_payoffs': 153.20797963184802, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15947329919531822, 0.16971470373079736, 0.16093635698610095, 0.1616678858814923, 0.17581077785905877, 0.17215313338210192]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 108.63560438156128 s
