Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 431, 'sum_payoffs': 98.48808640741454, 'action': [0.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 108.12690746624821, 'action': [2.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 107.8002440534522, 'action': [1.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 113.66692806965861, 'action': [2.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 102.45436822206746, 'action': [0.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 104.40478453813452, 'action': [1.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 107.15189582305959, 'action': [1.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 105.75838906169577, 'action': [0.0, 0.0]}, {'num_count': 471, 'sum_payoffs': 111.71900079001149, 'action': [2.0, 0.0]}])
Weights num count: [0.10509631797122652, 0.11216776396000976, 0.1119239209948793, 0.1163130943672275, 0.10802243355279201, 0.10948549134357474, 0.11143623506461839, 0.11046086320409657, 0.11485003657644477]
Actions to choose Agent 1: dict_values([{'num_count': 725, 'sum_payoffs': 159.308994565786, 'action': [1.0, 0.0]}, {'num_count': 705, 'sum_payoffs': 153.3214421411103, 'action': [1.0, -1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 138.1164791226024, 'action': [0.0, 1.5707963267948966]}, {'num_count': 674, 'sum_payoffs': 144.24321538247244, 'action': [0.0, 0.0]}, {'num_count': 649, 'sum_payoffs': 136.99398803992557, 'action': [0.0, -1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 150.19296676335654, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1767861497195806, 0.17190929041697148, 0.15922945623018775, 0.16435015849792733, 0.15825408436966593, 0.16922701780053645]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 110.51814603805542 s
