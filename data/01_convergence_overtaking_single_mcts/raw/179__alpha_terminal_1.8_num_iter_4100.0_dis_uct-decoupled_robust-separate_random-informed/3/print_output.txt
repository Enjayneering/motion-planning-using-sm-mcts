Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 463, 'sum_payoffs': 109.79731289174936, 'action': [1.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 117.79067951486095, 'action': [2.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 103.54671027500892, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 105.21228109784059, 'action': [0.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 103.75620654984031, 'action': [1.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 103.86721609786134, 'action': [1.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 107.74869844858866, 'action': [2.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 103.52099185567336, 'action': [0.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 110.728786725448, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11289929285540112, 0.11875152401853206, 0.10826627651792246, 0.10948549134357474, 0.10851011948305292, 0.10851011948305292, 0.11143623506461839, 0.10826627651792246, 0.11363082175079249]
Actions to choose Agent 1: dict_values([{'num_count': 699, 'sum_payoffs': 150.7676384378219, 'action': [1.0, 1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 140.01741858171073, 'action': [0.0, 0.0]}, {'num_count': 655, 'sum_payoffs': 137.98274258207258, 'action': [0.0, 1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 137.40936147522558, 'action': [0.0, -1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 156.37566602330733, 'action': [1.0, 0.0]}, {'num_count': 713, 'sum_payoffs': 154.9091923011378, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17044623262618874, 0.16142404291636187, 0.15971714216044866, 0.15922945623018775, 0.1750792489636674, 0.17386003413801512]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 106.37482118606567 s
