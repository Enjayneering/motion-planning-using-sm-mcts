Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 474, 'sum_payoffs': 113.13853880410514, 'action': [2.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 109.8168390390779, 'action': [2.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 98.54158821489092, 'action': [0.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 111.75304800778981, 'action': [2.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 103.54968150489118, 'action': [0.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 111.05433208741768, 'action': [1.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 103.87693011077977, 'action': [0.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 103.2192990768713, 'action': [1.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 108.08176081461373, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11558156547183614, 0.11314313582053158, 0.10485247500609607, 0.11460619361131431, 0.10851011948305292, 0.1141185076810534, 0.10875396244818337, 0.10826627651792246, 0.1119239209948793]
Actions to choose Agent 1: dict_values([{'num_count': 670, 'sum_payoffs': 142.8633597138269, 'action': [0.0, 1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 137.9357466217089, 'action': [0.0, -1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 157.83298711591215, 'action': [1.0, 0.0]}, {'num_count': 697, 'sum_payoffs': 150.80018794951113, 'action': [1.0, -1.5707963267948966]}, {'num_count': 668, 'sum_payoffs': 142.22168113430496, 'action': [0.0, 0.0]}, {'num_count': 691, 'sum_payoffs': 148.99058023367877, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1633747866374055, 0.15922945623018775, 0.17581077785905877, 0.16995854669592783, 0.1628871007071446, 0.1684954889051451]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 105.67741203308105 s
