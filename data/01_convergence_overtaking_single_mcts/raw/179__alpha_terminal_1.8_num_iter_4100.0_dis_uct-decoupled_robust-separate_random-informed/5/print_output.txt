Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 468, 'sum_payoffs': 110.68737956122955, 'action': [2.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 99.8634154588613, 'action': [0.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 106.03958905622625, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 102.81608611349415, 'action': [1.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 112.28756705557834, 'action': [2.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 114.29965471149475, 'action': [2.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 103.41799069275704, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 105.11937123798234, 'action': [1.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 104.72364062010156, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1141185076810534, 0.10607168983174835, 0.11070470616922702, 0.10826627651792246, 0.11533772250670568, 0.11680078029748842, 0.10875396244818337, 0.10997317727383565, 0.1097293343087052]
Actions to choose Agent 1: dict_values([{'num_count': 659, 'sum_payoffs': 139.9515740696589, 'action': [0.0, 0.0]}, {'num_count': 657, 'sum_payoffs': 139.3732467121662, 'action': [0.0, 1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 137.87350424423153, 'action': [0.0, -1.5707963267948966]}, {'num_count': 729, 'sum_payoffs': 160.52873439255282, 'action': [1.0, 0.0]}, {'num_count': 700, 'sum_payoffs': 151.9080189997063, 'action': [1.0, -1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 152.75564972304275, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16069251402097048, 0.16020482809070957, 0.1589856132650573, 0.17776152158010242, 0.17069007559131918, 0.17142160448671057]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 108.88885521888733 s
