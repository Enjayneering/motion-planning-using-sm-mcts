Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 478, 'sum_payoffs': 113.96184104473681, 'action': [2.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 102.1973940042187, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 104.47283468380114, 'action': [0.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 109.01449678966019, 'action': [2.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 103.14540777046085, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 105.02313619263852, 'action': [1.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 112.03976635885701, 'action': [2.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 102.7512160204146, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 106.72380282804671, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11655693733235796, 0.10777859058766155, 0.10948549134357474, 0.11289929285540112, 0.10851011948305292, 0.10997317727383565, 0.11509387954157523, 0.10826627651792246, 0.11119239209948793]
Actions to choose Agent 1: dict_values([{'num_count': 666, 'sum_payoffs': 141.59715209501667, 'action': [0.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 153.69410326315304, 'action': [1.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 136.37163960100068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 156.70230218971716, 'action': [1.0, 0.0]}, {'num_count': 655, 'sum_payoffs': 138.52366507432018, 'action': [0.0, -1.5707963267948966]}, {'num_count': 707, 'sum_payoffs': 153.70175304316552, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1623994147768837, 0.1723969763472324, 0.15801024140453548, 0.17483540599853695, 0.15971714216044866, 0.1723969763472324]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 109.87902235984802 s
