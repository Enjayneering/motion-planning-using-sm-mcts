Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 458, 'sum_payoffs': 107.53213892932423, 'action': [1.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 112.55791888784051, 'action': [2.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 108.96735720546762, 'action': [1.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 108.60927168151416, 'action': [2.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 101.33712788834126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 113.26329717311343, 'action': [2.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 101.01475192635556, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 101.66108523394429, 'action': [0.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 106.29311909388773, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11168007802974884, 0.11533772250670568, 0.11265544989027067, 0.11241160692514021, 0.10704706169227018, 0.11582540843696659, 0.10680321872713973, 0.10729090465740064, 0.11070470616922702]
Actions to choose Agent 1: dict_values([{'num_count': 712, 'sum_payoffs': 155.13299811608388, 'action': [1.0, 0.0]}, {'num_count': 662, 'sum_payoffs': 140.47415007811028, 'action': [0.0, -1.5707963267948966]}, {'num_count': 651, 'sum_payoffs': 137.21609421014128, 'action': [0.0, 1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 153.85318442663947, 'action': [1.0, -1.5707963267948966]}, {'num_count': 685, 'sum_payoffs': 147.20067803064742, 'action': [1.0, 1.5707963267948966]}, {'num_count': 682, 'sum_payoffs': 146.26073334684858, 'action': [0.0, 0.0]}])
Weights num count: [0.17361619117288465, 0.16142404291636187, 0.15874177029992684, 0.17264081931236283, 0.16703243111436236, 0.16630090221897098]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 105.40641069412231 s
