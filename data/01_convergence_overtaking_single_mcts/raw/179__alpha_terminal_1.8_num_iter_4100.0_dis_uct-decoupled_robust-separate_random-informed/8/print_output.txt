Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 446, 'sum_payoffs': 103.50009339546756, 'action': [0.0, 0.0]}, {'num_count': 434, 'sum_payoffs': 99.59312961939645, 'action': [0.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 114.06114482644915, 'action': [2.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 104.52430437121352, 'action': [1.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 107.48365995877954, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 101.84125461582003, 'action': [0.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 110.10554334838737, 'action': [1.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 110.78257265152544, 'action': [2.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 108.14216317344929, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10875396244818337, 0.10582784686661789, 0.11655693733235796, 0.10948549134357474, 0.11168007802974884, 0.1075347476225311, 0.11363082175079249, 0.1141185076810534, 0.11216776396000976]
Actions to choose Agent 1: dict_values([{'num_count': 648, 'sum_payoffs': 136.17351071406645, 'action': [0.0, 1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 137.43294409737229, 'action': [0.0, -1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 153.7682051994402, 'action': [1.0, 1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 155.83421800155259, 'action': [1.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 151.06875677688924, 'action': [1.0, -1.5707963267948966]}, {'num_count': 678, 'sum_payoffs': 145.00161640927968, 'action': [0.0, 0.0]}])
Weights num count: [0.15801024140453548, 0.1589856132650573, 0.17264081931236283, 0.17434772006827604, 0.17044623262618874, 0.16532553035844916]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.2374975681305 s
