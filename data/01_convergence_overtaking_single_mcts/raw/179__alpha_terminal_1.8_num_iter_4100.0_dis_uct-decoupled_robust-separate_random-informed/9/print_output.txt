Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 448, 'sum_payoffs': 103.99111178553028, 'action': [0.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 104.10645109749404, 'action': [1.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 101.09990843802733, 'action': [0.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 119.25664913713638, 'action': [2.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 107.69455608608864, 'action': [1.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 107.35390490609055, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 104.69583613680847, 'action': [2.0, 1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 109.96833267199888, 'action': [2.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 100.82471656161339, 'action': [0.0, 0.0]}])
Weights num count: [0.10924164837844429, 0.10924164837844429, 0.10704706169227018, 0.12045842477444525, 0.1119239209948793, 0.11168007802974884, 0.1097293343087052, 0.11363082175079249, 0.10680321872713973]
Actions to choose Agent 1: dict_values([{'num_count': 728, 'sum_payoffs': 160.28697237921324, 'action': [1.0, 0.0]}, {'num_count': 657, 'sum_payoffs': 139.37203479671714, 'action': [0.0, -1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 141.7404735508468, 'action': [0.0, 1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 137.4006575893431, 'action': [0.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 151.77833401698277, 'action': [1.0, -1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 152.37665642507915, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17751767861497195, 0.16020482809070957, 0.16215557181175322, 0.1584979273347964, 0.17044623262618874, 0.17093391855644965]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.97655701637268 s
