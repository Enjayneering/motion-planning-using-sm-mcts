Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 322, 'sum_payoffs': 72.4570323413631, 'action': [1.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 90.89976749242986, 'action': [2.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 72.38750188920943, 'action': [1.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 58.35973316721398, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 57.797622918077195, 'action': [0.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 91.52554156181257, 'action': [2.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 114.26594671777039, 'action': [2.0, 0.0]}, {'num_count': 325, 'sum_payoffs': 73.47978183601766, 'action': [0.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 90.89705145916076, 'action': [1.0, 0.0]}])
Weights num count: [0.1038374717832957, 0.12125120928732668, 0.1038374717832957, 0.09029345372460497, 0.08964850048371494, 0.12189616252821671, 0.14285714285714285, 0.10480490164463076, 0.12125120928732668]
Actions to choose Agent 1: dict_values([{'num_count': 615, 'sum_payoffs': 178.9172804604647, 'action': [1.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 136.60111970730605, 'action': [0.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 118.6485018163225, 'action': [0.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 150.4645503085172, 'action': [1.0, 1.5707963267948966]}, {'num_count': 547, 'sum_payoffs': 153.75855547929544, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 118.59613669452257, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1983231215736859, 0.16123831022250887, 0.14511447920025797, 0.17349242179941954, 0.1763947113834247, 0.14511447920025797]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.539921998977661 s
