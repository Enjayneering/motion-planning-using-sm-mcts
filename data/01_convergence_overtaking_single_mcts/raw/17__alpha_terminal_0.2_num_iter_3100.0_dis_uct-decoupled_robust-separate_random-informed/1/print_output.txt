Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 439, 'sum_payoffs': 113.04623048912794, 'action': [2.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 93.39551237323407, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 92.00356342036883, 'action': [2.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.23974967838296, 'action': [1.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 90.22771221583966, 'action': [2.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 72.91180495497322, 'action': [1.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 58.498794071521246, 'action': [0.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 58.45251286431374, 'action': [0.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.17811382966782, 'action': [0.0, 0.0]}])
Weights num count: [0.14156723637536278, 0.1235085456304418, 0.12221863914866173, 0.10351499516285069, 0.12060625604643663, 0.10415994840374072, 0.09029345372460497, 0.09029345372460497, 0.10351499516285069]
Actions to choose Agent 1: dict_values([{'num_count': 442, 'sum_payoffs': 115.86141709826934, 'action': [0.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 137.43751310468042, 'action': [0.0, 0.0]}, {'num_count': 553, 'sum_payoffs': 156.099341607573, 'action': [1.0, -1.5707963267948966]}, {'num_count': 604, 'sum_payoffs': 174.97830791841903, 'action': [1.0, 0.0]}, {'num_count': 547, 'sum_payoffs': 153.83688587931073, 'action': [1.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 119.40747015810467, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14253466623669783, 0.1618832634633989, 0.17832957110609482, 0.1947758787487907, 0.1763947113834247, 0.14575943244114803]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.541833877563477 s
