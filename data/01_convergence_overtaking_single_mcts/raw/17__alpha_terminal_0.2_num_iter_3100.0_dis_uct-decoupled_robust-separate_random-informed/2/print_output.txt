Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 446, 'sum_payoffs': 115.3040508539326, 'action': [2.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 89.12956563712368, 'action': [2.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 90.81285442723785, 'action': [2.0, 1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 71.41690023367008, 'action': [1.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 73.01914259049991, 'action': [1.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 73.69999781495159, 'action': [0.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 92.23551266311829, 'action': [1.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 57.72809246592356, 'action': [0.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 58.46402884544443, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1438245727184779, 0.11963882618510158, 0.12125120928732668, 0.10287004192196066, 0.10448242502418574, 0.10512737826507579, 0.12254111576910674, 0.08964850048371494, 0.09029345372460497]
Actions to choose Agent 1: dict_values([{'num_count': 446, 'sum_payoffs': 117.14186383120395, 'action': [0.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 116.76813765087815, 'action': [0.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 156.00971250911184, 'action': [1.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 135.51065044934387, 'action': [0.0, 0.0]}, {'num_count': 619, 'sum_payoffs': 180.37177060646775, 'action': [1.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 151.26415050828396, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1438245727184779, 0.14350209609803288, 0.17832957110609482, 0.16027088036117382, 0.199613028055466, 0.17413737504030957]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.661664962768555 s
