Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 325, 'sum_payoffs': 73.47300985968687, 'action': [0.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 57.119701009579295, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 116.03140456827667, 'action': [2.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 90.89976749242989, 'action': [2.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 71.83125827198039, 'action': [1.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 92.99176497163143, 'action': [1.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 57.855637389100124, 'action': [0.0, -1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 89.82487015866002, 'action': [2.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 72.39901787034016, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10480490164463076, 0.0890035472428249, 0.14446952595936793, 0.12125120928732668, 0.10319251854240567, 0.12318606900999678, 0.08964850048371494, 0.12028377942599161, 0.1038374717832957]
Actions to choose Agent 1: dict_values([{'num_count': 443, 'sum_payoffs': 116.05545051630342, 'action': [0.0, 1.5707963267948966]}, {'num_count': 550, 'sum_payoffs': 154.79303823776561, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 138.7933569216762, 'action': [0.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 154.50307452399315, 'action': [1.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 115.99743604528045, 'action': [0.0, -1.5707963267948966]}, {'num_count': 609, 'sum_payoffs': 176.6245862282409, 'action': [1.0, 0.0]}])
Weights num count: [0.14285714285714285, 0.17736214124475974, 0.16317316994517897, 0.17703966462431472, 0.14285714285714285, 0.1963882618510158]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.785435199737549 s
