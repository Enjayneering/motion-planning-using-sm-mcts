Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 375, 'sum_payoffs': 90.55493990627319, 'action': [2.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 73.776300243436, 'action': [0.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 93.33275389741121, 'action': [1.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 72.83662515359657, 'action': [1.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 57.780240305038795, 'action': [0.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 89.53805704352628, 'action': [2.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 57.85563738910017, 'action': [0.0, -1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.15283661319096, 'action': [1.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 114.7053646898729, 'action': [2.0, 0.0]}])
Weights num count: [0.12092873266688164, 0.10512737826507579, 0.1235085456304418, 0.10415994840374072, 0.08964850048371494, 0.1199613028055466, 0.08964850048371494, 0.10351499516285069, 0.14317961947758787]
Actions to choose Agent 1: dict_values([{'num_count': 545, 'sum_payoffs': 153.1501640229512, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 138.55282501375007, 'action': [0.0, 0.0]}, {'num_count': 615, 'sum_payoffs': 179.0276600532874, 'action': [1.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 117.18542900514232, 'action': [0.0, 1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 153.45435975112352, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 116.0843491104725, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17574975814253466, 0.16285069332473395, 0.1983231215736859, 0.1438245727184779, 0.17607223476297967, 0.14285714285714285]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.984233856201172 s
