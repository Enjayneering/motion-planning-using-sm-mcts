Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 374, 'sum_payoffs': 90.22184558393192, 'action': [2.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 114.02563209252894, 'action': [2.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 89.19909608927732, 'action': [2.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 72.52656279351673, 'action': [1.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 75.15629864980099, 'action': [0.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 72.15283661319096, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.211980956387514, 'action': [0.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 93.04391281074668, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 58.1309345231032, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12060625604643663, 0.14253466623669783, 0.11963882618510158, 0.1038374717832957, 0.10641728474685586, 0.10351499516285069, 0.08997097710415995, 0.12318606900999678, 0.08997097710415995]
Actions to choose Agent 1: dict_values([{'num_count': 452, 'sum_payoffs': 119.3986702102432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 133.4170595639811, 'action': [0.0, 0.0]}, {'num_count': 619, 'sum_payoffs': 180.4085638040756, 'action': [1.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 119.3379397059511, 'action': [0.0, -1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 150.93963885111953, 'action': [1.0, 1.5707963267948966]}, {'num_count': 547, 'sum_payoffs': 153.88610040247207, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14575943244114803, 0.15833602063850372, 0.199613028055466, 0.14575943244114803, 0.17381489841986456, 0.1763947113834247]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.665911912918091 s
