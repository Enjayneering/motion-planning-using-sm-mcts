Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 115.08495750210639, 'action': [2.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 90.58970513235002, 'action': [2.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.159833117272285, 'action': [0.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 91.30825889883248, 'action': [2.0, 1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 73.4538165578023, 'action': [0.0, 0.0]}, {'num_count': 318, 'sum_payoffs': 71.14746973157476, 'action': [1.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 92.68959721499007, 'action': [1.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 72.09482214216798, 'action': [1.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.15396648536456, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14350209609803288, 0.12092873266688164, 0.08997097710415995, 0.12157368590777169, 0.10480490164463076, 0.10254756530151564, 0.12286359238955176, 0.10351499516285069, 0.08997097710415995]
Actions to choose Agent 1: dict_values([{'num_count': 609, 'sum_payoffs': 176.8257175466324, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 136.07964131615378, 'action': [0.0, 0.0]}, {'num_count': 548, 'sum_payoffs': 154.2656932147055, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 118.7006496554377, 'action': [0.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 117.58805377963719, 'action': [0.0, 1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 154.2395106538056, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1963882618510158, 0.16059335698161883, 0.1767171880038697, 0.14511447920025797, 0.14414704933892292, 0.1767171880038697]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.547975778579712 s
