Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 278, 'sum_payoffs': 57.815005531115624, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 90.57818915121929, 'action': [2.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 90.67661819754207, 'action': [2.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 91.59822261260486, 'action': [1.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 74.53932452827996, 'action': [0.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 58.53355929759805, 'action': [0.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 71.23438279676675, 'action': [1.0, -1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.18173520736008, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 115.83059917391192, 'action': [2.0, 0.0]}])
Weights num count: [0.08964850048371494, 0.12092873266688164, 0.12092873266688164, 0.12189616252821671, 0.10577233150596582, 0.09029345372460497, 0.10254756530151564, 0.10351499516285069, 0.14414704933892292]
Actions to choose Agent 1: dict_values([{'num_count': 612, 'sum_payoffs': 177.80196855139488, 'action': [1.0, 0.0]}, {'num_count': 453, 'sum_payoffs': 119.6450687500771, 'action': [0.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 115.74832147217738, 'action': [0.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 151.50174910025626, 'action': [1.0, -1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 138.05358188177846, 'action': [0.0, 0.0]}, {'num_count': 548, 'sum_payoffs': 154.11783236253683, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.19735569171235084, 0.14608190906159305, 0.14253466623669783, 0.1744598516607546, 0.16252821670428894, 0.1767171880038697]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.626245498657227 s
