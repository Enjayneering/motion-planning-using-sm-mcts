Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 382, 'sum_payoffs': 93.02653019770823, 'action': [1.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 91.22134583364043, 'action': [2.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 73.88939586952796, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 113.23845321831587, 'action': [2.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 58.22936356942592, 'action': [0.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 58.54507527872876, 'action': [0.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 88.91228297414358, 'action': [2.0, -1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.17021922622934, 'action': [1.0, 1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 73.44501660994092, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.12318606900999678, 0.12157368590777169, 0.10512737826507579, 0.1418897129958078, 0.08997097710415995, 0.09029345372460497, 0.11931634956465656, 0.10351499516285069, 0.10480490164463076]
Actions to choose Agent 1: dict_values([{'num_count': 538, 'sum_payoffs': 150.46455030851718, 'action': [1.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 121.84114462482431, 'action': [0.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 138.126950994309, 'action': [0.0, 0.0]}, {'num_count': 544, 'sum_payoffs': 152.66627553248728, 'action': [1.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 117.87193357881709, 'action': [0.0, -1.5707963267948966]}, {'num_count': 607, 'sum_payoffs': 175.9796188569736, 'action': [1.0, 0.0]}])
Weights num count: [0.17349242179941954, 0.14801676878426315, 0.16252821670428894, 0.17542728152208964, 0.14446952595936793, 0.19574330861012576]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.631263017654419 s
