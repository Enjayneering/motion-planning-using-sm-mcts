Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 508, 'sum_payoffs': 117.7586559012185, 'action': [1.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 118.14259435965373, 'action': [2.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 128.19174621545895, 'action': [2.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 111.98777210562527, 'action': [0.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 111.99061437474165, 'action': [0.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 122.60598163295651, 'action': [1.0, -1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 123.29152966850128, 'action': [2.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 120.44319169716093, 'action': [1.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 114.88159046316146, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11041078026515974, 0.11062812432079983, 0.11736579004564225, 0.10649858726363834, 0.10649858726363834, 0.11367094109976092, 0.11410562921104107, 0.11214953271028037, 0.10845468376439904]
Actions to choose Agent 1: dict_values([{'num_count': 745, 'sum_payoffs': 157.41647632659328, 'action': [0.0, 1.5707963267948966]}, {'num_count': 788, 'sum_payoffs': 169.7609377835607, 'action': [1.0, -1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 159.15502513836032, 'action': [0.0, 0.0]}, {'num_count': 731, 'sum_payoffs': 153.42445659356704, 'action': [0.0, -1.5707963267948966]}, {'num_count': 778, 'sum_payoffs': 166.83458101719154, 'action': [1.0, 1.5707963267948966]}, {'num_count': 807, 'sum_payoffs': 175.21849619297973, 'action': [1.0, 0.0]}])
Weights num count: [0.16192132145185828, 0.17126711584438165, 0.16322538578569876, 0.1588785046728972, 0.16909367528798086, 0.17539665290154313]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 118.71855664253235 s
