Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 514, 'sum_payoffs': 119.60428741887856, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 115.64195694158613, 'action': [0.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 110.56218939105192, 'action': [0.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 121.4981199873954, 'action': [2.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 117.89371011290616, 'action': [1.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 124.75664096059961, 'action': [2.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 120.25506355108844, 'action': [2.0, 1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 118.54475270743494, 'action': [0.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 118.90127357981441, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11171484459900022, 0.10910671593131928, 0.10562921104107803, 0.11301890893284068, 0.11062812432079983, 0.11519234948924147, 0.11214953271028037, 0.11106281243207998, 0.11128015648772006]
Actions to choose Agent 1: dict_values([{'num_count': 803, 'sum_payoffs': 174.29089452555803, 'action': [1.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 154.18289926342646, 'action': [0.0, -1.5707963267948966]}, {'num_count': 781, 'sum_payoffs': 167.93827608633347, 'action': [1.0, 1.5707963267948966]}, {'num_count': 781, 'sum_payoffs': 167.93302984592492, 'action': [1.0, -1.5707963267948966]}, {'num_count': 757, 'sum_payoffs': 161.06891401620658, 'action': [0.0, 0.0]}, {'num_count': 745, 'sum_payoffs': 157.53366935653966, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17452727667898282, 0.15931319278417735, 0.1697457074549011, 0.1697457074549011, 0.16452945011953923, 0.16192132145185828]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.83726143836975 s
