Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 505, 'sum_payoffs': 117.25468567773548, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 115.09635429578734, 'action': [0.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 122.82314035954703, 'action': [2.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 122.82980298351218, 'action': [2.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 112.158259888738, 'action': [0.0, 1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 130.64308085658328, 'action': [2.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 115.01780054059724, 'action': [1.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 114.77469166215411, 'action': [0.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 123.0665553132281, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10975874809823952, 0.10823733970875897, 0.11345359704412085, 0.11345359704412085, 0.10628124320799825, 0.11866985437948271, 0.10823733970875897, 0.10801999565311889, 0.11367094109976092]
Actions to choose Agent 1: dict_values([{'num_count': 729, 'sum_payoffs': 152.49717521580627, 'action': [0.0, -1.5707963267948966]}, {'num_count': 794, 'sum_payoffs': 171.23830895609734, 'action': [1.0, 1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 175.5956307084909, 'action': [1.0, 0.0]}, {'num_count': 727, 'sum_payoffs': 152.06592690869164, 'action': [0.0, 1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 172.1349372644586, 'action': [1.0, -1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 156.88626831640343, 'action': [0.0, 0.0]}])
Weights num count: [0.15844381656161705, 0.17257118017822212, 0.1758313410128233, 0.15800912845033688, 0.17322321234514235, 0.16170397739621822]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.61030530929565 s
