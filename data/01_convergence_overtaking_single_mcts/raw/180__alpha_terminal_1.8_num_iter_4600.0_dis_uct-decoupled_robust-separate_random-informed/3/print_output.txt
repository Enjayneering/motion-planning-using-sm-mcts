Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 526, 'sum_payoffs': 123.30982320446147, 'action': [2.0, -1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 131.66951140005696, 'action': [2.0, 0.0]}, {'num_count': 512, 'sum_payoffs': 118.77345901826409, 'action': [1.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 114.8972843513146, 'action': [0.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 112.97487648790239, 'action': [1.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 115.25277127165344, 'action': [0.0, -1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 118.1094386955378, 'action': [2.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 120.05488772011762, 'action': [1.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 111.33510820089057, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11432297326668116, 0.11997391871332319, 0.11128015648772006, 0.10867202782003912, 0.10736796348619865, 0.1088893718756792, 0.11084546837643991, 0.11214953271028037, 0.10628124320799825]
Actions to choose Agent 1: dict_values([{'num_count': 780, 'sum_payoffs': 167.57334136281486, 'action': [1.0, 1.5707963267948966]}, {'num_count': 750, 'sum_payoffs': 158.9033718533226, 'action': [0.0, 1.5707963267948966]}, {'num_count': 742, 'sum_payoffs': 156.67007097116968, 'action': [0.0, 0.0]}, {'num_count': 735, 'sum_payoffs': 154.69117313433452, 'action': [0.0, -1.5707963267948966]}, {'num_count': 812, 'sum_payoffs': 176.80422798107713, 'action': [1.0, 0.0]}, {'num_count': 781, 'sum_payoffs': 167.8881451685557, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16952836339926103, 0.16300804173005867, 0.16126928928493806, 0.15974788089545752, 0.17648337317974352, 0.1697457074549011]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 115.1906521320343 s
