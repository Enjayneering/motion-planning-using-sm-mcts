Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 513, 'sum_payoffs': 119.36752025758977, 'action': [1.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 119.9612887821556, 'action': [2.0, 1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 125.8041729399266, 'action': [2.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 111.58618394553045, 'action': [0.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 114.44745876782501, 'action': [1.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 119.25562384954473, 'action': [1.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 113.73999963623496, 'action': [0.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 117.10834237023793, 'action': [0.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 127.10552046411712, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11149750054336013, 0.1119321886546403, 0.1158443816561617, 0.10628124320799825, 0.10823733970875897, 0.11149750054336013, 0.10780265159747882, 0.10997609215387959, 0.11671375787872201]
Actions to choose Agent 1: dict_values([{'num_count': 784, 'sum_payoffs': 168.52347831394576, 'action': [1.0, -1.5707963267948966]}, {'num_count': 732, 'sum_payoffs': 153.55359393325725, 'action': [0.0, 1.5707963267948966]}, {'num_count': 785, 'sum_payoffs': 168.78043341730054, 'action': [1.0, 1.5707963267948966]}, {'num_count': 817, 'sum_payoffs': 178.07149950325473, 'action': [1.0, 0.0]}, {'num_count': 754, 'sum_payoffs': 159.82046676391113, 'action': [0.0, 0.0]}, {'num_count': 728, 'sum_payoffs': 152.44577953311108, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17039773962182134, 0.15909584872853727, 0.17061508367746142, 0.17757009345794392, 0.163877417952619, 0.15822647250597696]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 121.36244416236877 s
