Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 513, 'sum_payoffs': 119.30652651845541, 'action': [1.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 108.70240614860785, 'action': [0.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 114.84248232380997, 'action': [0.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 124.26476528994674, 'action': [2.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 115.4065124652861, 'action': [1.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 112.62580941904886, 'action': [0.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 126.17537372074757, 'action': [2.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 122.24623140699383, 'action': [1.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 125.1997852927218, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11149750054336013, 0.10432514670723755, 0.10845468376439904, 0.11475766137796131, 0.1088893718756792, 0.1069332753749185, 0.11606172571180179, 0.11345359704412085, 0.11540969354488155]
Actions to choose Agent 1: dict_values([{'num_count': 784, 'sum_payoffs': 168.41740354125275, 'action': [1.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 154.55790234986878, 'action': [0.0, -1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 175.6568187135196, 'action': [1.0, 0.0]}, {'num_count': 730, 'sum_payoffs': 152.9537692862292, 'action': [0.0, 1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 171.57137568039118, 'action': [1.0, -1.5707963267948966]}, {'num_count': 746, 'sum_payoffs': 157.53103166902739, 'action': [0.0, 0.0]}])
Weights num count: [0.17039773962182134, 0.15996522495109758, 0.1758313410128233, 0.15866116061725713, 0.1727885242338622, 0.16213866550749836]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 120.48959946632385 s
