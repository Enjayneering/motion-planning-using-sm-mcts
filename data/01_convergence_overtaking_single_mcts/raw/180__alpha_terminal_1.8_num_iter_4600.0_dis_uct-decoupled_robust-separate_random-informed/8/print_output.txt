Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 491, 'sum_payoffs': 112.80096387665887, 'action': [0.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 111.12235416718318, 'action': [0.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 129.3795195397049, 'action': [2.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 115.40205698323106, 'action': [1.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 122.13972247095418, 'action': [1.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 119.2505048270977, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 114.07292364528698, 'action': [0.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 125.10528275587076, 'action': [2.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 124.4022333957295, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10671593131927842, 0.10562921104107803, 0.1178004781569224, 0.10845468376439904, 0.11301890893284068, 0.11106281243207998, 0.10758530754183873, 0.1149750054336014, 0.11454031732232123]
Actions to choose Agent 1: dict_values([{'num_count': 779, 'sum_payoffs': 166.67099340215182, 'action': [1.0, 1.5707963267948966]}, {'num_count': 729, 'sum_payoffs': 152.30822989114094, 'action': [0.0, -1.5707963267948966]}, {'num_count': 822, 'sum_payoffs': 179.11469511607845, 'action': [1.0, 0.0]}, {'num_count': 772, 'sum_payoffs': 164.62384797105352, 'action': [1.0, -1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 157.7978044613802, 'action': [0.0, 0.0]}, {'num_count': 750, 'sum_payoffs': 158.3299255814306, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16931101934362094, 0.15844381656161705, 0.1786568137361443, 0.1677896109541404, 0.16257335361877853, 0.16300804173005867]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 118.14840531349182 s
