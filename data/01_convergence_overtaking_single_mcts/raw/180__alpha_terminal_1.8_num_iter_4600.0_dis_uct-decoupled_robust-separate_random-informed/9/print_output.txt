Searching game tree in timestep 0...
Max timehorizon: 12
Actions to choose Agent 0: dict_values([{'num_count': 545, 'sum_payoffs': 130.11306243593347, 'action': [2.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 120.32296806815364, 'action': [1.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 113.53659253249916, 'action': [0.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 122.03330711485724, 'action': [2.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 120.37397519073853, 'action': [1.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 109.04467449321697, 'action': [0.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 112.99470279213627, 'action': [0.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 125.244172156555, 'action': [2.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 118.4393735023168, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11845251032384264, 0.1119321886546403, 0.10736796348619865, 0.11301890893284068, 0.1119321886546403, 0.10432514670723755, 0.1069332753749185, 0.11519234948924147, 0.11062812432079983]
Actions to choose Agent 1: dict_values([{'num_count': 727, 'sum_payoffs': 152.05562655089318, 'action': [0.0, 1.5707963267948966]}, {'num_count': 812, 'sum_payoffs': 176.50874672719186, 'action': [1.0, 0.0]}, {'num_count': 749, 'sum_payoffs': 158.36546425805423, 'action': [0.0, 0.0]}, {'num_count': 748, 'sum_payoffs': 158.07466973072758, 'action': [0.0, -1.5707963267948966]}, {'num_count': 772, 'sum_payoffs': 164.94863176297062, 'action': [1.0, -1.5707963267948966]}, {'num_count': 792, 'sum_payoffs': 170.6837138395399, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15800912845033688, 0.17648337317974352, 0.16279069767441862, 0.16257335361877853, 0.1677896109541404, 0.17213649206694198]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 119.55018210411072 s
