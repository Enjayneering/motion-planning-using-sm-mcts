Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 178, 'sum_payoffs': 41.78407074687475, 'action': [1.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 41.72390093108772, 'action': [1.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 41.238976694736614, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 40.934661617904744, 'action': [0.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 40.23758366335987, 'action': [0.0, -1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 43.60807556155725, 'action': [2.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 42.48654929512141, 'action': [2.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 44.004600814195705, 'action': [2.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.765012597281356, 'action': [0.0, 0.0]}])
Weights num count: [0.11118051217988757, 0.11118051217988757, 0.11055590256089944, 0.1099312929419113, 0.10868207370393504, 0.11430356027482823, 0.11242973141786383, 0.11492816989381636, 0.1061836352279825]
Actions to choose Agent 1: dict_values([{'num_count': 270, 'sum_payoffs': 57.751037192802556, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 57.51648658028417, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 60.81507738779578, 'action': [1.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 52.50360401167826, 'action': [0.0, 1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 56.73622029986099, 'action': [0.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 54.873643337726996, 'action': [0.0, 0.0]}])
Weights num count: [0.16864459712679575, 0.1680199875078076, 0.17426608369768895, 0.15865084322298564, 0.16677076826983137, 0.16302311055590257]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 56.945656299591064 s
