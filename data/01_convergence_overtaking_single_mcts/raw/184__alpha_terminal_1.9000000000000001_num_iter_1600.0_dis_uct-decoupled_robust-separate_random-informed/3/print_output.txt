Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 179, 'sum_payoffs': 42.131487870924026, 'action': [2.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 39.91881060572473, 'action': [0.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 42.13347077735244, 'action': [2.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 39.800664418949914, 'action': [0.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 42.08959376764275, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 41.00553018343987, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 43.2287661185332, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 40.65315492732355, 'action': [1.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 44.081070565060806, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11180512179887571, 0.1080574640849469, 0.11180512179887571, 0.1080574640849469, 0.11180512179887571, 0.1099312929419113, 0.1136789506558401, 0.10930668332292318, 0.11492816989381636]
Actions to choose Agent 1: dict_values([{'num_count': 259, 'sum_payoffs': 54.16468343323789, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 58.441923360070824, 'action': [1.0, 0.0]}, {'num_count': 260, 'sum_payoffs': 54.48579856132843, 'action': [0.0, -1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 57.137086104108285, 'action': [1.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 56.45329266736251, 'action': [0.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 59.38116432000207, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1617738913179263, 0.16989381636477202, 0.16239850093691444, 0.16739537788881947, 0.16614615865084323, 0.1717676452217364]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.958489418029785 s
