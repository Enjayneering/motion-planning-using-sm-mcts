Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 183, 'sum_payoffs': 43.45492319409296, 'action': [1.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 43.798412364610286, 'action': [2.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 42.0325075066427, 'action': [0.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.51559174689022, 'action': [0.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 39.71113636532869, 'action': [0.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 42.71470700153543, 'action': [2.0, 1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 43.517234072457235, 'action': [2.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 39.69209224536038, 'action': [1.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 40.09147818737295, 'action': [1.0, 0.0]}])
Weights num count: [0.11430356027482823, 0.11492816989381636, 0.11180512179887571, 0.1061836352279825, 0.1080574640849469, 0.11305434103685197, 0.11430356027482823, 0.1080574640849469, 0.10868207370393504]
Actions to choose Agent 1: dict_values([{'num_count': 266, 'sum_payoffs': 56.49782497007092, 'action': [0.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 59.0759015594461, 'action': [1.0, 0.0]}, {'num_count': 256, 'sum_payoffs': 53.080549681622784, 'action': [0.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 59.63714352009653, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 57.41082819573556, 'action': [1.0, 1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 54.195514231113194, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16614615865084323, 0.1711430356027483, 0.1599000624609619, 0.17239225484072454, 0.1680199875078076, 0.1617738913179263]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.61627531051636 s
