Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 40.73392083845667, 'action': [0.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 38.0873484786329, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 43.040058452011074, 'action': [2.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.355265526691156, 'action': [1.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 42.2213888653238, 'action': [1.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 41.84719821793487, 'action': [1.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 45.6230690713913, 'action': [2.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 42.20416654029701, 'action': [2.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 40.37790371116579, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1099312929419113, 0.10555902560899438, 0.1136789506558401, 0.1061836352279825, 0.11242973141786383, 0.11180512179887571, 0.11805121798875702, 0.11242973141786383, 0.10930668332292318]
Actions to choose Agent 1: dict_values([{'num_count': 280, 'sum_payoffs': 61.152704355299036, 'action': [1.0, 1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 57.8679308878264, 'action': [1.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 60.383550828558, 'action': [1.0, 0.0]}, {'num_count': 261, 'sum_payoffs': 54.822686787255805, 'action': [0.0, 1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 54.14608765046651, 'action': [0.0, -1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 51.90578773551405, 'action': [0.0, 0.0]}])
Weights num count: [0.1748906933166771, 0.16864459712679575, 0.1736414740787008, 0.16302311055590257, 0.1617738913179263, 0.15740162398500937]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.11992907524109 s
