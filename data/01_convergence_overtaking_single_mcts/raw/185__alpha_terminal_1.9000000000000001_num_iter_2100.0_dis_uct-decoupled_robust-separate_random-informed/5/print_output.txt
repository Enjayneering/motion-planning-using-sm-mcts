Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 222, 'sum_payoffs': 50.52165994539268, 'action': [0.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 53.32827908144472, 'action': [1.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 57.014060933214104, 'action': [1.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 56.991421992312496, 'action': [2.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 57.38437509128976, 'action': [2.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 51.26655216607861, 'action': [0.0, 1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 54.87867797214744, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 56.723992458286574, 'action': [2.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 53.4314686064374, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10566396953831508, 0.10947168015230842, 0.1142313184198001, 0.1142313184198001, 0.11470728224654926, 0.10661589719181343, 0.11137553545930509, 0.11375535459305093, 0.10947168015230842]
Actions to choose Agent 1: dict_values([{'num_count': 361, 'sum_payoffs': 77.59326747559679, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 79.48007804736247, 'action': [1.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 68.45470187795831, 'action': [0.0, 1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 73.12425639135033, 'action': [0.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 69.43394278143137, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 76.68295728718272, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1718229414564493, 0.17467872441694432, 0.15801999048072346, 0.16515944788196096, 0.15944788196097096, 0.1703950499762018]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.85076355934143 s
