Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 234, 'sum_payoffs': 54.921305657948196, 'action': [2.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 53.02802983805893, 'action': [0.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 54.024760070634684, 'action': [0.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 55.900933691040755, 'action': [2.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 52.372371208821995, 'action': [1.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 57.393910807134354, 'action': [1.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 50.95803073444669, 'action': [0.0, 1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 54.39082479138478, 'action': [1.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 58.54230952344257, 'action': [2.0, 0.0]}])
Weights num count: [0.11137553545930509, 0.10899571632555925, 0.11042360780580676, 0.1128034269395526, 0.10804378867206092, 0.11470728224654926, 0.10613993336506425, 0.11089957163255593, 0.11613517372679677]
Actions to choose Agent 1: dict_values([{'num_count': 364, 'sum_payoffs': 78.55072922638678, 'action': [1.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 69.39482811244447, 'action': [0.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 70.66384426207894, 'action': [0.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 77.96016631584489, 'action': [1.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 77.24246340010224, 'action': [1.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 71.01242813019772, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17325083293669682, 0.15944788196097096, 0.16135173726796764, 0.17229890528319847, 0.17134697762970014, 0.1618277010947168]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 62.27395820617676 s
