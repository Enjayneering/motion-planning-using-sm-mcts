Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 235, 'sum_payoffs': 55.01646325647559, 'action': [1.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 56.071492229498034, 'action': [2.0, -1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 53.206581701421094, 'action': [0.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 55.07852361743025, 'action': [2.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 50.427968460119544, 'action': [0.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 53.223626044737465, 'action': [1.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 56.47180433052339, 'action': [1.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 58.736622370595036, 'action': [2.0, 0.0]}, {'num_count': 226, 'sum_payoffs': 51.862595562759026, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11185149928605426, 0.11327939076630177, 0.10947168015230842, 0.11185149928605426, 0.10566396953831508, 0.10947168015230842, 0.11375535459305093, 0.11661113755354593, 0.10756782484531176]
Actions to choose Agent 1: dict_values([{'num_count': 367, 'sum_payoffs': 80.02278879496073, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 70.81541704646621, 'action': [0.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 77.06473236447569, 'action': [1.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 72.05938025071676, 'action': [0.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 76.78505193370269, 'action': [1.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 70.79548946628552, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17467872441694432, 0.16087577344121848, 0.1703950499762018, 0.16277962874821514, 0.16991908614945264, 0.16087577344121848]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.194483518600464 s
