Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 233, 'sum_payoffs': 54.44551211538617, 'action': [1.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 53.77172397003417, 'action': [1.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 58.431929461651734, 'action': [2.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 54.7921939893471, 'action': [0.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.468354315786925, 'action': [1.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 53.04261279417081, 'action': [2.0, 1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 50.86048710258023, 'action': [0.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.80986282210324, 'action': [0.0, 1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 58.83714480605795, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11089957163255593, 0.1099476439790576, 0.11613517372679677, 0.11137553545930509, 0.11470728224654926, 0.10899571632555925, 0.10613993336506425, 0.10471204188481675, 0.11661113755354593]
Actions to choose Agent 1: dict_values([{'num_count': 347, 'sum_payoffs': 73.36285519516372, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 77.48243455840068, 'action': [1.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 71.12106910799729, 'action': [0.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 79.16431309958605, 'action': [1.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 75.60189260085015, 'action': [1.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 69.27900279667946, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16515944788196096, 0.17134697762970014, 0.1618277010947168, 0.173726796763446, 0.16849119466920515, 0.1589719181342218]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60.432026863098145 s
