Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 68.88097230918001, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 65.79074363205707, 'action': [2.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 71.01030093427545, 'action': [2.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 63.3307365797076, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 63.71032372718217, 'action': [0.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 71.04070790079312, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 65.43635855367208, 'action': [1.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 68.55580275538449, 'action': [2.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 66.47428061149695, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11303344867358708, 0.10957324106113034, 0.11534025374855825, 0.10688196847366398, 0.10726643598615918, 0.11534025374855825, 0.10918877354863514, 0.11264898116109189, 0.11034217608612072]
Actions to choose Agent 1: dict_values([{'num_count': 445, 'sum_payoffs': 95.86062666176139, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 88.13504452558905, 'action': [0.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 89.93795557990286, 'action': [0.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 96.41586179442808, 'action': [1.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 88.38873075714191, 'action': [0.0, 1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 94.54455974379634, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1710880430603614, 0.16147635524798154, 0.1637831603229527, 0.17185697808535177, 0.16186082276047675, 0.1695501730103806]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.4931571483612 s
