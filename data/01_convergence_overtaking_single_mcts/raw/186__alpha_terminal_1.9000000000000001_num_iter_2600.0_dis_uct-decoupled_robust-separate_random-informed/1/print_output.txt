Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 285, 'sum_payoffs': 66.09130338038449, 'action': [0.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 70.44347617160247, 'action': [2.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 64.85301020339358, 'action': [0.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 67.63433289749025, 'action': [1.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 68.71324054679357, 'action': [1.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 72.88928405564386, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 65.55419193420876, 'action': [0.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 63.70416372265014, 'action': [1.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 68.29512108821969, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10957324106113034, 0.11418685121107267, 0.10803537101114956, 0.1111111111111111, 0.1122645136485967, 0.11687812379853903, 0.10880430603613994, 0.10688196847366398, 0.1118800461361015]
Actions to choose Agent 1: dict_values([{'num_count': 445, 'sum_payoffs': 95.13830429873046, 'action': [1.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 92.34363864231534, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 96.329052220727, 'action': [1.0, 0.0]}, {'num_count': 430, 'sum_payoffs': 90.54990409083945, 'action': [0.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 87.20092064915693, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 87.71514102691562, 'action': [0.0, 0.0]}])
Weights num count: [0.1710880430603614, 0.16762783544790466, 0.17262591311034217, 0.1653210303729335, 0.16109188773548636, 0.16186082276047675]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.17488813400269 s
