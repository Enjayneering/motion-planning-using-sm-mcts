Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 278, 'sum_payoffs': 63.504328130507155, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 66.26439180097366, 'action': [0.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 66.93588379937746, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 63.89404381408623, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 65.15714915305469, 'action': [1.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 70.45633157369691, 'action': [1.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 68.69684977521504, 'action': [2.0, 0.0]}, {'num_count': 295, 'sum_payoffs': 69.45366517682096, 'action': [2.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 71.17290570178157, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10688196847366398, 0.10995770857362552, 0.11072664359861592, 0.10726643598615918, 0.10880430603613994, 0.11457131872356786, 0.11264898116109189, 0.11341791618608228, 0.11534025374855825]
Actions to choose Agent 1: dict_values([{'num_count': 443, 'sum_payoffs': 94.76992343970254, 'action': [1.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 88.25412720943278, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 97.20011572209278, 'action': [1.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 87.12872708896543, 'action': [0.0, 1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 92.84635167720542, 'action': [1.0, -1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 90.39360519280469, 'action': [0.0, 0.0]}])
Weights num count: [0.170319108035371, 0.16224529027297194, 0.17339484813533257, 0.16070742022299117, 0.16801230296039985, 0.16493656286043828]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.22055435180664 s
