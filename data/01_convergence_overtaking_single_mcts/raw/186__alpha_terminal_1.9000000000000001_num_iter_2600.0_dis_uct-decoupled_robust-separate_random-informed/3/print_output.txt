Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 274, 'sum_payoffs': 62.15542002677343, 'action': [1.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 67.37714536917974, 'action': [2.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 69.73519696735822, 'action': [1.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 71.97050124190756, 'action': [2.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 65.61477087782407, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 65.29502047873002, 'action': [0.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 67.34029794560311, 'action': [1.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 70.85946923328409, 'action': [2.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 65.62946573383442, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1053440984236832, 0.1111111111111111, 0.11380238369857747, 0.11610918877354863, 0.10918877354863514, 0.10880430603613994, 0.1111111111111111, 0.11495578623606305, 0.10918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 441, 'sum_payoffs': 94.65533816010473, 'action': [1.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 99.00069472763612, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 83.9208299554023, 'action': [0.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 90.74434055710569, 'action': [0.0, 1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 87.04440209464533, 'action': [0.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 98.72721631505594, 'action': [1.0, 0.0]}])
Weights num count: [0.1695501730103806, 0.17493271818531334, 0.15609381007304882, 0.1645520953479431, 0.15993848519800077, 0.17454825067281815]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.62211108207703 s
