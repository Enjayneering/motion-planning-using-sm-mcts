Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 68.79848720863718, 'action': [1.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 68.7226816810236, 'action': [2.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 63.587105161048456, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 65.95540648058746, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 65.66124088648544, 'action': [0.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 68.07709255872201, 'action': [2.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 67.03879109097366, 'action': [1.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 65.15679315595163, 'action': [1.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 69.83900321371371, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11303344867358708, 0.11303344867358708, 0.10726643598615918, 0.10995770857362552, 0.10957324106113034, 0.1122645136485967, 0.1111111111111111, 0.10918877354863514, 0.11418685121107267]
Actions to choose Agent 1: dict_values([{'num_count': 449, 'sum_payoffs': 97.08337351739743, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 89.36406492061997, 'action': [0.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 96.12491273251347, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 94.61804092468789, 'action': [1.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 89.68592425765601, 'action': [0.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 86.61062623766212, 'action': [0.0, 0.0]}])
Weights num count: [0.17262591311034217, 0.16301422529796233, 0.1714725105728566, 0.1695501730103806, 0.16339869281045752, 0.15955401768550556]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 75.10038876533508 s
