Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 69.40722188665761, 'action': [1.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 65.21510412862287, 'action': [2.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 63.73293552569189, 'action': [0.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 69.04009303986221, 'action': [1.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 70.67617351170635, 'action': [2.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 70.03086626880652, 'action': [1.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 70.74885520839, 'action': [2.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 64.43629302274358, 'action': [0.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 64.8224850790475, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11303344867358708, 0.10841983852364476, 0.10688196847366398, 0.11264898116109189, 0.11457131872356786, 0.11380238369857747, 0.11457131872356786, 0.10765090349865436, 0.10803537101114956]
Actions to choose Agent 1: dict_values([{'num_count': 444, 'sum_payoffs': 95.59889964762127, 'action': [1.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 96.2524122539713, 'action': [1.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 83.97213005372379, 'action': [0.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 87.30160805592261, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 98.37785690252662, 'action': [1.0, 0.0]}, {'num_count': 434, 'sum_payoffs': 92.40320365186784, 'action': [0.0, 0.0]}])
Weights num count: [0.1707035755478662, 0.1714725105728566, 0.15609381007304882, 0.16032295271049596, 0.17416378316032297, 0.16685890042291426]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.37782287597656 s
