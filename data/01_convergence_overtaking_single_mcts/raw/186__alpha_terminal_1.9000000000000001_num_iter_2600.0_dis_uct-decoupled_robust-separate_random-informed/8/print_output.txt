Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 305, 'sum_payoffs': 73.09519731717133, 'action': [2.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 66.44904435948409, 'action': [0.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 70.20446858204087, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 65.8054179170382, 'action': [0.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 66.74830936147214, 'action': [2.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 66.8555681490554, 'action': [1.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 61.60018201401539, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 66.39014532740822, 'action': [1.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 69.82960678306537, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11726259131103421, 0.10995770857362552, 0.11418685121107267, 0.10918877354863514, 0.11034217608612072, 0.11034217608612072, 0.10457516339869281, 0.10995770857362552, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 423, 'sum_payoffs': 88.91916355950644, 'action': [0.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 96.39610871682791, 'action': [1.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 88.69519957530304, 'action': [0.0, -1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 88.71113141097658, 'action': [0.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 96.10673963751604, 'action': [1.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 94.2133291422881, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16262975778546712, 0.17185697808535177, 0.16224529027297194, 0.16224529027297194, 0.1714725105728566, 0.16916570549788543]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.80651044845581 s
