Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 65.66110648367717, 'action': [0.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 66.37321645544603, 'action': [1.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 70.84862274323996, 'action': [2.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 68.72277250146551, 'action': [1.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 63.94702311487603, 'action': [0.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 69.14240072515209, 'action': [2.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 63.27050794588775, 'action': [1.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 72.60210999764384, 'action': [2.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 65.71840605892221, 'action': [0.0, 0.0]}])
Weights num count: [0.10918877354863514, 0.10995770857362552, 0.11495578623606305, 0.11264898116109189, 0.10726643598615918, 0.11303344867358708, 0.10649750096116878, 0.11687812379853903, 0.10918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 416, 'sum_payoffs': 86.3866353967053, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 87.34985176609017, 'action': [0.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 88.24553585180122, 'action': [0.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 94.12030939478215, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 95.97975221279195, 'action': [1.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 98.40875862704556, 'action': [1.0, 0.0]}])
Weights num count: [0.15993848519800077, 0.16109188773548636, 0.16224529027297194, 0.1695501730103806, 0.17185697808535177, 0.17493271818531334]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 75.73258090019226 s
