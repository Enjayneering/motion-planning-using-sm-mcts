Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 348, 'sum_payoffs': 81.26163131715039, 'action': [2.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 78.17262754115626, 'action': [0.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 82.50879195045823, 'action': [1.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 81.19911439517799, 'action': [2.0, 1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 80.79250917786676, 'action': [2.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 75.38871412353853, 'action': [0.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 81.2318844228815, 'action': [0.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 79.53321214734008, 'action': [1.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 79.8820641505211, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11222186391486617, 0.10931957433086101, 0.11351177039664624, 0.11222186391486617, 0.11189938729442116, 0.10673976136730087, 0.11222186391486617, 0.11060948081264109, 0.1109319574330861]
Actions to choose Agent 1: dict_values([{'num_count': 507, 'sum_payoffs': 105.96696017032423, 'action': [0.0, 1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 111.33914290171435, 'action': [1.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 114.87270093850962, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 105.00480674566788, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 112.13745041382101, 'action': [1.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 103.55612334765979, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.163495646565624, 0.16930022573363432, 0.17316994517897452, 0.16252821670428894, 0.17026765559496937, 0.16091583360206385]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 88.83006167411804 s
