Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 338, 'sum_payoffs': 77.70001008032607, 'action': [0.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 86.44526199463984, 'action': [2.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 82.836498845431, 'action': [2.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 78.01888306741816, 'action': [0.0, 1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 74.01618331199444, 'action': [0.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 78.36482416167415, 'action': [1.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 82.15841349360605, 'action': [1.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 78.73367805082874, 'action': [1.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 80.79102138298441, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10899709771041599, 0.11738148984198646, 0.11383424701709126, 0.10931957433086101, 0.1054498548855208, 0.10964205095130602, 0.11318929377620122, 0.10996452757175104, 0.11189938729442116]
Actions to choose Agent 1: dict_values([{'num_count': 530, 'sum_payoffs': 113.17519600523592, 'action': [1.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 113.49810917756496, 'action': [1.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 105.06584542386685, 'action': [0.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 106.298614710959, 'action': [0.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 114.88321570077508, 'action': [1.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 102.13479690135135, 'action': [0.0, 0.0]}])
Weights num count: [0.1709126088358594, 0.17123508545630442, 0.16220574008384392, 0.163495646565624, 0.1728474685585295, 0.15898097387939375]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.49602556228638 s
