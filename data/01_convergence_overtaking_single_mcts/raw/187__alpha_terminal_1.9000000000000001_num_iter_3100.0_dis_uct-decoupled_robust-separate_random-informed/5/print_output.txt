Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 80.40160665045775, 'action': [1.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 79.3086185268194, 'action': [0.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 82.45734465024442, 'action': [2.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 72.59547175039368, 'action': [0.0, 1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 83.1121100493678, 'action': [2.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 80.07243463533902, 'action': [1.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 74.54038444799176, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 84.4884110834888, 'action': [2.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 81.80757684702307, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11157691067397614, 0.11060948081264109, 0.11351177039664624, 0.10415994840374072, 0.11415672363753628, 0.11125443405353112, 0.10609480812641084, 0.11544663011931634, 0.11286681715575621]
Actions to choose Agent 1: dict_values([{'num_count': 505, 'sum_payoffs': 105.32294534969807, 'action': [0.0, -1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 110.76169468216725, 'action': [1.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 104.10504787195141, 'action': [0.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 113.72838478523524, 'action': [1.0, -1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 114.60026260919471, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 104.46466325748693, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16285069332473395, 0.16865527249274428, 0.16156078684295389, 0.17188003869719445, 0.1728474685585295, 0.1618832634633989]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.283438205719 s
