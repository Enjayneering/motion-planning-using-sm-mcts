Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 360, 'sum_payoffs': 84.8784472455653, 'action': [2.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 80.87833009142686, 'action': [1.0, -1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 79.12462473607349, 'action': [1.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 76.74704958163233, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 78.43567696658432, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 79.8923480885376, 'action': [2.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 82.49670421815232, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 76.73458021923662, 'action': [0.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 77.43930754200031, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11609158336020639, 0.11222186391486617, 0.11060948081264109, 0.10835214446952596, 0.10996452757175104, 0.11125443405353112, 0.11383424701709126, 0.10835214446952596, 0.10899709771041599]
Actions to choose Agent 1: dict_values([{'num_count': 534, 'sum_payoffs': 113.80354631707927, 'action': [1.0, 0.0]}, {'num_count': 524, 'sum_payoffs': 110.72180177514849, 'action': [1.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 113.77601059268707, 'action': [1.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 102.41381998956973, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 105.92523545699075, 'action': [0.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 104.82084569653738, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17220251531763947, 0.1689777491131893, 0.17220251531763947, 0.1599484037407288, 0.163818123186069, 0.16252821670428894]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 87.72539687156677 s
