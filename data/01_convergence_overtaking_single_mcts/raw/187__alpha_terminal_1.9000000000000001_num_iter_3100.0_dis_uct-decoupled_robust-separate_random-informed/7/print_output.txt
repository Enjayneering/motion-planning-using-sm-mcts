Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 342, 'sum_payoffs': 79.23619244367595, 'action': [1.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 76.52226060462225, 'action': [0.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 87.07967375323753, 'action': [2.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 77.57392658917233, 'action': [1.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 79.85065026761717, 'action': [2.0, 1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 76.15959503582573, 'action': [0.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 79.94660940694065, 'action': [1.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 84.36647314458902, 'action': [2.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 79.88941594183208, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11028700419219607, 0.10770719122863592, 0.11770396646243148, 0.10867462108997097, 0.1109319574330861, 0.1073847146081909, 0.1109319574330861, 0.11512415349887133, 0.1109319574330861]
Actions to choose Agent 1: dict_values([{'num_count': 495, 'sum_payoffs': 102.2377217897945, 'action': [0.0, -1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 113.5703061041568, 'action': [1.0, -1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 110.56874873186493, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 105.23864389943914, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 104.25052123826693, 'action': [0.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 116.25520741276989, 'action': [1.0, 0.0]}])
Weights num count: [0.15962592712028378, 0.17188003869719445, 0.16865527249274428, 0.16285069332473395, 0.1618832634633989, 0.1747823282811996]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.1316409111023 s
