Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 345, 'sum_payoffs': 80.00659736316281, 'action': [2.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 74.47073366870296, 'action': [0.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 79.00244501052009, 'action': [1.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 85.35764378276205, 'action': [1.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 74.91719473248583, 'action': [0.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 77.22570269725695, 'action': [0.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 83.42100899988304, 'action': [2.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 83.0708635067046, 'action': [2.0, 0.0]}, {'num_count': 347, 'sum_payoffs': 80.71108330605577, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11125443405353112, 0.10609480812641084, 0.11028700419219607, 0.11641405998065141, 0.10641728474685586, 0.10867462108997097, 0.11447920025798129, 0.11415672363753628, 0.11189938729442116]
Actions to choose Agent 1: dict_values([{'num_count': 491, 'sum_payoffs': 101.41916078893138, 'action': [0.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 116.90682176017818, 'action': [1.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 106.16844182309292, 'action': [0.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 113.40163386379278, 'action': [1.0, -1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 105.30564566405204, 'action': [0.0, 0.0]}, {'num_count': 524, 'sum_payoffs': 111.23127059030364, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15833602063850372, 0.17510480490164462, 0.163495646565624, 0.17123508545630442, 0.16252821670428894, 0.1689777491131893]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 87.76435256004333 s
