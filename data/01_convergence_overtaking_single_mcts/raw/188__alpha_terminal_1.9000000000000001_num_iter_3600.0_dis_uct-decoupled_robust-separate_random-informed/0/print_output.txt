Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 380, 'sum_payoffs': 85.64356227682654, 'action': [0.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 102.3619943054987, 'action': [2.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 90.34269977624261, 'action': [0.0, 0.0]}, {'num_count': 384, 'sum_payoffs': 87.02610472458963, 'action': [0.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 89.36833484150269, 'action': [1.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 93.64023578920008, 'action': [1.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 91.34787044122245, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 95.53111379530716, 'action': [2.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 95.48874045489319, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10552624271035824, 0.11941127464593168, 0.1094140516523188, 0.10663704526520411, 0.10858094973618439, 0.1121910580394335, 0.1102471535684532, 0.11385726187170231, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 623, 'sum_payoffs': 131.56837526162553, 'action': [1.0, 0.0]}, {'num_count': 617, 'sum_payoffs': 129.7708979383983, 'action': [1.0, -1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 120.2091182466702, 'action': [0.0, 0.0]}, {'num_count': 580, 'sum_payoffs': 119.03752673155077, 'action': [0.0, -1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 127.1759944455796, 'action': [1.0, 1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 121.2428410759076, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1730074979172452, 0.1713412940849764, 0.16217717300749793, 0.16106637045265204, 0.16884198833657318, 0.16328797556234378]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.34837126731873 s
