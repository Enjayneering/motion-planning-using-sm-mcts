Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 386, 'sum_payoffs': 87.45988711760197, 'action': [0.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 92.04207799051191, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 93.73453378167007, 'action': [2.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 94.34976503360276, 'action': [1.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 87.39649344973353, 'action': [0.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 93.65362045595147, 'action': [1.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 85.10553472057231, 'action': [0.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 101.378036201171, 'action': [2.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 93.35857311073582, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10719244654262705, 0.11108025548458761, 0.11246875867814496, 0.1130241599555679, 0.10719244654262705, 0.11246875867814496, 0.10524854207164676, 0.11885587336850875, 0.1121910580394335]
Actions to choose Agent 1: dict_values([{'num_count': 583, 'sum_payoffs': 119.886914178908, 'action': [0.0, 1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 129.79652863752202, 'action': [1.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 120.1409925616537, 'action': [0.0, 0.0]}, {'num_count': 588, 'sum_payoffs': 121.29350554889604, 'action': [0.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 130.96291411218445, 'action': [1.0, 1.5707963267948966]}, {'num_count': 607, 'sum_payoffs': 126.80887380468268, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16189947236878643, 0.1713412940849764, 0.16217717300749793, 0.16328797556234378, 0.17245209663982228, 0.1685642876978617]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.75721526145935 s
