Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 90.09124271859542, 'action': [1.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 101.49676528921759, 'action': [2.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 95.16314937104596, 'action': [2.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 91.16918638752713, 'action': [1.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 85.28289948541624, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 91.50800817112236, 'action': [1.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 88.49596693977985, 'action': [0.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 88.56978291876987, 'action': [0.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 97.78531783789032, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1094140516523188, 0.11885587336850875, 0.11357956123299083, 0.1102471535684532, 0.10524854207164676, 0.11052485420716468, 0.10802554845876146, 0.10802554845876146, 0.11580116634268259]
Actions to choose Agent 1: dict_values([{'num_count': 612, 'sum_payoffs': 128.67177824523196, 'action': [1.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 119.25966409967602, 'action': [0.0, 1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 121.62409691334493, 'action': [0.0, 0.0]}, {'num_count': 611, 'sum_payoffs': 128.30606225937984, 'action': [1.0, -1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 118.19799356278183, 'action': [0.0, -1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 134.8200614315812, 'action': [1.0, 0.0]}])
Weights num count: [0.16995279089141904, 0.16106637045265204, 0.16328797556234378, 0.16967509025270758, 0.15995556789780616, 0.1757845043043599]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.02570867538452 s
