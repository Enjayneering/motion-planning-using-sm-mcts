Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 390, 'sum_payoffs': 88.47841992160079, 'action': [0.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 92.13953471251929, 'action': [1.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 95.39064801715857, 'action': [2.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 95.08574535828502, 'action': [1.0, -1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 92.48253699069821, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 89.48852414528287, 'action': [1.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 86.88856263939746, 'action': [0.0, 0.0]}, {'num_count': 425, 'sum_payoffs': 100.14679528755738, 'action': [2.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 86.1553544138072, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10830324909747292, 0.11135795612329909, 0.11413496251041377, 0.11385726187170231, 0.11163565676201055, 0.10913635101360733, 0.10691474590391557, 0.11802277145237434, 0.10635934462649264]
Actions to choose Agent 1: dict_values([{'num_count': 585, 'sum_payoffs': 120.80503705915864, 'action': [0.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 118.27736714989527, 'action': [0.0, 1.5707963267948966]}, {'num_count': 613, 'sum_payoffs': 128.99975770836562, 'action': [1.0, 1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 131.9136975514538, 'action': [1.0, -1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 119.98263564048871, 'action': [0.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 131.3886571181304, 'action': [1.0, 0.0]}])
Weights num count: [0.1624548736462094, 0.15995556789780616, 0.1702304915301305, 0.1730074979172452, 0.16162177173007497, 0.17245209663982228]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.7991590499878 s
