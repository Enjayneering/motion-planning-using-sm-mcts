Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 411, 'sum_payoffs': 95.02998586631892, 'action': [2.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 94.72311550916079, 'action': [2.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 90.86451254701112, 'action': [1.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 87.92776642802379, 'action': [0.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 94.20748994249848, 'action': [1.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 87.27306409522055, 'action': [0.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 86.53343633291927, 'action': [0.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 92.83560625406571, 'action': [1.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 94.10789508627597, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11413496251041377, 0.11385726187170231, 0.11052485420716468, 0.10802554845876146, 0.11330186059427937, 0.10747014718133852, 0.10691474590391557, 0.1121910580394335, 0.11330186059427937]
Actions to choose Agent 1: dict_values([{'num_count': 590, 'sum_payoffs': 122.34980128052875, 'action': [0.0, 0.0]}, {'num_count': 579, 'sum_payoffs': 119.11718373809698, 'action': [0.0, -1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 129.8905139745946, 'action': [1.0, -1.5707963267948966]}, {'num_count': 603, 'sum_payoffs': 126.17607677384139, 'action': [1.0, 1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 131.70747120538883, 'action': [1.0, 0.0]}, {'num_count': 590, 'sum_payoffs': 122.40881819171591, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16384337683976674, 0.16078866981394058, 0.17106359344626493, 0.16745348514301583, 0.17272979727853374, 0.16384337683976674]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.8131000995636 s
