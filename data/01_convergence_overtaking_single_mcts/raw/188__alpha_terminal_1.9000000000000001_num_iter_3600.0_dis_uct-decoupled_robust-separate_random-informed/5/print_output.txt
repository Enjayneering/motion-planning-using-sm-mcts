Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 407, 'sum_payoffs': 94.05093071632814, 'action': [2.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 98.69444031204388, 'action': [2.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 85.81072521596677, 'action': [0.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 88.14825344455042, 'action': [0.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 87.40996831583993, 'action': [1.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 90.47693032514128, 'action': [0.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 91.76577515927224, 'action': [1.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 94.70866397856953, 'action': [2.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 94.74397160579919, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1130241599555679, 0.11691196889752846, 0.10608164398778117, 0.10802554845876146, 0.10747014718133852, 0.10996945292974174, 0.11108025548458761, 0.11357956123299083, 0.11357956123299083]
Actions to choose Agent 1: dict_values([{'num_count': 580, 'sum_payoffs': 119.83640778762432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 595, 'sum_payoffs': 124.22085296324602, 'action': [0.0, 0.0]}, {'num_count': 618, 'sum_payoffs': 130.8385647427289, 'action': [1.0, 1.5707963267948966]}, {'num_count': 610, 'sum_payoffs': 128.5849852396484, 'action': [1.0, 0.0]}, {'num_count': 580, 'sum_payoffs': 119.73319489132628, 'action': [0.0, -1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 130.61387154504908, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16106637045265204, 0.16523188003332406, 0.17161899472368786, 0.1693973896139961, 0.16106637045265204, 0.1713412940849764]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.94723558425903 s
