Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 387, 'sum_payoffs': 87.13047825669331, 'action': [0.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 96.69128761114953, 'action': [2.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 92.74118011914614, 'action': [1.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 92.73174225356388, 'action': [1.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 88.34585225724697, 'action': [1.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 90.6569881508398, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 95.0460692938052, 'action': [2.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 87.10288372481754, 'action': [0.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 92.01257015523029, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10747014718133852, 0.11552346570397112, 0.1121910580394335, 0.1121910580394335, 0.10858094973618439, 0.11052485420716468, 0.11413496251041377, 0.10747014718133852, 0.11163565676201055]
Actions to choose Agent 1: dict_values([{'num_count': 572, 'sum_payoffs': 117.54221446662469, 'action': [0.0, -1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 119.55290680395461, 'action': [0.0, 1.5707963267948966]}, {'num_count': 595, 'sum_payoffs': 124.27098163595925, 'action': [0.0, 0.0]}, {'num_count': 617, 'sum_payoffs': 130.72331451482825, 'action': [1.0, 0.0]}, {'num_count': 608, 'sum_payoffs': 128.0070027419492, 'action': [1.0, 1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 134.18133800981295, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1588447653429603, 0.16078866981394058, 0.16523188003332406, 0.1713412940849764, 0.16884198833657318, 0.17467370174951402]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.56463837623596 s
