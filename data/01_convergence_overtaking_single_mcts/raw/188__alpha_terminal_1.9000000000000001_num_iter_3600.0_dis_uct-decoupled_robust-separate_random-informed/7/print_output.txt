Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 422, 'sum_payoffs': 98.85901917168043, 'action': [2.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 96.81969040004365, 'action': [2.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 89.55716409839768, 'action': [1.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 92.21256052337137, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 89.26738488523934, 'action': [0.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 88.88625334481908, 'action': [0.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 91.19155150644349, 'action': [1.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 90.62998699051845, 'action': [1.0, 1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 86.54375527577812, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11718966953623994, 0.11552346570397112, 0.1094140516523188, 0.11163565676201055, 0.10913635101360733, 0.10885865037489587, 0.11080255484587614, 0.1102471535684532, 0.10691474590391557]
Actions to choose Agent 1: dict_values([{'num_count': 577, 'sum_payoffs': 118.82456198902534, 'action': [0.0, 1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 131.5740412446534, 'action': [1.0, 0.0]}, {'num_count': 579, 'sum_payoffs': 119.4240128524809, 'action': [0.0, -1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 130.18447188053247, 'action': [1.0, 1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 132.7932711476445, 'action': [1.0, -1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 120.28340226340538, 'action': [0.0, 0.0]}])
Weights num count: [0.16023326853651762, 0.17245209663982228, 0.16078866981394058, 0.17106359344626493, 0.17356289919466814, 0.16162177173007497]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.36124753952026 s
