Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 90.02301479250394, 'action': [1.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 89.01981632130868, 'action': [0.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 91.72852323468447, 'action': [1.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 91.66507426118812, 'action': [1.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 88.75405588895141, 'action': [0.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 98.6336080116532, 'action': [2.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 93.30787033676616, 'action': [2.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 89.3594787385209, 'action': [0.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 95.683466701665, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1094140516523188, 0.10858094973618439, 0.11080255484587614, 0.11080255484587614, 0.10830324909747292, 0.116634268258817, 0.1121910580394335, 0.10885865037489587, 0.11413496251041377]
Actions to choose Agent 1: dict_values([{'num_count': 621, 'sum_payoffs': 131.33142038020333, 'action': [1.0, 0.0]}, {'num_count': 591, 'sum_payoffs': 122.58195757165392, 'action': [0.0, 0.0]}, {'num_count': 585, 'sum_payoffs': 120.80906367082152, 'action': [0.0, -1.5707963267948966]}, {'num_count': 604, 'sum_payoffs': 126.26239158930812, 'action': [1.0, -1.5707963267948966]}, {'num_count': 613, 'sum_payoffs': 129.01536552419427, 'action': [1.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 121.06002171807036, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17245209663982228, 0.1641210774784782, 0.1624548736462094, 0.1677311857817273, 0.1702304915301305, 0.16273257428492086]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.32036113739014 s
