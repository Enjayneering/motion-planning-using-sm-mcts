Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 429, 'sum_payoffs': 101.39832320106653, 'action': [2.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 92.77801465001963, 'action': [1.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 88.13764293885424, 'action': [0.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 85.51557102200543, 'action': [0.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 90.32817518038073, 'action': [1.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 93.11065733495165, 'action': [2.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 87.81215918971722, 'action': [0.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 93.36830279179239, 'action': [2.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 93.36280069385614, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11913357400722022, 0.11191335740072202, 0.10802554845876146, 0.1058039433490697, 0.10996945292974174, 0.1121910580394335, 0.10774784782004998, 0.11246875867814496, 0.11246875867814496]
Actions to choose Agent 1: dict_values([{'num_count': 572, 'sum_payoffs': 117.12284127668201, 'action': [0.0, 0.0]}, {'num_count': 629, 'sum_payoffs': 133.7368397627343, 'action': [1.0, 0.0]}, {'num_count': 592, 'sum_payoffs': 122.89925727659931, 'action': [0.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 116.47998006674212, 'action': [0.0, 1.5707963267948966]}, {'num_count': 607, 'sum_payoffs': 127.18754022116087, 'action': [1.0, 1.5707963267948966]}, {'num_count': 630, 'sum_payoffs': 134.02545333476917, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1588447653429603, 0.17467370174951402, 0.16439877811718967, 0.15828936406553734, 0.1685642876978617, 0.17495140238822549]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 112.7231011390686 s
