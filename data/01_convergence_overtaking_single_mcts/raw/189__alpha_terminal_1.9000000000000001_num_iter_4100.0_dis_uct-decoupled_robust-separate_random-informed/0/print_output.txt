Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 100.26432698795404, 'action': [1.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 106.69708580186695, 'action': [2.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 112.60001305419301, 'action': [2.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 104.0841143271989, 'action': [2.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 101.51558838440404, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 106.33949944852782, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 99.59421057052481, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 103.80651630956768, 'action': [1.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 97.99067678193643, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10851011948305292, 0.11338697878566203, 0.11777615215801024, 0.11143623506461839, 0.10948549134357474, 0.11314313582053158, 0.10802243355279201, 0.11119239209948793, 0.10680321872713973]
Actions to choose Agent 1: dict_values([{'num_count': 665, 'sum_payoffs': 136.00672290129646, 'action': [0.0, 0.0]}, {'num_count': 708, 'sum_payoffs': 148.26486764031776, 'action': [1.0, -1.5707963267948966]}, {'num_count': 707, 'sum_payoffs': 147.97309148891617, 'action': [1.0, 0.0]}, {'num_count': 670, 'sum_payoffs': 137.51290372078057, 'action': [0.0, 1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 132.6501176588254, 'action': [0.0, -1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 145.15583396770305, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16215557181175322, 0.17264081931236283, 0.1723969763472324, 0.1633747866374055, 0.15922945623018775, 0.16995854669592783]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 119.63772249221802 s
