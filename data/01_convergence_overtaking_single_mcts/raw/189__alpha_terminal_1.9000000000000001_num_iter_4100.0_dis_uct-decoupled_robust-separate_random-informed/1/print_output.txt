Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 462, 'sum_payoffs': 105.864243620091, 'action': [2.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 97.20130031001376, 'action': [0.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 104.88912007685883, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 113.38951681427025, 'action': [2.0, 0.0]}, {'num_count': 471, 'sum_payoffs': 108.81635109724529, 'action': [2.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 99.4705305147944, 'action': [0.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 96.18114575503432, 'action': [0.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 102.63186775452415, 'action': [1.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 105.96354277737417, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11265544989027067, 0.10607168983174835, 0.1119239209948793, 0.11826383808827115, 0.11485003657644477, 0.10777859058766155, 0.10534016093635698, 0.11021702023896611, 0.11265544989027067]
Actions to choose Agent 1: dict_values([{'num_count': 664, 'sum_payoffs': 135.6523885503018, 'action': [0.0, 1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 144.94553814387618, 'action': [1.0, 1.5707963267948966]}, {'num_count': 704, 'sum_payoffs': 146.91636622236632, 'action': [1.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 138.18551041550103, 'action': [0.0, 0.0]}, {'num_count': 655, 'sum_payoffs': 133.05364959016097, 'action': [0.0, -1.5707963267948966]}, {'num_count': 707, 'sum_payoffs': 147.87802881565054, 'action': [1.0, 0.0]}])
Weights num count: [0.16191172884662278, 0.16995854669592783, 0.171665447451841, 0.1641063155327969, 0.15971714216044866, 0.1723969763472324]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 113.98552107810974 s
