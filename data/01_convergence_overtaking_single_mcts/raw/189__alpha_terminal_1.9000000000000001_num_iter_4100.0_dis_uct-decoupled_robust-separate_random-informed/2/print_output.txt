Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 100.17455990852173, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 99.93572950507715, 'action': [1.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 101.52413655504824, 'action': [0.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 108.99150649961494, 'action': [2.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 103.45620236851495, 'action': [1.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 107.30167595812513, 'action': [2.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 97.26687637965843, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 104.76189382630209, 'action': [1.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 109.30987207565991, 'action': [2.0, 0.0]}])
Weights num count: [0.10851011948305292, 0.10826627651792246, 0.10948549134357474, 0.11509387954157523, 0.11094854913435748, 0.11387466471592295, 0.1063155327968788, 0.1119239209948793, 0.11533772250670568]
Actions to choose Agent 1: dict_values([{'num_count': 692, 'sum_payoffs': 143.73857443399137, 'action': [1.0, 1.5707963267948966]}, {'num_count': 685, 'sum_payoffs': 141.79205892790378, 'action': [1.0, -1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 149.2643849445701, 'action': [1.0, 0.0]}, {'num_count': 661, 'sum_payoffs': 135.0361014910971, 'action': [0.0, -1.5707963267948966]}, {'num_count': 674, 'sum_payoffs': 138.65330518982393, 'action': [0.0, 1.5707963267948966]}, {'num_count': 677, 'sum_payoffs': 139.53292661999228, 'action': [0.0, 0.0]}])
Weights num count: [0.16873933187027554, 0.16703243111436236, 0.1733723482077542, 0.1611801999512314, 0.16435015849792733, 0.16508168739331872]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 117.33641815185547 s
