Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 463, 'sum_payoffs': 105.78594883234882, 'action': [2.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 104.58002731169603, 'action': [1.0, 0.0]}, {'num_count': 478, 'sum_payoffs': 110.70335226922607, 'action': [2.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 100.0062963670803, 'action': [0.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 99.70604331143667, 'action': [1.0, 1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 106.75773152539455, 'action': [2.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 103.85437561292848, 'action': [1.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 97.77345672374514, 'action': [0.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 101.64338624015116, 'action': [0.0, 0.0]}])
Weights num count: [0.11289929285540112, 0.1119239209948793, 0.11655693733235796, 0.10851011948305292, 0.10826627651792246, 0.11363082175079249, 0.11143623506461839, 0.10680321872713973, 0.1097293343087052]
Actions to choose Agent 1: dict_values([{'num_count': 710, 'sum_payoffs': 148.84458556501968, 'action': [1.0, -1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 135.69689920007272, 'action': [0.0, 0.0]}, {'num_count': 671, 'sum_payoffs': 137.75254617098037, 'action': [0.0, -1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 145.11360443040238, 'action': [1.0, 1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 146.56061318042669, 'action': [1.0, 0.0]}, {'num_count': 656, 'sum_payoffs': 133.50314256549234, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17312850524262374, 0.16191172884662278, 0.16361862960253595, 0.16995854669592783, 0.1711777615215801, 0.15996098512557913]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 114.50495386123657 s
