Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 458, 'sum_payoffs': 104.57520317573463, 'action': [1.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 107.19937752663199, 'action': [2.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 98.43174124372648, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 102.28836615086594, 'action': [1.0, 1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 96.74765931762487, 'action': [0.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 111.0661295231032, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 100.71680629211221, 'action': [0.0, -1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 110.36604548826826, 'action': [2.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 102.51084352023683, 'action': [1.0, 0.0]}])
Weights num count: [0.11168007802974884, 0.11363082175079249, 0.10704706169227018, 0.10997317727383565, 0.10582784686661789, 0.11655693733235796, 0.10875396244818337, 0.11606925140209705, 0.11021702023896611]
Actions to choose Agent 1: dict_values([{'num_count': 697, 'sum_payoffs': 145.17891145872923, 'action': [1.0, -1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 135.8232655402092, 'action': [0.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 134.07636052406977, 'action': [0.0, 1.5707963267948966]}, {'num_count': 723, 'sum_payoffs': 152.62502570608245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 146.90430241455823, 'action': [1.0, 0.0]}, {'num_count': 655, 'sum_payoffs': 133.20479552031773, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16995854669592783, 0.16191172884662278, 0.16044867105584004, 0.17629846378931968, 0.17142160448671057, 0.15971714216044866]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 117.4304735660553 s
