Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 459, 'sum_payoffs': 104.56586038897824, 'action': [2.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 103.0140882701803, 'action': [1.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 99.76451182226315, 'action': [1.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 98.42884949107741, 'action': [0.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 108.82632766090467, 'action': [2.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 110.77470977373366, 'action': [2.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 100.7644882017123, 'action': [0.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 101.64626979250637, 'action': [0.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 103.59643818795645, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1119239209948793, 0.11070470616922702, 0.10826627651792246, 0.10729090465740064, 0.11509387954157523, 0.11655693733235796, 0.10899780541331383, 0.1097293343087052, 0.11119239209948793]
Actions to choose Agent 1: dict_values([{'num_count': 664, 'sum_payoffs': 135.98402400557367, 'action': [0.0, 1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 148.54025812742, 'action': [1.0, 1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 147.8804469692996, 'action': [1.0, 0.0]}, {'num_count': 693, 'sum_payoffs': 144.21331125649283, 'action': [1.0, -1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 132.53103280207657, 'action': [0.0, 0.0]}, {'num_count': 677, 'sum_payoffs': 139.7100273248859, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16191172884662278, 0.17264081931236283, 0.17215313338210192, 0.168983174835406, 0.1589856132650573, 0.16508168739331872]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 118.86383104324341 s
