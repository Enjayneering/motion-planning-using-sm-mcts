Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 441, 'sum_payoffs': 99.00895591612954, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 105.41206737248285, 'action': [2.0, 1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 108.67870362498255, 'action': [2.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 119.81941203749847, 'action': [2.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 99.02557808221431, 'action': [0.0, 0.0]}, {'num_count': 432, 'sum_payoffs': 96.1413124630898, 'action': [0.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 103.56280055010104, 'action': [1.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 100.33134850727288, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 101.5738569506681, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1075347476225311, 0.11241160692514021, 0.11485003657644477, 0.12314069739088028, 0.1075347476225311, 0.10534016093635698, 0.11094854913435748, 0.10851011948305292, 0.10948549134357474]
Actions to choose Agent 1: dict_values([{'num_count': 654, 'sum_payoffs': 132.87803602689007, 'action': [0.0, 1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 146.77167889797454, 'action': [1.0, -1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 149.10326726640824, 'action': [1.0, 0.0]}, {'num_count': 702, 'sum_payoffs': 146.41808110367248, 'action': [1.0, 1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 135.11329167866717, 'action': [0.0, -1.5707963267948966]}, {'num_count': 668, 'sum_payoffs': 136.79018460893292, 'action': [0.0, 0.0]}])
Weights num count: [0.15947329919531822, 0.17142160448671057, 0.1733723482077542, 0.1711777615215801, 0.16142404291636187, 0.1628871007071446]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 119.87063646316528 s
