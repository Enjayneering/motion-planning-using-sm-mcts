Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 453, 'sum_payoffs': 102.62712654007098, 'action': [1.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 101.68951558519777, 'action': [1.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 104.52576790300215, 'action': [1.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 99.1282374124902, 'action': [0.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 99.68113370958466, 'action': [0.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 107.8060212081071, 'action': [2.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 105.47812884790616, 'action': [2.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 98.41980158808414, 'action': [0.0, 1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 111.66833951616871, 'action': [2.0, 0.0]}])
Weights num count: [0.11046086320409657, 0.1097293343087052, 0.1119239209948793, 0.10777859058766155, 0.10826627651792246, 0.11436235064618386, 0.11265544989027067, 0.10729090465740064, 0.11728846622774933]
Actions to choose Agent 1: dict_values([{'num_count': 672, 'sum_payoffs': 138.38949723994716, 'action': [0.0, 0.0]}, {'num_count': 657, 'sum_payoffs': 134.07466821130458, 'action': [0.0, 1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 149.59871517308224, 'action': [1.0, 1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 149.58683004059654, 'action': [1.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 134.45014650982296, 'action': [0.0, -1.5707963267948966]}, {'num_count': 691, 'sum_payoffs': 143.79531851934797, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16386247256766642, 0.16020482809070957, 0.1733723482077542, 0.1733723482077542, 0.16044867105584004, 0.1684954889051451]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 125.97699499130249 s
