Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 437, 'sum_payoffs': 97.50974476871446, 'action': [0.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 100.30095822475614, 'action': [1.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 96.22923312037385, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 107.18498498069337, 'action': [2.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 106.45832983766606, 'action': [1.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 100.05373158824122, 'action': [0.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 113.33048469220387, 'action': [2.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 108.09437984676579, 'action': [2.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 101.92347582522056, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10655937576200927, 0.10875396244818337, 0.10558400390148744, 0.11387466471592295, 0.11338697878566203, 0.10851011948305292, 0.1185076810534016, 0.11460619361131431, 0.10997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 701, 'sum_payoffs': 146.42966858079703, 'action': [1.0, -1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 137.30260178087013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 145.5759390580966, 'action': [1.0, 1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 134.133663987207, 'action': [0.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 148.66426599107845, 'action': [1.0, 0.0]}, {'num_count': 665, 'sum_payoffs': 136.18861019874868, 'action': [0.0, 0.0]}])
Weights num count: [0.17093391855644965, 0.16313094367227504, 0.17020238966105827, 0.16044867105584004, 0.1728846622774933, 0.16215557181175322]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 129.6807005405426 s
