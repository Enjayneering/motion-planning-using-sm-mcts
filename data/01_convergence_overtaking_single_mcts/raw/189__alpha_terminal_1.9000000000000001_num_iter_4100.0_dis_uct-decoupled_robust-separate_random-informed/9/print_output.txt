Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 448, 'sum_payoffs': 101.10547053918253, 'action': [1.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 106.31888660025018, 'action': [1.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 98.16164920277144, 'action': [0.0, 1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 110.52538845153951, 'action': [2.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 106.32858393305783, 'action': [2.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 112.7824801459772, 'action': [2.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 99.87862804932766, 'action': [1.0, 1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 98.86389133390058, 'action': [0.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 98.18049726133768, 'action': [0.0, 0.0]}])
Weights num count: [0.10924164837844429, 0.11314313582053158, 0.10704706169227018, 0.1163130943672275, 0.11314313582053158, 0.1180199951231407, 0.10826627651792246, 0.1075347476225311, 0.10704706169227018]
Actions to choose Agent 1: dict_values([{'num_count': 682, 'sum_payoffs': 140.98558512286513, 'action': [1.0, -1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 145.53608934735345, 'action': [1.0, 1.5707963267948966]}, {'num_count': 667, 'sum_payoffs': 136.72907055707253, 'action': [0.0, 1.5707963267948966]}, {'num_count': 656, 'sum_payoffs': 133.57172582780407, 'action': [0.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 150.75659580194056, 'action': [1.0, 0.0]}, {'num_count': 681, 'sum_payoffs': 140.7446134119388, 'action': [0.0, 0.0]}])
Weights num count: [0.16630090221897098, 0.17020238966105827, 0.16264325774201413, 0.15996098512557913, 0.17459156303340648, 0.16605705925384054]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 120.15270161628723 s
