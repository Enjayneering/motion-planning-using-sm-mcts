Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 370, 'sum_payoffs': 82.8807335325006, 'action': [1.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 84.84315811699457, 'action': [0.0, 0.0]}, {'num_count': 435, 'sum_payoffs': 104.57119264713833, 'action': [2.0, -1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 66.91480345672235, 'action': [0.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 107.95920878346662, 'action': [1.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 134.71173831131563, 'action': [2.0, 0.0]}, {'num_count': 322, 'sum_payoffs': 67.29439626895581, 'action': [0.0, -1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 82.57653780432841, 'action': [1.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 105.97918430324943, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10274923632324354, 0.10441544015551235, 0.12079977783948903, 0.08914190502638156, 0.12357678422660372, 0.14523743404609832, 0.08941960566509304, 0.10247153568453207, 0.1219105803943349]
Actions to choose Agent 1: dict_values([{'num_count': 522, 'sum_payoffs': 137.78675877160998, 'action': [0.0, 1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 180.9826608133966, 'action': [1.0, 1.5707963267948966]}, {'num_count': 707, 'sum_payoffs': 204.27286353427894, 'action': [1.0, 0.0]}, {'num_count': 577, 'sum_payoffs': 157.28360454840035, 'action': [0.0, 0.0]}, {'num_count': 517, 'sum_payoffs': 136.0425221945191, 'action': [0.0, -1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 177.694522274526, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14495973340738683, 0.17856151069147458, 0.1963343515690086, 0.16023326853651762, 0.1435712302138295, 0.17606220494307137]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.019017219543457 s
