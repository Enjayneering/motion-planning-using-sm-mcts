Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 372, 'sum_payoffs': 83.66881775113671, 'action': [1.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 106.00243354819554, 'action': [2.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 107.03669902398084, 'action': [2.0, -1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 83.98181342717022, 'action': [0.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 138.57281501871546, 'action': [2.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 65.43445667384617, 'action': [0.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 82.92136539048514, 'action': [1.0, -1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 65.32429436370803, 'action': [0.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.6655005654774, 'action': [1.0, 0.0]}])
Weights num count: [0.10330463760066648, 0.1219105803943349, 0.12274368231046931, 0.10358233823937794, 0.14829214107192445, 0.08775340183282422, 0.10274923632324354, 0.08775340183282422, 0.12163287975562344]
Actions to choose Agent 1: dict_values([{'num_count': 510, 'sum_payoffs': 133.42372289896963, 'action': [0.0, 1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 180.88694780034294, 'action': [1.0, 1.5707963267948966]}, {'num_count': 724, 'sum_payoffs': 210.3094829047491, 'action': [1.0, 0.0]}, {'num_count': 510, 'sum_payoffs': 133.40047365402344, 'action': [0.0, -1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 155.78464388403535, 'action': [0.0, 0.0]}, {'num_count': 640, 'sum_payoffs': 179.81520106521177, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1416273257428492, 0.17856151069147458, 0.2010552624271036, 0.1416273257428492, 0.15912246598167176, 0.17772840877534019]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.186646461486816 s
