Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 318, 'sum_payoffs': 66.01394953602136, 'action': [0.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 85.24103888668952, 'action': [0.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 105.97940158593416, 'action': [1.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 84.55250634148405, 'action': [1.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 106.33552787053681, 'action': [2.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.65760596203884, 'action': [2.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 82.96764659769268, 'action': [1.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 65.96745104612914, 'action': [0.0, -1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 135.4052321440059, 'action': [2.0, 0.0]}])
Weights num count: [0.08830880311024715, 0.10469314079422383, 0.1219105803943349, 0.10413773951680089, 0.12218828103304638, 0.12163287975562344, 0.10274923632324354, 0.08830880311024715, 0.14579283532352125]
Actions to choose Agent 1: dict_values([{'num_count': 637, 'sum_payoffs': 178.5258457430734, 'action': [1.0, 1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 133.7335679763647, 'action': [0.0, -1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 206.57584247918328, 'action': [1.0, 0.0]}, {'num_count': 580, 'sum_payoffs': 158.19800242176265, 'action': [0.0, 0.0]}, {'num_count': 521, 'sum_payoffs': 137.22747319708478, 'action': [0.0, 1.5707963267948966]}, {'num_count': 637, 'sum_payoffs': 178.6186254401732, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17689530685920576, 0.1419050263815607, 0.1982782560399889, 0.16106637045265204, 0.14468203276867536, 0.17689530685920576]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.004518747329712 s
