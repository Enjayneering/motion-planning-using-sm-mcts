Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 372, 'sum_payoffs': 83.6399191569676, 'action': [1.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 105.35341023386673, 'action': [2.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 66.12976119538247, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 107.75749804467891, 'action': [1.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 135.59350757147442, 'action': [2.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 106.38767570965202, 'action': [2.0, 1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 66.02546551715207, 'action': [0.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 82.39988699933289, 'action': [1.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 85.68925680665353, 'action': [0.0, 0.0]}])
Weights num count: [0.10330463760066648, 0.12135517911691197, 0.08830880311024715, 0.12329908358789225, 0.14579283532352125, 0.12218828103304638, 0.08830880311024715, 0.10219383504582061, 0.1049708414329353]
Actions to choose Agent 1: dict_values([{'num_count': 731, 'sum_payoffs': 212.78447729119125, 'action': [1.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 134.7445842072039, 'action': [0.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 177.20770046810813, 'action': [1.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 174.97685936728433, 'action': [1.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 156.40860726457205, 'action': [0.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 136.88851224283582, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.20299916689808387, 0.14273812829769508, 0.1757845043043599, 0.1741183004720911, 0.1596778672590947, 0.1444043321299639]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.985116720199585 s
