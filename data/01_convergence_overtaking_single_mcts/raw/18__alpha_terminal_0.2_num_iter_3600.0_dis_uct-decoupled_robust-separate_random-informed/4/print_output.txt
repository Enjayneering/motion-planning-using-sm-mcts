Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 321, 'sum_payoffs': 66.93805270166847, 'action': [0.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 106.29489601255226, 'action': [2.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 81.55965494158157, 'action': [1.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 65.0200986355359, 'action': [0.0, 1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 105.22586531069017, 'action': [2.0, -1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 82.86335091946222, 'action': [1.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 84.241538636981, 'action': [0.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 137.478724383061, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 108.28462578505413, 'action': [1.0, 0.0]}])
Weights num count: [0.08914190502638156, 0.12218828103304638, 0.10163843376839767, 0.08747570119411274, 0.12135517911691197, 0.10274923632324354, 0.10386003887808942, 0.14745903915579006, 0.1238544848653152]
Actions to choose Agent 1: dict_values([{'num_count': 636, 'sum_payoffs': 178.32616297581626, 'action': [1.0, 1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 134.15944199580576, 'action': [0.0, -1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 158.19032510100885, 'action': [0.0, 0.0]}, {'num_count': 722, 'sum_payoffs': 209.61215041168202, 'action': [1.0, 0.0]}, {'num_count': 643, 'sum_payoffs': 180.89281443225056, 'action': [1.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 132.42400536657644, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1766176062204943, 0.14218272702027215, 0.16106637045265204, 0.20049986114968063, 0.17856151069147458, 0.1407942238267148]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.956140995025635 s
