Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 440, 'sum_payoffs': 106.24861480534474, 'action': [2.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.64246860321603, 'action': [1.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 83.56452207290624, 'action': [1.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.64022334900048, 'action': [2.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 136.36591122457574, 'action': [2.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 66.59322511551183, 'action': [0.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 67.24811506174832, 'action': [0.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 82.19129564287198, 'action': [1.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 84.22799468431953, 'action': [0.0, 0.0]}])
Weights num count: [0.12218828103304638, 0.12163287975562344, 0.10330463760066648, 0.12163287975562344, 0.14662593723965564, 0.08886420438767009, 0.08941960566509304, 0.10219383504582061, 0.10386003887808942]
Actions to choose Agent 1: dict_values([{'num_count': 632, 'sum_payoffs': 176.86873951385923, 'action': [1.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 134.46363772397788, 'action': [0.0, 1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 209.24226289173282, 'action': [1.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 136.62788168860203, 'action': [0.0, -1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 158.18061980872426, 'action': [0.0, 0.0]}, {'num_count': 635, 'sum_payoffs': 177.96373549393644, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17550680366564844, 0.14246042765898362, 0.20022216051096917, 0.14412663149125243, 0.16106637045265204, 0.17633990558178284]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.006100177764893 s
