Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 434, 'sum_payoffs': 104.33066073921208, 'action': [2.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 104.9738174216332, 'action': [2.0, 1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 137.10974220753522, 'action': [2.0, 0.0]}, {'num_count': 322, 'sum_payoffs': 67.22486581680222, 'action': [0.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 85.58383850131537, 'action': [0.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 82.22019423704111, 'action': [1.0, 1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 83.24294373169575, 'action': [1.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 66.00808290411364, 'action': [0.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 107.28422019092262, 'action': [1.0, 0.0]}])
Weights num count: [0.12052207720077757, 0.1210774784782005, 0.1471813385170786, 0.08941960566509304, 0.1049708414329353, 0.10219383504582061, 0.10302693696195502, 0.08830880311024715, 0.12302138294918079]
Actions to choose Agent 1: dict_values([{'num_count': 631, 'sum_payoffs': 176.70947132190196, 'action': [1.0, 1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 136.67709621176334, 'action': [0.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 180.27584031072948, 'action': [1.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 133.22360556634317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 159.8089360851116, 'action': [0.0, 0.0]}, {'num_count': 716, 'sum_payoffs': 207.5205874978498, 'action': [1.0, 0.0]}])
Weights num count: [0.17522910302693695, 0.14412663149125243, 0.17800610941405165, 0.14134962510413773, 0.16217717300749793, 0.19883365731741184]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.078999996185303 s
