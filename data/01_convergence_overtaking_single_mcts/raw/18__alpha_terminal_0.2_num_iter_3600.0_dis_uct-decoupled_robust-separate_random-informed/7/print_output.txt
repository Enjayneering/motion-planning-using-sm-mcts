Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 107.93979819889748, 'action': [1.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 66.96695129583757, 'action': [0.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 82.24344348198714, 'action': [1.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.59394214179294, 'action': [2.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 104.56532601523064, 'action': [2.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 84.61142615692987, 'action': [0.0, 0.0]}, {'num_count': 524, 'sum_payoffs': 135.09223646797213, 'action': [2.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 83.93824825323198, 'action': [1.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 66.97281792774525, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12357678422660372, 0.08914190502638156, 0.10219383504582061, 0.12163287975562344, 0.12079977783948903, 0.10413773951680089, 0.14551513468480978, 0.10358233823937794, 0.08914190502638156]
Actions to choose Agent 1: dict_values([{'num_count': 710, 'sum_payoffs': 205.19696669992624, 'action': [1.0, 0.0]}, {'num_count': 632, 'sum_payoffs': 176.85135690082083, 'action': [1.0, -1.5707963267948966]}, {'num_count': 568, 'sum_payoffs': 153.88508641670668, 'action': [0.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 138.745627163334, 'action': [0.0, -1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 180.86663187135073, 'action': [1.0, 1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 137.68246309337954, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.19716745348514303, 0.17550680366564844, 0.1577339627881144, 0.14579283532352125, 0.17856151069147458, 0.14495973340738683]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.113915205001831 s
