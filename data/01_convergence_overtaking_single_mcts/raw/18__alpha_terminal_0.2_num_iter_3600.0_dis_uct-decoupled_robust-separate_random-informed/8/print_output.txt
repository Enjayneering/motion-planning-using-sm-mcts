Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 316, 'sum_payoffs': 65.28952913763119, 'action': [0.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 84.18352416595806, 'action': [0.0, 0.0]}, {'num_count': 527, 'sum_payoffs': 136.07638207617308, 'action': [2.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 83.59928729898313, 'action': [1.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 106.89763811967359, 'action': [2.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 82.57653780432841, 'action': [1.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 106.60698634416296, 'action': [1.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 66.33552787723195, 'action': [0.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 106.29489601255227, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.08775340183282422, 0.10386003887808942, 0.14634823660094418, 0.10330463760066648, 0.12274368231046931, 0.10247153568453207, 0.12246598167175785, 0.08858650374895863, 0.12218828103304638]
Actions to choose Agent 1: dict_values([{'num_count': 642, 'sum_payoffs': 180.44097513459442, 'action': [1.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 134.3767246587859, 'action': [0.0, -1.5707963267948966]}, {'num_count': 713, 'sum_payoffs': 206.1653231012502, 'action': [1.0, 0.0]}, {'num_count': 582, 'sum_payoffs': 158.88744031139132, 'action': [0.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 135.12439430212203, 'action': [0.0, 1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 177.90300498964447, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17828381005276311, 0.14246042765898362, 0.19800055540127742, 0.16162177173007497, 0.14301582893640655, 0.17633990558178284]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.230027198791504 s
