Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 373, 'sum_payoffs': 83.85720181994772, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 105.88640460614967, 'action': [2.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 67.26549767478673, 'action': [0.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 83.23707709978801, 'action': [1.0, 1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 106.66996210267058, 'action': [1.0, 0.0]}, {'num_count': 318, 'sum_payoffs': 65.96745104612909, 'action': [0.0, 1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 83.2352664109417, 'action': [0.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 104.8869043564412, 'action': [2.0, 1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 136.7098334663095, 'action': [2.0, 0.0]}])
Weights num count: [0.10358233823937794, 0.1219105803943349, 0.08941960566509304, 0.10302693696195502, 0.12246598167175785, 0.08830880311024715, 0.10302693696195502, 0.1210774784782005, 0.14690363787836713]
Actions to choose Agent 1: dict_values([{'num_count': 644, 'sum_payoffs': 181.28685654156857, 'action': [1.0, -1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 176.9673858428666, 'action': [1.0, 1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 207.85752048056807, 'action': [1.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 135.6543467171088, 'action': [0.0, -1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 159.05945575292927, 'action': [0.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 133.21208958521248, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17883921133018607, 0.17550680366564844, 0.1991113579561233, 0.143293529575118, 0.16162177173007497, 0.14134962510413773]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.78252100944519 s
