Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 534, 'sum_payoffs': 122.26613419057222, 'action': [2.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 111.49607528066466, 'action': [0.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 108.99533559421505, 'action': [0.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 115.5336479529354, 'action': [1.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 109.03215763210257, 'action': [0.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 116.21517925476621, 'action': [1.0, -1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 122.29451418510111, 'action': [2.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 116.89431169437556, 'action': [2.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 112.49945137576034, 'action': [1.0, 0.0]}])
Weights num count: [0.11606172571180179, 0.10867202782003912, 0.1069332753749185, 0.11149750054336013, 0.1069332753749185, 0.1119321886546403, 0.11606172571180179, 0.11236687676592046, 0.10932405998695936]
Actions to choose Agent 1: dict_values([{'num_count': 730, 'sum_payoffs': 147.14468253799816, 'action': [0.0, 0.0]}, {'num_count': 782, 'sum_payoffs': 161.69121548394523, 'action': [1.0, -1.5707963267948966]}, {'num_count': 792, 'sum_payoffs': 164.49381760962655, 'action': [1.0, 1.5707963267948966]}, {'num_count': 757, 'sum_payoffs': 154.6061687697644, 'action': [0.0, -1.5707963267948966]}, {'num_count': 794, 'sum_payoffs': 165.0267708815833, 'action': [1.0, 0.0]}, {'num_count': 745, 'sum_payoffs': 151.33650444964624, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15866116061725713, 0.1699630515105412, 0.17213649206694198, 0.16452945011953923, 0.17257118017822212, 0.16192132145185828]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 129.4318664073944 s
