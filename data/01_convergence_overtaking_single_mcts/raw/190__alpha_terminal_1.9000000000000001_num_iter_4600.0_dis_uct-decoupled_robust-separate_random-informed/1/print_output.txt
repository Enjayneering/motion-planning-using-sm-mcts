Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 543, 'sum_payoffs': 125.64744892400766, 'action': [2.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 113.62850024234064, 'action': [1.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 112.92441959781408, 'action': [1.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 123.47012419624562, 'action': [2.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 111.75361312506158, 'action': [0.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 121.61859833968032, 'action': [2.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 106.97104665964356, 'action': [0.0, 1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 114.55177469487987, 'action': [1.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 109.52714834259072, 'action': [0.0, 0.0]}])
Weights num count: [0.11801782221256249, 0.10975874809823952, 0.10932405998695936, 0.11649641382308194, 0.10845468376439904, 0.11519234948924147, 0.10519452292979788, 0.11041078026515974, 0.1069332753749185]
Actions to choose Agent 1: dict_values([{'num_count': 788, 'sum_payoffs': 163.3323723484455, 'action': [1.0, -1.5707963267948966]}, {'num_count': 782, 'sum_payoffs': 161.70975663662213, 'action': [1.0, 1.5707963267948966]}, {'num_count': 804, 'sum_payoffs': 167.84159796542193, 'action': [1.0, 0.0]}, {'num_count': 740, 'sum_payoffs': 149.9492046565502, 'action': [0.0, -1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 153.00389617480684, 'action': [0.0, 0.0]}, {'num_count': 735, 'sum_payoffs': 148.5484758030926, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17126711584438165, 0.1699630515105412, 0.1747446207346229, 0.1608346011736579, 0.16322538578569876, 0.15974788089545752]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 123.94182634353638 s
