Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 511, 'sum_payoffs': 115.35140015328953, 'action': [2.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 116.82483092212603, 'action': [2.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 116.32265101954177, 'action': [1.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 110.00446313043976, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 111.54486028574597, 'action': [0.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 116.93169945267285, 'action': [1.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 110.93125271012578, 'action': [0.0, 1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 125.4339810230735, 'action': [2.0, 0.0]}, {'num_count': 510, 'sum_payoffs': 114.98997412061765, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11106281243207998, 0.11214953271028037, 0.11171484459900022, 0.10736796348619865, 0.10845468376439904, 0.11214953271028037, 0.10801999565311889, 0.11801782221256249, 0.11084546837643991]
Actions to choose Agent 1: dict_values([{'num_count': 747, 'sum_payoffs': 152.15817560439862, 'action': [0.0, -1.5707963267948966]}, {'num_count': 753, 'sum_payoffs': 153.8804090261641, 'action': [0.0, 0.0]}, {'num_count': 734, 'sum_payoffs': 148.54531361012272, 'action': [0.0, 1.5707963267948966]}, {'num_count': 810, 'sum_payoffs': 169.88794035652938, 'action': [1.0, 0.0]}, {'num_count': 762, 'sum_payoffs': 156.38354491763926, 'action': [1.0, 1.5707963267948966]}, {'num_count': 794, 'sum_payoffs': 165.30122645889148, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16235600956313845, 0.16366007389697892, 0.15953053683981744, 0.17604868506846338, 0.16561617039773963, 0.17257118017822212]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 129.93160605430603 s
