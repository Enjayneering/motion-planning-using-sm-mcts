Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 522, 'sum_payoffs': 118.60179790665536, 'action': [2.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 111.34113480161234, 'action': [0.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 115.70010529857453, 'action': [2.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 116.96898546754971, 'action': [1.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 110.67544977106841, 'action': [0.0, 0.0]}, {'num_count': 541, 'sum_payoffs': 124.66643053551219, 'action': [2.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 117.9320205228512, 'action': [1.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 105.38607802957186, 'action': [0.0, 1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 115.05011739410645, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11345359704412085, 0.10845468376439904, 0.11149750054336013, 0.11236687676592046, 0.10801999565311889, 0.11758313410128234, 0.11301890893284068, 0.10432514670723755, 0.11106281243207998]
Actions to choose Agent 1: dict_values([{'num_count': 797, 'sum_payoffs': 166.16549682147792, 'action': [1.0, 0.0]}, {'num_count': 749, 'sum_payoffs': 152.77015526990087, 'action': [0.0, 0.0]}, {'num_count': 786, 'sum_payoffs': 163.02970379716487, 'action': [1.0, -1.5707963267948966]}, {'num_count': 798, 'sum_payoffs': 166.44101878940978, 'action': [1.0, 1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 149.3406717871197, 'action': [0.0, 1.5707963267948966]}, {'num_count': 733, 'sum_payoffs': 148.2483330449234, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17322321234514235, 0.16279069767441862, 0.1708324277331015, 0.17344055640078243, 0.16018256900673766, 0.15931319278417735]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 135.32428336143494 s
