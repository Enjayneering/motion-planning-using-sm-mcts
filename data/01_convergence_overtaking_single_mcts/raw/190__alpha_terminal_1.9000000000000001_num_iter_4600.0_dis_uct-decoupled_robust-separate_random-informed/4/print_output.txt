Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 506, 'sum_payoffs': 113.11122749355006, 'action': [1.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 108.4608620815781, 'action': [0.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 109.3676173402589, 'action': [0.0, 0.0]}, {'num_count': 521, 'sum_payoffs': 117.87319366066788, 'action': [2.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 115.05877226941068, 'action': [2.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 112.48138044176105, 'action': [1.0, 0.0]}, {'num_count': 543, 'sum_payoffs': 124.81663364284518, 'action': [2.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 111.52734314081783, 'action': [0.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 120.06815331671017, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10997609215387959, 0.10671593131927842, 0.10736796348619865, 0.11323625298848077, 0.11128015648772006, 0.10954140404259943, 0.11801782221256249, 0.1088893718756792, 0.11475766137796131]
Actions to choose Agent 1: dict_values([{'num_count': 754, 'sum_payoffs': 154.25496939414182, 'action': [0.0, -1.5707963267948966]}, {'num_count': 781, 'sum_payoffs': 161.8279168729819, 'action': [1.0, 1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 144.80821009774397, 'action': [0.0, 1.5707963267948966]}, {'num_count': 741, 'sum_payoffs': 150.65557649275874, 'action': [0.0, 0.0]}, {'num_count': 815, 'sum_payoffs': 171.43962005676153, 'action': [1.0, 0.0]}, {'num_count': 789, 'sum_payoffs': 164.07330254130798, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.163877417952619, 0.1697457074549011, 0.15648772006085634, 0.16105194522929797, 0.17713540534666378, 0.17148445990002173]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 134.00511765480042 s
