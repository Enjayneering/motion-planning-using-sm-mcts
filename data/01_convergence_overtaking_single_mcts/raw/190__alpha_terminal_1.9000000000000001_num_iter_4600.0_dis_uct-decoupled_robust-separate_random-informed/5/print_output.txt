Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 552, 'sum_payoffs': 128.25042678156922, 'action': [2.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 109.85116370690062, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 111.16138075440904, 'action': [0.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 121.5921060215631, 'action': [2.0, -1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 119.7001330917195, 'action': [2.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 118.37471519645194, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 108.66844742776546, 'action': [0.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 112.10380424443777, 'action': [1.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 108.00170563335396, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11997391871332319, 0.10736796348619865, 0.10823733970875897, 0.11540969354488155, 0.11410562921104107, 0.11323625298848077, 0.10649858726363834, 0.1088893718756792, 0.10606389915235818]
Actions to choose Agent 1: dict_values([{'num_count': 809, 'sum_payoffs': 169.55075874141434, 'action': [1.0, 0.0]}, {'num_count': 785, 'sum_payoffs': 162.72824264622906, 'action': [1.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 149.0278164524924, 'action': [0.0, -1.5707963267948966]}, {'num_count': 784, 'sum_payoffs': 162.4618628702155, 'action': [1.0, -1.5707963267948966]}, {'num_count': 742, 'sum_payoffs': 150.6920605373808, 'action': [0.0, 0.0]}, {'num_count': 744, 'sum_payoffs': 151.3199616155324, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1758313410128233, 0.17061508367746142, 0.15996522495109758, 0.17039773962182134, 0.16126928928493806, 0.16170397739621822]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 132.79453420639038 s
