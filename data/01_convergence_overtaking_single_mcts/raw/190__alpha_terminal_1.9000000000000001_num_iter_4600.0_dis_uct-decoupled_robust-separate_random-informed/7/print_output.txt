Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 524, 'sum_payoffs': 119.51960183389137, 'action': [2.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 110.03771848694483, 'action': [0.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 108.8180613789906, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 114.48607971085414, 'action': [2.0, 1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 119.80350261525871, 'action': [1.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 127.5414104305693, 'action': [2.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 109.12699132842931, 'action': [1.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 111.62147522273662, 'action': [0.0, -1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 118.19007336141165, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.113888285155401, 0.10736796348619865, 0.10649858726363834, 0.11041078026515974, 0.11410562921104107, 0.11932188654640295, 0.10671593131927842, 0.10845468376439904, 0.11301890893284068]
Actions to choose Agent 1: dict_values([{'num_count': 801, 'sum_payoffs': 166.9888620238971, 'action': [1.0, 1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 149.12881548131517, 'action': [0.0, 0.0]}, {'num_count': 728, 'sum_payoffs': 146.59424868063627, 'action': [0.0, -1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 169.2557239299141, 'action': [1.0, 0.0]}, {'num_count': 780, 'sum_payoffs': 161.15813585382537, 'action': [1.0, -1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 151.40516691602025, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17409258856770268, 0.16018256900673766, 0.15822647250597696, 0.1758313410128233, 0.16952836339926103, 0.16192132145185828]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 135.0683102607727 s
