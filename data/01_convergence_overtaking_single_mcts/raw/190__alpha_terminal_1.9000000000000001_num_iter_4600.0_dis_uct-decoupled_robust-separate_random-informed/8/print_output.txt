Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 525, 'sum_payoffs': 119.14557017675745, 'action': [2.0, -1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 112.55471406612742, 'action': [1.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 122.62794927576851, 'action': [2.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 113.13779340836813, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 111.88149499530286, 'action': [0.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 117.8834940760043, 'action': [1.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 108.9650141450297, 'action': [0.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 110.86092744773923, 'action': [0.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 115.70521571854736, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11410562921104107, 0.10954140404259943, 0.11649641382308194, 0.10997609215387959, 0.10910671593131928, 0.11323625298848077, 0.10715061943055858, 0.10845468376439904, 0.11171484459900022]
Actions to choose Agent 1: dict_values([{'num_count': 731, 'sum_payoffs': 147.50629233454393, 'action': [0.0, 1.5707963267948966]}, {'num_count': 773, 'sum_payoffs': 159.21070664234574, 'action': [1.0, 1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 152.25939262685228, 'action': [0.0, -1.5707963267948966]}, {'num_count': 789, 'sum_payoffs': 163.68673098709013, 'action': [1.0, -1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 153.10499408996148, 'action': [0.0, 0.0]}, {'num_count': 808, 'sum_payoffs': 169.02917889476393, 'action': [1.0, 0.0]}])
Weights num count: [0.1588785046728972, 0.1680069550097805, 0.16257335361877853, 0.17148445990002173, 0.16322538578569876, 0.17561399695718322]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 140.61246156692505 s
