Searching game tree in timestep 0...
Max timehorizon: 13
Actions to choose Agent 0: dict_values([{'num_count': 496, 'sum_payoffs': 110.48884810983408, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 111.83064876913053, 'action': [0.0, -1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 119.72988084278843, 'action': [2.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 113.63274321914845, 'action': [1.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 116.18097441505094, 'action': [1.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 112.7164058644017, 'action': [1.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 109.5385127632874, 'action': [0.0, 0.0]}, {'num_count': 524, 'sum_payoffs': 119.24773982544978, 'action': [2.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 124.19186358258294, 'action': [2.0, 0.0]}])
Weights num count: [0.10780265159747882, 0.10867202782003912, 0.11410562921104107, 0.10997609215387959, 0.11171484459900022, 0.10932405998695936, 0.10715061943055858, 0.113888285155401, 0.11714844599000217]
Actions to choose Agent 1: dict_values([{'num_count': 732, 'sum_payoffs': 148.29322906359357, 'action': [0.0, -1.5707963267948966]}, {'num_count': 749, 'sum_payoffs': 152.98343657919946, 'action': [0.0, 0.0]}, {'num_count': 812, 'sum_payoffs': 170.71803235953027, 'action': [1.0, 0.0]}, {'num_count': 787, 'sum_payoffs': 163.67120723229021, 'action': [1.0, -1.5707963267948966]}, {'num_count': 732, 'sum_payoffs': 148.21199887996346, 'action': [0.0, 1.5707963267948966]}, {'num_count': 788, 'sum_payoffs': 163.91215241896577, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15909584872853727, 0.16279069767441862, 0.17648337317974352, 0.1710497717887416, 0.15909584872853727, 0.17126711584438165]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 134.26688981056213 s
