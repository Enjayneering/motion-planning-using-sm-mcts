Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 125, 'sum_payoffs': 29.03106328492173, 'action': [1.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 27.901392661881573, 'action': [0.0, -1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 30.234608486680575, 'action': [2.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 26.71094822535154, 'action': [0.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 27.962458246010666, 'action': [2.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 27.096313613228006, 'action': [1.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 27.878073359801657, 'action': [2.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 27.903809013553744, 'action': [1.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 27.138068008837642, 'action': [0.0, 0.0]}])
Weights num count: [0.11353315168029064, 0.11080835603996367, 0.11625794732061762, 0.1080835603996367, 0.11080835603996367, 0.10899182561307902, 0.11080835603996367, 0.11080835603996367, 0.10899182561307902]
Actions to choose Agent 1: dict_values([{'num_count': 176, 'sum_payoffs': 35.04263180543481, 'action': [0.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 36.11068132537374, 'action': [0.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 37.73910110709746, 'action': [1.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 39.08635223605222, 'action': [1.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 39.54278727170329, 'action': [1.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 37.76412029010988, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15985467756584923, 0.16257947320617622, 0.16712079927338783, 0.17075386012715713, 0.17166212534059946, 0.16712079927338783]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.249491930007935 s
