Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 174, 'sum_payoffs': 39.153591327767124, 'action': [2.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 41.680428170750886, 'action': [1.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 39.15284797876627, 'action': [1.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.587296970767675, 'action': [0.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 42.06996987168644, 'action': [2.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 38.053583799096245, 'action': [0.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.58072622024891, 'action': [1.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 43.66484837218426, 'action': [2.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 39.794378649874766, 'action': [0.0, 0.0]}])
Weights num count: [0.10868207370393504, 0.11305434103685197, 0.10868207370393504, 0.11118051217988757, 0.1136789506558401, 0.10680824484697064, 0.11118051217988757, 0.11617738913179262, 0.1099312929419113]
Actions to choose Agent 1: dict_values([{'num_count': 276, 'sum_payoffs': 57.80761479154907, 'action': [1.0, 1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 55.87847901077665, 'action': [1.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 57.60249001270162, 'action': [1.0, -1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 51.69450150528001, 'action': [0.0, 0.0]}, {'num_count': 265, 'sum_payoffs': 54.232074415506474, 'action': [0.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 51.71129846694562, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17239225484072454, 0.16864459712679575, 0.1717676452217364, 0.16052467207995003, 0.1655215490318551, 0.16052467207995003]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 52.63623070716858 s
