Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 185, 'sum_payoffs': 43.374170935502924, 'action': [2.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 42.28799990562069, 'action': [1.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 37.809641452490915, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 39.999950498639656, 'action': [2.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 42.213838806955636, 'action': [2.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 42.54936356048101, 'action': [1.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 40.44910264165381, 'action': [0.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 38.56347398187414, 'action': [0.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 38.98204600458961, 'action': [0.0, 0.0]}])
Weights num count: [0.1155527795128045, 0.1136789506558401, 0.1061836352279825, 0.1099312929419113, 0.1136789506558401, 0.11430356027482823, 0.11055590256089944, 0.10743285446595878, 0.1080574640849469]
Actions to choose Agent 1: dict_values([{'num_count': 263, 'sum_payoffs': 53.40789585139624, 'action': [0.0, -1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 57.370639718090686, 'action': [1.0, 1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 53.87769685419272, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 56.38593706540303, 'action': [1.0, 0.0]}, {'num_count': 259, 'sum_payoffs': 52.244850760871955, 'action': [0.0, 1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 54.832887006421814, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16427232979387882, 0.1717676452217364, 0.16489693941286696, 0.16989381636477202, 0.1617738913179263, 0.16677076826983137]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.68390703201294 s
