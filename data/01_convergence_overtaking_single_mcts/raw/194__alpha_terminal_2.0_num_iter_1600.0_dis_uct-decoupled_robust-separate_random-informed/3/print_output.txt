Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 188, 'sum_payoffs': 44.300412328847514, 'action': [2.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 39.77616565611824, 'action': [1.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.656693849390784, 'action': [1.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 38.71315051908423, 'action': [0.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.5849648143267, 'action': [2.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 40.27068974544345, 'action': [0.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 40.20212249422798, 'action': [2.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 37.261617549294726, 'action': [0.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 42.885328089412965, 'action': [1.0, 0.0]}])
Weights num count: [0.1174266083697689, 0.1099312929419113, 0.11118051217988757, 0.1080574640849469, 0.11118051217988757, 0.11055590256089944, 0.11055590256089944, 0.10555902560899438, 0.11492816989381636]
Actions to choose Agent 1: dict_values([{'num_count': 261, 'sum_payoffs': 53.1466000339948, 'action': [0.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 56.74489191776833, 'action': [1.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 54.90519142581473, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 56.82128628059533, 'action': [1.0, 0.0]}, {'num_count': 259, 'sum_payoffs': 52.60481324262404, 'action': [0.0, 1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 56.11728324363226, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16302311055590257, 0.16989381636477202, 0.16614615865084323, 0.16989381636477202, 0.1617738913179263, 0.16864459712679575]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.620306968688965 s
