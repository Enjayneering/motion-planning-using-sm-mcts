Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 39.90111389400957, 'action': [2.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 42.09125555297184, 'action': [2.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 39.837013158868096, 'action': [0.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 43.51979900954041, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 39.75003020442782, 'action': [1.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 43.165846415487145, 'action': [2.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 36.92267382376014, 'action': [0.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 39.78187949444859, 'action': [1.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 39.52670255753842, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1099312929419113, 0.1136789506558401, 0.1099312929419113, 0.11617738913179262, 0.1099312929419113, 0.1155527795128045, 0.10493441599000625, 0.1099312929419113, 0.10930668332292318]
Actions to choose Agent 1: dict_values([{'num_count': 261, 'sum_payoffs': 52.911794817943615, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 58.160554285379675, 'action': [1.0, 1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 50.39895648781891, 'action': [0.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 57.7884180795099, 'action': [1.0, 0.0]}, {'num_count': 260, 'sum_payoffs': 52.54949990117201, 'action': [0.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 56.78434952741576, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16302311055590257, 0.17301686445971268, 0.1580262336039975, 0.17239225484072454, 0.16239850093691444, 0.17051842598376016]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 54.000314235687256 s
