Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 175, 'sum_payoffs': 39.521151062940035, 'action': [0.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 38.88978773935414, 'action': [2.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 41.761937499145205, 'action': [1.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 39.25331549612191, 'action': [0.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 42.9023643576231, 'action': [2.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 39.520403211169246, 'action': [1.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 42.24092693793218, 'action': [2.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 42.87498037028686, 'action': [1.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 38.399079811637634, 'action': [0.0, 0.0]}])
Weights num count: [0.10930668332292318, 0.1080574640849469, 0.11305434103685197, 0.10868207370393504, 0.11492816989381636, 0.10930668332292318, 0.1136789506558401, 0.11492816989381636, 0.10743285446595878]
Actions to choose Agent 1: dict_values([{'num_count': 262, 'sum_payoffs': 53.44373290127815, 'action': [0.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 57.00959916084394, 'action': [1.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 55.96776534931152, 'action': [1.0, 0.0]}, {'num_count': 265, 'sum_payoffs': 54.36401810918021, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 56.614224765293066, 'action': [1.0, 1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 52.14862511929873, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16364772017489068, 0.17051842598376016, 0.16864459712679575, 0.1655215490318551, 0.16989381636477202, 0.16114928169893816]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.159472942352295 s
