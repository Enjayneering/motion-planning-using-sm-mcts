Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 173, 'sum_payoffs': 38.546723321869806, 'action': [0.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 39.32714603227272, 'action': [1.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 40.188026432703026, 'action': [0.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 43.178869096765204, 'action': [1.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 41.961561595266794, 'action': [2.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 38.67438953425824, 'action': [1.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 41.60776585516265, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 37.88453631666399, 'action': [0.0, 1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 42.350333917867296, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1080574640849469, 0.10930668332292318, 0.11055590256089944, 0.1155527795128045, 0.1136789506558401, 0.1080574640849469, 0.11305434103685197, 0.10680824484697064, 0.11430356027482823]
Actions to choose Agent 1: dict_values([{'num_count': 275, 'sum_payoffs': 57.65719237233377, 'action': [1.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 52.35297612885835, 'action': [0.0, 1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 53.08689327525732, 'action': [0.0, 0.0]}, {'num_count': 264, 'sum_payoffs': 54.00578368904906, 'action': [0.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 56.00492452595753, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 56.27964168395197, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1717676452217364, 0.1617738913179263, 0.16302311055590257, 0.16489693941286696, 0.16864459712679575, 0.16926920674578388]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 57.635589838027954 s
