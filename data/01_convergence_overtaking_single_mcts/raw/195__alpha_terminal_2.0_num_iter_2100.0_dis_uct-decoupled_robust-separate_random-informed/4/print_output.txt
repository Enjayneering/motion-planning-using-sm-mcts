Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 231, 'sum_payoffs': 52.286242443666254, 'action': [0.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 49.81311819246906, 'action': [0.0, 1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 52.8903283792099, 'action': [1.0, 0.0]}, {'num_count': 226, 'sum_payoffs': 50.479779800712954, 'action': [0.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 54.41503100205419, 'action': [1.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 56.55782008556403, 'action': [2.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 54.35995576831689, 'action': [2.0, 1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 54.38393147223638, 'action': [2.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 52.55096809936629, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1099476439790576, 0.10661589719181343, 0.11089957163255593, 0.10756782484531176, 0.1128034269395526, 0.11565920990004759, 0.1128034269395526, 0.1128034269395526, 0.11042360780580676]
Actions to choose Agent 1: dict_values([{'num_count': 346, 'sum_payoffs': 70.83526203696425, 'action': [0.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 77.00990531991035, 'action': [1.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 75.19769595846573, 'action': [1.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 68.00216135465601, 'action': [0.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 68.96214891345001, 'action': [0.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 72.3833519874226, 'action': [1.0, 0.0]}])
Weights num count: [0.1646834840552118, 0.17420276059019515, 0.17134697762970014, 0.1603998096144693, 0.1618277010947168, 0.16706330318895765]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 68.78478121757507 s
