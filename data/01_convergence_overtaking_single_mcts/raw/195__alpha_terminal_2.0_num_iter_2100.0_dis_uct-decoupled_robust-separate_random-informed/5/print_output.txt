Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 232, 'sum_payoffs': 52.51268982443592, 'action': [0.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 55.360353982280365, 'action': [1.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 50.84191410342704, 'action': [0.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 53.63097944360458, 'action': [2.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 56.10067726945747, 'action': [2.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 52.60081982408683, 'action': [1.0, 1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 52.5680620417869, 'action': [1.0, 0.0]}, {'num_count': 225, 'sum_payoffs': 50.14907023678361, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 53.555422938483034, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11042360780580676, 0.1142313184198001, 0.10804378867206092, 0.11185149928605426, 0.11518324607329843, 0.11042360780580676, 0.11042360780580676, 0.10709186101856259, 0.11185149928605426]
Actions to choose Agent 1: dict_values([{'num_count': 345, 'sum_payoffs': 70.27837401073981, 'action': [0.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 74.10869211152063, 'action': [1.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 68.53764820355069, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 75.02656755698838, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 67.88862902188264, 'action': [0.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 75.59037645334313, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16420752022846263, 0.16991908614945264, 0.16135173726796764, 0.17134697762970014, 0.1603998096144693, 0.17229890528319847]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 70.03905892372131 s
