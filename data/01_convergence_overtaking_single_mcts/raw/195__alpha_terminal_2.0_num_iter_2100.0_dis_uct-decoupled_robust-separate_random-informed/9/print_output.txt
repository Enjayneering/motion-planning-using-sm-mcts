Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 245, 'sum_payoffs': 57.08703786234581, 'action': [2.0, 0.0]}, {'num_count': 232, 'sum_payoffs': 52.517268170954196, 'action': [2.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 50.75645065016001, 'action': [1.0, 0.0]}, {'num_count': 225, 'sum_payoffs': 50.00444743303351, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 53.6755673475375, 'action': [2.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 54.71340946541251, 'action': [1.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 50.77738812115969, 'action': [0.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 54.95616593514107, 'action': [1.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 52.554486487824036, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11661113755354593, 0.11042360780580676, 0.10804378867206092, 0.10709186101856259, 0.11185149928605426, 0.11327939076630177, 0.10804378867206092, 0.11375535459305093, 0.11042360780580676]
Actions to choose Agent 1: dict_values([{'num_count': 360, 'sum_payoffs': 75.43547125640778, 'action': [1.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 69.5193549909471, 'action': [0.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 73.5973480750315, 'action': [1.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 67.99057894774377, 'action': [0.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 77.67072582531195, 'action': [1.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 69.87357508761878, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17134697762970014, 0.16230366492146597, 0.16849119466920515, 0.15992384578772012, 0.17467872441694432, 0.16277962874821514]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 69.88582634925842 s
