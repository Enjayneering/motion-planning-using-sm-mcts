Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 291, 'sum_payoffs': 66.50919972773613, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 64.37195109649011, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 63.772620763464495, 'action': [0.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 62.94927089306847, 'action': [1.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 68.12278717609338, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 63.72564691502472, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 61.99911889953206, 'action': [0.0, 0.0]}, {'num_count': 307, 'sum_payoffs': 71.93034608304941, 'action': [2.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 68.25034183257203, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1118800461361015, 0.10957324106113034, 0.10880430603613994, 0.10803537101114956, 0.11380238369857747, 0.10880430603613994, 0.10688196847366398, 0.11803152633602461, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 423, 'sum_payoffs': 85.98158643192238, 'action': [0.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 90.79623577577026, 'action': [1.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 92.66600667244019, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 86.34667979781388, 'action': [0.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 85.40990642298789, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 93.51394818583846, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16262975778546712, 0.16878123798539024, 0.1710880430603614, 0.16301422529796233, 0.16186082276047675, 0.172241445597847]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 91.08590960502625 s
