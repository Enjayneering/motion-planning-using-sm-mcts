Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 306, 'sum_payoffs': 71.76384720526536, 'action': [2.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 63.472346560426736, 'action': [1.0, 0.0]}, {'num_count': 302, 'sum_payoffs': 70.40354211483485, 'action': [2.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 65.510204285916, 'action': [0.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 65.48548242987611, 'action': [1.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 67.2048637116393, 'action': [2.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 65.8939898513102, 'action': [1.0, -1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 61.11756294969751, 'action': [0.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 61.66264478139243, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11764705882352941, 0.10841983852364476, 0.11610918877354863, 0.11072664359861592, 0.11072664359861592, 0.11264898116109189, 0.1111111111111111, 0.1057285659361784, 0.10649750096116878]
Actions to choose Agent 1: dict_values([{'num_count': 449, 'sum_payoffs': 93.9239609250124, 'action': [1.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 92.14348672078962, 'action': [1.0, 0.0]}, {'num_count': 435, 'sum_payoffs': 89.73263353144527, 'action': [0.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 91.50431603098713, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 82.53103138933554, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 85.50226378423051, 'action': [0.0, 0.0]}])
Weights num count: [0.17262591311034217, 0.170319108035371, 0.16724336793540945, 0.1695501730103806, 0.1580161476355248, 0.16186082276047675]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 86.05071258544922 s
