Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 291, 'sum_payoffs': 66.34071922945101, 'action': [2.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 67.35512803276137, 'action': [1.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 63.870860180598925, 'action': [1.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 63.83889643323916, 'action': [0.0, -1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 60.843182688678596, 'action': [0.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 74.92769516637232, 'action': [2.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 62.11625787025065, 'action': [0.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 66.94727309920242, 'action': [2.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 63.90858505018702, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1118800461361015, 0.11303344867358708, 0.10918877354863514, 0.10918877354863514, 0.1057285659361784, 0.12149173394848135, 0.10726643598615918, 0.11264898116109189, 0.10918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 433, 'sum_payoffs': 89.00445388006789, 'action': [0.0, -1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 86.96002109615311, 'action': [0.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 89.6659099496961, 'action': [1.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 90.19923260714245, 'action': [1.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 84.23423973571546, 'action': [0.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 94.76995810070447, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16647443291041908, 0.1637831603229527, 0.16724336793540945, 0.16801230296039985, 0.16032295271049596, 0.17377931564782775]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 85.75948357582092 s
