Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 292, 'sum_payoffs': 66.77820900676193, 'action': [1.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 63.006594264982866, 'action': [0.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 70.90170545042466, 'action': [2.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 62.339995357810146, 'action': [0.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 64.36238327664837, 'action': [1.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 69.13563538062328, 'action': [2.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 66.02485901575467, 'action': [0.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 60.553911312716146, 'action': [1.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 68.22091750188761, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1122645136485967, 0.10803537101114956, 0.11687812379853903, 0.10726643598615918, 0.10957324106113034, 0.11495578623606305, 0.1114955786236063, 0.1053440984236832, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 449, 'sum_payoffs': 93.64130208810631, 'action': [1.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 88.82461734212254, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 94.22946689814859, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 83.41768422979813, 'action': [0.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 81.61604728817929, 'action': [0.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 91.85276433090942, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17262591311034217, 0.16647443291041908, 0.17339484813533257, 0.15955401768550556, 0.1572472126105344, 0.170319108035371]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 87.61774110794067 s
