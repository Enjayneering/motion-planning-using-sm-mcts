Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 308, 'sum_payoffs': 72.31817064532113, 'action': [2.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 68.91108202109449, 'action': [2.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 65.4186088626992, 'action': [1.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 66.87861075993187, 'action': [2.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 65.15823905766534, 'action': [0.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 65.85693787506521, 'action': [1.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 61.3788829576407, 'action': [0.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 63.44025985719214, 'action': [1.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 62.69929937388287, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1184159938485198, 0.11457131872356786, 0.11072664359861592, 0.1122645136485967, 0.11034217608612072, 0.1111111111111111, 0.1061130334486736, 0.10841983852364476, 0.10765090349865436]
Actions to choose Agent 1: dict_values([{'num_count': 419, 'sum_payoffs': 84.45401985379662, 'action': [0.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 85.57655156438253, 'action': [0.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 93.41959194976154, 'action': [1.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 93.74285254040824, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 85.93217993775319, 'action': [0.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 89.30148871962457, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16109188773548636, 0.16262975778546712, 0.17262591311034217, 0.17301038062283736, 0.16301422529796233, 0.16724336793540945]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 91.26398921012878 s
