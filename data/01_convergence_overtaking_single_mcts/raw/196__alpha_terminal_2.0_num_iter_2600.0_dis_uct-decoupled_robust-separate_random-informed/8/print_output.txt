Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 291, 'sum_payoffs': 66.41705489988438, 'action': [1.0, -1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 67.9005664327004, 'action': [2.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 67.1353368366317, 'action': [2.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 59.97105794024818, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 64.72571255512953, 'action': [0.0, -1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 73.72000483873084, 'action': [2.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 63.684706570232045, 'action': [1.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 67.19683191431619, 'action': [1.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 60.942824706249795, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1118800461361015, 0.11341791618608228, 0.11264898116109189, 0.10457516339869281, 0.10995770857362552, 0.11995386389850057, 0.10880430603613994, 0.11264898116109189, 0.1057285659361784]
Actions to choose Agent 1: dict_values([{'num_count': 439, 'sum_payoffs': 90.9166476589557, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 83.37350295532137, 'action': [0.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 96.70568360283701, 'action': [1.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 90.93456123539454, 'action': [1.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 87.05579495180524, 'action': [0.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 86.43533828901045, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16878123798539024, 0.15916955017301038, 0.17608612072279892, 0.16878123798539024, 0.1637831603229527, 0.16301422529796233]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 82.21306467056274 s
