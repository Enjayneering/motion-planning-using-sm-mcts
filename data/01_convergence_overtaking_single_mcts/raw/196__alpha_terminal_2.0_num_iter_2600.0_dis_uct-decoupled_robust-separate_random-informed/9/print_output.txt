Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 67.88763492160567, 'action': [2.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 62.66603466504105, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 64.37604506154476, 'action': [0.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 63.280571745964515, 'action': [0.0, 0.0]}, {'num_count': 295, 'sum_payoffs': 68.15208231870855, 'action': [1.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 66.1683452507391, 'action': [1.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 67.86324549381163, 'action': [2.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 70.64185209689124, 'action': [2.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 63.684220860203915, 'action': [1.0, 0.0]}])
Weights num count: [0.11303344867358708, 0.10726643598615918, 0.10918877354863514, 0.10803537101114956, 0.11341791618608228, 0.1111111111111111, 0.11303344867358708, 0.11610918877354863, 0.10841983852364476]
Actions to choose Agent 1: dict_values([{'num_count': 422, 'sum_payoffs': 85.51067122498455, 'action': [0.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 94.6231800523779, 'action': [1.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 82.86403695458367, 'action': [0.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 85.24259282907458, 'action': [0.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 92.47116521698307, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 93.08779992663926, 'action': [1.0, 0.0]}])
Weights num count: [0.16224529027297194, 0.17377931564782775, 0.1587850826605152, 0.16186082276047675, 0.1710880430603614, 0.17185697808535177]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 79.69898700714111 s
