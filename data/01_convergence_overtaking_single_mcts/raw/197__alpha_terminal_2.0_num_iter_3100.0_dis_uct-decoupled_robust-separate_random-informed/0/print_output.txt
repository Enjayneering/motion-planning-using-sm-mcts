Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 342, 'sum_payoffs': 77.06309651125274, 'action': [2.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 81.6881737978757, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 74.61087496690662, 'action': [0.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 83.33233932840102, 'action': [2.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 73.25314983283563, 'action': [0.0, 0.0]}, {'num_count': 327, 'sum_payoffs': 72.05343044127962, 'action': [0.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 78.32942616980044, 'action': [1.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 79.6321769332077, 'action': [1.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 80.30043197372581, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11028700419219607, 0.11480167687842631, 0.10802966784908094, 0.11641405998065141, 0.10673976136730087, 0.1054498548855208, 0.11157691067397614, 0.11286681715575621, 0.11351177039664624]
Actions to choose Agent 1: dict_values([{'num_count': 500, 'sum_payoffs': 100.15409243905651, 'action': [0.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 109.44906173933923, 'action': [1.0, -1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 110.6139648129508, 'action': [1.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 101.83501345885824, 'action': [0.0, 0.0]}, {'num_count': 521, 'sum_payoffs': 106.28231443323666, 'action': [1.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 101.56089291009985, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16123831022250887, 0.17155756207674944, 0.1728474685585295, 0.16317316994517897, 0.16801031925185425, 0.16285069332473395]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 94.15057253837585 s
