Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 337, 'sum_payoffs': 74.73171876785456, 'action': [0.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 73.07870692964156, 'action': [0.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 73.80280105158793, 'action': [1.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 77.16025136036232, 'action': [0.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 79.05264881269589, 'action': [1.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 78.71267538180028, 'action': [2.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 79.10295076307338, 'action': [2.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 83.10825091239145, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 76.47483713441073, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10867462108997097, 0.10706223798774589, 0.10770719122863592, 0.1109319574330861, 0.11286681715575621, 0.11254434053531119, 0.11286681715575621, 0.11673653660109642, 0.11028700419219607]
Actions to choose Agent 1: dict_values([{'num_count': 528, 'sum_payoffs': 109.4677657165788, 'action': [1.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 101.81678933448076, 'action': [0.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 103.3297693868332, 'action': [0.0, -1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 111.3022947152488, 'action': [1.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 102.71290604695184, 'action': [0.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 108.29596672034732, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17026765559496937, 0.1618832634633989, 0.163495646565624, 0.17220251531763947, 0.16285069332473395, 0.1689777491131893]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 96.71374130249023 s
