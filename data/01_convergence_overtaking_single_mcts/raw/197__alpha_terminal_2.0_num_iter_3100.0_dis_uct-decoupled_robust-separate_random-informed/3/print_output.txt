Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 365, 'sum_payoffs': 84.76603138466287, 'action': [2.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 77.38335058268818, 'action': [1.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 80.67761657487988, 'action': [2.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 75.45290780297773, 'action': [0.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 77.72469567251038, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 75.10544528787943, 'action': [0.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 78.39441994352043, 'action': [1.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 72.78063315076685, 'action': [0.0, 1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 78.66400387687538, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11770396646243148, 0.11060948081264109, 0.11383424701709126, 0.10867462108997097, 0.1109319574330861, 0.10835214446952596, 0.11157691067397614, 0.10609480812641084, 0.11189938729442116]
Actions to choose Agent 1: dict_values([{'num_count': 534, 'sum_payoffs': 110.1568369850777, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 101.38346940117367, 'action': [0.0, 1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 107.58847785734748, 'action': [1.0, 1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 108.955588741961, 'action': [1.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 101.18237053783108, 'action': [0.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 101.43446471850926, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17220251531763947, 0.16252821670428894, 0.16930022573363432, 0.1709126088358594, 0.16220574008384392, 0.16252821670428894]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 96.01888465881348 s
