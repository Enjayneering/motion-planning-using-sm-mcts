Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 358, 'sum_payoffs': 82.31179916905785, 'action': [2.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 80.26672869018525, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 75.01891062014462, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 77.0009259458229, 'action': [1.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 77.93602428525013, 'action': [1.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 84.74510747407237, 'action': [2.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 74.63057225219494, 'action': [1.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 73.31102697140854, 'action': [0.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 74.9783088209492, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11544663011931634, 0.11351177039664624, 0.10835214446952596, 0.11028700419219607, 0.11125443405353112, 0.11770396646243148, 0.10802966784908094, 0.10673976136730087, 0.10835214446952596]
Actions to choose Agent 1: dict_values([{'num_count': 517, 'sum_payoffs': 105.42112176675498, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 100.83780214674825, 'action': [0.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 102.23698952816942, 'action': [0.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 103.98348839672867, 'action': [0.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 109.54341679074732, 'action': [1.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 110.10183072755616, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16672041277007418, 0.16156078684295389, 0.16317316994517897, 0.16510802966784907, 0.17123508545630442, 0.17188003869719445]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 93.65304517745972 s
