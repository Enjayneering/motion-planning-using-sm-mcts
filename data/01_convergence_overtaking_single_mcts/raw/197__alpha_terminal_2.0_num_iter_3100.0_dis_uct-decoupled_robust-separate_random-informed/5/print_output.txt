Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 330, 'sum_payoffs': 72.35332327652924, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 74.94008959781284, 'action': [0.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 75.9952341445546, 'action': [1.0, 1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 77.95302286545876, 'action': [2.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 79.54513922939208, 'action': [1.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 79.96541422203161, 'action': [1.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 73.60430158607925, 'action': [0.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 79.95506528372162, 'action': [2.0, 0.0]}, {'num_count': 352, 'sum_payoffs': 79.60123943802186, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10641728474685586, 0.10899709771041599, 0.10996452757175104, 0.11189938729442116, 0.11351177039664624, 0.11383424701709126, 0.10770719122863592, 0.11383424701709126, 0.11351177039664624]
Actions to choose Agent 1: dict_values([{'num_count': 534, 'sum_payoffs': 110.98607378097212, 'action': [1.0, 1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 108.58154330168298, 'action': [1.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 110.04791985626288, 'action': [1.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 101.22329391760489, 'action': [0.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 100.16971035751877, 'action': [0.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 104.19805609831332, 'action': [0.0, 0.0]}])
Weights num count: [0.17220251531763947, 0.16962270235407934, 0.17123508545630442, 0.16156078684295389, 0.16027088036117382, 0.16478555304740405]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 91.78493213653564 s
