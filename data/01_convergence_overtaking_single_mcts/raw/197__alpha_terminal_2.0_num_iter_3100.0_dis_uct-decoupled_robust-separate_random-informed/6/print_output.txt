Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 341, 'sum_payoffs': 76.8542161862637, 'action': [1.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 75.42097412920207, 'action': [0.0, 1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 79.57987388075279, 'action': [1.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 80.15277814292652, 'action': [2.0, 1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 73.20475012224563, 'action': [0.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 76.54081405561443, 'action': [2.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 84.56760007964601, 'action': [2.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 76.52330119319697, 'action': [0.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 79.16350876731688, 'action': [1.0, 0.0]}])
Weights num count: [0.10996452757175104, 0.10867462108997097, 0.11254434053531119, 0.11318929377620122, 0.10641728474685586, 0.10964205095130602, 0.11738148984198646, 0.10964205095130602, 0.11222186391486617]
Actions to choose Agent 1: dict_values([{'num_count': 539, 'sum_payoffs': 111.67161201477387, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 99.76132615245943, 'action': [0.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 104.0785000318805, 'action': [0.0, 1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 108.67593119407225, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 98.90796031606888, 'action': [0.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 107.92329321612702, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17381489841986456, 0.16059335698161883, 0.1654305062882941, 0.1705901322154144, 0.15962592712028378, 0.16962270235407934]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.98548555374146 s
