Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 344, 'sum_payoffs': 77.60589790029306, 'action': [1.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 83.59212715220548, 'action': [2.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 74.51020641623728, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 76.955788016124, 'action': [2.0, 1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 78.51249090677271, 'action': [1.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 79.95366993102694, 'action': [1.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 75.181637185677, 'action': [0.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 81.29368214606527, 'action': [2.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 71.94423717618976, 'action': [0.0, 0.0]}])
Weights num count: [0.1109319574330861, 0.11673653660109642, 0.10802966784908094, 0.11028700419219607, 0.11189938729442116, 0.11318929377620122, 0.10867462108997097, 0.11447920025798129, 0.1054498548855208]
Actions to choose Agent 1: dict_values([{'num_count': 528, 'sum_payoffs': 108.68496156340078, 'action': [1.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 104.2933570539448, 'action': [0.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 109.31387112510775, 'action': [1.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 100.49331539616655, 'action': [0.0, -1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 110.41708739954795, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 99.10214555438898, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17026765559496937, 0.1654305062882941, 0.1709126088358594, 0.16123831022250887, 0.17220251531763947, 0.15962592712028378]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 97.48628354072571 s
