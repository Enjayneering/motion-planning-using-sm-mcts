Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 78.49167062251742, 'action': [1.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 87.2436477327552, 'action': [2.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 75.91857449825565, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 83.30003342995293, 'action': [2.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 74.23957698542296, 'action': [0.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 72.78053798287736, 'action': [0.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 78.20154292827709, 'action': [1.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 73.82173179639437, 'action': [0.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 78.18958467725933, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11157691067397614, 0.1199613028055466, 0.10899709771041599, 0.11609158336020639, 0.1073847146081909, 0.10609480812641084, 0.11125443405353112, 0.10706223798774589, 0.11125443405353112]
Actions to choose Agent 1: dict_values([{'num_count': 499, 'sum_payoffs': 100.07233362201767, 'action': [0.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 104.17862868475257, 'action': [1.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 103.08012756581597, 'action': [0.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 110.35927322145403, 'action': [1.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 104.52677366478495, 'action': [0.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 109.46525876452803, 'action': [1.0, 0.0]}])
Weights num count: [0.16091583360206385, 0.1654305062882941, 0.16414059980651402, 0.17220251531763947, 0.1657529829087391, 0.17123508545630442]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 94.17933464050293 s
