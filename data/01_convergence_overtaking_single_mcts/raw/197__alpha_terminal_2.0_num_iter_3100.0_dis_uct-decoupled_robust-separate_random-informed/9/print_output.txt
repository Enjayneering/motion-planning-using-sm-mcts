Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 335, 'sum_payoffs': 74.7573516923919, 'action': [0.0, 1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 79.76915642407988, 'action': [1.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 76.06789438546635, 'action': [1.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 80.70434700276383, 'action': [2.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 74.75221535775643, 'action': [0.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 80.48447186364857, 'action': [2.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 73.76857352151917, 'action': [0.0, 0.0]}, {'num_count': 348, 'sum_payoffs': 79.06784521363593, 'action': [1.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 81.73657579718318, 'action': [2.0, 0.0]}])
Weights num count: [0.10802966784908094, 0.11286681715575621, 0.10931957433086101, 0.11383424701709126, 0.10802966784908094, 0.11351177039664624, 0.10706223798774589, 0.11222186391486617, 0.11480167687842631]
Actions to choose Agent 1: dict_values([{'num_count': 509, 'sum_payoffs': 103.2091299947803, 'action': [0.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 103.99123909049844, 'action': [0.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 107.20376984933127, 'action': [1.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 106.90691303387838, 'action': [1.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 109.03813818238662, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 102.02852148432484, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16414059980651402, 0.16510802966784907, 0.16865527249274428, 0.16833279587229927, 0.1705901322154144, 0.16285069332473395]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 97.85692691802979 s
