Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 409, 'sum_payoffs': 92.66651504866569, 'action': [2.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 90.93343155315962, 'action': [2.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 82.24400624130364, 'action': [0.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 88.02940046246393, 'action': [0.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 88.74889570454066, 'action': [1.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 92.28589591826977, 'action': [1.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 89.67485682050697, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 96.26707414961578, 'action': [2.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 86.4068263134966, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11357956123299083, 0.1121910580394335, 0.10469314079422383, 0.10969175229103027, 0.1102471535684532, 0.11330186059427937, 0.11108025548458761, 0.116634268258817, 0.10830324909747292]
Actions to choose Agent 1: dict_values([{'num_count': 590, 'sum_payoffs': 118.39735419849048, 'action': [0.0, -1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 116.41978471395622, 'action': [0.0, 1.5707963267948966]}, {'num_count': 599, 'sum_payoffs': 120.86544187231006, 'action': [0.0, 0.0]}, {'num_count': 611, 'sum_payoffs': 124.36616626241918, 'action': [1.0, 0.0]}, {'num_count': 604, 'sum_payoffs': 122.27973358567205, 'action': [1.0, 1.5707963267948966]}, {'num_count': 613, 'sum_payoffs': 124.86064841103097, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16384337683976674, 0.16189947236878643, 0.16634268258816995, 0.16967509025270758, 0.1677311857817273, 0.1702304915301305]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 104.7289469242096 s
