Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 404, 'sum_payoffs': 90.75506999620956, 'action': [2.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 99.33785299549649, 'action': [2.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 84.64310506527764, 'action': [0.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 89.76836054483671, 'action': [0.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 84.67756077817856, 'action': [1.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 87.85945461065909, 'action': [1.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 83.63983465777949, 'action': [0.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 89.48232332372801, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 95.41439490378752, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1121910580394335, 0.11941127464593168, 0.10691474590391557, 0.11135795612329909, 0.10691474590391557, 0.10969175229103027, 0.10608164398778117, 0.11108025548458761, 0.11607886698139405]
Actions to choose Agent 1: dict_values([{'num_count': 580, 'sum_payoffs': 115.53435419716504, 'action': [0.0, -1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 129.1130909000477, 'action': [1.0, 0.0]}, {'num_count': 608, 'sum_payoffs': 123.46697439976354, 'action': [1.0, -1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 118.64913867950912, 'action': [0.0, 0.0]}, {'num_count': 608, 'sum_payoffs': 123.43249507990872, 'action': [1.0, 1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 116.91734117364506, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16106637045265204, 0.17439600111080256, 0.16884198833657318, 0.1641210774784782, 0.16884198833657318, 0.1624548736462094]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 109.87335801124573 s
