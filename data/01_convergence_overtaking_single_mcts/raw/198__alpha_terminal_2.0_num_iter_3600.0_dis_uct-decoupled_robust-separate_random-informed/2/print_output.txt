Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 387, 'sum_payoffs': 85.4805426519911, 'action': [0.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 91.67908816917748, 'action': [1.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 87.77877528669774, 'action': [1.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 90.98056531540087, 'action': [1.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 82.28774952574132, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 90.33216229402917, 'action': [2.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 85.78024657422155, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 93.88572958211952, 'action': [2.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 99.21759883738159, 'action': [2.0, 0.0]}])
Weights num count: [0.10747014718133852, 0.11274645931685642, 0.1094140516523188, 0.1121910580394335, 0.10469314079422383, 0.11163565676201055, 0.10774784782004998, 0.11469036378783672, 0.11913357400722022]
Actions to choose Agent 1: dict_values([{'num_count': 576, 'sum_payoffs': 114.36206900914631, 'action': [0.0, -1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 116.14687326833193, 'action': [0.0, 1.5707963267948966]}, {'num_count': 597, 'sum_payoffs': 120.3679293170229, 'action': [0.0, 0.0]}, {'num_count': 622, 'sum_payoffs': 127.50516154117561, 'action': [1.0, -1.5707963267948966]}, {'num_count': 613, 'sum_payoffs': 124.95008068541044, 'action': [1.0, 0.0]}, {'num_count': 610, 'sum_payoffs': 124.05473507306037, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15995556789780616, 0.16162177173007497, 0.16578728131074702, 0.17272979727853374, 0.1702304915301305, 0.1693973896139961]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 103.95743012428284 s
