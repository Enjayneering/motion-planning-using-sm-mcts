Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 87.29606723391743, 'action': [0.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 83.79507547071243, 'action': [0.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 96.06258718859904, 'action': [2.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 94.04410291661739, 'action': [2.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 82.69611835501068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 86.61220052385063, 'action': [1.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 89.19551673835134, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 90.81511629109028, 'action': [1.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 92.45832824830018, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1094140516523188, 0.10635934462649264, 0.11691196889752846, 0.11524576506525964, 0.10552624271035824, 0.10885865037489587, 0.11108025548458761, 0.11246875867814496, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 622, 'sum_payoffs': 127.40683024781953, 'action': [1.0, 1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 117.6887226150089, 'action': [0.0, 0.0]}, {'num_count': 577, 'sum_payoffs': 114.6740895758614, 'action': [0.0, 1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 125.64497913576837, 'action': [1.0, -1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 125.06162761120892, 'action': [1.0, 0.0]}, {'num_count': 583, 'sum_payoffs': 116.35700996960831, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17272979727853374, 0.16328797556234378, 0.16023326853651762, 0.17106359344626493, 0.170508192168842, 0.16189947236878643]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 105.81433725357056 s
