Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 392, 'sum_payoffs': 86.62525614259332, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 93.1454807446816, 'action': [2.0, -1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 85.63360130076225, 'action': [0.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 92.51536900834392, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 93.79625747602654, 'action': [2.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 88.875038409457, 'action': [2.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 88.19250226997269, 'action': [0.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 88.20899023232111, 'action': [1.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 85.92118232368931, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10885865037489587, 0.11441266314912524, 0.10802554845876146, 0.11385726187170231, 0.11496806442654818, 0.11080255484587614, 0.1102471535684532, 0.1102471535684532, 0.10830324909747292]
Actions to choose Agent 1: dict_values([{'num_count': 582, 'sum_payoffs': 116.37453548343599, 'action': [0.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 114.33784379053813, 'action': [0.0, -1.5707963267948966]}, {'num_count': 624, 'sum_payoffs': 128.3792324056223, 'action': [1.0, 1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 113.77912148422595, 'action': [0.0, 0.0]}, {'num_count': 618, 'sum_payoffs': 126.67602121094453, 'action': [1.0, -1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 129.47651174826345, 'action': [1.0, 0.0]}])
Weights num count: [0.16162177173007497, 0.1596778672590947, 0.17328519855595667, 0.15912246598167176, 0.17161899472368786, 0.17439600111080256]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 110.12943077087402 s
