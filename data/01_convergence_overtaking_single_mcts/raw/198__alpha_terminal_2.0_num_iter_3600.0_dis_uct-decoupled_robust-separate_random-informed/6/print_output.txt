Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 404, 'sum_payoffs': 90.90612014100837, 'action': [2.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 97.39752122428139, 'action': [2.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 87.5684378649936, 'action': [1.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 86.88046093313761, 'action': [0.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 87.67927520195948, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 85.34839385836545, 'action': [0.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 92.13609027905964, 'action': [2.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 87.27393648076651, 'action': [0.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 90.83036877306357, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1121910580394335, 0.11774507081366287, 0.1094140516523188, 0.10885865037489587, 0.1094140516523188, 0.10747014718133852, 0.11330186059427937, 0.10913635101360733, 0.1121910580394335]
Actions to choose Agent 1: dict_values([{'num_count': 591, 'sum_payoffs': 118.47892339089326, 'action': [0.0, -1.5707963267948966]}, {'num_count': 610, 'sum_payoffs': 123.91682732836247, 'action': [1.0, -1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 115.7087289521925, 'action': [0.0, 1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 125.93114318379934, 'action': [1.0, 0.0]}, {'num_count': 615, 'sum_payoffs': 125.34665131675064, 'action': [1.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 117.09505230687904, 'action': [0.0, 0.0]}])
Weights num count: [0.1641210774784782, 0.1693973896139961, 0.1613440710913635, 0.1713412940849764, 0.17078589280755346, 0.16273257428492086]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 105.57236981391907 s
