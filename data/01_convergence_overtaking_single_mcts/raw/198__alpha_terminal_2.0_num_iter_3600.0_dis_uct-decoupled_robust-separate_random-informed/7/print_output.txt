Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 393, 'sum_payoffs': 87.2445826064269, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 93.11633739782778, 'action': [2.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 85.58062196298776, 'action': [0.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 87.86481197074696, 'action': [1.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 86.77037598247873, 'action': [0.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 90.49384850131275, 'action': [2.0, 1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 89.78628120451327, 'action': [1.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 95.08081812383142, 'action': [2.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 89.5358060458133, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10913635101360733, 0.11413496251041377, 0.10774784782004998, 0.10969175229103027, 0.10885865037489587, 0.11191335740072202, 0.11135795612329909, 0.11580116634268259, 0.11108025548458761]
Actions to choose Agent 1: dict_values([{'num_count': 598, 'sum_payoffs': 121.0315604461712, 'action': [0.0, 1.5707963267948966]}, {'num_count': 607, 'sum_payoffs': 123.62683483760917, 'action': [1.0, -1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 116.85040475735467, 'action': [0.0, 0.0]}, {'num_count': 581, 'sum_payoffs': 116.20803956142475, 'action': [0.0, -1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 126.4997475438357, 'action': [1.0, 1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 125.659178941482, 'action': [1.0, 0.0]}])
Weights num count: [0.16606498194945848, 0.1685642876978617, 0.16189947236878643, 0.1613440710913635, 0.1713412940849764, 0.170508192168842]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 104.62046694755554 s
