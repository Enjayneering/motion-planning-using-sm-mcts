Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 397, 'sum_payoffs': 88.2562371470313, 'action': [1.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 81.15460354883136, 'action': [0.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 85.30950900001353, 'action': [0.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 90.78042330478691, 'action': [2.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 91.76235101034287, 'action': [2.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 91.08620860854761, 'action': [1.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 87.61031623888367, 'action': [0.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 88.86623965551387, 'action': [1.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 97.8906005409564, 'action': [2.0, 0.0]}])
Weights num count: [0.1102471535684532, 0.10413773951680089, 0.10774784782004998, 0.11246875867814496, 0.11330186059427937, 0.11274645931685642, 0.10969175229103027, 0.11080255484587614, 0.11857817272979727]
Actions to choose Agent 1: dict_values([{'num_count': 619, 'sum_payoffs': 126.24192831172425, 'action': [1.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 116.95155017550722, 'action': [0.0, -1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 116.87327717680155, 'action': [0.0, 0.0]}, {'num_count': 611, 'sum_payoffs': 124.00324418561567, 'action': [1.0, 0.0]}, {'num_count': 612, 'sum_payoffs': 124.24432839455551, 'action': [1.0, -1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 116.94235362778402, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17189669536239932, 0.16273257428492086, 0.16273257428492086, 0.16967509025270758, 0.16995279089141904, 0.16273257428492086]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 101.56196331977844 s
