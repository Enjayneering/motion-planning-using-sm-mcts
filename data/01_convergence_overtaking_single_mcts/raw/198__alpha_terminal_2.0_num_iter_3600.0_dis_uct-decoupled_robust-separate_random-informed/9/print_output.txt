Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 403, 'sum_payoffs': 90.33767118649033, 'action': [1.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 97.50689067812421, 'action': [2.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 88.99037771397963, 'action': [2.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 88.70142162941994, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 94.25073080719619, 'action': [2.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 88.76659854760895, 'action': [1.0, 1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 81.5923364407503, 'action': [0.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 89.00065352586857, 'action': [0.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 85.0974856715561, 'action': [0.0, 0.0]}])
Weights num count: [0.11191335740072202, 0.11802277145237434, 0.11080255484587614, 0.11052485420716468, 0.11524576506525964, 0.11052485420716468, 0.10441544015551235, 0.11080255484587614, 0.10747014718133852]
Actions to choose Agent 1: dict_values([{'num_count': 612, 'sum_payoffs': 124.8478479445016, 'action': [1.0, -1.5707963267948966]}, {'num_count': 594, 'sum_payoffs': 119.68449664984993, 'action': [0.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 114.94981985769927, 'action': [0.0, -1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 125.77586069795046, 'action': [1.0, 1.5707963267948966]}, {'num_count': 624, 'sum_payoffs': 128.25498921638842, 'action': [1.0, 0.0]}, {'num_count': 578, 'sum_payoffs': 115.16544896514851, 'action': [0.0, 0.0]}])
Weights num count: [0.16995279089141904, 0.1649541793946126, 0.16023326853651762, 0.17078589280755346, 0.17328519855595667, 0.1605109691752291]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 112.15510535240173 s
