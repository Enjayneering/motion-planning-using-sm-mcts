Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 460, 'sum_payoffs': 102.20752759615827, 'action': [1.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 103.80419090732674, 'action': [2.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 101.24158364573246, 'action': [1.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 100.5453219420587, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 102.53018717625929, 'action': [2.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 97.4304400698019, 'action': [0.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 95.1939106489398, 'action': [0.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 105.01416674605028, 'action': [2.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 98.95680106556804, 'action': [0.0, 0.0]}])
Weights num count: [0.11216776396000976, 0.11338697878566203, 0.11143623506461839, 0.11094854913435748, 0.11241160692514021, 0.10851011948305292, 0.10680321872713973, 0.11436235064618386, 0.1097293343087052]
Actions to choose Agent 1: dict_values([{'num_count': 667, 'sum_payoffs': 132.06909460194544, 'action': [0.0, 1.5707963267948966]}, {'num_count': 692, 'sum_payoffs': 139.06312988940363, 'action': [1.0, -1.5707963267948966]}, {'num_count': 668, 'sum_payoffs': 132.3354069510782, 'action': [0.0, -1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 141.4810536012356, 'action': [1.0, 0.0]}, {'num_count': 697, 'sum_payoffs': 140.39264706224319, 'action': [1.0, 1.5707963267948966]}, {'num_count': 675, 'sum_payoffs': 134.30274935440187, 'action': [0.0, 0.0]}])
Weights num count: [0.16264325774201413, 0.16873933187027554, 0.1628871007071446, 0.17093391855644965, 0.16995854669592783, 0.1645940014630578]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 118.90789437294006 s
