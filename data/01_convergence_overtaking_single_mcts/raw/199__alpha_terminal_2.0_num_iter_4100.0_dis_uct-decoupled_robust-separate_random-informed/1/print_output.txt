Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 457, 'sum_payoffs': 101.14600269843365, 'action': [2.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 96.04200500386457, 'action': [0.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 100.20816554892176, 'action': [2.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 101.12181915900621, 'action': [1.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 101.37086098914646, 'action': [1.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 100.78115377630682, 'action': [0.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 101.4689079780779, 'action': [1.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 108.78608468275114, 'action': [2.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 95.05724175471478, 'action': [0.0, 0.0]}])
Weights num count: [0.11143623506461839, 0.1075347476225311, 0.11070470616922702, 0.11143623506461839, 0.11168007802974884, 0.11119239209948793, 0.11168007802974884, 0.11728846622774933, 0.10680321872713973]
Actions to choose Agent 1: dict_values([{'num_count': 704, 'sum_payoffs': 142.54311714433834, 'action': [1.0, 0.0]}, {'num_count': 694, 'sum_payoffs': 139.83114057726866, 'action': [1.0, -1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 131.77348157659625, 'action': [0.0, 1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 130.90884710362388, 'action': [0.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 133.95268169397866, 'action': [0.0, 0.0]}, {'num_count': 702, 'sum_payoffs': 142.09804689474112, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.171665447451841, 0.16922701780053645, 0.16215557181175322, 0.16142404291636187, 0.1641063155327969, 0.1711777615215801]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 124.09500861167908 s
