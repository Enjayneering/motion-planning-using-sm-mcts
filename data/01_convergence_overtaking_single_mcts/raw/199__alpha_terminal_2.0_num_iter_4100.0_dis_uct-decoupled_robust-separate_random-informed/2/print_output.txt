Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 457, 'sum_payoffs': 101.01347767095501, 'action': [1.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 93.79167014413984, 'action': [0.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 98.82342722794374, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 101.90655468363136, 'action': [2.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 102.24274349395267, 'action': [1.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 94.32936433296335, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 100.72281955966777, 'action': [1.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 102.87552764106408, 'action': [2.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 109.30552294676797, 'action': [2.0, 0.0]}])
Weights num count: [0.11143623506461839, 0.10582784686661789, 0.1097293343087052, 0.11216776396000976, 0.11241160692514021, 0.1063155327968788, 0.11119239209948793, 0.11289929285540112, 0.11777615215801024]
Actions to choose Agent 1: dict_values([{'num_count': 674, 'sum_payoffs': 134.94066296996988, 'action': [0.0, 0.0]}, {'num_count': 694, 'sum_payoffs': 140.48123432302305, 'action': [1.0, -1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 143.84837059883773, 'action': [1.0, 0.0]}, {'num_count': 709, 'sum_payoffs': 144.71118216841097, 'action': [1.0, 1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 130.6905651439653, 'action': [0.0, 1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 130.50484949814614, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16435015849792733, 0.16922701780053645, 0.17215313338210192, 0.1728846622774933, 0.16069251402097048, 0.16044867105584004]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 118.94675326347351 s
