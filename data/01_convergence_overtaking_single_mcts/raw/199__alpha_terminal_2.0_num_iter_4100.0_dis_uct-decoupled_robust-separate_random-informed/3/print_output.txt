Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 430, 'sum_payoffs': 92.65050627172442, 'action': [0.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 111.68886798624482, 'action': [2.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 95.69880389416404, 'action': [0.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 95.79632857799697, 'action': [0.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 103.26683586561924, 'action': [2.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 97.66506798856636, 'action': [1.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 103.38183890298788, 'action': [1.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 100.81008020655754, 'action': [1.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 105.27888620285776, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10485247500609607, 0.11948305291392343, 0.10729090465740064, 0.10729090465740064, 0.11314313582053158, 0.10875396244818337, 0.11314313582053158, 0.11119239209948793, 0.11460619361131431]
Actions to choose Agent 1: dict_values([{'num_count': 663, 'sum_payoffs': 131.3015572499366, 'action': [0.0, 0.0]}, {'num_count': 673, 'sum_payoffs': 134.05334580326257, 'action': [0.0, -1.5707963267948966]}, {'num_count': 700, 'sum_payoffs': 141.58450882838457, 'action': [1.0, -1.5707963267948966]}, {'num_count': 670, 'sum_payoffs': 133.24622800371367, 'action': [0.0, 1.5707963267948966]}, {'num_count': 692, 'sum_payoffs': 139.34311498946062, 'action': [1.0, 1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 142.12818943056274, 'action': [1.0, 0.0]}])
Weights num count: [0.1616678858814923, 0.1641063155327969, 0.17069007559131918, 0.1633747866374055, 0.16873933187027554, 0.1711777615215801]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 124.53771328926086 s
