Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 457, 'sum_payoffs': 101.2363339082306, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 99.4074873425583, 'action': [1.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 105.37806472569793, 'action': [2.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 96.8791016358231, 'action': [0.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 94.93302485799855, 'action': [0.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 101.64409944411535, 'action': [1.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 99.42276475989232, 'action': [1.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 103.74073955663081, 'action': [2.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 104.76201205329207, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11143623506461839, 0.10997317727383565, 0.11460619361131431, 0.10802243355279201, 0.10655937576200927, 0.11168007802974884, 0.10997317727383565, 0.11338697878566203, 0.1141185076810534]
Actions to choose Agent 1: dict_values([{'num_count': 664, 'sum_payoffs': 131.11787782035483, 'action': [0.0, 1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 141.49199180402493, 'action': [1.0, 1.5707963267948966]}, {'num_count': 670, 'sum_payoffs': 132.85314384042078, 'action': [0.0, 0.0]}, {'num_count': 655, 'sum_payoffs': 128.65751879306336, 'action': [0.0, -1.5707963267948966]}, {'num_count': 705, 'sum_payoffs': 142.5170544108498, 'action': [1.0, 0.0]}, {'num_count': 705, 'sum_payoffs': 142.58502764308687, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16191172884662278, 0.17093391855644965, 0.1633747866374055, 0.15971714216044866, 0.17190929041697148, 0.17190929041697148]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 128.78917598724365 s
