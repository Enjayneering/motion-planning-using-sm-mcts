Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 439, 'sum_payoffs': 95.14907582427922, 'action': [0.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 101.25926381329816, 'action': [1.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 113.32097736707749, 'action': [2.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 98.0481662716596, 'action': [1.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 94.02722298597357, 'action': [0.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 104.09768549089027, 'action': [2.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 94.56828671080778, 'action': [0.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 100.32792160523246, 'action': [1.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 103.38570063464155, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10704706169227018, 0.11168007802974884, 0.12094611070470616, 0.10924164837844429, 0.10607168983174835, 0.11387466471592295, 0.10655937576200927, 0.11094854913435748, 0.11338697878566203]
Actions to choose Agent 1: dict_values([{'num_count': 657, 'sum_payoffs': 129.68655495513374, 'action': [0.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 127.7291520825198, 'action': [0.0, 1.5707963267948966]}, {'num_count': 712, 'sum_payoffs': 144.96470878277398, 'action': [1.0, 0.0]}, {'num_count': 713, 'sum_payoffs': 145.3072413975981, 'action': [1.0, -1.5707963267948966]}, {'num_count': 691, 'sum_payoffs': 139.09950536924163, 'action': [1.0, 1.5707963267948966]}, {'num_count': 677, 'sum_payoffs': 135.21192198782813, 'action': [0.0, 0.0]}])
Weights num count: [0.16020482809070957, 0.1584979273347964, 0.17361619117288465, 0.17386003413801512, 0.1684954889051451, 0.16508168739331872]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 122.92245435714722 s
