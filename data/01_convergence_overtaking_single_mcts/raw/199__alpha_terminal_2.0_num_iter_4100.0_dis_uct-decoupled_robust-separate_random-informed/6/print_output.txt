Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 455, 'sum_payoffs': 100.95078465252658, 'action': [1.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 98.72365704187735, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 105.10071393136283, 'action': [2.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 113.70842041079106, 'action': [2.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 92.6879671380728, 'action': [0.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 99.98778755386616, 'action': [0.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 101.60330649743449, 'action': [1.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 96.19479738080393, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 101.26116033346192, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11094854913435748, 0.10924164837844429, 0.1141185076810534, 0.12070226773957571, 0.10460863204096561, 0.11021702023896611, 0.11143623506461839, 0.10729090465740064, 0.11119239209948793]
Actions to choose Agent 1: dict_values([{'num_count': 702, 'sum_payoffs': 141.51882245840903, 'action': [1.0, 0.0]}, {'num_count': 657, 'sum_payoffs': 129.06138613594786, 'action': [0.0, 1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 140.62633719338552, 'action': [1.0, -1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 130.08104995639562, 'action': [0.0, 0.0]}, {'num_count': 666, 'sum_payoffs': 131.47559792641476, 'action': [0.0, -1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 145.1165110238222, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1711777615215801, 0.16020482809070957, 0.17044623262618874, 0.1611801999512314, 0.1623994147768837, 0.17434772006827604]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 122.96326279640198 s
