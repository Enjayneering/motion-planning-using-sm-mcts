Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 442, 'sum_payoffs': 96.30422423455468, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 101.34941685617692, 'action': [1.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 102.99635513735709, 'action': [1.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 102.68797627726974, 'action': [2.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 101.91335809353134, 'action': [2.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 97.00818180389005, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 98.13516177915257, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 97.94460100858967, 'action': [0.0, -1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 107.0894647972749, 'action': [2.0, 0.0]}])
Weights num count: [0.10777859058766155, 0.11168007802974884, 0.11289929285540112, 0.11265544989027067, 0.11216776396000976, 0.10826627651792246, 0.10924164837844429, 0.10899780541331383, 0.11606925140209705]
Actions to choose Agent 1: dict_values([{'num_count': 669, 'sum_payoffs': 133.19290135218256, 'action': [0.0, 0.0]}, {'num_count': 718, 'sum_payoffs': 146.92850215106293, 'action': [1.0, 0.0]}, {'num_count': 666, 'sum_payoffs': 132.3837119892891, 'action': [0.0, -1.5707963267948966]}, {'num_count': 682, 'sum_payoffs': 136.85404213816003, 'action': [1.0, -1.5707963267948966]}, {'num_count': 696, 'sum_payoffs': 140.65876799142347, 'action': [1.0, 1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 133.1879865898943, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16313094367227504, 0.1750792489636674, 0.1623994147768837, 0.16630090221897098, 0.16971470373079736, 0.16313094367227504]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 120.64187693595886 s
