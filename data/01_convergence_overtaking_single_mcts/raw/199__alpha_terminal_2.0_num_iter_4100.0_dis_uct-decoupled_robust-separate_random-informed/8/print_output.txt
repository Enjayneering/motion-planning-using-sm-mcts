Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 453, 'sum_payoffs': 99.79261230909941, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 104.51399387392554, 'action': [2.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 108.3875031791262, 'action': [2.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 106.75872393633449, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 93.15396461879699, 'action': [0.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 98.85691698041119, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 99.21662904522891, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 95.33121352585793, 'action': [0.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 99.51531912208583, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11046086320409657, 0.1141185076810534, 0.11704462326261887, 0.11582540843696659, 0.10534016093635698, 0.1097293343087052, 0.10997317727383565, 0.10704706169227018, 0.11021702023896611]
Actions to choose Agent 1: dict_values([{'num_count': 691, 'sum_payoffs': 139.11422461413952, 'action': [1.0, -1.5707963267948966]}, {'num_count': 686, 'sum_payoffs': 137.7296305163756, 'action': [0.0, 1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 129.17164251893863, 'action': [0.0, -1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 131.40303580297046, 'action': [0.0, 0.0]}, {'num_count': 693, 'sum_payoffs': 139.68762916158, 'action': [1.0, 1.5707963267948966]}, {'num_count': 712, 'sum_payoffs': 145.01425607730533, 'action': [1.0, 0.0]}])
Weights num count: [0.1684954889051451, 0.1672762740794928, 0.15971714216044866, 0.1616678858814923, 0.168983174835406, 0.17361619117288465]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 118.80683779716492 s
