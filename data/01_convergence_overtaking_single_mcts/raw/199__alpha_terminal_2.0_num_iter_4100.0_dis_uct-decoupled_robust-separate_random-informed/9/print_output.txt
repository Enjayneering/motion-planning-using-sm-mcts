Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 460, 'sum_payoffs': 102.39028382448122, 'action': [2.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 96.74033103884358, 'action': [0.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 103.45056685479875, 'action': [2.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 104.11378649614281, 'action': [1.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 97.75037403148619, 'action': [1.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 96.15871876688095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 101.20758793669893, 'action': [1.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 97.09083057335656, 'action': [0.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 110.80547931516726, 'action': [2.0, 0.0]}])
Weights num count: [0.11216776396000976, 0.10777859058766155, 0.11289929285540112, 0.11338697878566203, 0.10851011948305292, 0.10729090465740064, 0.11119239209948793, 0.10802243355279201, 0.1185076810534016]
Actions to choose Agent 1: dict_values([{'num_count': 704, 'sum_payoffs': 142.4550784499245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 129.44726019820956, 'action': [0.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 143.32804902711723, 'action': [1.0, -1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 144.24134946422598, 'action': [1.0, 0.0]}, {'num_count': 657, 'sum_payoffs': 129.4485475977228, 'action': [0.0, 1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 131.6912556015397, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.171665447451841, 0.16020482809070957, 0.1723969763472324, 0.17312850524262374, 0.16020482809070957, 0.16215557181175322]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 117.1416482925415 s
