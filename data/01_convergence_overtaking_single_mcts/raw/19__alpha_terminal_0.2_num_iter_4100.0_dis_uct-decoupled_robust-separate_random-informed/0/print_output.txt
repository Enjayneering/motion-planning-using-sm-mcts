Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 421, 'sum_payoffs': 94.4228972313461, 'action': [0.0, 0.0]}, {'num_count': 363, 'sum_payoffs': 75.82643459620269, 'action': [0.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 118.76692086761055, 'action': [2.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 94.40258130235412, 'action': [1.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 93.7883232141021, 'action': [1.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 75.21217650795079, 'action': [0.0, 1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 157.77372180848334, 'action': [2.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 123.98123400007948, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 119.72013991011153, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10265788831992197, 0.08851499634235552, 0.12070226773957571, 0.10265788831992197, 0.10217020238966106, 0.0880273104120946, 0.1489880516947086, 0.12460375518166301, 0.12143379663496708]
Actions to choose Agent 1: dict_values([{'num_count': 734, 'sum_payoffs': 206.1961410256074, 'action': [1.0, 1.5707963267948966]}, {'num_count': 814, 'sum_payoffs': 234.79187938358146, 'action': [1.0, 0.0]}, {'num_count': 586, 'sum_payoffs': 154.06253392478285, 'action': [0.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 176.44590745171436, 'action': [0.0, 0.0]}, {'num_count': 587, 'sum_payoffs': 154.42181080802422, 'action': [0.0, 1.5707963267948966]}, {'num_count': 729, 'sum_payoffs': 204.35369268487864, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17898073640575468, 0.19848817361619117, 0.1428919775664472, 0.1584979273347964, 0.14313582053157767, 0.17776152158010242]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.588350296020508 s
