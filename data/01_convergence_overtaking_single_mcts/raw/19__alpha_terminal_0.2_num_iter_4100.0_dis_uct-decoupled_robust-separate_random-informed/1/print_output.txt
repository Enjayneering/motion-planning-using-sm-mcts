Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 503, 'sum_payoffs': 121.27588377704907, 'action': [2.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 94.4199639153925, 'action': [1.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 74.6096516835142, 'action': [0.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 121.34653685631051, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 121.61484473129806, 'action': [2.0, -1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 156.68911918242887, 'action': [2.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 94.10131889013543, 'action': [0.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 95.40794818397028, 'action': [1.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 74.21267625824221, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12265301146061935, 0.10265788831992197, 0.0875396244818337, 0.12265301146061935, 0.12289685442574981, 0.14825652279931725, 0.10241404535479151, 0.10338941721531333, 0.08729578151670324]
Actions to choose Agent 1: dict_values([{'num_count': 732, 'sum_payoffs': 205.35884228381028, 'action': [1.0, -1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 177.406803814969, 'action': [0.0, 0.0]}, {'num_count': 821, 'sum_payoffs': 237.23572992163935, 'action': [1.0, 0.0]}, {'num_count': 582, 'sum_payoffs': 152.59087844842594, 'action': [0.0, -1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 203.55409248511182, 'action': [1.0, 1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 153.59037869813437, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17849305047549377, 0.15922945623018775, 0.20019507437210438, 0.14191660570592537, 0.1772738356498415, 0.14264813460131676]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.613795757293701 s
