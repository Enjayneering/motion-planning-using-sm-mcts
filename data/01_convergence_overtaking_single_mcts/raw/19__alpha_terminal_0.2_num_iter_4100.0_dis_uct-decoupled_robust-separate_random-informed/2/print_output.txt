Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 605, 'sum_payoffs': 155.58825657044386, 'action': [2.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 121.25850116401068, 'action': [2.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 120.94843880393081, 'action': [2.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 95.02835537173675, 'action': [1.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 121.92693506290874, 'action': [1.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 74.51687198641447, 'action': [0.0, -1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 95.95651448044534, 'action': [0.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 94.07513632923583, 'action': [1.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 73.9084805300702, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14752499390392587, 0.12265301146061935, 0.1224091684954889, 0.10314557425018288, 0.12314069739088028, 0.0875396244818337, 0.10387710314557425, 0.10241404535479151, 0.08705193855157278]
Actions to choose Agent 1: dict_values([{'num_count': 819, 'sum_payoffs': 236.5521586639185, 'action': [1.0, 0.0]}, {'num_count': 582, 'sum_payoffs': 152.58501181651815, 'action': [0.0, 1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 153.67435844737258, 'action': [0.0, -1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 201.415813798703, 'action': [1.0, 1.5707963267948966]}, {'num_count': 729, 'sum_payoffs': 204.33044343993268, 'action': [1.0, -1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 181.2687858667918, 'action': [0.0, 0.0]}])
Weights num count: [0.19970738844184346, 0.14191660570592537, 0.14264813460131676, 0.17581077785905877, 0.17776152158010242, 0.16191172884662278]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.0978100299835205 s
