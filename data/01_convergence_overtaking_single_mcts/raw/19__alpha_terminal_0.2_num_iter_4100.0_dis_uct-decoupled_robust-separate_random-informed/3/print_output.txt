Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 359, 'sum_payoffs': 74.67331550376, 'action': [0.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 96.70306149667391, 'action': [0.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 93.16254914471939, 'action': [1.0, -1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 121.65547658928257, 'action': [2.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 74.04167480246976, 'action': [0.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 121.3706914456796, 'action': [1.0, 0.0]}, {'num_count': 617, 'sum_payoffs': 159.82508742970018, 'action': [2.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 93.82308844017892, 'action': [1.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 119.03635136970593, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.0875396244818337, 0.10436478907583516, 0.10168251645940014, 0.12289685442574981, 0.08705193855157278, 0.12265301146061935, 0.15045110948549134, 0.10217020238966106, 0.12094611070470616]
Actions to choose Agent 1: dict_values([{'num_count': 719, 'sum_payoffs': 200.72050927716663, 'action': [1.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 178.8265287348955, 'action': [0.0, 0.0]}, {'num_count': 588, 'sum_payoffs': 154.6275774898734, 'action': [0.0, 1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 203.94498399579135, 'action': [1.0, 1.5707963267948966]}, {'num_count': 823, 'sum_payoffs': 237.95450097080644, 'action': [1.0, 0.0]}, {'num_count': 585, 'sum_payoffs': 153.67729176332642, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17532309192879786, 0.16020482809070957, 0.1433796634967081, 0.17751767861497195, 0.2006827603023653, 0.14264813460131676]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.064091205596924 s
