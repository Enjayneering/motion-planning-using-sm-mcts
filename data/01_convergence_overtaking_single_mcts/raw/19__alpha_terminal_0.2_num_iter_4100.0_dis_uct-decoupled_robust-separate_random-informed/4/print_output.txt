Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 502, 'sum_payoffs': 121.01796925608444, 'action': [2.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 74.6037850516064, 'action': [0.0, -1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 157.6511381727914, 'action': [2.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 74.48797339224531, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 94.38519868931564, 'action': [1.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 93.14516653168097, 'action': [1.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 122.31601588474209, 'action': [2.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 120.94481742623863, 'action': [1.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 95.0554432770596, 'action': [0.0, 0.0]}])
Weights num count: [0.1224091684954889, 0.0875396244818337, 0.1489880516947086, 0.0875396244818337, 0.10265788831992197, 0.10168251645940014, 0.12338454035601074, 0.1224091684954889, 0.10314557425018288]
Actions to choose Agent 1: dict_values([{'num_count': 736, 'sum_payoffs': 206.86548026892856, 'action': [1.0, -1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 236.23644695461562, 'action': [1.0, 0.0]}, {'num_count': 660, 'sum_payoffs': 179.92807940861888, 'action': [0.0, 0.0]}, {'num_count': 726, 'sum_payoffs': 203.2614127380704, 'action': [1.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 151.90415659206647, 'action': [0.0, -1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 151.94207241678174, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1794684223360156, 0.199463545476713, 0.16093635698610095, 0.17702999268471104, 0.14142891977566446, 0.14142891977566446]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.063143253326416 s
