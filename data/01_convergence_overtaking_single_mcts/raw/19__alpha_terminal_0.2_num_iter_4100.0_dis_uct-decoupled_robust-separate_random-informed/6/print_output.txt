Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 612, 'sum_payoffs': 157.90590209511743, 'action': [2.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 73.92586314310856, 'action': [0.0, 1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 122.51906653332239, 'action': [1.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 74.22419223937298, 'action': [0.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 94.95295828767549, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 120.61534448158962, 'action': [2.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 95.02835537173652, 'action': [0.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 94.3678160762773, 'action': [1.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 119.54631377972744, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.14923189465983908, 0.08705193855157278, 0.12362838332114119, 0.08729578151670324, 0.10314557425018288, 0.12216532553035844, 0.10314557425018288, 0.10265788831992197, 0.12143379663496708]
Actions to choose Agent 1: dict_values([{'num_count': 830, 'sum_payoffs': 240.46809924370243, 'action': [1.0, 0.0]}, {'num_count': 727, 'sum_payoffs': 203.56854178219635, 'action': [1.0, 1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 152.88920754469032, 'action': [0.0, 1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 152.22280161732326, 'action': [0.0, -1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 203.5367098720735, 'action': [1.0, -1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 177.0831975022277, 'action': [0.0, 0.0]}])
Weights num count: [0.20238966105827846, 0.1772738356498415, 0.14216044867105584, 0.14167276274079493, 0.1772738356498415, 0.1589856132650573]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.054098844528198 s
