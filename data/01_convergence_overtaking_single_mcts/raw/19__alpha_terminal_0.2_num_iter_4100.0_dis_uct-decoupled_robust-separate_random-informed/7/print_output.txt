Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 357, 'sum_payoffs': 74.01277620830068, 'action': [0.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 74.56901982552964, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 94.48949436754611, 'action': [1.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 97.04111710649978, 'action': [0.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 119.39269493699325, 'action': [2.0, -1.5707963267948966]}, {'num_count': 609, 'sum_payoffs': 157.09105589518543, 'action': [2.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 121.32803161616431, 'action': [2.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 123.68945959746114, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 92.53677507533669, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.08705193855157278, 0.0875396244818337, 0.10265788831992197, 0.10460863204096561, 0.12118995366983662, 0.1485003657644477, 0.12265301146061935, 0.12435991221653256, 0.10119483052913923]
Actions to choose Agent 1: dict_values([{'num_count': 583, 'sum_payoffs': 152.92983940267482, 'action': [0.0, 1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 152.19390302315398, 'action': [0.0, -1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 201.45644565668752, 'action': [1.0, -1.5707963267948966]}, {'num_count': 824, 'sum_payoffs': 238.31084453809396, 'action': [1.0, 0.0]}, {'num_count': 730, 'sum_payoffs': 204.64322183328147, 'action': [1.0, 1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 180.2266257875679, 'action': [0.0, 0.0]}])
Weights num count: [0.14216044867105584, 0.14167276274079493, 0.17581077785905877, 0.20092660326749573, 0.17800536454523286, 0.1611801999512314]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.982138633728027 s
