Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 497, 'sum_payoffs': 119.2883992587628, 'action': [2.0, 1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 122.64911020708333, 'action': [2.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 124.00719927829475, 'action': [1.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 94.33891748210812, 'action': [1.0, -1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 94.67201180444944, 'action': [1.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 73.97214435031603, 'action': [0.0, -1.5707963267948966]}, {'num_count': 606, 'sum_payoffs': 155.978460019385, 'action': [2.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 74.29372269152665, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 94.3736827081847, 'action': [0.0, 0.0]}])
Weights num count: [0.12118995366983662, 0.12362838332114119, 0.12460375518166301, 0.10265788831992197, 0.10290173128505242, 0.08705193855157278, 0.14776883686905634, 0.08729578151670324, 0.10265788831992197]
Actions to choose Agent 1: dict_values([{'num_count': 584, 'sum_payoffs': 153.29204960186993, 'action': [0.0, -1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 152.593594481695, 'action': [0.0, 1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 180.56174808143984, 'action': [0.0, 0.0]}, {'num_count': 823, 'sum_payoffs': 237.96985561231415, 'action': [1.0, 0.0]}, {'num_count': 719, 'sum_payoffs': 200.73224254098187, 'action': [1.0, -1.5707963267948966]}, {'num_count': 730, 'sum_payoffs': 204.68972032317376, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1424042916361863, 0.14191660570592537, 0.16142404291636187, 0.2006827603023653, 0.17532309192879786, 0.17800536454523286]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.0985119342803955 s
