Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 604, 'sum_payoffs': 155.39038449203304, 'action': [2.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 94.10990155531266, 'action': [1.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 121.0644677459767, 'action': [2.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 73.69119786709007, 'action': [0.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 74.36911977558793, 'action': [0.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 119.73752252314992, 'action': [2.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 95.16741627604408, 'action': [1.0, 1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 124.43893992964352, 'action': [1.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 96.47900685736298, 'action': [0.0, 0.0]}])
Weights num count: [0.1472811509387954, 0.10241404535479151, 0.1224091684954889, 0.08680809558644233, 0.08729578151670324, 0.12143379663496708, 0.10314557425018288, 0.12484759814679347, 0.1041209461107047]
Actions to choose Agent 1: dict_values([{'num_count': 815, 'sum_payoffs': 235.007134075031, 'action': [1.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 153.28324965400836, 'action': [0.0, 1.5707963267948966]}, {'num_count': 730, 'sum_payoffs': 204.58835796089718, 'action': [1.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 178.77416361309562, 'action': [0.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 204.2317971109251, 'action': [1.0, 1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 153.57864543431893, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.19873201658132164, 0.1424042916361863, 0.17800536454523286, 0.16020482809070957, 0.17776152158010242, 0.14264813460131676]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.014401435852051 s
