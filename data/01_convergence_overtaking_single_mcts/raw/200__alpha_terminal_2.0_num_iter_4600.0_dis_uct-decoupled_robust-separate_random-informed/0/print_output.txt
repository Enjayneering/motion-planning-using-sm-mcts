Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 524, 'sum_payoffs': 116.150432385251, 'action': [2.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 110.27290267346976, 'action': [1.0, 1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 124.64891439390253, 'action': [2.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 110.34969100230032, 'action': [1.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 117.76016703286534, 'action': [2.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 110.34939678842066, 'action': [1.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 108.78843201450354, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 106.60856848480555, 'action': [0.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 105.07895333474744, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.113888285155401, 0.10975874809823952, 0.11975657465768311, 0.10975874809823952, 0.1149750054336014, 0.10975874809823952, 0.10867202782003912, 0.10715061943055858, 0.10606389915235818]
Actions to choose Agent 1: dict_values([{'num_count': 785, 'sum_payoffs': 157.25221006152796, 'action': [1.0, 1.5707963267948966]}, {'num_count': 757, 'sum_payoffs': 149.60348559546435, 'action': [0.0, 0.0]}, {'num_count': 740, 'sum_payoffs': 145.0647053951948, 'action': [0.0, 1.5707963267948966]}, {'num_count': 773, 'sum_payoffs': 154.03731812488533, 'action': [1.0, -1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 161.42515123928956, 'action': [1.0, 0.0]}, {'num_count': 745, 'sum_payoffs': 146.32265082699618, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17061508367746142, 0.16452945011953923, 0.1608346011736579, 0.1680069550097805, 0.1738752445120626, 0.16192132145185828]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 132.62622952461243 s
