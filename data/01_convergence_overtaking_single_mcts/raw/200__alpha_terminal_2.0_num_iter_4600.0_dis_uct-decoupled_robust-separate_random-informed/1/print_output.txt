Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 518, 'sum_payoffs': 114.54015196873881, 'action': [1.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 122.0296704969167, 'action': [2.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 113.54040697376351, 'action': [2.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 114.80044045435424, 'action': [1.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 106.11766959190018, 'action': [0.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 104.3015133743686, 'action': [0.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 108.2530341267962, 'action': [0.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 112.2810062682242, 'action': [1.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 115.43099215610151, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11258422082156053, 0.1178004781569224, 0.1119321886546403, 0.11280156487720061, 0.10671593131927842, 0.10541186698543795, 0.10823733970875897, 0.11106281243207998, 0.11323625298848077]
Actions to choose Agent 1: dict_values([{'num_count': 781, 'sum_payoffs': 156.168206447155, 'action': [1.0, 1.5707963267948966]}, {'num_count': 752, 'sum_payoffs': 148.25779535044884, 'action': [0.0, 0.0]}, {'num_count': 734, 'sum_payoffs': 143.42395224572198, 'action': [0.0, -1.5707963267948966]}, {'num_count': 749, 'sum_payoffs': 147.46467865416338, 'action': [0.0, 1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 159.40485172631944, 'action': [1.0, 0.0]}, {'num_count': 791, 'sum_payoffs': 158.91250670676683, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1697457074549011, 0.16344272984133884, 0.15953053683981744, 0.16279069767441862, 0.17235383612258204, 0.1719191480113019]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 131.80718636512756 s
