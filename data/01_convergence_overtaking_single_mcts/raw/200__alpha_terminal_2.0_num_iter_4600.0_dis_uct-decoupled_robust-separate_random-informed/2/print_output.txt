Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 489, 'sum_payoffs': 105.21469708147649, 'action': [0.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 106.11583940119255, 'action': [0.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 108.84480307863609, 'action': [0.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 112.61599436322636, 'action': [1.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 111.38538246344102, 'action': [1.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 120.69077374940734, 'action': [2.0, 0.0]}, {'num_count': 521, 'sum_payoffs': 115.03521577200007, 'action': [2.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 115.02777631035302, 'action': [2.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 113.13109830782665, 'action': [1.0, 0.0]}])
Weights num count: [0.10628124320799825, 0.1069332753749185, 0.1088893718756792, 0.11149750054336013, 0.11062812432079983, 0.11714844599000217, 0.11323625298848077, 0.11323625298848077, 0.1119321886546403]
Actions to choose Agent 1: dict_values([{'num_count': 781, 'sum_payoffs': 156.65467365657452, 'action': [1.0, 1.5707963267948966]}, {'num_count': 740, 'sum_payoffs': 145.40627774143934, 'action': [0.0, -1.5707963267948966]}, {'num_count': 742, 'sum_payoffs': 145.98525621428442, 'action': [0.0, 1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 148.4076857505623, 'action': [0.0, 0.0]}, {'num_count': 794, 'sum_payoffs': 160.18426864636172, 'action': [1.0, -1.5707963267948966]}, {'num_count': 792, 'sum_payoffs': 159.63470826048075, 'action': [1.0, 0.0]}])
Weights num count: [0.1697457074549011, 0.1608346011736579, 0.16126928928493806, 0.16322538578569876, 0.17257118017822212, 0.17213649206694198]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 134.3986155986786 s
