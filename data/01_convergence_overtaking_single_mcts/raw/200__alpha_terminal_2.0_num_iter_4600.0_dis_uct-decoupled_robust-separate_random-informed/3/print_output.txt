Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 522, 'sum_payoffs': 115.62870925656598, 'action': [2.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 115.33076860997423, 'action': [1.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 107.91159768669648, 'action': [0.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 111.0083449686453, 'action': [1.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 112.81726908907693, 'action': [2.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 109.47973833588595, 'action': [0.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 108.21662003978982, 'action': [1.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 120.37265133506958, 'action': [2.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 109.64812126895353, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11345359704412085, 0.11323625298848077, 0.10801999565311889, 0.11019343620951967, 0.11149750054336013, 0.10910671593131928, 0.10823733970875897, 0.11671375787872201, 0.10932405998695936]
Actions to choose Agent 1: dict_values([{'num_count': 753, 'sum_payoffs': 148.27699521662043, 'action': [0.0, 0.0]}, {'num_count': 747, 'sum_payoffs': 146.58018210711072, 'action': [0.0, 1.5707963267948966]}, {'num_count': 771, 'sum_payoffs': 153.2013332467329, 'action': [1.0, 1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 144.5302575165799, 'action': [0.0, -1.5707963267948966]}, {'num_count': 788, 'sum_payoffs': 157.72035360177836, 'action': [1.0, -1.5707963267948966]}, {'num_count': 802, 'sum_payoffs': 161.5889274601934, 'action': [1.0, 0.0]}])
Weights num count: [0.16366007389697892, 0.16235600956313845, 0.16757226689850033, 0.16061725711801783, 0.17126711584438165, 0.17430993262334274]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 143.64346504211426 s
