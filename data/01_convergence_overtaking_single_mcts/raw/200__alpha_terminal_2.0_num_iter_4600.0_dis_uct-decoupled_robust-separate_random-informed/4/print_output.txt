Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 496, 'sum_payoffs': 107.49895066422111, 'action': [0.0, 1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 124.05444720224041, 'action': [2.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 113.77122956604875, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 107.16538077489629, 'action': [0.0, 0.0]}, {'num_count': 512, 'sum_payoffs': 112.44834740400435, 'action': [2.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 108.16446482028208, 'action': [0.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 113.95997002401741, 'action': [2.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 112.37490632594233, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 110.33107559693251, 'action': [1.0, 0.0]}])
Weights num count: [0.10780265159747882, 0.11932188654640295, 0.11214953271028037, 0.10758530754183873, 0.11128015648772006, 0.10823733970875897, 0.11236687676592046, 0.11128015648772006, 0.10975874809823952]
Actions to choose Agent 1: dict_values([{'num_count': 754, 'sum_payoffs': 148.85623918846431, 'action': [0.0, 0.0]}, {'num_count': 777, 'sum_payoffs': 155.0694204153204, 'action': [1.0, -1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 146.37544824810763, 'action': [0.0, -1.5707963267948966]}, {'num_count': 782, 'sum_payoffs': 156.50974502601105, 'action': [1.0, 1.5707963267948966]}, {'num_count': 749, 'sum_payoffs': 147.43285386432942, 'action': [0.0, 1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 159.5253472326446, 'action': [1.0, 0.0]}])
Weights num count: [0.163877417952619, 0.1688763312323408, 0.16192132145185828, 0.1699630515105412, 0.16279069767441862, 0.17235383612258204]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 142.0344798564911 s
