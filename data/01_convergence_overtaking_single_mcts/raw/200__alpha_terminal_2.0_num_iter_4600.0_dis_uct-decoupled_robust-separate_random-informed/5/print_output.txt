Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 543, 'sum_payoffs': 121.74669307541693, 'action': [2.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 115.25272316342834, 'action': [2.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 107.1873280913904, 'action': [0.0, 0.0]}, {'num_count': 510, 'sum_payoffs': 111.50548516936152, 'action': [1.0, 1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 112.195918046788, 'action': [1.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 114.26541834116222, 'action': [2.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 106.00183426076333, 'action': [0.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 106.2988476687875, 'action': [0.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 112.48182502127672, 'action': [1.0, 0.0]}])
Weights num count: [0.11801782221256249, 0.11345359704412085, 0.10780265159747882, 0.11084546837643991, 0.11128015648772006, 0.11280156487720061, 0.1069332753749185, 0.10715061943055858, 0.11149750054336013]
Actions to choose Agent 1: dict_values([{'num_count': 776, 'sum_payoffs': 155.10248759632307, 'action': [1.0, -1.5707963267948966]}, {'num_count': 781, 'sum_payoffs': 156.4737825679078, 'action': [1.0, 1.5707963267948966]}, {'num_count': 804, 'sum_payoffs': 162.75149112831065, 'action': [1.0, 0.0]}, {'num_count': 740, 'sum_payoffs': 145.2695338506495, 'action': [0.0, 1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 146.38278224473714, 'action': [0.0, -1.5707963267948966]}, {'num_count': 755, 'sum_payoffs': 149.39705159451685, 'action': [0.0, 0.0]}])
Weights num count: [0.16865898717670072, 0.1697457074549011, 0.1747446207346229, 0.1608346011736579, 0.16170397739621822, 0.16409476200825907]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 143.6073760986328 s
