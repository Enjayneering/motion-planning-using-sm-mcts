Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 491, 'sum_payoffs': 106.10114082010182, 'action': [0.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 106.40912147372278, 'action': [0.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 107.94317490005376, 'action': [0.0, 0.0]}, {'num_count': 510, 'sum_payoffs': 111.94200426977189, 'action': [1.0, -1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 115.0487545989231, 'action': [2.0, 1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 116.64009038563972, 'action': [1.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 121.85554944260839, 'action': [2.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 115.02392822371894, 'action': [2.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 109.7836671029134, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10671593131927842, 0.1069332753749185, 0.10801999565311889, 0.11084546837643991, 0.11301890893284068, 0.11410562921104107, 0.1178004781569224, 0.11301890893284068, 0.10932405998695936]
Actions to choose Agent 1: dict_values([{'num_count': 751, 'sum_payoffs': 148.18148653287238, 'action': [0.0, 0.0]}, {'num_count': 732, 'sum_payoffs': 143.0298659956492, 'action': [0.0, 1.5707963267948966]}, {'num_count': 803, 'sum_payoffs': 162.37220409721152, 'action': [1.0, 0.0]}, {'num_count': 746, 'sum_payoffs': 146.74130163629403, 'action': [0.0, -1.5707963267948966]}, {'num_count': 773, 'sum_payoffs': 154.17554448386272, 'action': [1.0, 1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 160.2378046015187, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16322538578569876, 0.15909584872853727, 0.17452727667898282, 0.16213866550749836, 0.1680069550097805, 0.1727885242338622]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 132.05714058876038 s
