Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 480, 'sum_payoffs': 102.86097900890539, 'action': [0.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 118.68729638143886, 'action': [2.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 111.26471442584412, 'action': [1.0, 1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 124.37793742588255, 'action': [2.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 110.98756943028732, 'action': [1.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 106.8638769853446, 'action': [0.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 111.29345764242905, 'action': [0.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 113.76732471085187, 'action': [2.0, 1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 112.75438169087434, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10432514670723755, 0.11540969354488155, 0.11019343620951967, 0.11932188654640295, 0.10997609215387959, 0.10715061943055858, 0.11019343620951967, 0.1119321886546403, 0.11128015648772006]
Actions to choose Agent 1: dict_values([{'num_count': 787, 'sum_payoffs': 157.59191606441368, 'action': [1.0, 1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 144.2623997453068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 750, 'sum_payoffs': 147.5473754259512, 'action': [0.0, 0.0]}, {'num_count': 780, 'sum_payoffs': 155.7093477476995, 'action': [1.0, -1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 159.8031781034309, 'action': [1.0, 0.0]}, {'num_count': 750, 'sum_payoffs': 147.5433118593619, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1710497717887416, 0.16039991306237775, 0.16300804173005867, 0.16952836339926103, 0.1727885242338622, 0.16300804173005867]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 131.33269715309143 s
