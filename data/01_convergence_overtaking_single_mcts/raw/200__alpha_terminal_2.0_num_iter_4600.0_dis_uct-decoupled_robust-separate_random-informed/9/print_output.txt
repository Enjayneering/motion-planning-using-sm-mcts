Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 496, 'sum_payoffs': 107.15178462340269, 'action': [0.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 108.68000510890982, 'action': [0.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 113.11088713848565, 'action': [1.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 108.09738847150133, 'action': [1.0, 1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 126.79251416483422, 'action': [2.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 107.15500677402066, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 108.12665140676336, 'action': [1.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 113.1022352132459, 'action': [2.0, -1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 114.59265059797715, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10780265159747882, 0.1088893718756792, 0.1119321886546403, 0.10845468376439904, 0.12149532710280374, 0.10780265159747882, 0.10845468376439904, 0.1119321886546403, 0.11301890893284068]
Actions to choose Agent 1: dict_values([{'num_count': 750, 'sum_payoffs': 148.09730610473267, 'action': [0.0, 1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 160.95813704264927, 'action': [1.0, 0.0]}, {'num_count': 747, 'sum_payoffs': 147.1853508018959, 'action': [0.0, -1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 146.4488876616739, 'action': [0.0, 0.0]}, {'num_count': 782, 'sum_payoffs': 156.82301571699017, 'action': [1.0, -1.5707963267948966]}, {'num_count': 780, 'sum_payoffs': 156.20046392936726, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16300804173005867, 0.17322321234514235, 0.16235600956313845, 0.16170397739621822, 0.1699630515105412, 0.16952836339926103]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 131.5708417892456 s
