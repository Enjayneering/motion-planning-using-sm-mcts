Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 130, 'sum_payoffs': 31.077861031443472, 'action': [2.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 27.87243593306426, 'action': [1.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 25.87655542404854, 'action': [1.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 27.459861346113154, 'action': [0.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 26.94995078278849, 'action': [0.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 27.84946709168664, 'action': [1.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 29.39997275903188, 'action': [2.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 27.80996045753633, 'action': [2.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 26.955393318483726, 'action': [0.0, 0.0]}])
Weights num count: [0.11807447774750227, 0.11080835603996367, 0.10626702997275204, 0.10990009082652134, 0.10899182561307902, 0.11080835603996367, 0.11444141689373297, 0.11080835603996367, 0.10899182561307902]
Actions to choose Agent 1: dict_values([{'num_count': 189, 'sum_payoffs': 39.74687147472533, 'action': [1.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 34.30360842337944, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 34.620975876216306, 'action': [0.0, 1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 38.76493544774765, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 40.160189494881756, 'action': [1.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 39.3515243904077, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17166212534059946, 0.15712988192552224, 0.15803814713896458, 0.16893732970027248, 0.17257039055404177, 0.17075386012715713]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.77013444900513 s
