Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 173, 'sum_payoffs': 38.73325381152387, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 41.328813470762846, 'action': [1.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 41.78860488069187, 'action': [2.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 37.743701371638366, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 42.16474868042198, 'action': [2.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 42.08769609470273, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 39.848651601703054, 'action': [1.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 41.310654492213985, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 39.95886089195637, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1080574640849469, 0.11242973141786383, 0.11305434103685197, 0.1061836352279825, 0.1136789506558401, 0.1136789506558401, 0.1099312929419113, 0.11242973141786383, 0.1099312929419113]
Actions to choose Agent 1: dict_values([{'num_count': 268, 'sum_payoffs': 55.07494932365388, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 56.01177532405434, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 55.3998718011325, 'action': [1.0, 1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 52.46560949067455, 'action': [0.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 52.71375248210459, 'action': [0.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 56.015595412894065, 'action': [1.0, 0.0]}])
Weights num count: [0.16739537788881947, 0.16926920674578388, 0.1680199875078076, 0.16239850093691444, 0.16302311055590257, 0.16926920674578388]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.489624977111816 s
