Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 184, 'sum_payoffs': 42.83924202298652, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 39.89161920633637, 'action': [2.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.61831884899533, 'action': [1.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 37.643712307557266, 'action': [0.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 39.930022017457226, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 39.905237956861384, 'action': [0.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 38.85814986396665, 'action': [0.0, 1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 43.962022360272755, 'action': [2.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 41.45326033832267, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11492816989381636, 0.1099312929419113, 0.11118051217988757, 0.1061836352279825, 0.1099312929419113, 0.1099312929419113, 0.1080574640849469, 0.11680199875078076, 0.11242973141786383]
Actions to choose Agent 1: dict_values([{'num_count': 268, 'sum_payoffs': 55.214239380875064, 'action': [1.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 56.5972798596771, 'action': [1.0, -1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 51.653062353409084, 'action': [0.0, 1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 53.615302930013975, 'action': [0.0, 0.0]}, {'num_count': 268, 'sum_payoffs': 55.245259807433435, 'action': [0.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 56.59548351638476, 'action': [1.0, 0.0]}])
Weights num count: [0.16739537788881947, 0.16989381636477202, 0.16052467207995003, 0.16427232979387882, 0.16739537788881947, 0.16989381636477202]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.63497972488403 s
