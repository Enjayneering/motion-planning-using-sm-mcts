Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 175, 'sum_payoffs': 39.43316083839718, 'action': [0.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.56820198719811, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 39.075978172378264, 'action': [1.0, -1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 44.72391533259747, 'action': [2.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 40.151933064379605, 'action': [0.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 38.732018377627504, 'action': [1.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.597955447617544, 'action': [1.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 38.04684192598574, 'action': [2.0, 1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 43.138843082209135, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10930668332292318, 0.11118051217988757, 0.10868207370393504, 0.11805121798875702, 0.11055590256089944, 0.1080574640849469, 0.11118051217988757, 0.10680824484697064, 0.1155527795128045]
Actions to choose Agent 1: dict_values([{'num_count': 270, 'sum_payoffs': 56.173773988575824, 'action': [1.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 53.15928895924124, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 58.36991742082528, 'action': [1.0, 1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 53.24207106811356, 'action': [0.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 57.414317343173074, 'action': [1.0, 0.0]}, {'num_count': 257, 'sum_payoffs': 51.986349374465014, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16864459712679575, 0.16302311055590257, 0.17301686445971268, 0.16302311055590257, 0.1711430356027483, 0.16052467207995003]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.657450675964355 s
