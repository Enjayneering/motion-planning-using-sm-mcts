Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 172, 'sum_payoffs': 38.107993655515195, 'action': [1.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 40.70883633772511, 'action': [0.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 39.61778166477649, 'action': [0.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 38.819252480073025, 'action': [2.0, 1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 42.27884707126634, 'action': [1.0, -1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 42.25624492494972, 'action': [2.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 38.125452263651454, 'action': [0.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 41.04377130830647, 'action': [1.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 41.45991994098279, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10743285446595878, 0.11180512179887571, 0.1099312929419113, 0.10868207370393504, 0.11430356027482823, 0.11430356027482823, 0.10743285446595878, 0.11242973141786383, 0.11305434103685197]
Actions to choose Agent 1: dict_values([{'num_count': 258, 'sum_payoffs': 52.18852197966576, 'action': [0.0, -1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 50.51800357139251, 'action': [0.0, 0.0]}, {'num_count': 269, 'sum_payoffs': 55.73920375439971, 'action': [0.0, 1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 55.74706158020014, 'action': [1.0, -1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 57.64236301653858, 'action': [1.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 57.87281342844895, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16114928169893816, 0.1580262336039975, 0.1680199875078076, 0.1680199875078076, 0.1717676452217364, 0.17239225484072454]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 47.373100996017456 s
