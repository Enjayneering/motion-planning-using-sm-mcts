Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 186, 'sum_payoffs': 43.59546130850672, 'action': [2.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 38.79078718257408, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 39.162877231525556, 'action': [0.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 40.33209684723542, 'action': [1.0, 1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 42.45396277946394, 'action': [2.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 40.27011431146311, 'action': [1.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 41.44904545618059, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 39.9711476321621, 'action': [2.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 39.10725535116341, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11617738913179262, 0.1080574640849469, 0.10868207370393504, 0.11055590256089944, 0.11430356027482823, 0.11055590256089944, 0.11242973141786383, 0.1099312929419113, 0.10868207370393504]
Actions to choose Agent 1: dict_values([{'num_count': 276, 'sum_payoffs': 58.06008053774225, 'action': [1.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 54.85367314751489, 'action': [1.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 58.43268534689309, 'action': [1.0, -1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 54.175036385742956, 'action': [0.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 52.62944166637029, 'action': [0.0, 0.0]}, {'num_count': 258, 'sum_payoffs': 52.21641623031054, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17239225484072454, 0.16614615865084323, 0.17301686445971268, 0.16489693941286696, 0.1617738913179263, 0.16114928169893816]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.421101570129395 s
