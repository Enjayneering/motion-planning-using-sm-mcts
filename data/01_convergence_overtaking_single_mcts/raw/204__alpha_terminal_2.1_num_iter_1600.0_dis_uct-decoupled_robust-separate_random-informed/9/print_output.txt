Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 182, 'sum_payoffs': 41.856023337503984, 'action': [2.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 40.22321056173847, 'action': [2.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 38.51852316867686, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 41.09093932817625, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 39.47019020191036, 'action': [0.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.34805134233401, 'action': [1.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 38.88121409725378, 'action': [0.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 41.49091795203538, 'action': [1.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 40.34050555608338, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1136789506558401, 0.11118051217988757, 0.1080574640849469, 0.11242973141786383, 0.1099312929419113, 0.11118051217988757, 0.10868207370393504, 0.11305434103685197, 0.11118051217988757]
Actions to choose Agent 1: dict_values([{'num_count': 271, 'sum_payoffs': 56.56999114244914, 'action': [1.0, 0.0]}, {'num_count': 264, 'sum_payoffs': 54.3151523766591, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 56.661701586685645, 'action': [1.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 51.99083175017319, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 59.231603533060216, 'action': [1.0, -1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 52.353589331654476, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16926920674578388, 0.16489693941286696, 0.16926920674578388, 0.16052467207995003, 0.17426608369768895, 0.16114928169893816]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.651663064956665 s
