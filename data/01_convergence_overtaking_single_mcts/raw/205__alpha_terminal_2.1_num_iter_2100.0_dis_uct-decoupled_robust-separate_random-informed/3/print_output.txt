Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 240, 'sum_payoffs': 55.284573523307834, 'action': [2.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 53.135614347500734, 'action': [0.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 49.93415930160668, 'action': [1.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 52.11771730789223, 'action': [2.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 52.36273039263866, 'action': [0.0, 0.0]}, {'num_count': 227, 'sum_payoffs': 50.70153681305257, 'action': [0.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 55.02421853794508, 'action': [1.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 53.898687758935154, 'action': [2.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 53.94891027064061, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1142313184198001, 0.11137553545930509, 0.10709186101856259, 0.1099476439790576, 0.11042360780580676, 0.10804378867206092, 0.11375535459305093, 0.11232746311280342, 0.11232746311280342]
Actions to choose Agent 1: dict_values([{'num_count': 343, 'sum_payoffs': 69.89911385802489, 'action': [0.0, -1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 76.18856161873576, 'action': [1.0, 0.0]}, {'num_count': 330, 'sum_payoffs': 65.93883846531106, 'action': [0.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 69.88179582247669, 'action': [0.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 73.91102006967625, 'action': [1.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 76.70671487236014, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1632555925749643, 0.17277486910994763, 0.15706806282722513, 0.1632555925749643, 0.16944312232270348, 0.173726796763446]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 62.22337317466736 s
