Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 293, 'sum_payoffs': 66.85559461600594, 'action': [2.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 66.55776437142192, 'action': [1.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 66.19050503356306, 'action': [0.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 65.22841037074149, 'action': [2.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 65.82401068593236, 'action': [1.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 62.48271890446744, 'action': [0.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 68.91794143758219, 'action': [2.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 63.7998871935295, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 63.47221703666322, 'action': [0.0, 0.0]}])
Weights num count: [0.11264898116109189, 0.1122645136485967, 0.1118800461361015, 0.11072664359861592, 0.1114955786236063, 0.10765090349865436, 0.11495578623606305, 0.10918877354863514, 0.10880430603613994]
Actions to choose Agent 1: dict_values([{'num_count': 414, 'sum_payoffs': 82.94873821818635, 'action': [0.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 91.94999885992198, 'action': [1.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 95.29647744963157, 'action': [1.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 91.56276270182092, 'action': [1.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 84.4329837590596, 'action': [0.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 86.2409100860945, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15916955017301038, 0.1707035755478662, 0.17493271818531334, 0.170319108035371, 0.16109188773548636, 0.16339869281045752]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 79.27209234237671 s
