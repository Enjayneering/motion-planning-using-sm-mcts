Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 278, 'sum_payoffs': 61.65086971166519, 'action': [0.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 64.79729170330877, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 62.065562834531576, 'action': [0.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 63.430678741127004, 'action': [1.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 68.17032366204184, 'action': [2.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 67.90951785584804, 'action': [2.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 68.80411765224923, 'action': [2.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 68.13384672701281, 'action': [1.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 63.74166808552052, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10688196847366398, 0.11034217608612072, 0.10726643598615918, 0.10880430603613994, 0.11418685121107267, 0.11380238369857747, 0.11495578623606305, 0.11418685121107267, 0.10918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 426, 'sum_payoffs': 87.1270952771945, 'action': [0.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 93.3935481343212, 'action': [1.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 89.26501161096202, 'action': [1.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 94.96642588807562, 'action': [1.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 87.1705625507087, 'action': [0.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 84.05565343262727, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1637831603229527, 0.17185697808535177, 0.16647443291041908, 0.17377931564782775, 0.1637831603229527, 0.15993848519800077]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 83.34787201881409 s
