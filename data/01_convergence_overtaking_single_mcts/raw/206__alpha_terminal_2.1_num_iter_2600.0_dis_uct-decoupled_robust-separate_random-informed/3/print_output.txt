Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 286, 'sum_payoffs': 64.66020160289969, 'action': [0.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 63.066999143242136, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 64.09409023265574, 'action': [1.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 66.44604583386968, 'action': [2.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 66.11618185888382, 'action': [1.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 67.74217791290825, 'action': [1.0, -1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 69.96215094395014, 'action': [2.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 66.72794718896603, 'action': [2.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 62.67924801073927, 'action': [0.0, 0.0]}])
Weights num count: [0.10995770857362552, 0.10803537101114956, 0.10918877354863514, 0.1118800461361015, 0.1114955786236063, 0.11341791618608228, 0.11572472126105345, 0.1122645136485967, 0.10765090349865436]
Actions to choose Agent 1: dict_values([{'num_count': 439, 'sum_payoffs': 90.56579144034957, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 86.03923490658376, 'action': [0.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 92.09918686344358, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 93.02078429144386, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 84.88934448957805, 'action': [0.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 86.71750812408439, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16878123798539024, 0.16301422529796233, 0.1707035755478662, 0.17185697808535177, 0.16147635524798154, 0.1637831603229527]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 77.99778962135315 s
