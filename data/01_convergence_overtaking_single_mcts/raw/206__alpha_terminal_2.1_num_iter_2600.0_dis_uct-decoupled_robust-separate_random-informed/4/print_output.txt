Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 303, 'sum_payoffs': 70.52452993468218, 'action': [2.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 66.99760818364149, 'action': [0.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 67.35236538442778, 'action': [2.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 61.6190189058967, 'action': [0.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 61.940485380098714, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 63.680414955787526, 'action': [0.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 64.68887080222436, 'action': [1.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 66.05650352296185, 'action': [1.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 68.07894448988955, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11649365628604383, 0.11264898116109189, 0.11303344867358708, 0.10649750096116878, 0.10688196847366398, 0.10880430603613994, 0.10995770857362552, 0.1114955786236063, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 415, 'sum_payoffs': 83.49637487346153, 'action': [0.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 91.87472273022298, 'action': [1.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 94.02851038758277, 'action': [1.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 85.84744150945717, 'action': [0.0, 1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 83.73431608696866, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 94.89099834023322, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15955401768550556, 0.170319108035371, 0.17301038062283736, 0.16262975778546712, 0.15993848519800077, 0.17416378316032297]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 77.35716605186462 s
