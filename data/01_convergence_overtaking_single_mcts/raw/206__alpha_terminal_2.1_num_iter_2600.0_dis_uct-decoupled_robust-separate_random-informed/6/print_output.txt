Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 285, 'sum_payoffs': 64.17309888432051, 'action': [0.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 66.20347134215378, 'action': [1.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 60.38472748600484, 'action': [0.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 67.89494565300241, 'action': [2.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 62.12375769917523, 'action': [0.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 67.45911262402814, 'action': [1.0, -1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 72.32359025476538, 'action': [2.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 62.39535514687649, 'action': [2.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 66.17562670139142, 'action': [1.0, 0.0]}])
Weights num count: [0.10957324106113034, 0.1118800461361015, 0.1053440984236832, 0.11380238369857747, 0.10726643598615918, 0.11341791618608228, 0.11880046136101499, 0.10765090349865436, 0.1118800461361015]
Actions to choose Agent 1: dict_values([{'num_count': 422, 'sum_payoffs': 85.64745127339131, 'action': [0.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 85.32960519798044, 'action': [0.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 87.79759987401293, 'action': [0.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 93.73233973463384, 'action': [1.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 90.41187068956195, 'action': [1.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 91.41236903904891, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16224529027297194, 0.16186082276047675, 0.16493656286043828, 0.17262591311034217, 0.16839677047289503, 0.1695501730103806]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 77.77577495574951 s
