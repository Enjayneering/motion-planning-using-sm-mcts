Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 67.26385923618024, 'action': [2.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 64.13730589505933, 'action': [1.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 65.96514244770403, 'action': [1.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 68.59781673282518, 'action': [1.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 67.87671240578477, 'action': [2.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 63.539566384870064, 'action': [0.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 67.23645890549943, 'action': [2.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 63.154206504625705, 'action': [0.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 61.84931996272909, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11303344867358708, 0.10957324106113034, 0.1114955786236063, 0.11457131872356786, 0.11380238369857747, 0.10880430603613994, 0.11303344867358708, 0.10841983852364476, 0.10688196847366398]
Actions to choose Agent 1: dict_values([{'num_count': 456, 'sum_payoffs': 95.91970552787834, 'action': [1.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 83.79004026419591, 'action': [0.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 93.79324841705, 'action': [1.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 83.20887557083711, 'action': [0.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 86.8531640170252, 'action': [0.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 90.69560057991586, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17531718569780855, 0.15993848519800077, 0.17262591311034217, 0.15916955017301038, 0.1637831603229527, 0.16878123798539024]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 77.34669470787048 s
