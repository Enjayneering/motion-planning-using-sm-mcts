Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 290, 'sum_payoffs': 65.71868136630385, 'action': [1.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 66.7835281318327, 'action': [1.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 61.56335650729465, 'action': [0.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 63.273083493875475, 'action': [0.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 65.71610283982116, 'action': [2.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 65.56775824159172, 'action': [2.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 69.45205400864545, 'action': [2.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 66.03236982516296, 'action': [0.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 63.62144086927594, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1114955786236063, 0.11264898116109189, 0.10688196847366398, 0.10880430603613994, 0.1114955786236063, 0.1114955786236063, 0.11572472126105345, 0.1118800461361015, 0.10918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 430, 'sum_payoffs': 88.301131663979, 'action': [0.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 84.07400868270217, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 91.31879494473284, 'action': [1.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 89.71276992966602, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 84.60135367192952, 'action': [0.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 97.58950460790308, 'action': [1.0, 0.0]}])
Weights num count: [0.1653210303729335, 0.15993848519800077, 0.16916570549788543, 0.16724336793540945, 0.16070742022299117, 0.1772395232602845]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 76.8480064868927 s
