Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 285, 'sum_payoffs': 64.31045426041865, 'action': [1.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 68.50609554029866, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 62.59817291270525, 'action': [0.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 66.1355701125998, 'action': [2.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 73.68370865704183, 'action': [2.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 62.00480696887694, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 62.27841426575221, 'action': [0.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 69.44644413916463, 'action': [1.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 62.325540594538026, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10957324106113034, 0.11418685121107267, 0.10765090349865436, 0.1114955786236063, 0.11995386389850057, 0.10688196847366398, 0.10726643598615918, 0.11534025374855825, 0.10726643598615918]
Actions to choose Agent 1: dict_values([{'num_count': 421, 'sum_payoffs': 84.96453101570381, 'action': [0.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 82.95054255627674, 'action': [0.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 83.59299257121445, 'action': [0.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 93.41755805273974, 'action': [1.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 93.1091371366301, 'action': [1.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 94.34185071749826, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16186082276047675, 0.15916955017301038, 0.15993848519800077, 0.17262591311034217, 0.172241445597847, 0.17377931564782775]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 76.92196869850159 s
