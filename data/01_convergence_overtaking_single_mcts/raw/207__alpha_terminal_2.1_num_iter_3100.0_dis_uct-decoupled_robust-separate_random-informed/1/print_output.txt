Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 351, 'sum_payoffs': 79.85206539610589, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 76.89475460057122, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 77.90049009098571, 'action': [2.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 77.79402773265014, 'action': [1.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 83.59029160108568, 'action': [2.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 74.86731907474382, 'action': [0.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 78.54228567754546, 'action': [1.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 72.91007024028478, 'action': [0.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 76.87563865771912, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11318929377620122, 0.11028700419219607, 0.11125443405353112, 0.11125443405353112, 0.11673653660109642, 0.10835214446952596, 0.11189938729442116, 0.10641728474685586, 0.11028700419219607]
Actions to choose Agent 1: dict_values([{'num_count': 509, 'sum_payoffs': 103.11001264066931, 'action': [0.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 110.4102768684833, 'action': [1.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 100.74193407652699, 'action': [0.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 101.85914042106698, 'action': [0.0, -1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 107.73417656387102, 'action': [1.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 108.07911590876563, 'action': [1.0, 0.0]}])
Weights num count: [0.16414059980651402, 0.17220251531763947, 0.16156078684295389, 0.16285069332473395, 0.16930022573363432, 0.16962270235407934]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 91.52926397323608 s
