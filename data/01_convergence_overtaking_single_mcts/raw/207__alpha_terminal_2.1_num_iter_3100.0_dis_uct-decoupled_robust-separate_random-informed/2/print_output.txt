Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 332, 'sum_payoffs': 73.53116611434476, 'action': [0.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 77.89112838003035, 'action': [1.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 79.18617387323795, 'action': [2.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 75.79777766089629, 'action': [1.0, 1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 79.46678622034807, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 75.12084043802477, 'action': [0.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 83.16445071963881, 'action': [2.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 80.79856182237268, 'action': [2.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 73.91991296184956, 'action': [0.0, 0.0]}])
Weights num count: [0.10706223798774589, 0.11125443405353112, 0.11254434053531119, 0.10931957433086101, 0.11286681715575621, 0.10867462108997097, 0.11641405998065141, 0.11415672363753628, 0.1073847146081909]
Actions to choose Agent 1: dict_values([{'num_count': 534, 'sum_payoffs': 110.81945865065599, 'action': [1.0, 1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 102.88302126202593, 'action': [0.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 100.18439439951287, 'action': [0.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 101.33495445445229, 'action': [0.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 108.93300165710541, 'action': [1.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 109.92292097322292, 'action': [1.0, 0.0]}])
Weights num count: [0.17220251531763947, 0.163495646565624, 0.16059335698161883, 0.1618832634633989, 0.17026765559496937, 0.17123508545630442]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 90.39741539955139 s
