Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 349, 'sum_payoffs': 79.27722301283377, 'action': [1.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 72.9271419594877, 'action': [0.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 75.17775969880351, 'action': [0.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 80.53023028645063, 'action': [2.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 78.27054071539118, 'action': [1.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 74.13361132522623, 'action': [1.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 77.57014660422507, 'action': [2.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 76.47776926932198, 'action': [0.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 84.9493968736303, 'action': [2.0, 0.0]}])
Weights num count: [0.11254434053531119, 0.10641728474685586, 0.10867462108997097, 0.11383424701709126, 0.11157691067397614, 0.10770719122863592, 0.1109319574330861, 0.10996452757175104, 0.11802644308287649]
Actions to choose Agent 1: dict_values([{'num_count': 511, 'sum_payoffs': 103.48301674891358, 'action': [0.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 101.65952766196108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 108.13738574410782, 'action': [1.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 111.07836433530561, 'action': [1.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 107.56248746153918, 'action': [1.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 98.75090960596845, 'action': [0.0, 0.0]}])
Weights num count: [0.16478555304740405, 0.16285069332473395, 0.16994517897452435, 0.17316994517897452, 0.16930022573363432, 0.15962592712028378]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 91.28294110298157 s
