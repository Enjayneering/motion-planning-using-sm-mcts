Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 331, 'sum_payoffs': 73.22836536813246, 'action': [1.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 74.46747503565621, 'action': [0.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 73.1202165153386, 'action': [0.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 81.88671163847383, 'action': [2.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 79.55949597018743, 'action': [1.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 80.14241441591565, 'action': [1.0, -1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 83.8309083113901, 'action': [2.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 80.47453441303807, 'action': [2.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 72.2047765349986, 'action': [0.0, 0.0]}])
Weights num count: [0.10673976136730087, 0.10802966784908094, 0.10673976136730087, 0.11512415349887133, 0.11286681715575621, 0.11351177039664624, 0.11705901322154144, 0.11383424701709126, 0.10577233150596582]
Actions to choose Agent 1: dict_values([{'num_count': 529, 'sum_payoffs': 108.81333093695113, 'action': [1.0, -1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 102.74365324545299, 'action': [0.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 109.13355588464105, 'action': [1.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 101.2520265333741, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 108.5827531935412, 'action': [1.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 100.91995602815163, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1705901322154144, 0.163818123186069, 0.1709126088358594, 0.16220574008384392, 0.17026765559496937, 0.1618832634633989]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 91.09422826766968 s
