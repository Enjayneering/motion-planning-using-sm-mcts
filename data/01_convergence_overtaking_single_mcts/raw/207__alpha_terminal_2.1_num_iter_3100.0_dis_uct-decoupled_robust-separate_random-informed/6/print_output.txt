Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 351, 'sum_payoffs': 80.07733231691324, 'action': [1.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 73.82100938241884, 'action': [0.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 76.49774713992934, 'action': [1.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 72.80435749285739, 'action': [1.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 81.55435748552297, 'action': [2.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 84.86510254957732, 'action': [2.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 78.07272069412011, 'action': [0.0, -1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 78.7889064736968, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 75.12716525391758, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11318929377620122, 0.10706223798774589, 0.10964205095130602, 0.10609480812641084, 0.11447920025798129, 0.11770396646243148, 0.11125443405353112, 0.11189938729442116, 0.10835214446952596]
Actions to choose Agent 1: dict_values([{'num_count': 529, 'sum_payoffs': 109.21414507354423, 'action': [1.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 109.44352880467319, 'action': [1.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 99.4991627737297, 'action': [0.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 103.32903465006862, 'action': [0.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 109.39587846539506, 'action': [1.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 102.48791097504855, 'action': [0.0, 0.0]}])
Weights num count: [0.1705901322154144, 0.1709126088358594, 0.1599484037407288, 0.16414059980651402, 0.1709126088358594, 0.16317316994517897]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 90.63753318786621 s
