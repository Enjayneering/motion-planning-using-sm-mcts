Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 347, 'sum_payoffs': 78.35034058348971, 'action': [2.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 76.36217505184119, 'action': [0.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 74.02427748247837, 'action': [0.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 73.81379708311978, 'action': [0.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 79.80797883103254, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 77.72002176488499, 'action': [1.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 80.4755666646069, 'action': [2.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 77.37973901858199, 'action': [1.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 79.99309236884771, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11189938729442116, 0.10996452757175104, 0.10770719122863592, 0.1073847146081909, 0.11318929377620122, 0.11125443405353112, 0.11383424701709126, 0.1109319574330861, 0.11351177039664624]
Actions to choose Agent 1: dict_values([{'num_count': 520, 'sum_payoffs': 106.79448964049078, 'action': [1.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 106.50449748495967, 'action': [0.0, -1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 106.10832799775575, 'action': [1.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 100.28555627134337, 'action': [0.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 105.01473006759737, 'action': [0.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 110.03000418080356, 'action': [1.0, 0.0]}])
Weights num count: [0.16768784263140923, 0.16736536601096422, 0.1670428893905192, 0.16059335698161883, 0.1657529829087391, 0.17123508545630442]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 91.37232279777527 s
