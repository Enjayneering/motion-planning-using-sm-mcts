Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 338, 'sum_payoffs': 75.64853720086948, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 82.94256494541517, 'action': [2.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 81.909600333455, 'action': [2.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 77.31998892575557, 'action': [1.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 77.98120702339311, 'action': [1.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 80.33966694626415, 'action': [2.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 75.32412253813655, 'action': [0.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 72.58388108664154, 'action': [0.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 75.96186171964877, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10899709771041599, 0.11609158336020639, 0.11512415349887133, 0.11060948081264109, 0.11125443405353112, 0.11351177039664624, 0.10867462108997097, 0.10609480812641084, 0.10931957433086101]
Actions to choose Agent 1: dict_values([{'num_count': 505, 'sum_payoffs': 101.6214940305258, 'action': [0.0, 0.0]}, {'num_count': 527, 'sum_payoffs': 108.05549931576957, 'action': [1.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 98.19424944008271, 'action': [0.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 109.49597947307035, 'action': [1.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 108.29445699344829, 'action': [1.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 104.53965588622533, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16285069332473395, 0.16994517897452435, 0.15898097387939375, 0.17155756207674944, 0.17026765559496937, 0.16607545952918412]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 64094.77697825432 s
