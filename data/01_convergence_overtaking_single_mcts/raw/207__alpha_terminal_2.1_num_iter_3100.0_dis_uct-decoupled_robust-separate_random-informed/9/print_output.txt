Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 354, 'sum_payoffs': 81.04761345832837, 'action': [2.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 84.39225728965722, 'action': [2.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 78.05551585556212, 'action': [2.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 78.0230141964574, 'action': [1.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 77.3566187626071, 'action': [1.0, 1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 78.69734748101186, 'action': [1.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 73.68369225917155, 'action': [0.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 74.68678225898579, 'action': [0.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 74.73509753660873, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11415672363753628, 0.11738148984198646, 0.11125443405353112, 0.11125443405353112, 0.11060948081264109, 0.11189938729442116, 0.10706223798774589, 0.10802966784908094, 0.10802966784908094]
Actions to choose Agent 1: dict_values([{'num_count': 534, 'sum_payoffs': 110.33934423919779, 'action': [1.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 99.2530904597903, 'action': [0.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 97.74278276284876, 'action': [0.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 109.345270545136, 'action': [1.0, -1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 110.82847540699902, 'action': [1.0, 1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 103.7958410871667, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17220251531763947, 0.1599484037407288, 0.15833602063850372, 0.17123508545630442, 0.1728474685585295, 0.16510802966784907]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 170.845201253891 s
