Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 404, 'sum_payoffs': 90.800108948083, 'action': [1.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 92.06971249970228, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 95.33993008759822, 'action': [2.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 85.25935983689274, 'action': [0.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 91.03094086594074, 'action': [2.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 84.53568579086888, 'action': [0.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 91.15465795691786, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 87.23200237570613, 'action': [1.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 87.8657220413954, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1121910580394335, 0.11330186059427937, 0.11607886698139405, 0.10747014718133852, 0.11246875867814496, 0.10691474590391557, 0.11246875867814496, 0.10913635101360733, 0.10969175229103027]
Actions to choose Agent 1: dict_values([{'num_count': 618, 'sum_payoffs': 126.67410846337252, 'action': [1.0, 1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 117.28180954433026, 'action': [0.0, -1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 127.23111568246169, 'action': [1.0, -1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 117.34538900959556, 'action': [0.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 114.77051136374742, 'action': [0.0, 1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 126.07980516638474, 'action': [1.0, 0.0]}])
Weights num count: [0.17161899472368786, 0.1624548736462094, 0.1721743960011108, 0.1624548736462094, 0.15995556789780616, 0.17106359344626493]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 307.44656014442444 s
