Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 392, 'sum_payoffs': 86.79151610221565, 'action': [0.0, -1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 90.36767644039718, 'action': [1.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 87.73584091154329, 'action': [1.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 91.33307563416191, 'action': [2.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 86.67346944274446, 'action': [1.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 83.8961650897491, 'action': [0.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 98.7188560539684, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 91.94678321281648, 'action': [2.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 86.70423827218, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10885865037489587, 0.11191335740072202, 0.10969175229103027, 0.11274645931685642, 0.10885865037489587, 0.10635934462649264, 0.11913357400722022, 0.11330186059427937, 0.10885865037489587]
Actions to choose Agent 1: dict_values([{'num_count': 622, 'sum_payoffs': 127.65864329611709, 'action': [1.0, -1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 125.30613237653911, 'action': [1.0, 0.0]}, {'num_count': 579, 'sum_payoffs': 115.37363057837909, 'action': [0.0, -1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 117.89636673021609, 'action': [0.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 116.81760347989358, 'action': [0.0, 1.5707963267948966]}, {'num_count': 613, 'sum_payoffs': 125.06427796783255, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17272979727853374, 0.170508192168842, 0.16078866981394058, 0.16328797556234378, 0.16217717300749793, 0.1702304915301305]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 330.17996525764465 s
