Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 391, 'sum_payoffs': 86.20839160016453, 'action': [1.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 85.82609921950791, 'action': [0.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 92.07884871324858, 'action': [2.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 84.58427395772618, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 93.38742494841685, 'action': [2.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 96.91722599284161, 'action': [2.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 87.88999273975466, 'action': [1.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 84.64945626431914, 'action': [0.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 90.82513503999643, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10858094973618439, 0.10830324909747292, 0.11357956123299083, 0.10719244654262705, 0.11469036378783672, 0.11774507081366287, 0.10996945292974174, 0.10719244654262705, 0.11246875867814496]
Actions to choose Agent 1: dict_values([{'num_count': 606, 'sum_payoffs': 122.59592865629217, 'action': [1.0, 0.0]}, {'num_count': 612, 'sum_payoffs': 124.31160512353325, 'action': [1.0, 1.5707963267948966]}, {'num_count': 587, 'sum_payoffs': 117.13987299354122, 'action': [0.0, 0.0]}, {'num_count': 587, 'sum_payoffs': 117.15694353604093, 'action': [0.0, 1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 125.16671228746141, 'action': [1.0, -1.5707963267948966]}, {'num_count': 593, 'sum_payoffs': 118.88937210323662, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16828658705915023, 0.16995279089141904, 0.16301027492363232, 0.16301027492363232, 0.17078589280755346, 0.16467647875590113]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 329.81677508354187 s
