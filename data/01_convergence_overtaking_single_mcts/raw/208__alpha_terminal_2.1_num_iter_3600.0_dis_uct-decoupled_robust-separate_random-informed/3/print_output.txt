Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 386, 'sum_payoffs': 85.17658203080266, 'action': [0.0, -1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 86.22434097256601, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 89.10671805351005, 'action': [1.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 87.71807610063034, 'action': [1.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 93.6960491035282, 'action': [2.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 91.73351107446133, 'action': [2.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 92.58522197247964, 'action': [1.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 87.45266805079366, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 94.00514082907578, 'action': [2.0, 0.0]}])
Weights num count: [0.10719244654262705, 0.10802554845876146, 0.11052485420716468, 0.1094140516523188, 0.11441266314912524, 0.11274645931685642, 0.11357956123299083, 0.10913635101360733, 0.11469036378783672]
Actions to choose Agent 1: dict_values([{'num_count': 585, 'sum_payoffs': 116.667225001743, 'action': [0.0, 1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 115.82638044250382, 'action': [0.0, -1.5707963267948966]}, {'num_count': 605, 'sum_payoffs': 122.3623676459656, 'action': [1.0, -1.5707963267948966]}, {'num_count': 590, 'sum_payoffs': 118.10356893502791, 'action': [0.0, 0.0]}, {'num_count': 624, 'sum_payoffs': 127.81475151989824, 'action': [1.0, 0.0]}, {'num_count': 614, 'sum_payoffs': 124.92982556742368, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1624548736462094, 0.16162177173007497, 0.16800888642043876, 0.16384337683976674, 0.17328519855595667, 0.170508192168842]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 327.31999826431274 s
