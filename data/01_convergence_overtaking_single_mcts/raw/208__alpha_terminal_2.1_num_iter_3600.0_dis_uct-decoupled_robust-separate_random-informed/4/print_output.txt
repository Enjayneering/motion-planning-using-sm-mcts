Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 402, 'sum_payoffs': 90.0472853968115, 'action': [1.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 85.22950998091842, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 95.8877201960082, 'action': [2.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 88.14271065572915, 'action': [0.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 86.79425632867894, 'action': [0.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 91.30979143512535, 'action': [2.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 83.27761038595877, 'action': [0.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 90.33926977837675, 'action': [1.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 93.6513418025043, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11163565676201055, 0.10747014718133852, 0.116634268258817, 0.10996945292974174, 0.10885865037489587, 0.11274645931685642, 0.1058039433490697, 0.11191335740072202, 0.11469036378783672]
Actions to choose Agent 1: dict_values([{'num_count': 582, 'sum_payoffs': 116.09589833997984, 'action': [0.0, 0.0]}, {'num_count': 586, 'sum_payoffs': 117.19448730533388, 'action': [0.0, 1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 125.49176517342562, 'action': [1.0, -1.5707963267948966]}, {'num_count': 619, 'sum_payoffs': 126.59773111397658, 'action': [1.0, 0.0]}, {'num_count': 621, 'sum_payoffs': 127.0957044275451, 'action': [1.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 114.66774883513347, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16162177173007497, 0.16273257428492086, 0.17078589280755346, 0.17189669536239932, 0.17245209663982228, 0.16023326853651762]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 306.58301734924316 s
