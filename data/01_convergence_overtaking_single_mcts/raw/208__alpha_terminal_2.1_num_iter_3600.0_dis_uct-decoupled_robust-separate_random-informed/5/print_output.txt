Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 399, 'sum_payoffs': 89.05278666600627, 'action': [2.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 86.66915717488054, 'action': [0.0, -1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 98.5149790004643, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 91.91120369194255, 'action': [2.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 90.96391935914582, 'action': [1.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 89.27517050295708, 'action': [1.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 83.15625050061068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 84.18447814130151, 'action': [0.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 90.30846953547764, 'action': [1.0, 0.0]}])
Weights num count: [0.11080255484587614, 0.10885865037489587, 0.11885587336850875, 0.11330186059427937, 0.11246875867814496, 0.11108025548458761, 0.1058039433490697, 0.10663704526520411, 0.11191335740072202]
Actions to choose Agent 1: dict_values([{'num_count': 579, 'sum_payoffs': 115.67129053634339, 'action': [0.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 114.83529958374027, 'action': [0.0, -1.5707963267948966]}, {'num_count': 596, 'sum_payoffs': 120.46450927376002, 'action': [0.0, 1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 127.68346526168368, 'action': [1.0, 0.0]}, {'num_count': 616, 'sum_payoffs': 126.26591402811852, 'action': [1.0, 1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 125.02409546348918, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16078866981394058, 0.15995556789780616, 0.16550958067203556, 0.17245209663982228, 0.17106359344626493, 0.16995279089141904]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 302.18137383461 s
