Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 402, 'sum_payoffs': 89.66072152994684, 'action': [1.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 93.28895443589295, 'action': [2.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 86.17889687604837, 'action': [1.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 91.56637280200694, 'action': [2.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 83.54021758815065, 'action': [0.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 93.54639308900022, 'action': [2.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 84.45069333062291, 'action': [0.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 86.49407487938704, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 92.59541573975386, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11163565676201055, 0.11469036378783672, 0.10858094973618439, 0.11330186059427937, 0.10635934462649264, 0.11496806442654818, 0.10719244654262705, 0.10885865037489587, 0.11413496251041377]
Actions to choose Agent 1: dict_values([{'num_count': 591, 'sum_payoffs': 118.67050339508305, 'action': [0.0, 0.0]}, {'num_count': 587, 'sum_payoffs': 117.4507181687058, 'action': [0.0, 1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 126.045509225552, 'action': [1.0, 0.0]}, {'num_count': 607, 'sum_payoffs': 123.18117694118204, 'action': [1.0, -1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 117.26180976258834, 'action': [0.0, -1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 124.61748082214123, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1641210774784782, 0.16301027492363232, 0.1713412940849764, 0.1685642876978617, 0.16273257428492086, 0.16995279089141904]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18383.51257276535 s
