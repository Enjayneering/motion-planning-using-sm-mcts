Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 413, 'sum_payoffs': 93.57347781657087, 'action': [2.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 88.50889812169224, 'action': [1.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 96.23969717303622, 'action': [2.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 87.77988481978002, 'action': [1.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 81.9732102711259, 'action': [0.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 90.13898279056772, 'action': [2.0, -1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 90.07325157597326, 'action': [1.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 88.09464265916586, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 88.47292045665638, 'action': [0.0, 0.0]}])
Weights num count: [0.11469036378783672, 0.1102471535684532, 0.11691196889752846, 0.10969175229103027, 0.10469314079422383, 0.11163565676201055, 0.11163565676201055, 0.10996945292974174, 0.1102471535684532]
Actions to choose Agent 1: dict_values([{'num_count': 581, 'sum_payoffs': 115.79469280629456, 'action': [0.0, 0.0]}, {'num_count': 619, 'sum_payoffs': 126.61902587221489, 'action': [1.0, -1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 116.89689940894289, 'action': [0.0, 1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 125.75136056600019, 'action': [1.0, 1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 127.1468131775613, 'action': [1.0, 0.0]}, {'num_count': 578, 'sum_payoffs': 114.93337877314165, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1613440710913635, 0.17189669536239932, 0.1624548736462094, 0.17106359344626493, 0.17245209663982228, 0.1605109691752291]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 172.00626015663147 s
