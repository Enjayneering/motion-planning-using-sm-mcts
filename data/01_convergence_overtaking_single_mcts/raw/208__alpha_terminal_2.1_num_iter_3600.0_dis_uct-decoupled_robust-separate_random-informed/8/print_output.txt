Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 414, 'sum_payoffs': 93.91741124292551, 'action': [2.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 89.33839722228917, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 94.00454077608283, 'action': [2.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 86.80406656715972, 'action': [1.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 88.76318553259108, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 87.11077882113128, 'action': [1.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 82.35333461123932, 'action': [0.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 97.82515217834192, 'action': [2.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 84.57248178458278, 'action': [0.0, 0.0]}])
Weights num count: [0.11496806442654818, 0.11108025548458761, 0.11496806442654818, 0.10885865037489587, 0.11052485420716468, 0.10913635101360733, 0.1049708414329353, 0.11830047209108581, 0.10691474590391557]
Actions to choose Agent 1: dict_values([{'num_count': 612, 'sum_payoffs': 124.73571977585608, 'action': [1.0, -1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 125.85694343286355, 'action': [1.0, 1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 116.45920182573929, 'action': [0.0, 0.0]}, {'num_count': 619, 'sum_payoffs': 126.72200450476605, 'action': [1.0, 0.0]}, {'num_count': 582, 'sum_payoffs': 116.19514472276644, 'action': [0.0, 1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 117.8887962471632, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16995279089141904, 0.17106359344626493, 0.16189947236878643, 0.17189669536239932, 0.16162177173007497, 0.16328797556234378]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 256.3533957004547 s
