Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 386, 'sum_payoffs': 84.83408809580243, 'action': [0.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 87.77400937784677, 'action': [1.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 96.95884337672331, 'action': [2.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 89.68677345429056, 'action': [1.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 88.4972079127876, 'action': [0.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 92.24192115539113, 'action': [2.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 87.41740973546054, 'action': [1.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 89.7460918920914, 'action': [2.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 87.47684943650522, 'action': [0.0, 0.0]}])
Weights num count: [0.10719244654262705, 0.10969175229103027, 0.1174673701749514, 0.11135795612329909, 0.1102471535684532, 0.11357956123299083, 0.1094140516523188, 0.11135795612329909, 0.1094140516523188]
Actions to choose Agent 1: dict_values([{'num_count': 612, 'sum_payoffs': 124.55180987792602, 'action': [1.0, 1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 123.42979971517424, 'action': [1.0, -1.5707963267948966]}, {'num_count': 596, 'sum_payoffs': 120.03281960045032, 'action': [0.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 116.51877608100033, 'action': [0.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 114.635573786228, 'action': [0.0, -1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 127.75844971977168, 'action': [1.0, 0.0]}])
Weights num count: [0.16995279089141904, 0.16884198833657318, 0.16550958067203556, 0.16217717300749793, 0.16023326853651762, 0.1730074979172452]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 274.8772609233856 s
