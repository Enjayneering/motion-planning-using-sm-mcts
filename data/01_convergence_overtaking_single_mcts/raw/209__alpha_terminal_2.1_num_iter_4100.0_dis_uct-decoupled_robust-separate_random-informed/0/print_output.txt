Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 438, 'sum_payoffs': 95.48106565282642, 'action': [1.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 96.762293637567, 'action': [0.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 106.61026504134118, 'action': [2.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 101.78863675712797, 'action': [1.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 107.86564005361296, 'action': [2.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 103.30777936901853, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 100.19033812053475, 'action': [0.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 93.79862791632186, 'action': [0.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 103.39498065109694, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10680321872713973, 0.10777859058766155, 0.11533772250670568, 0.11168007802974884, 0.1163130943672275, 0.11289929285540112, 0.11046086320409657, 0.10558400390148744, 0.11289929285540112]
Actions to choose Agent 1: dict_values([{'num_count': 694, 'sum_payoffs': 139.5778756945182, 'action': [1.0, 0.0]}, {'num_count': 679, 'sum_payoffs': 135.26192768918196, 'action': [0.0, 1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 142.07445070590032, 'action': [1.0, 1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 128.72913581437163, 'action': [0.0, 0.0]}, {'num_count': 702, 'sum_payoffs': 141.73469555959633, 'action': [1.0, -1.5707963267948966]}, {'num_count': 667, 'sum_payoffs': 132.02908663321298, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16922701780053645, 0.16556937332357963, 0.17142160448671057, 0.15971714216044866, 0.1711777615215801, 0.16264325774201413]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 321.44932866096497 s
