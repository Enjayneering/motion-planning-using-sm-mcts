Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 449, 'sum_payoffs': 98.83604000041329, 'action': [0.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 107.31532635959289, 'action': [2.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 100.60819484024084, 'action': [0.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 103.23179307157542, 'action': [2.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 101.01776174278815, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 99.16027283111596, 'action': [1.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 104.55205749240385, 'action': [2.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 94.42402688517888, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 98.73447204151397, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10948549134357474, 0.11606925140209705, 0.11094854913435748, 0.11289929285540112, 0.11119239209948793, 0.1097293343087052, 0.11387466471592295, 0.10607168983174835, 0.10948549134357474]
Actions to choose Agent 1: dict_values([{'num_count': 697, 'sum_payoffs': 140.52408694049058, 'action': [1.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 131.38495402193595, 'action': [0.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 129.76402128588506, 'action': [0.0, 1.5707963267948966]}, {'num_count': 671, 'sum_payoffs': 133.31717653640274, 'action': [0.0, -1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 144.20727079980887, 'action': [1.0, 0.0]}, {'num_count': 700, 'sum_payoffs': 141.33710778758348, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16995854669592783, 0.16191172884662278, 0.16044867105584004, 0.16361862960253595, 0.17312850524262374, 0.17069007559131918]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 276.2125954627991 s
