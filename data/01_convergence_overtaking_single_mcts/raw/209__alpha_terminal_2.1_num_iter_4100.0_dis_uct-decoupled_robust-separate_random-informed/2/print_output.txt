Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 465, 'sum_payoffs': 103.63832196643651, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 97.66692441064394, 'action': [0.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 103.44011810151193, 'action': [2.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 98.93578787907715, 'action': [1.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 102.39646968896872, 'action': [1.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 104.32827690760107, 'action': [2.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 99.00742279438067, 'action': [0.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 102.14874155183016, 'action': [1.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 94.81370267921407, 'action': [0.0, 0.0]}])
Weights num count: [0.11338697878566203, 0.10875396244818337, 0.11314313582053158, 0.1097293343087052, 0.11241160692514021, 0.11387466471592295, 0.1097293343087052, 0.11216776396000976, 0.10655937576200927]
Actions to choose Agent 1: dict_values([{'num_count': 653, 'sum_payoffs': 128.3922612368175, 'action': [0.0, -1.5707963267948966]}, {'num_count': 674, 'sum_payoffs': 134.23420759754674, 'action': [0.0, 1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 140.96867448594693, 'action': [1.0, 0.0]}, {'num_count': 685, 'sum_payoffs': 137.3313325899946, 'action': [0.0, 0.0]}, {'num_count': 700, 'sum_payoffs': 141.50351478807093, 'action': [1.0, -1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 138.60132594156156, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15922945623018775, 0.16435015849792733, 0.17020238966105827, 0.16703243111436236, 0.17069007559131918, 0.16825164594001463]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 275.75522780418396 s
