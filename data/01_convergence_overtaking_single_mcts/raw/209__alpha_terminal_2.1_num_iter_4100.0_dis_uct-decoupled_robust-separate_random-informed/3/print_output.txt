Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 444, 'sum_payoffs': 97.20425365542366, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 101.35059843109407, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 98.11979122465912, 'action': [0.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 98.10578050311588, 'action': [1.0, 0.0]}, {'num_count': 435, 'sum_payoffs': 94.421599664887, 'action': [0.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 103.24282014924518, 'action': [2.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 103.5740284985427, 'action': [2.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 107.38003863354676, 'action': [2.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 104.42570307014493, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10826627651792246, 0.11143623506461839, 0.10899780541331383, 0.10899780541331383, 0.10607168983174835, 0.11289929285540112, 0.11314313582053158, 0.11606925140209705, 0.11387466471592295]
Actions to choose Agent 1: dict_values([{'num_count': 703, 'sum_payoffs': 141.36827148061045, 'action': [1.0, -1.5707963267948966]}, {'num_count': 700, 'sum_payoffs': 140.5445093671311, 'action': [1.0, 1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 130.85791781346848, 'action': [0.0, 0.0]}, {'num_count': 672, 'sum_payoffs': 132.74446753311742, 'action': [0.0, -1.5707963267948966]}, {'num_count': 644, 'sum_payoffs': 125.02096365898095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 144.91735170155133, 'action': [1.0, 0.0]}])
Weights num count: [0.17142160448671057, 0.17069007559131918, 0.16215557181175322, 0.16386247256766642, 0.15703486954401366, 0.17459156303340648]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 319.08969259262085 s
