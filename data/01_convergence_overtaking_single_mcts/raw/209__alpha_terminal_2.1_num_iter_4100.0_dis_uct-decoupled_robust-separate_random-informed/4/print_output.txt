Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 458, 'sum_payoffs': 101.56379896468167, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 104.77358832921104, 'action': [2.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 109.16051933508973, 'action': [2.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 92.4441460628911, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 104.42681699495769, 'action': [2.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 98.78247533356037, 'action': [1.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 99.70898563184008, 'action': [1.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 96.11794948295632, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 100.31196166329059, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11168007802974884, 0.1141185076810534, 0.11753230919287978, 0.10460863204096561, 0.11387466471592295, 0.10948549134357474, 0.11021702023896611, 0.1075347476225311, 0.11070470616922702]
Actions to choose Agent 1: dict_values([{'num_count': 706, 'sum_payoffs': 142.9262395772667, 'action': [1.0, 0.0]}, {'num_count': 666, 'sum_payoffs': 131.92345092188805, 'action': [0.0, 0.0]}, {'num_count': 650, 'sum_payoffs': 127.44891211586554, 'action': [0.0, -1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 131.09937592736796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 143.75898969067046, 'action': [1.0, -1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 142.9601127508989, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17215313338210192, 0.1623994147768837, 0.1584979273347964, 0.1616678858814923, 0.1728846622774933, 0.17215313338210192]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 290.50855469703674 s
