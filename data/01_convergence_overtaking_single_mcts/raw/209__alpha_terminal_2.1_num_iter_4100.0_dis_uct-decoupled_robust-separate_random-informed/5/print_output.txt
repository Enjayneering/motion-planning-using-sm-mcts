Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 472, 'sum_payoffs': 106.20050558978123, 'action': [2.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 100.45752634517113, 'action': [1.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 101.41147852944715, 'action': [1.0, -1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 109.00472714093894, 'action': [2.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 97.20497287555963, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 98.53645224810337, 'action': [1.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 104.15592823303137, 'action': [2.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 95.59062941515224, 'action': [0.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 95.67366713696971, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11509387954157523, 0.11070470616922702, 0.11143623506461839, 0.11728846622774933, 0.10826627651792246, 0.10924164837844429, 0.11363082175079249, 0.10704706169227018, 0.10704706169227018]
Actions to choose Agent 1: dict_values([{'num_count': 664, 'sum_payoffs': 131.44025745022924, 'action': [0.0, -1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 144.04036552880777, 'action': [1.0, 0.0]}, {'num_count': 680, 'sum_payoffs': 135.93472021825906, 'action': [0.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 141.23246402811824, 'action': [1.0, -1.5707963267948966]}, {'num_count': 654, 'sum_payoffs': 128.72216660696614, 'action': [0.0, 1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 139.78960355117013, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16191172884662278, 0.1728846622774933, 0.16581321628871007, 0.17044623262618874, 0.15947329919531822, 0.16922701780053645]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 366.4188287258148 s
