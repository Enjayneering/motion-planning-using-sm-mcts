Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 457, 'sum_payoffs': 101.81030473871355, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 99.58363932234992, 'action': [1.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 98.05013827497791, 'action': [0.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 99.50036859014592, 'action': [1.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 104.3404980492354, 'action': [2.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 115.60170312280297, 'action': [2.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 96.32072229560634, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 101.52447771737091, 'action': [2.0, 1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 95.40000994682678, 'action': [0.0, 0.0]}])
Weights num count: [0.11143623506461839, 0.1097293343087052, 0.10851011948305292, 0.1097293343087052, 0.11338697878566203, 0.12192148256522799, 0.10729090465740064, 0.11119239209948793, 0.10655937576200927]
Actions to choose Agent 1: dict_values([{'num_count': 673, 'sum_payoffs': 133.66188516478113, 'action': [0.0, 0.0]}, {'num_count': 706, 'sum_payoffs': 142.77339953488854, 'action': [1.0, 0.0]}, {'num_count': 651, 'sum_payoffs': 127.51693619280108, 'action': [0.0, 1.5707963267948966]}, {'num_count': 686, 'sum_payoffs': 137.277652844829, 'action': [1.0, 1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 145.3404074146661, 'action': [1.0, -1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 132.54259608842284, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1641063155327969, 0.17215313338210192, 0.15874177029992684, 0.1672762740794928, 0.17434772006827604, 0.16313094367227504]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 364.61996579170227 s
