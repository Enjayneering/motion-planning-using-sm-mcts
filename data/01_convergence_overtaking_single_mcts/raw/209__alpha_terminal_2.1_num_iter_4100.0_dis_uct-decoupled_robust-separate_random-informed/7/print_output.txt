Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 467, 'sum_payoffs': 104.34852057179475, 'action': [2.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 97.9536480388503, 'action': [0.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 94.84952876943501, 'action': [0.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 101.72516710754229, 'action': [2.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 101.8399340378336, 'action': [1.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 97.67240265446162, 'action': [0.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 98.87930181649537, 'action': [1.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 98.33589089985476, 'action': [1.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 110.7371999097031, 'action': [2.0, 0.0]}])
Weights num count: [0.11387466471592295, 0.10899780541331383, 0.10655937576200927, 0.1119239209948793, 0.1119239209948793, 0.10875396244818337, 0.1097293343087052, 0.10924164837844429, 0.11875152401853206]
Actions to choose Agent 1: dict_values([{'num_count': 696, 'sum_payoffs': 140.13983428856298, 'action': [1.0, 1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 132.511117345849, 'action': [0.0, 0.0]}, {'num_count': 657, 'sum_payoffs': 129.26816887889214, 'action': [0.0, -1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 145.45787544773012, 'action': [1.0, 0.0]}, {'num_count': 665, 'sum_payoffs': 131.5120878067924, 'action': [0.0, 1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 140.6766358433354, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16971470373079736, 0.16313094367227504, 0.16020482809070957, 0.17434772006827604, 0.16215557181175322, 0.17020238966105827]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 358.5684247016907 s
