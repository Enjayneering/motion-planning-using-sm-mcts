Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 440, 'sum_payoffs': 95.94414543062047, 'action': [0.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 95.25634539328145, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 100.70371447581442, 'action': [1.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 104.45214217070229, 'action': [2.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 100.67942065212335, 'action': [1.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 101.34738573288104, 'action': [1.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 99.68513325287778, 'action': [2.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 101.65425107042668, 'action': [0.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 108.01516365963367, 'action': [2.0, 0.0]}])
Weights num count: [0.10729090465740064, 0.10680321872713973, 0.11094854913435748, 0.11387466471592295, 0.11094854913435748, 0.11143623506461839, 0.11021702023896611, 0.11168007802974884, 0.11655693733235796]
Actions to choose Agent 1: dict_values([{'num_count': 688, 'sum_payoffs': 137.89289780910184, 'action': [1.0, 1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 129.26148388984822, 'action': [0.0, -1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 131.72302482243964, 'action': [0.0, 1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 145.06141961155635, 'action': [1.0, -1.5707963267948966]}, {'num_count': 675, 'sum_payoffs': 134.23728278672934, 'action': [0.0, 0.0]}, {'num_count': 700, 'sum_payoffs': 141.16338208620866, 'action': [1.0, 0.0]}])
Weights num count: [0.16776396000975372, 0.16020482809070957, 0.1623994147768837, 0.17410387710314557, 0.1645940014630578, 0.17069007559131918]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 60570.37096762657 s
