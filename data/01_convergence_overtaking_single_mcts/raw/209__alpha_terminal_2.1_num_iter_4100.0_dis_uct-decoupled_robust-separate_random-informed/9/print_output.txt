Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 460, 'sum_payoffs': 102.35610187725375, 'action': [2.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 100.07474016644886, 'action': [1.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 99.83408662699667, 'action': [0.0, -1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 105.51402784592962, 'action': [2.0, -1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 107.46477875636631, 'action': [2.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 94.76058673254013, 'action': [0.0, 0.0]}, {'num_count': 434, 'sum_payoffs': 94.10421823069659, 'action': [0.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 102.02058247224075, 'action': [1.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 102.37327509606655, 'action': [1.0, 0.0]}])
Weights num count: [0.11216776396000976, 0.11046086320409657, 0.11021702023896611, 0.11460619361131431, 0.11606925140209705, 0.1063155327968788, 0.10582784686661789, 0.1119239209948793, 0.11216776396000976]
Actions to choose Agent 1: dict_values([{'num_count': 709, 'sum_payoffs': 144.0118269086008, 'action': [1.0, -1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 131.73510793145647, 'action': [0.0, -1.5707963267948966]}, {'num_count': 692, 'sum_payoffs': 139.2482824904131, 'action': [1.0, 1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 132.8423953725288, 'action': [0.0, 0.0]}, {'num_count': 713, 'sum_payoffs': 145.0643397353968, 'action': [1.0, 0.0]}, {'num_count': 652, 'sum_payoffs': 128.09473154056997, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1728846622774933, 0.16215557181175322, 0.16873933187027554, 0.16313094367227504, 0.17386003413801512, 0.1589856132650573]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 149.53811025619507 s
