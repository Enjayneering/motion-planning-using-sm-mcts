Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 569, 'sum_payoffs': 137.30851963036937, 'action': [2.0, -1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 105.6317493251661, 'action': [1.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.7121439054473, 'action': [2.0, 1.5707963267948966]}, {'num_count': 692, 'sum_payoffs': 178.09117177577096, 'action': [2.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 104.64376505658832, 'action': [1.0, -1.5707963267948966]}, {'num_count': 578, 'sum_payoffs': 140.24357384193354, 'action': [1.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 105.31784830470906, 'action': [0.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 81.08163308302497, 'action': [0.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 81.66112594520023, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12366876765920452, 0.10236905020647685, 0.12258204738100413, 0.15040208650293416, 0.10171701803955661, 0.12562486415996524, 0.10215170615083677, 0.08541621386655075, 0.0858509019778309]
Actions to choose Agent 1: dict_values([{'num_count': 929, 'sum_payoffs': 267.88127670835246, 'action': [1.0, 0.0]}, {'num_count': 660, 'sum_payoffs': 173.68185469592262, 'action': [0.0, 1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 167.91930119100547, 'action': [0.0, -1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 228.57907998716954, 'action': [1.0, 1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 198.23117423498186, 'action': [0.0, 0.0]}, {'num_count': 819, 'sum_payoffs': 228.95280616749545, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.2019126276896327, 0.14344707672245163, 0.13975222777657031, 0.177787437513584, 0.1588785046728972, 0.17800478156922409]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.11719822883606 s
