Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 565, 'sum_payoffs': 136.03372224665787, 'action': [2.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 82.6084783557935, 'action': [0.0, 1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 106.59851232032827, 'action': [0.0, 0.0]}, {'num_count': 695, 'sum_payoffs': 179.03265755445645, 'action': [2.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 105.00597525578348, 'action': [1.0, 1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 137.66508048034157, 'action': [1.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 81.94793906033392, 'action': [0.0, -1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 136.30315274875318, 'action': [2.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 104.28742148930098, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.12279939143664421, 0.08650293414475115, 0.10302108237339709, 0.15105411866985438, 0.1019343620951967, 0.1238861117148446, 0.08606824603347099, 0.12301673549228428, 0.10149967398391654]
Actions to choose Agent 1: dict_values([{'num_count': 742, 'sum_payoffs': 202.41372064224984, 'action': [0.0, 0.0]}, {'num_count': 812, 'sum_payoffs': 226.8785172253594, 'action': [1.0, -1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 169.58824932537755, 'action': [0.0, -1.5707963267948966]}, {'num_count': 927, 'sum_payoffs': 267.578312248631, 'action': [1.0, 0.0]}, {'num_count': 652, 'sum_payoffs': 171.2482888705452, 'action': [0.0, 1.5707963267948966]}, {'num_count': 820, 'sum_payoffs': 229.69471782026628, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16126928928493806, 0.17648337317974352, 0.14062160399913062, 0.20147793957835253, 0.14170832427733102, 0.17822212562486417]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.143732070922852 s
