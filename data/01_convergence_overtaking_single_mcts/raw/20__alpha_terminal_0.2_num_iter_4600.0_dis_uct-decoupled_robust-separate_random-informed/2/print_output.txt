Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 694, 'sum_payoffs': 178.73320583108426, 'action': [2.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 82.30428262762126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.659996066332, 'action': [2.0, -1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 106.82347230406225, 'action': [0.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 104.63224907545768, 'action': [1.0, 1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 135.33255109321377, 'action': [2.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 83.24011905708387, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 104.34543596032383, 'action': [1.0, -1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 138.2463840313626, 'action': [1.0, 0.0]}])
Weights num count: [0.1508367746142143, 0.08628559008911106, 0.12258204738100413, 0.10323842642903716, 0.10171701803955661, 0.12236470332536405, 0.0869376222560313, 0.10149967398391654, 0.12432079982612476]
Actions to choose Agent 1: dict_values([{'num_count': 654, 'sum_payoffs': 171.68578751245934, 'action': [0.0, 1.5707963267948966]}, {'num_count': 826, 'sum_payoffs': 231.5835560095449, 'action': [1.0, -1.5707963267948966]}, {'num_count': 807, 'sum_payoffs': 224.88234140055386, 'action': [1.0, 1.5707963267948966]}, {'num_count': 725, 'sum_payoffs': 196.25520569782643, 'action': [0.0, 0.0]}, {'num_count': 933, 'sum_payoffs': 269.48453305094824, 'action': [1.0, 0.0]}, {'num_count': 655, 'sum_payoffs': 172.07374570718497, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14214301238861118, 0.17952618995870462, 0.17539665290154313, 0.15757444033905674, 0.202782003912193, 0.14236035644425124]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.2319066524505615 s
