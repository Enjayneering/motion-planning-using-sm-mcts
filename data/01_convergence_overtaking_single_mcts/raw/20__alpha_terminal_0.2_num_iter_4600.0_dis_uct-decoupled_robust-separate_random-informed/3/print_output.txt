Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 398, 'sum_payoffs': 82.60847835579354, 'action': [0.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 106.83520556787755, 'action': [0.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 135.0457379780801, 'action': [2.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.68889466050118, 'action': [2.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 81.99422026754146, 'action': [0.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 104.97121002970661, 'action': [1.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 104.69591289570359, 'action': [1.0, 1.5707963267948966]}, {'num_count': 693, 'sum_payoffs': 178.39446215952012, 'action': [2.0, 0.0]}, {'num_count': 575, 'sum_payoffs': 139.27590550234805, 'action': [1.0, 0.0]}])
Weights num count: [0.08650293414475115, 0.10323842642903716, 0.12214735926972398, 0.12258204738100413, 0.08606824603347099, 0.1019343620951967, 0.10171701803955661, 0.1506194305585742, 0.12497283199304499]
Actions to choose Agent 1: dict_values([{'num_count': 929, 'sum_payoffs': 268.30092195817457, 'action': [1.0, 0.0]}, {'num_count': 654, 'sum_payoffs': 171.92327746308936, 'action': [0.0, 1.5707963267948966]}, {'num_count': 817, 'sum_payoffs': 228.6024378734581, 'action': [1.0, 1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 228.95856415806063, 'action': [1.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 168.8435129979949, 'action': [0.0, -1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 200.66292937151294, 'action': [0.0, 0.0]}])
Weights num count: [0.2019126276896327, 0.14214301238861118, 0.17757009345794392, 0.177787437513584, 0.14018691588785046, 0.16018256900673766]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.230745077133179 s
