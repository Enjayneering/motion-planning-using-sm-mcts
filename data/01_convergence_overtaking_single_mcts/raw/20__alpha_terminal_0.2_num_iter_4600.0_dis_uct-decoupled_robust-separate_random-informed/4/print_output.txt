Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 479, 'sum_payoffs': 108.14567352208897, 'action': [0.0, 0.0]}, {'num_count': 563, 'sum_payoffs': 135.2630206410602, 'action': [2.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 139.92109015629995, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 104.57423460443472, 'action': [1.0, 1.5707963267948966]}, {'num_count': 686, 'sum_payoffs': 176.039806154554, 'action': [2.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 80.67314167662228, 'action': [0.0, 1.5707963267948966]}, {'num_count': 567, 'sum_payoffs': 136.65362968413288, 'action': [2.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 104.988592642745, 'action': [1.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 82.89529147092725, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10410780265159748, 0.12236470332536405, 0.12540752010432515, 0.10171701803955661, 0.14909802216909368, 0.08519886981091067, 0.12323407954792437, 0.1019343620951967, 0.08672027820039122]
Actions to choose Agent 1: dict_values([{'num_count': 647, 'sum_payoffs': 169.39117395004718, 'action': [0.0, -1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 201.14761456505747, 'action': [0.0, 0.0]}, {'num_count': 828, 'sum_payoffs': 232.33959103537353, 'action': [1.0, -1.5707963267948966]}, {'num_count': 644, 'sum_payoffs': 168.37994043652344, 'action': [0.0, 1.5707963267948966]}, {'num_count': 924, 'sum_payoffs': 266.2620863038538, 'action': [1.0, 0.0]}, {'num_count': 818, 'sum_payoffs': 228.79353797553816, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14062160399913062, 0.16061725711801783, 0.1799608780699848, 0.1399695718322104, 0.2008259074114323, 0.177787437513584]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.019123792648315 s
