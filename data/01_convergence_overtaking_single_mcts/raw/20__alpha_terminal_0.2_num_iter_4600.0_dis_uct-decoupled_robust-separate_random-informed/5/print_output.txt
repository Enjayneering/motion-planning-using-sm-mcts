Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 691, 'sum_payoffs': 177.64860320502981, 'action': [2.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 134.93579295062673, 'action': [2.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 82.19412031748317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 104.53946937835781, 'action': [1.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 104.58010123634241, 'action': [1.0, 1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 107.48603957105252, 'action': [0.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 82.83162765068134, 'action': [0.0, -1.5707963267948966]}, {'num_count': 574, 'sum_payoffs': 138.87418607227613, 'action': [1.0, 0.0]}, {'num_count': 564, 'sum_payoffs': 135.6193642083476, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.15018474244729407, 0.12214735926972398, 0.08628559008911106, 0.10171701803955661, 0.10171701803955661, 0.10367311454031732, 0.08672027820039122, 0.12475548793740492, 0.12258204738100413]
Actions to choose Agent 1: dict_values([{'num_count': 649, 'sum_payoffs': 170.1966407817217, 'action': [0.0, 1.5707963267948966]}, {'num_count': 813, 'sum_payoffs': 227.23192747669307, 'action': [1.0, 1.5707963267948966]}, {'num_count': 824, 'sum_payoffs': 231.06207761839272, 'action': [1.0, -1.5707963267948966]}, {'num_count': 928, 'sum_payoffs': 267.92313983478715, 'action': [1.0, 0.0]}, {'num_count': 741, 'sum_payoffs': 202.03412783001622, 'action': [0.0, 0.0]}, {'num_count': 645, 'sum_payoffs': 168.86969555889488, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1410562921104108, 0.1767007172353836, 0.17909150184742448, 0.2016952836339926, 0.16105194522929797, 0.14018691588785046]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.025921583175659 s
