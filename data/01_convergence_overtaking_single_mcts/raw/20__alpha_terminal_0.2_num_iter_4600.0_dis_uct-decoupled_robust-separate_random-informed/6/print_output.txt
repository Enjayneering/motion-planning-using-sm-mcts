Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 571, 'sum_payoffs': 137.96837086409045, 'action': [1.0, 0.0]}, {'num_count': 472, 'sum_payoffs': 105.91856244029991, 'action': [1.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 82.30428262762128, 'action': [0.0, -1.5707963267948966]}, {'num_count': 696, 'sum_payoffs': 179.48359150768965, 'action': [2.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 136.0452382277885, 'action': [2.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 104.67288093344214, 'action': [1.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 81.40907805614322, 'action': [0.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 106.92302397749262, 'action': [0.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 134.9820741578342, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12410345577048468, 0.10258639426211694, 0.08628559008911106, 0.15127146272549447, 0.12279939143664421, 0.10171701803955661, 0.08563355792219082, 0.10323842642903716, 0.12214735926972398]
Actions to choose Agent 1: dict_values([{'num_count': 949, 'sum_payoffs': 275.27699913579943, 'action': [1.0, 0.0]}, {'num_count': 808, 'sum_payoffs': 225.3228819997643, 'action': [1.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 168.78843184292586, 'action': [0.0, -1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 170.17925816868328, 'action': [0.0, 1.5707963267948966]}, {'num_count': 810, 'sum_payoffs': 226.06446772850808, 'action': [1.0, 1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 201.1862184515111, 'action': [0.0, 0.0]}])
Weights num count: [0.20625950880243427, 0.17561399695718322, 0.14018691588785046, 0.1410562921104108, 0.17604868506846338, 0.16061725711801783]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.057728052139282 s
