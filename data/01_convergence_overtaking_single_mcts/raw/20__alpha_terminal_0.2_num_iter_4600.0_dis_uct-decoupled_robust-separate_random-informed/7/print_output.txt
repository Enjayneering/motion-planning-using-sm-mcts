Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 474, 'sum_payoffs': 106.52423786337467, 'action': [0.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 104.29328812120865, 'action': [1.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 82.5389479036399, 'action': [0.0, 1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 105.59698409908928, 'action': [1.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 82.25213478850605, 'action': [0.0, -1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.61936420834752, 'action': [2.0, -1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 179.753022009785, 'action': [2.0, 0.0]}, {'num_count': 575, 'sum_payoffs': 139.18989778157896, 'action': [1.0, 0.0]}, {'num_count': 557, 'sum_payoffs': 133.33355059379693, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10302108237339709, 0.10149967398391654, 0.08650293414475115, 0.10236905020647685, 0.08628559008911106, 0.12258204738100413, 0.15148880678113455, 0.12497283199304499, 0.12106063899152358]
Actions to choose Agent 1: dict_values([{'num_count': 942, 'sum_payoffs': 272.83858066427996, 'action': [1.0, 0.0]}, {'num_count': 737, 'sum_payoffs': 200.6358414661898, 'action': [0.0, 0.0]}, {'num_count': 654, 'sum_payoffs': 171.8914455529665, 'action': [0.0, -1.5707963267948966]}, {'num_count': 644, 'sum_payoffs': 168.48152008148463, 'action': [0.0, 1.5707963267948966]}, {'num_count': 811, 'sum_payoffs': 226.522173658072, 'action': [1.0, 1.5707963267948966]}, {'num_count': 812, 'sum_payoffs': 226.82636938624427, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.2047381004129537, 0.16018256900673766, 0.14214301238861118, 0.1399695718322104, 0.17626602912410347, 0.17648337317974352]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.053866863250732 s
