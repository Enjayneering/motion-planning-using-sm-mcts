Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 561, 'sum_payoffs': 134.7474088818156, 'action': [2.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 106.02285811853028, 'action': [1.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 82.63172760073961, 'action': [0.0, -1.5707963267948966]}, {'num_count': 693, 'sum_payoffs': 178.52088445558888, 'action': [2.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 105.35080284194001, 'action': [1.0, -1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 106.9056413644543, 'action': [0.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 81.11639830910181, 'action': [0.0, 1.5707963267948966]}, {'num_count': 568, 'sum_payoffs': 137.07791029741242, 'action': [1.0, 0.0]}, {'num_count': 570, 'sum_payoffs': 137.7343936498103, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12193001521408389, 0.10258639426211694, 0.08650293414475115, 0.1506194305585742, 0.10215170615083677, 0.10323842642903716, 0.08541621386655075, 0.12345142360356444, 0.1238861117148446]
Actions to choose Agent 1: dict_values([{'num_count': 650, 'sum_payoffs': 170.26312927657912, 'action': [0.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 199.92981766660355, 'action': [0.0, 0.0]}, {'num_count': 651, 'sum_payoffs': 170.63098882499722, 'action': [0.0, -1.5707963267948966]}, {'num_count': 926, 'sum_payoffs': 266.7659647992832, 'action': [1.0, 0.0]}, {'num_count': 825, 'sum_payoffs': 230.98363857703526, 'action': [1.0, 1.5707963267948966]}, {'num_count': 812, 'sum_payoffs': 226.47578380952226, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14127363616605085, 0.15996522495109758, 0.14149098022169093, 0.20126059552271244, 0.17930884590306456, 0.17648337317974352]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.077403545379639 s
