Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 700, 'sum_payoffs': 180.77577150443946, 'action': [2.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 81.98270428641077, 'action': [0.0, 1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 105.56221887301245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 568, 'sum_payoffs': 136.88376823803608, 'action': [1.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 134.9473089317574, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 81.67264192633087, 'action': [0.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 104.6437650565883, 'action': [1.0, -1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 106.87380945433128, 'action': [0.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 135.9815744075426, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.15214083894805477, 0.08606824603347099, 0.10236905020647685, 0.12345142360356444, 0.12214735926972398, 0.0858509019778309, 0.10171701803955661, 0.10323842642903716, 0.12279939143664421]
Actions to choose Agent 1: dict_values([{'num_count': 734, 'sum_payoffs': 199.52820687787414, 'action': [0.0, 0.0]}, {'num_count': 810, 'sum_payoffs': 226.15996345887703, 'action': [1.0, -1.5707963267948966]}, {'num_count': 946, 'sum_payoffs': 274.273660225714, 'action': [1.0, 0.0]}, {'num_count': 651, 'sum_payoffs': 170.8224148511043, 'action': [0.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 170.55298434900917, 'action': [0.0, 1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 225.79503722641283, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15953053683981744, 0.17604868506846338, 0.205607476635514, 0.14149098022169093, 0.14127363616605085, 0.1758313410128233]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.066130638122559 s
