Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 496, 'sum_payoffs': 107.38735999541379, 'action': [0.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 110.21548757337163, 'action': [1.0, 1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 106.47703638534502, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 105.00965445911602, 'action': [0.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 116.99678072959932, 'action': [2.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 110.52045276294601, 'action': [1.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 114.53367309898258, 'action': [2.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 113.89571417219518, 'action': [1.0, -1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 123.94295761420177, 'action': [2.0, 0.0]}])
Weights num count: [0.10780265159747882, 0.10975874809823952, 0.10715061943055858, 0.10606389915235818, 0.11454031732232123, 0.10997609215387959, 0.11280156487720061, 0.11236687676592046, 0.11932188654640295]
Actions to choose Agent 1: dict_values([{'num_count': 776, 'sum_payoffs': 154.57469336172537, 'action': [1.0, 1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 146.95395445320045, 'action': [0.0, 0.0]}, {'num_count': 786, 'sum_payoffs': 157.28269136675996, 'action': [1.0, -1.5707963267948966]}, {'num_count': 741, 'sum_payoffs': 144.94738792144724, 'action': [0.0, 1.5707963267948966]}, {'num_count': 796, 'sum_payoffs': 160.0434988335569, 'action': [1.0, 0.0]}, {'num_count': 753, 'sum_payoffs': 148.27179470997538, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16865898717670072, 0.16257335361877853, 0.1708324277331015, 0.16105194522929797, 0.1730058682895023, 0.16366007389697892]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 138.40012574195862 s
