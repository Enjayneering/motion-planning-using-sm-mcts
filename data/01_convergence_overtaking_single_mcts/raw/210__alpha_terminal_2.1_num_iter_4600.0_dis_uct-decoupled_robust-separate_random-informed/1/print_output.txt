Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 499, 'sum_payoffs': 108.6409391991661, 'action': [1.0, 1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 125.7152769708737, 'action': [2.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 110.78105192250031, 'action': [1.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 108.6551295215908, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 117.57034282455456, 'action': [2.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 113.00673793448361, 'action': [1.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 105.90121204298822, 'action': [0.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 107.13447467628323, 'action': [0.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 114.27638389643644, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10845468376439904, 0.12040860682460335, 0.10997609215387959, 0.10845468376439904, 0.11475766137796131, 0.11149750054336013, 0.10649858726363834, 0.10736796348619865, 0.11236687676592046]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 143.20349929534245, 'action': [0.0, -1.5707963267948966]}, {'num_count': 742, 'sum_payoffs': 145.65677764666677, 'action': [0.0, 1.5707963267948966]}, {'num_count': 798, 'sum_payoffs': 160.89839795989812, 'action': [1.0, 0.0]}, {'num_count': 792, 'sum_payoffs': 159.37164606802457, 'action': [1.0, -1.5707963267948966]}, {'num_count': 762, 'sum_payoffs': 151.1454640631852, 'action': [0.0, 0.0]}, {'num_count': 773, 'sum_payoffs': 154.17454796064501, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15931319278417735, 0.16126928928493806, 0.17344055640078243, 0.17213649206694198, 0.16561617039773963, 0.1680069550097805]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 128.2244827747345 s
