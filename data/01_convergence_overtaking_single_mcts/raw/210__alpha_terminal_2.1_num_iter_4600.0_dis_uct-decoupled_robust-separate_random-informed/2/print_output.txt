Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 517, 'sum_payoffs': 114.14104056889451, 'action': [1.0, -1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 111.91010189897143, 'action': [1.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 109.08998844915457, 'action': [0.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 111.61258035993262, 'action': [1.0, 1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 104.54725393804603, 'action': [0.0, 1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 116.93913597830125, 'action': [2.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 108.72406632420594, 'action': [0.0, 0.0]}, {'num_count': 536, 'sum_payoffs': 120.0167669072244, 'action': [2.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 113.43755020791329, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11236687676592046, 0.11084546837643991, 0.1088893718756792, 0.11062812432079983, 0.10562921104107803, 0.11432297326668116, 0.10867202782003912, 0.11649641382308194, 0.1119321886546403]
Actions to choose Agent 1: dict_values([{'num_count': 796, 'sum_payoffs': 160.27554281388674, 'action': [1.0, 0.0]}, {'num_count': 744, 'sum_payoffs': 146.04140276599574, 'action': [0.0, -1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 160.0078821658118, 'action': [1.0, 1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 144.44407789970865, 'action': [0.0, 1.5707963267948966]}, {'num_count': 756, 'sum_payoffs': 149.32818493317995, 'action': [0.0, 0.0]}, {'num_count': 771, 'sum_payoffs': 153.4282922002012, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1730058682895023, 0.16170397739621822, 0.1727885242338622, 0.16039991306237775, 0.16431210606389915, 0.16757226689850033]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 119.19104790687561 s
