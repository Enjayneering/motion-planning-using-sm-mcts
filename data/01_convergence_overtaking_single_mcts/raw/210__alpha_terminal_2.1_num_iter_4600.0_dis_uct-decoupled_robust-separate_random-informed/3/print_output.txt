Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 510, 'sum_payoffs': 111.5673724636999, 'action': [2.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 111.60315202370893, 'action': [1.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 106.70301308820214, 'action': [0.0, -1.5707963267948966]}, {'num_count': 560, 'sum_payoffs': 127.21407771986209, 'action': [2.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 116.2900026564857, 'action': [2.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 113.51072590635776, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 103.93094169788816, 'action': [0.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 103.89182225668964, 'action': [0.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 113.09073684917644, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11084546837643991, 0.11084546837643991, 0.10736796348619865, 0.12171267115844382, 0.11410562921104107, 0.11214953271028037, 0.10541186698543795, 0.10541186698543795, 0.1119321886546403]
Actions to choose Agent 1: dict_values([{'num_count': 735, 'sum_payoffs': 143.62995380666652, 'action': [0.0, -1.5707963267948966]}, {'num_count': 779, 'sum_payoffs': 155.54172854729998, 'action': [1.0, -1.5707963267948966]}, {'num_count': 762, 'sum_payoffs': 150.89774929760625, 'action': [0.0, 0.0]}, {'num_count': 781, 'sum_payoffs': 156.13498897331834, 'action': [1.0, 1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 160.48119755304683, 'action': [1.0, 0.0]}, {'num_count': 746, 'sum_payoffs': 146.5740372120553, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15974788089545752, 0.16931101934362094, 0.16561617039773963, 0.1697457074549011, 0.17322321234514235, 0.16213866550749836]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 120.20881986618042 s
