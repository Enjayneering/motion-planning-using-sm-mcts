Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 495, 'sum_payoffs': 106.95881560117242, 'action': [0.0, 1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 114.15246311533795, 'action': [1.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 117.84483090798547, 'action': [2.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 104.23918625782802, 'action': [0.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 115.41129097370731, 'action': [2.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 113.19912861654356, 'action': [1.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 115.37057486131462, 'action': [2.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 112.9062035109447, 'action': [1.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 107.98201143759168, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10758530754183873, 0.11258422082156053, 0.11519234948924147, 0.10562921104107803, 0.11345359704412085, 0.1119321886546403, 0.11345359704412085, 0.11171484459900022, 0.10823733970875897]
Actions to choose Agent 1: dict_values([{'num_count': 744, 'sum_payoffs': 146.16733004535592, 'action': [0.0, 0.0]}, {'num_count': 765, 'sum_payoffs': 151.8795916904894, 'action': [1.0, 1.5707963267948966]}, {'num_count': 791, 'sum_payoffs': 158.95741067073166, 'action': [1.0, -1.5707963267948966]}, {'num_count': 802, 'sum_payoffs': 161.95728475895385, 'action': [1.0, 0.0]}, {'num_count': 748, 'sum_payoffs': 147.2507178872171, 'action': [0.0, -1.5707963267948966]}, {'num_count': 750, 'sum_payoffs': 147.73774091332862, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16170397739621822, 0.16626820256465985, 0.1719191480113019, 0.17430993262334274, 0.16257335361877853, 0.16300804173005867]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 133.86224699020386 s
