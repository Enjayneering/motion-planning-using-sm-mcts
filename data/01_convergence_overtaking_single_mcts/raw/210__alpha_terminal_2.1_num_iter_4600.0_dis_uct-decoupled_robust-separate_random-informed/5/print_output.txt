Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 498, 'sum_payoffs': 108.47780121012326, 'action': [0.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 109.37013519700429, 'action': [0.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 114.07757930040928, 'action': [2.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 110.68736734257486, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 108.7727275321503, 'action': [1.0, 1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 114.67779526974388, 'action': [2.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 113.42264853513123, 'action': [1.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 110.01284492138193, 'action': [1.0, 0.0]}, {'num_count': 546, 'sum_payoffs': 123.44780705385966, 'action': [2.0, 0.0]}])
Weights num count: [0.10823733970875897, 0.1088893718756792, 0.11214953271028037, 0.10975874809823952, 0.10845468376439904, 0.11258422082156053, 0.11171484459900022, 0.10932405998695936, 0.11866985437948271]
Actions to choose Agent 1: dict_values([{'num_count': 777, 'sum_payoffs': 154.6191944378041, 'action': [1.0, 1.5707963267948966]}, {'num_count': 777, 'sum_payoffs': 154.57709280052237, 'action': [1.0, -1.5707963267948966]}, {'num_count': 748, 'sum_payoffs': 146.63243654121422, 'action': [0.0, 0.0]}, {'num_count': 742, 'sum_payoffs': 145.0320764836648, 'action': [0.0, -1.5707963267948966]}, {'num_count': 750, 'sum_payoffs': 147.26307280706962, 'action': [0.0, 1.5707963267948966]}, {'num_count': 806, 'sum_payoffs': 162.56909174750703, 'action': [1.0, 0.0]}])
Weights num count: [0.1688763312323408, 0.1688763312323408, 0.16257335361877853, 0.16126928928493806, 0.16300804173005867, 0.17517930884590308]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 124.33853459358215 s
