Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 521, 'sum_payoffs': 115.15887403166617, 'action': [2.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 108.39893623745087, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 104.03591645712146, 'action': [0.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 114.61744120208431, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 110.52643412867847, 'action': [0.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 106.22710493921174, 'action': [0.0, 1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 122.41652398185381, 'action': [2.0, 0.0]}, {'num_count': 518, 'sum_payoffs': 114.2811335438363, 'action': [2.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 113.68023050118263, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11323625298848077, 0.10845468376439904, 0.10541186698543795, 0.11280156487720061, 0.10997609215387959, 0.1069332753749185, 0.11823516626820256, 0.11258422082156053, 0.11214953271028037]
Actions to choose Agent 1: dict_values([{'num_count': 787, 'sum_payoffs': 157.90465050028794, 'action': [1.0, -1.5707963267948966]}, {'num_count': 749, 'sum_payoffs': 147.498174555511, 'action': [0.0, 0.0]}, {'num_count': 779, 'sum_payoffs': 155.73474295153196, 'action': [1.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 143.98042437065533, 'action': [0.0, -1.5707963267948966]}, {'num_count': 747, 'sum_payoffs': 146.9830252444463, 'action': [0.0, 1.5707963267948966]}, {'num_count': 802, 'sum_payoffs': 162.0809050317754, 'action': [1.0, 0.0]}])
Weights num count: [0.1710497717887416, 0.16279069767441862, 0.16931101934362094, 0.15996522495109758, 0.16235600956313845, 0.17430993262334274]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 119.45186257362366 s
