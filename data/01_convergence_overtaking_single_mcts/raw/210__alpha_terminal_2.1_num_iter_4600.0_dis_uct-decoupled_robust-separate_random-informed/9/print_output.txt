Searching game tree in timestep 0...
Max timehorizon: 14
Actions to choose Agent 0: dict_values([{'num_count': 510, 'sum_payoffs': 111.88935465887297, 'action': [0.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 113.14168596765182, 'action': [1.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 105.67090126942728, 'action': [0.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 103.86719640529087, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 111.32828386028257, 'action': [1.0, -1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 114.99089725985574, 'action': [1.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 115.06298572871756, 'action': [2.0, -1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 121.56870016147516, 'action': [2.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 112.86397848802089, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11084546837643991, 0.11171484459900022, 0.10649858726363834, 0.10519452292979788, 0.11041078026515974, 0.11301890893284068, 0.11301890893284068, 0.11758313410128234, 0.11149750054336013]
Actions to choose Agent 1: dict_values([{'num_count': 734, 'sum_payoffs': 143.5922302774797, 'action': [0.0, 1.5707963267948966]}, {'num_count': 762, 'sum_payoffs': 151.27783380903813, 'action': [0.0, 0.0]}, {'num_count': 779, 'sum_payoffs': 155.88301142883088, 'action': [1.0, -1.5707963267948966]}, {'num_count': 790, 'sum_payoffs': 158.94838736967128, 'action': [1.0, 1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 160.2998478098165, 'action': [1.0, 0.0]}, {'num_count': 740, 'sum_payoffs': 145.1945465032041, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15953053683981744, 0.16561617039773963, 0.16931101934362094, 0.1717018039556618, 0.1727885242338622, 0.1608346011736579]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 119.59890174865723 s
