Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 125, 'sum_payoffs': 28.40014443632312, 'action': [2.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 26.75398674142382, 'action': [0.0, 0.0]}, {'num_count': 125, 'sum_payoffs': 28.375148365633198, 'action': [2.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 28.362226142371828, 'action': [2.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 26.810739834943497, 'action': [0.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 25.523300922480235, 'action': [1.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 26.67314018756178, 'action': [0.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 26.653695862171666, 'action': [1.0, 1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 27.501308084390445, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11353315168029064, 0.10990009082652134, 0.11353315168029064, 0.11353315168029064, 0.10990009082652134, 0.10717529518619437, 0.10990009082652134, 0.10990009082652134, 0.11171662125340599]
Actions to choose Agent 1: dict_values([{'num_count': 193, 'sum_payoffs': 40.46531078394493, 'action': [1.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 38.81847713756443, 'action': [1.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 36.427221930539595, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 34.74709144531449, 'action': [0.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 34.76368319196573, 'action': [0.0, 0.0]}, {'num_count': 186, 'sum_payoffs': 38.116726661159255, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17529518619436876, 0.17075386012715713, 0.16439600363306087, 0.15985467756584923, 0.15985467756584923, 0.16893732970027248]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.17510366439819 s
