Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 183, 'sum_payoffs': 41.421433376353235, 'action': [1.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 36.30498825335821, 'action': [0.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 39.17411066031751, 'action': [2.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 40.95950337481696, 'action': [0.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 36.243482140450176, 'action': [0.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 41.309573965498686, 'action': [1.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 42.046984412120196, 'action': [2.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 39.50096151822606, 'action': [2.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 38.05357479687873, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11430356027482823, 0.10555902560899438, 0.11055590256089944, 0.1136789506558401, 0.10555902560899438, 0.11430356027482823, 0.1155527795128045, 0.11118051217988757, 0.10868207370393504]
Actions to choose Agent 1: dict_values([{'num_count': 261, 'sum_payoffs': 51.513211133372536, 'action': [0.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 56.07562457935662, 'action': [1.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 51.56272078706545, 'action': [0.0, -1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 51.31670975297391, 'action': [0.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 55.391335665385256, 'action': [1.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 54.43776817780063, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16302311055590257, 0.1717676452217364, 0.16302311055590257, 0.16239850093691444, 0.17051842598376016, 0.16864459712679575]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 47.91013169288635 s
