Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 174, 'sum_payoffs': 38.22017431174125, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 38.325302416756635, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 40.42091147420147, 'action': [2.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 41.18575721922888, 'action': [2.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 37.9365963012472, 'action': [0.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 40.87328259338964, 'action': [1.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 40.48949612425384, 'action': [2.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 40.393386110630814, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 39.014661722337436, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10868207370393504, 0.10868207370393504, 0.11242973141786383, 0.1136789506558401, 0.1080574640849469, 0.11305434103685197, 0.11242973141786383, 0.11242973141786383, 0.1099312929419113]
Actions to choose Agent 1: dict_values([{'num_count': 262, 'sum_payoffs': 52.12983656798995, 'action': [0.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 54.96962090913284, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 54.35556248505544, 'action': [1.0, 1.5707963267948966]}, {'num_count': 262, 'sum_payoffs': 52.02394651955339, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 55.3503949932481, 'action': [1.0, 0.0]}, {'num_count': 264, 'sum_payoffs': 52.71778556153589, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16364772017489068, 0.16926920674578388, 0.1680199875078076, 0.16364772017489068, 0.16989381636477202, 0.16489693941286696]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.62588453292847 s
