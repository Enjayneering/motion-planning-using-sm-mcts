Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 237, 'sum_payoffs': 53.15017253567447, 'action': [2.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 51.03556210755376, 'action': [1.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 51.04156645375447, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 53.83335504089822, 'action': [1.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 48.86372238654017, 'action': [1.0, 0.0]}, {'num_count': 243, 'sum_payoffs': 55.24971166553347, 'action': [2.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 48.52165281733685, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 54.9319591324736, 'action': [2.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 50.011920494591166, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1128034269395526, 0.1099476439790576, 0.1099476439790576, 0.11375535459305093, 0.10709186101856259, 0.11565920990004759, 0.10661589719181343, 0.11518324607329843, 0.10851975249881009]
Actions to choose Agent 1: dict_values([{'num_count': 360, 'sum_payoffs': 72.8922896351914, 'action': [1.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 67.1236097995176, 'action': [0.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 73.49352280054583, 'action': [1.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 64.7663254238834, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 72.34641880240859, 'action': [1.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 68.62596378658188, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17134697762970014, 0.16230366492146597, 0.17229890528319847, 0.15849595430747263, 0.1703950499762018, 0.1646834840552118]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 59.822508096694946 s
