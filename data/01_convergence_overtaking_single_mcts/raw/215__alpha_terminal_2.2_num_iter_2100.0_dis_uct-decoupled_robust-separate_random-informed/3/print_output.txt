Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 235, 'sum_payoffs': 52.45923858386937, 'action': [2.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 51.271351978371875, 'action': [0.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 51.318674263968006, 'action': [1.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 51.015379176299284, 'action': [0.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 51.04970743639793, 'action': [0.0, 1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 51.69144813724849, 'action': [1.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 47.57003158902624, 'action': [1.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 56.98048730101459, 'action': [2.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 53.14154609364726, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11185149928605426, 0.11042360780580676, 0.11042360780580676, 0.1099476439790576, 0.1099476439790576, 0.11089957163255593, 0.10518800571156592, 0.11803902903379343, 0.1128034269395526]
Actions to choose Agent 1: dict_values([{'num_count': 363, 'sum_payoffs': 74.02025785147836, 'action': [1.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 70.91040615485929, 'action': [1.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 67.56204372194074, 'action': [0.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 70.39258899178546, 'action': [0.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 71.60345804050655, 'action': [1.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 65.77088744317246, 'action': [0.0, 0.0]}])
Weights num count: [0.17277486910994763, 0.16801523084245598, 0.16277962874821514, 0.16706330318895765, 0.1689671584959543, 0.15992384578772012]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 62.6148886680603 s
