Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 226, 'sum_payoffs': 49.5138707741963, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 53.965925400537756, 'action': [1.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 53.660729849517786, 'action': [2.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 49.145721942270704, 'action': [0.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 54.04503625871027, 'action': [2.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 53.63754594940656, 'action': [2.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 52.99966199181793, 'action': [1.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 50.53042077784346, 'action': [0.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 50.90202154812864, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10756782484531176, 0.11375535459305093, 0.11327939076630177, 0.10709186101856259, 0.11375535459305093, 0.11327939076630177, 0.11232746311280342, 0.10899571632555925, 0.10947168015230842]
Actions to choose Agent 1: dict_values([{'num_count': 366, 'sum_payoffs': 74.9093461527717, 'action': [1.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 67.2724886135496, 'action': [0.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 72.12998138345036, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 73.14263337847365, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 66.43284697190332, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 66.3539075297835, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17420276059019515, 0.16230366492146597, 0.16991908614945264, 0.17134697762970014, 0.16087577344121848, 0.16087577344121848]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 61.707807302474976 s
