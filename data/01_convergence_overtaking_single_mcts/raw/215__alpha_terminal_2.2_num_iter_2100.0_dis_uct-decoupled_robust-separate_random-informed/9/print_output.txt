Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 252, 'sum_payoffs': 58.30269548581254, 'action': [2.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 53.09490458994451, 'action': [1.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 48.47432479229026, 'action': [0.0, 1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 48.174630086631026, 'action': [0.0, 0.0]}, {'num_count': 243, 'sum_payoffs': 55.17289424961124, 'action': [2.0, -1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 50.955237104244574, 'action': [1.0, -1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 52.05686132441199, 'action': [2.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 49.92005592749189, 'action': [1.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 49.87613138149985, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1199428843407901, 0.1128034269395526, 0.10661589719181343, 0.10613993336506425, 0.11565920990004759, 0.1099476439790576, 0.11137553545930509, 0.10851975249881009, 0.10851975249881009]
Actions to choose Agent 1: dict_values([{'num_count': 344, 'sum_payoffs': 68.25517847149236, 'action': [0.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 72.20253460136793, 'action': [1.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 72.28557496510217, 'action': [1.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 73.43351413338672, 'action': [1.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 68.27054609956457, 'action': [0.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 66.09500240989509, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16373155640171347, 0.16991908614945264, 0.16991908614945264, 0.1718229414564493, 0.16373155640171347, 0.1603998096144693]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 65.79747247695923 s
