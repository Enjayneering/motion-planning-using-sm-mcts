Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 290, 'sum_payoffs': 64.57353232891846, 'action': [1.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 66.26149837008384, 'action': [2.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 66.53216730963662, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 62.973220352284734, 'action': [0.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 58.92107862123877, 'action': [0.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 67.26664034410844, 'action': [2.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 64.30175670492575, 'action': [2.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 65.33037484716945, 'action': [0.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 61.85810841518032, 'action': [1.0, 0.0]}])
Weights num count: [0.1114955786236063, 0.11341791618608228, 0.11380238369857747, 0.10957324106113034, 0.104959630911188, 0.11457131872356786, 0.1111111111111111, 0.1122645136485967, 0.10841983852364476]
Actions to choose Agent 1: dict_values([{'num_count': 451, 'sum_payoffs': 91.47751527294858, 'action': [1.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 82.73696325700443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 86.18642617438603, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 89.45981810900881, 'action': [1.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 82.65875774317557, 'action': [0.0, 0.0]}, {'num_count': 430, 'sum_payoffs': 85.2700282095294, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17339484813533257, 0.16186082276047675, 0.16647443291041908, 0.1707035755478662, 0.16186082276047675, 0.1653210303729335]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 75.41850137710571 s
