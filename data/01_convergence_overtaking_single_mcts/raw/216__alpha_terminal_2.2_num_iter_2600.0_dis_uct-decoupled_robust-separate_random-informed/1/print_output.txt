Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 279, 'sum_payoffs': 60.92813109731157, 'action': [0.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 61.974201318786726, 'action': [0.0, 0.0]}, {'num_count': 295, 'sum_payoffs': 66.26361522122477, 'action': [2.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 65.68414962294617, 'action': [2.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 68.69756164890408, 'action': [2.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 64.65724899913133, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 61.00652956305095, 'action': [0.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 64.31432407879207, 'action': [1.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 64.94203760940803, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10726643598615918, 0.10841983852364476, 0.11341791618608228, 0.11264898116109189, 0.11610918877354863, 0.1114955786236063, 0.10726643598615918, 0.1111111111111111, 0.1118800461361015]
Actions to choose Agent 1: dict_values([{'num_count': 434, 'sum_payoffs': 86.89948622770811, 'action': [0.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 87.25798493904108, 'action': [1.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 84.27801446031059, 'action': [0.0, 0.0]}, {'num_count': 435, 'sum_payoffs': 87.26229831212147, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 82.80446973048555, 'action': [0.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 91.98521169412727, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16685890042291426, 0.16724336793540945, 0.16339869281045752, 0.16724336793540945, 0.16147635524798154, 0.17339484813533257]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 72.85833239555359 s
