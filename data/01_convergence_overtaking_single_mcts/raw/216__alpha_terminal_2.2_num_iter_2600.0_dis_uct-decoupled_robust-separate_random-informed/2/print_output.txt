Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 290, 'sum_payoffs': 64.8398851713274, 'action': [1.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 62.12337314318838, 'action': [1.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 59.8109309214923, 'action': [0.0, 1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 71.28865982014972, 'action': [2.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 66.1910964983581, 'action': [1.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 62.11825398776874, 'action': [0.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 58.78653401151108, 'action': [0.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 67.29993656759561, 'action': [2.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 68.0303549314559, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1114955786236063, 0.10841983852364476, 0.1057285659361784, 0.11880046136101499, 0.11303344867358708, 0.10841983852364476, 0.10457516339869281, 0.11418685121107267, 0.11495578623606305]
Actions to choose Agent 1: dict_values([{'num_count': 432, 'sum_payoffs': 86.36699825717452, 'action': [0.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 81.9762621438278, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 88.78652095625941, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 91.76127225388143, 'action': [1.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 82.59198074656054, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 89.33983261700499, 'action': [1.0, 0.0]}])
Weights num count: [0.16608996539792387, 0.16032295271049596, 0.16916570549788543, 0.17301038062283736, 0.16109188773548636, 0.16993464052287582]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.63756394386292 s
