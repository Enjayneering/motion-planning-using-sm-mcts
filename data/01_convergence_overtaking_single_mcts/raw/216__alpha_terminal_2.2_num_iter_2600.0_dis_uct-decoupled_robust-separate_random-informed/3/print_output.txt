Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 274, 'sum_payoffs': 59.18766789008357, 'action': [0.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 59.497547433011, 'action': [0.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 63.82459199692787, 'action': [1.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 61.802806443475326, 'action': [0.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 64.51082076077104, 'action': [2.0, -1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 70.35891045615071, 'action': [2.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 63.16313304245963, 'action': [1.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 67.5779574464556, 'action': [2.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 67.64641181013857, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1053440984236832, 0.1057285659361784, 0.11072664359861592, 0.10841983852364476, 0.1114955786236063, 0.11803152633602461, 0.10995770857362552, 0.11495578623606305, 0.11495578623606305]
Actions to choose Agent 1: dict_values([{'num_count': 436, 'sum_payoffs': 87.62587905191228, 'action': [1.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 85.04556566734553, 'action': [0.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 84.41460026999081, 'action': [0.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 85.36792594727477, 'action': [0.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 88.56858270456385, 'action': [1.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 90.33213756545382, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16762783544790466, 0.16416762783544792, 0.16339869281045752, 0.1645520953479431, 0.16878123798539024, 0.1710880430603614]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 74.84340929985046 s
