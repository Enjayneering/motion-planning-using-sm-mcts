Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 289, 'sum_payoffs': 63.99677008756133, 'action': [1.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 62.94703426115383, 'action': [0.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 68.34847635965052, 'action': [2.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 61.533607139607184, 'action': [0.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 61.65364972301835, 'action': [0.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 63.004966620561056, 'action': [1.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 65.371424222105, 'action': [2.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 65.0402456606699, 'action': [2.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 63.66939387428142, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1111111111111111, 0.10995770857362552, 0.11610918877354863, 0.10841983852364476, 0.10841983852364476, 0.10995770857362552, 0.11264898116109189, 0.1122645136485967, 0.11072664359861592]
Actions to choose Agent 1: dict_values([{'num_count': 421, 'sum_payoffs': 83.28813029203006, 'action': [0.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 93.44497192598635, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 83.07569223831135, 'action': [0.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 90.75394371733181, 'action': [1.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 88.41263462218842, 'action': [1.0, 1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 83.05990348267267, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16186082276047675, 0.17493271818531334, 0.16147635524798154, 0.1714725105728566, 0.16839677047289503, 0.16147635524798154]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 75.99829149246216 s
