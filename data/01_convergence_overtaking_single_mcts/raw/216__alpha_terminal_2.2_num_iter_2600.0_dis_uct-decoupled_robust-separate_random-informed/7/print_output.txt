Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 296, 'sum_payoffs': 66.52460106084563, 'action': [2.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 62.82485013341432, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 63.478104574492384, 'action': [1.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 60.417921294778274, 'action': [0.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 68.1825133009228, 'action': [2.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 62.430584443043216, 'action': [1.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 66.56984177900937, 'action': [2.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 64.19791950105551, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 62.555471600762175, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11380238369857747, 0.10957324106113034, 0.11034217608612072, 0.10688196847366398, 0.11572472126105345, 0.10918877354863514, 0.11380238369857747, 0.1111111111111111, 0.10918877354863514]
Actions to choose Agent 1: dict_values([{'num_count': 414, 'sum_payoffs': 81.17599103784325, 'action': [0.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 82.60505453666903, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 91.48278041697716, 'action': [1.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 89.36424522072812, 'action': [1.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 91.13977855079499, 'action': [1.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 85.19112775128627, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15916955017301038, 0.16109188773548636, 0.17262591311034217, 0.16993464052287582, 0.172241445597847, 0.1645520953479431]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 76.44961190223694 s
