Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 296, 'sum_payoffs': 66.37186431769683, 'action': [2.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 65.32597754851146, 'action': [1.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 66.72745517411278, 'action': [2.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 63.629218740819134, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 60.62896755481962, 'action': [1.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 58.21893733161222, 'action': [0.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 63.69445317921152, 'action': [0.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 65.60487940391347, 'action': [2.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 65.35828595612654, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11380238369857747, 0.11264898116109189, 0.11418685121107267, 0.11072664359861592, 0.10726643598615918, 0.10457516339869281, 0.11072664359861592, 0.11303344867358708, 0.11264898116109189]
Actions to choose Agent 1: dict_values([{'num_count': 446, 'sum_payoffs': 90.17705334083432, 'action': [1.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 77.21060919461202, 'action': [0.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 86.12271049404794, 'action': [0.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 89.66691535385664, 'action': [1.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 83.0883679635935, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 92.56634662915347, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1714725105728566, 0.15455594002306805, 0.16608996539792387, 0.1707035755478662, 0.16224529027297194, 0.17454825067281815]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 76.37575936317444 s
