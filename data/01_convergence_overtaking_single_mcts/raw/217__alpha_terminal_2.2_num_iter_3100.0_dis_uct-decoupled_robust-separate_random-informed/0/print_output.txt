Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 345, 'sum_payoffs': 75.71939242905582, 'action': [0.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 76.03869477120803, 'action': [1.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 77.77419302746188, 'action': [2.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 74.07455275077865, 'action': [1.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 78.09377132650245, 'action': [2.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 73.83559533710181, 'action': [0.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 76.04860034297684, 'action': [2.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 76.75217254875304, 'action': [1.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 71.78145588026902, 'action': [0.0, 0.0]}])
Weights num count: [0.11125443405353112, 0.11157691067397614, 0.11318929377620122, 0.10964205095130602, 0.11351177039664624, 0.10931957433086101, 0.11157691067397614, 0.11222186391486617, 0.1073847146081909]
Actions to choose Agent 1: dict_values([{'num_count': 531, 'sum_payoffs': 106.63515896735332, 'action': [1.0, 1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 107.17071818754869, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 98.07114160681058, 'action': [0.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 106.34431208292567, 'action': [1.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 100.06185734521344, 'action': [0.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 96.91732233510595, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17123508545630442, 0.17188003869719445, 0.16156078684295389, 0.1709126088358594, 0.163818123186069, 0.16027088036117382]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 86.27972793579102 s
