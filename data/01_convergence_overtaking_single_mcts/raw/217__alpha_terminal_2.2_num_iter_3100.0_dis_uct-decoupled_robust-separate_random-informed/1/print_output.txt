Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 337, 'sum_payoffs': 73.5801967262721, 'action': [1.0, 0.0]}, {'num_count': 335, 'sum_payoffs': 72.87871672282826, 'action': [0.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 77.801698456061, 'action': [1.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 79.43594349742642, 'action': [1.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 78.79206427813455, 'action': [2.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 70.29389082328241, 'action': [0.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 81.43442012522151, 'action': [2.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 75.76575430040451, 'action': [2.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 73.85690426300548, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10867462108997097, 0.10802966784908094, 0.11286681715575621, 0.11447920025798129, 0.11383424701709126, 0.1054498548855208, 0.11641405998065141, 0.1109319574330861, 0.10899709771041599]
Actions to choose Agent 1: dict_values([{'num_count': 501, 'sum_payoffs': 97.73088888610654, 'action': [0.0, -1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 107.49053384063338, 'action': [1.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 105.74525329556612, 'action': [1.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 106.37140090480567, 'action': [1.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 98.98071868004541, 'action': [0.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 97.25087024985416, 'action': [0.0, 0.0]}])
Weights num count: [0.16156078684295389, 0.1725249919380845, 0.1705901322154144, 0.17123508545630442, 0.16285069332473395, 0.16091583360206385]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 86.35212779045105 s
