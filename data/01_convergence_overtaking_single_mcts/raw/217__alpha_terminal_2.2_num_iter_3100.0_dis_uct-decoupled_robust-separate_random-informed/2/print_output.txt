Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 76.45882560923815, 'action': [2.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 72.83673064661554, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 73.57571610286796, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 81.14000571072995, 'action': [2.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 73.86964319646461, 'action': [0.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 72.51203366046852, 'action': [0.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 80.09401579742246, 'action': [2.0, 0.0]}, {'num_count': 348, 'sum_payoffs': 77.05533994731675, 'action': [1.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 76.180249321517, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11157691067397614, 0.10802966784908094, 0.10867462108997097, 0.11609158336020639, 0.10899709771041599, 0.10770719122863592, 0.11512415349887133, 0.11222186391486617, 0.11125443405353112]
Actions to choose Agent 1: dict_values([{'num_count': 497, 'sum_payoffs': 97.44148885334471, 'action': [0.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 106.35215311534722, 'action': [1.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 106.38866155615419, 'action': [1.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 100.92350856901813, 'action': [0.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 101.43090499578433, 'action': [0.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 106.04953726679925, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16027088036117382, 0.17026765559496937, 0.17026765559496937, 0.16414059980651402, 0.16478555304740405, 0.16994517897452435]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 90.60462164878845 s
