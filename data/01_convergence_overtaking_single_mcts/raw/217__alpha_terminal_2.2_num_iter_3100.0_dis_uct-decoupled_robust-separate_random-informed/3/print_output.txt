Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 369, 'sum_payoffs': 84.42738003021668, 'action': [2.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 75.19407602427229, 'action': [1.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 76.7913804046705, 'action': [1.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 79.46185632437168, 'action': [2.0, -1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 75.78248203048061, 'action': [2.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 76.43445801335949, 'action': [1.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 72.21570868605392, 'action': [0.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 72.52648812252548, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 73.86936213335669, 'action': [0.0, 0.0]}])
Weights num count: [0.11899387294421154, 0.10996452757175104, 0.11157691067397614, 0.11415672363753628, 0.11060948081264109, 0.11125443405353112, 0.10706223798774589, 0.1073847146081909, 0.10867462108997097]
Actions to choose Agent 1: dict_values([{'num_count': 527, 'sum_payoffs': 105.9968484800794, 'action': [1.0, 1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 105.73426071130133, 'action': [1.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 101.35911801286346, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 100.53917513574358, 'action': [0.0, 1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 101.3473069149896, 'action': [0.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 103.15492050115523, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16994517897452435, 0.16962270235407934, 0.16478555304740405, 0.163818123186069, 0.16478555304740405, 0.16672041277007418]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 105.09931015968323 s
