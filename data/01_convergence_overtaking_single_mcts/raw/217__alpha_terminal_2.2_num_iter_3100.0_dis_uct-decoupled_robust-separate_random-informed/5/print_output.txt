Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 349, 'sum_payoffs': 77.37107545650797, 'action': [2.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 74.19196480651793, 'action': [1.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 79.33895085735014, 'action': [2.0, 0.0]}, {'num_count': 334, 'sum_payoffs': 72.5023042671451, 'action': [0.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 77.75568453369299, 'action': [1.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 76.39874921203122, 'action': [1.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 74.84275912851102, 'action': [0.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 72.46858765972554, 'action': [0.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 78.35758155849182, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11254434053531119, 0.10931957433086101, 0.11447920025798129, 0.10770719122863592, 0.11286681715575621, 0.11157691067397614, 0.10996452757175104, 0.10770719122863592, 0.11351177039664624]
Actions to choose Agent 1: dict_values([{'num_count': 530, 'sum_payoffs': 106.37930396958588, 'action': [1.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 105.84497594301395, 'action': [1.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 99.5605193224342, 'action': [0.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 106.45580590432239, 'action': [1.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 99.25048581467594, 'action': [0.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 98.06991076474984, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1709126088358594, 0.17026765559496937, 0.16317316994517897, 0.1709126088358594, 0.16285069332473395, 0.16156078684295389]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.83319091796875 s
