Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 347, 'sum_payoffs': 76.67176892814474, 'action': [1.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 76.36426773529122, 'action': [1.0, 1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 84.72610065764776, 'action': [2.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 77.43587247579487, 'action': [2.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 75.74479986715595, 'action': [2.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 72.47767248071625, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 74.84360462472841, 'action': [0.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 71.43599353385409, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 73.48332058530117, 'action': [1.0, 0.0]}])
Weights num count: [0.11189938729442116, 0.11157691067397614, 0.11963882618510158, 0.11254434053531119, 0.1109319574330861, 0.10770719122863592, 0.10996452757175104, 0.10673976136730087, 0.10867462108997097]
Actions to choose Agent 1: dict_values([{'num_count': 511, 'sum_payoffs': 101.10690681082309, 'action': [0.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 97.414476199555, 'action': [0.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 104.33058426783704, 'action': [1.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 105.763724936763, 'action': [1.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 107.41209285404015, 'action': [1.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 100.58081441963004, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16478555304740405, 0.16059335698161883, 0.16833279587229927, 0.16994517897452435, 0.17188003869719445, 0.16414059980651402]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 100.0610842704773 s
