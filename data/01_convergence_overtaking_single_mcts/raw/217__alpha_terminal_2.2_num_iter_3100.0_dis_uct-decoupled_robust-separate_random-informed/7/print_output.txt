Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 353, 'sum_payoffs': 79.06023651194053, 'action': [2.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 80.05315150790386, 'action': [2.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 75.01194623170512, 'action': [0.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 76.3747692611051, 'action': [1.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 75.0832767384181, 'action': [1.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 72.366734639784, 'action': [1.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 74.3126845028743, 'action': [0.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 81.73314125405078, 'action': [2.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 71.7318007453092, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11383424701709126, 0.11480167687842631, 0.10996452757175104, 0.11125443405353112, 0.10996452757175104, 0.1073847146081909, 0.10931957433086101, 0.11641405998065141, 0.10673976136730087]
Actions to choose Agent 1: dict_values([{'num_count': 505, 'sum_payoffs': 99.42766648110687, 'action': [0.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 105.70244306227579, 'action': [1.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 107.13527906974552, 'action': [1.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 98.23074367799299, 'action': [0.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 107.47363406215561, 'action': [1.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 98.43601458914182, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16285069332473395, 0.16994517897452435, 0.17155756207674944, 0.16156078684295389, 0.17188003869719445, 0.1618832634633989]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 91.96155858039856 s
