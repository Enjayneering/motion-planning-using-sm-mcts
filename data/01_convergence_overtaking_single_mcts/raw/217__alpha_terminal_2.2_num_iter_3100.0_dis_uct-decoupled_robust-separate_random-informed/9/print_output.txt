Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 347, 'sum_payoffs': 76.89660237419506, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 73.91826691750302, 'action': [1.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 73.26821661390966, 'action': [1.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 74.4617425152926, 'action': [0.0, 1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 79.07091042069185, 'action': [2.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 83.07397737182885, 'action': [2.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 77.80807136862268, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 75.09404237687053, 'action': [0.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 70.30859099542164, 'action': [0.0, 0.0]}])
Weights num count: [0.11189938729442116, 0.10899709771041599, 0.10835214446952596, 0.10964205095130602, 0.11415672363753628, 0.11802644308287649, 0.11286681715575621, 0.11028700419219607, 0.1054498548855208]
Actions to choose Agent 1: dict_values([{'num_count': 539, 'sum_payoffs': 109.32485337650915, 'action': [1.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 106.07645654438971, 'action': [1.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 97.21430813476007, 'action': [0.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 106.11205122151763, 'action': [1.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 100.09828450979333, 'action': [0.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 98.36824619695537, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17381489841986456, 0.17026765559496937, 0.16027088036117382, 0.17026765559496937, 0.163495646565624, 0.16156078684295389]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 88.10842156410217 s
