Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 389, 'sum_payoffs': 83.52158142179509, 'action': [0.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 82.8895193170041, 'action': [0.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 90.96301251323573, 'action': [2.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 91.47828419981536, 'action': [2.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 91.27035919715945, 'action': [2.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 83.90315743739173, 'action': [1.0, 0.0]}, {'num_count': 391, 'sum_payoffs': 84.2015286001873, 'action': [0.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 85.17939011441635, 'action': [1.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 90.23246129233038, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10802554845876146, 0.10747014718133852, 0.11441266314912524, 0.11496806442654818, 0.11469036378783672, 0.10830324909747292, 0.10858094973618439, 0.1094140516523188, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 608, 'sum_payoffs': 120.5044999770071, 'action': [1.0, -1.5707963267948966]}, {'num_count': 609, 'sum_payoffs': 120.7808257659076, 'action': [1.0, 1.5707963267948966]}, {'num_count': 594, 'sum_payoffs': 116.62637850490282, 'action': [0.0, -1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 114.96582290493751, 'action': [0.0, 0.0]}, {'num_count': 582, 'sum_payoffs': 113.30068192078825, 'action': [0.0, 1.5707963267948966]}, {'num_count': 619, 'sum_payoffs': 123.53871715573521, 'action': [1.0, 0.0]}])
Weights num count: [0.16884198833657318, 0.16911968897528465, 0.1649541793946126, 0.16328797556234378, 0.16162177173007497, 0.17189669536239932]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.71056509017944 s
