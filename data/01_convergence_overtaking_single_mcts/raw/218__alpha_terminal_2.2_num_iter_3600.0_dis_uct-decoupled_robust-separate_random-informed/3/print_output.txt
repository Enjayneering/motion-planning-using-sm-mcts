Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 397, 'sum_payoffs': 86.35501795801324, 'action': [1.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 80.33471800622952, 'action': [0.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 81.01790503104577, 'action': [0.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 93.16768954162625, 'action': [2.0, -1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 96.68098136047705, 'action': [2.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 89.22752791959333, 'action': [1.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 84.52004378574958, 'action': [0.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 90.26237481163986, 'action': [2.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 84.81575444666109, 'action': [1.0, 0.0]}])
Weights num count: [0.1102471535684532, 0.1049708414329353, 0.10552624271035824, 0.11607886698139405, 0.11913357400722022, 0.11274645931685642, 0.10858094973618439, 0.11357956123299083, 0.10885865037489587]
Actions to choose Agent 1: dict_values([{'num_count': 587, 'sum_payoffs': 114.27873630271813, 'action': [0.0, 1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 124.03093658599828, 'action': [1.0, 1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 120.9903590970395, 'action': [1.0, -1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 112.57201791760333, 'action': [0.0, 0.0]}, {'num_count': 596, 'sum_payoffs': 116.79015729806322, 'action': [0.0, -1.5707963267948966]}, {'num_count': 603, 'sum_payoffs': 118.68574333327543, 'action': [1.0, 0.0]}])
Weights num count: [0.16301027492363232, 0.17272979727853374, 0.16967509025270758, 0.1613440710913635, 0.16550958067203556, 0.16745348514301583]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.75129294395447 s
