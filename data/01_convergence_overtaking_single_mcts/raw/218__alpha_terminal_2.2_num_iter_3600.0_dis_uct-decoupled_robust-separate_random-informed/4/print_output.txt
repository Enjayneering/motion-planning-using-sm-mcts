Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 390, 'sum_payoffs': 84.08364781954215, 'action': [2.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 85.30584208235145, 'action': [0.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 85.57683183105749, 'action': [0.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 100.98961300111478, 'action': [2.0, 0.0]}, {'num_count': 391, 'sum_payoffs': 84.37581019944332, 'action': [1.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 87.274861309927, 'action': [1.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 83.3925613248183, 'action': [1.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 91.64231770406849, 'action': [2.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 82.49798503916398, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10830324909747292, 0.1094140516523188, 0.10969175229103027, 0.12302138294918079, 0.10858094973618439, 0.11108025548458761, 0.10774784782004998, 0.11496806442654818, 0.10691474590391557]
Actions to choose Agent 1: dict_values([{'num_count': 585, 'sum_payoffs': 114.10342292076588, 'action': [0.0, 1.5707963267948966]}, {'num_count': 616, 'sum_payoffs': 122.75336959023645, 'action': [1.0, -1.5707963267948966]}, {'num_count': 606, 'sum_payoffs': 119.95554710367175, 'action': [1.0, 1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 112.96308717007373, 'action': [0.0, -1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 114.99376045129468, 'action': [0.0, 0.0]}, {'num_count': 624, 'sum_payoffs': 124.99675260493132, 'action': [1.0, 0.0]}])
Weights num count: [0.1624548736462094, 0.17106359344626493, 0.16828658705915023, 0.1613440710913635, 0.16328797556234378, 0.17328519855595667]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.7302303314209 s
