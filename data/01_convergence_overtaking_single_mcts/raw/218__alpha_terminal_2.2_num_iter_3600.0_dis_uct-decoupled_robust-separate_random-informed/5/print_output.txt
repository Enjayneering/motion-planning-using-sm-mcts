Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 407, 'sum_payoffs': 90.09184216722807, 'action': [2.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 95.83350374028626, 'action': [2.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 86.18714345346339, 'action': [1.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 84.82545141904285, 'action': [0.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 80.11533356594276, 'action': [0.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 85.92386558028109, 'action': [0.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 89.05763688372731, 'action': [1.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 87.14604576272825, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 90.99543494673412, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1130241599555679, 0.11802277145237434, 0.10969175229103027, 0.10858094973618439, 0.10441544015551235, 0.1094140516523188, 0.1121910580394335, 0.11052485420716468, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 585, 'sum_payoffs': 113.55305962279421, 'action': [0.0, -1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 119.9757418540367, 'action': [1.0, -1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 121.15156663243094, 'action': [1.0, 1.5707963267948966]}, {'num_count': 610, 'sum_payoffs': 120.57538265422984, 'action': [1.0, 0.0]}, {'num_count': 595, 'sum_payoffs': 116.32504256505649, 'action': [0.0, 0.0]}, {'num_count': 590, 'sum_payoffs': 115.01651963538585, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1624548736462094, 0.16884198833657318, 0.16995279089141904, 0.1693973896139961, 0.16523188003332406, 0.16384337683976674]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 107.10302948951721 s
