Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 402, 'sum_payoffs': 87.88494811167689, 'action': [1.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 86.10297699626841, 'action': [0.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 84.05831350892424, 'action': [0.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 85.68951386908654, 'action': [1.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 89.81206858054567, 'action': [2.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 87.34908317092587, 'action': [2.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 83.12630602808127, 'action': [0.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 87.34051972989255, 'action': [1.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 94.42907080769012, 'action': [2.0, 0.0]}])
Weights num count: [0.11163565676201055, 0.10996945292974174, 0.10830324909747292, 0.10969175229103027, 0.11330186059427937, 0.11108025548458761, 0.10747014718133852, 0.11108025548458761, 0.11718966953623994]
Actions to choose Agent 1: dict_values([{'num_count': 616, 'sum_payoffs': 122.56820587154725, 'action': [1.0, 0.0]}, {'num_count': 581, 'sum_payoffs': 112.79617794847712, 'action': [0.0, 1.5707963267948966]}, {'num_count': 578, 'sum_payoffs': 111.95760199636437, 'action': [0.0, 0.0]}, {'num_count': 590, 'sum_payoffs': 115.30925719884077, 'action': [0.0, -1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 122.2741149810033, 'action': [1.0, 1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 123.67056229401183, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17106359344626493, 0.1613440710913635, 0.1605109691752291, 0.16384337683976674, 0.17078589280755346, 0.1721743960011108]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 106.8853669166565 s
