Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 396, 'sum_payoffs': 86.2195926792971, 'action': [1.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 85.59570318524564, 'action': [0.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 90.36716324356341, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 85.9129927651898, 'action': [1.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 99.12622985835999, 'action': [2.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 83.35095808250246, 'action': [0.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 84.22229841896598, 'action': [0.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 86.83739410319312, 'action': [2.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 85.90092596824356, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10996945292974174, 0.1094140516523188, 0.11357956123299083, 0.10969175229103027, 0.1210774784782005, 0.10747014718133852, 0.10830324909747292, 0.11052485420716468, 0.10969175229103027]
Actions to choose Agent 1: dict_values([{'num_count': 587, 'sum_payoffs': 114.23508693799172, 'action': [0.0, -1.5707963267948966]}, {'num_count': 607, 'sum_payoffs': 119.88718626890436, 'action': [1.0, -1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 112.32671211081635, 'action': [0.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 114.0376039203577, 'action': [0.0, 0.0]}, {'num_count': 620, 'sum_payoffs': 123.51318437774489, 'action': [1.0, 1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 123.53874209538183, 'action': [1.0, 0.0]}])
Weights num count: [0.16301027492363232, 0.1685642876978617, 0.16106637045265204, 0.16273257428492086, 0.1721743960011108, 0.1721743960011108]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.4195396900177 s
