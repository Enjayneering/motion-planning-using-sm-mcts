Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 420, 'sum_payoffs': 94.04687030819395, 'action': [2.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 87.84499419689679, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 83.4085301579623, 'action': [0.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 89.4416262582553, 'action': [2.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 87.80230898957572, 'action': [1.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 88.20497687526074, 'action': [2.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 86.8533479948462, 'action': [0.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 82.79321158048062, 'action': [0.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 87.55403916546115, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.116634268258817, 0.11135795612329909, 0.10747014718133852, 0.11274645931685642, 0.11135795612329909, 0.11163565676201055, 0.11052485420716468, 0.10691474590391557, 0.11108025548458761]
Actions to choose Agent 1: dict_values([{'num_count': 614, 'sum_payoffs': 122.02807912891157, 'action': [1.0, 1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 123.63449358669658, 'action': [1.0, 0.0]}, {'num_count': 603, 'sum_payoffs': 118.83581348374013, 'action': [1.0, -1.5707963267948966]}, {'num_count': 592, 'sum_payoffs': 115.82007228748722, 'action': [0.0, 0.0]}, {'num_count': 588, 'sum_payoffs': 114.68560885899063, 'action': [0.0, -1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 113.37725710779766, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.170508192168842, 0.1721743960011108, 0.16745348514301583, 0.16439877811718967, 0.16328797556234378, 0.16189947236878643]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 99.21814179420471 s
