Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 396, 'sum_payoffs': 85.62416019179672, 'action': [1.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 91.07508582824028, 'action': [2.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 82.75058294460527, 'action': [0.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 85.65799244225703, 'action': [0.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 86.6182404933768, 'action': [1.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 82.75787954068967, 'action': [0.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 83.80564072846104, 'action': [1.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 92.62365144655539, 'action': [2.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 91.38070371564308, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10996945292974174, 0.11469036378783672, 0.10747014718133852, 0.10996945292974174, 0.11080255484587614, 0.10747014718133852, 0.10830324909747292, 0.11607886698139405, 0.11496806442654818]
Actions to choose Agent 1: dict_values([{'num_count': 608, 'sum_payoffs': 120.32799008284687, 'action': [1.0, 1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 120.32781526323855, 'action': [1.0, -1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 115.57659490842322, 'action': [0.0, -1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 125.3929588371515, 'action': [1.0, 0.0]}, {'num_count': 590, 'sum_payoffs': 115.33325991987898, 'action': [0.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 111.61298654613027, 'action': [0.0, 0.0]}])
Weights num count: [0.16884198833657318, 0.16884198833657318, 0.1641210774784782, 0.17384059983337963, 0.16384337683976674, 0.16023326853651762]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 103.36506748199463 s
