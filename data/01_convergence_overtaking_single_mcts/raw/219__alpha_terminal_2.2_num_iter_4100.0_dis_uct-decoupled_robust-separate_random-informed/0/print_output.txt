Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 469, 'sum_payoffs': 102.96889859066057, 'action': [1.0, -1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 90.07667673372114, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 100.1221627226354, 'action': [1.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 97.31046452692559, 'action': [2.0, 1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 103.51564831977838, 'action': [2.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 95.05320679064687, 'action': [0.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 108.28109351658203, 'action': [2.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 94.05751795516959, 'action': [0.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 97.02599754841904, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11436235064618386, 0.10436478907583516, 0.11216776396000976, 0.10997317727383565, 0.11485003657644477, 0.10826627651792246, 0.1185076810534016, 0.1075347476225311, 0.1097293343087052]
Actions to choose Agent 1: dict_values([{'num_count': 696, 'sum_payoffs': 136.83353074059323, 'action': [1.0, -1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 126.73237740764442, 'action': [0.0, 0.0]}, {'num_count': 676, 'sum_payoffs': 131.29758930026725, 'action': [0.0, 1.5707963267948966]}, {'num_count': 684, 'sum_payoffs': 133.4695897508619, 'action': [1.0, 1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 140.63684700838357, 'action': [1.0, 0.0]}, {'num_count': 675, 'sum_payoffs': 131.08382327869901, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16971470373079736, 0.16069251402097048, 0.16483784442818825, 0.1667885881492319, 0.17312850524262374, 0.1645940014630578]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 127.862548828125 s
