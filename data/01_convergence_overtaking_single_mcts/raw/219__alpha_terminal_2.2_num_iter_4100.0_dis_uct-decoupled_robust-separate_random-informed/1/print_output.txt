Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 436, 'sum_payoffs': 92.40774962163111, 'action': [0.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 101.11070764318956, 'action': [2.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 96.74130232099085, 'action': [1.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 93.31177411902016, 'action': [0.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 102.92025984645439, 'action': [2.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 97.38452405475388, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 95.78488568696393, 'action': [0.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 97.40595819047265, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 109.2535624215663, 'action': [2.0, 0.0]}])
Weights num count: [0.1063155327968788, 0.11314313582053158, 0.1097293343087052, 0.10704706169227018, 0.11460619361131431, 0.11021702023896611, 0.10899780541331383, 0.11021702023896611, 0.11948305291392343]
Actions to choose Agent 1: dict_values([{'num_count': 674, 'sum_payoffs': 130.68968638992658, 'action': [0.0, 0.0]}, {'num_count': 700, 'sum_payoffs': 137.78262368186313, 'action': [1.0, -1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 127.15900963038514, 'action': [0.0, -1.5707963267948966]}, {'num_count': 713, 'sum_payoffs': 141.33419496128087, 'action': [1.0, 0.0]}, {'num_count': 688, 'sum_payoffs': 134.47194378257456, 'action': [1.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 127.99857659757691, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16435015849792733, 0.17069007559131918, 0.1611801999512314, 0.17386003413801512, 0.16776396000975372, 0.16191172884662278]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.73828172683716 s
