Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 436, 'sum_payoffs': 92.63520827131255, 'action': [0.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 98.56218355103096, 'action': [1.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 98.27114654773426, 'action': [1.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 105.12995770119785, 'action': [2.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 97.294057372826, 'action': [1.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 100.45172090291427, 'action': [2.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 106.96300299039468, 'action': [2.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 95.44513185746138, 'action': [0.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 93.83079403621895, 'action': [0.0, 0.0]}])
Weights num count: [0.1063155327968788, 0.11094854913435748, 0.11070470616922702, 0.11606925140209705, 0.10997317727383565, 0.11241160692514021, 0.11753230919287978, 0.10851011948305292, 0.10729090465740064]
Actions to choose Agent 1: dict_values([{'num_count': 671, 'sum_payoffs': 129.3145203550457, 'action': [0.0, 0.0]}, {'num_count': 698, 'sum_payoffs': 136.66146874717512, 'action': [1.0, -1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 127.31435087156001, 'action': [0.0, -1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 135.56618437814365, 'action': [1.0, 1.5707963267948966]}, {'num_count': 671, 'sum_payoffs': 129.24249284505112, 'action': [0.0, 1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 137.6702775521881, 'action': [1.0, 0.0]}])
Weights num count: [0.16361862960253595, 0.17020238966105827, 0.16191172884662278, 0.16922701780053645, 0.16361862960253595, 0.1711777615215801]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.96540474891663 s
