Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 486, 'sum_payoffs': 107.98670640153222, 'action': [2.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 96.5708598564031, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 100.12190566979949, 'action': [2.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 101.0476301266293, 'action': [2.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 101.02143009132912, 'action': [1.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 95.15241653244367, 'action': [0.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 91.36562148139926, 'action': [0.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 93.86979292387663, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 98.60406164504427, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1185076810534016, 0.1097293343087052, 0.11241160692514021, 0.11314313582053158, 0.11314313582053158, 0.10851011948305292, 0.10558400390148744, 0.1075347476225311, 0.11119239209948793]
Actions to choose Agent 1: dict_values([{'num_count': 708, 'sum_payoffs': 139.99814240906466, 'action': [1.0, 0.0]}, {'num_count': 653, 'sum_payoffs': 125.01728588615224, 'action': [0.0, 0.0]}, {'num_count': 672, 'sum_payoffs': 130.210652004544, 'action': [0.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 126.92422814814607, 'action': [0.0, -1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 137.51249171768248, 'action': [1.0, 1.5707963267948966]}, {'num_count': 708, 'sum_payoffs': 139.9304557520945, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17264081931236283, 0.15922945623018775, 0.16386247256766642, 0.16093635698610095, 0.17044623262618874, 0.17264081931236283]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.32315707206726 s
