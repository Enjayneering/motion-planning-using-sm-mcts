Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 433, 'sum_payoffs': 91.41215535843538, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 101.07988997147122, 'action': [2.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 99.80883645861489, 'action': [1.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 94.2733130439826, 'action': [0.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 105.54384520093237, 'action': [2.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 101.08321726805929, 'action': [2.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 95.8239531289466, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 98.60408120121502, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 98.62038827934494, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10558400390148744, 0.11314313582053158, 0.11216776396000976, 0.10777859058766155, 0.11655693733235796, 0.11314313582053158, 0.10899780541331383, 0.11119239209948793, 0.11119239209948793]
Actions to choose Agent 1: dict_values([{'num_count': 698, 'sum_payoffs': 137.30713768945438, 'action': [1.0, -1.5707963267948966]}, {'num_count': 692, 'sum_payoffs': 135.69659279348824, 'action': [1.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 128.01431085840255, 'action': [0.0, 0.0]}, {'num_count': 667, 'sum_payoffs': 128.90710422258226, 'action': [0.0, 1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 141.77764777103818, 'action': [1.0, 0.0]}, {'num_count': 665, 'sum_payoffs': 128.34588179660352, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17020238966105827, 0.16873933187027554, 0.16191172884662278, 0.16264325774201413, 0.17410387710314557, 0.16215557181175322]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.66248369216919 s
