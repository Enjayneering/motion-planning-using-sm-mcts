Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 466, 'sum_payoffs': 101.6710085977207, 'action': [2.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 92.3922163089935, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 99.13119032234104, 'action': [2.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 95.42223690176802, 'action': [0.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 95.80464352717395, 'action': [1.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 93.60089727937907, 'action': [0.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 96.43533068505559, 'action': [1.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 98.87538482501013, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 112.66643666611714, 'action': [2.0, 0.0]}])
Weights num count: [0.11363082175079249, 0.1063155327968788, 0.11168007802974884, 0.10875396244818337, 0.10899780541331383, 0.10729090465740064, 0.10948549134357474, 0.11143623506461839, 0.12216532553035844]
Actions to choose Agent 1: dict_values([{'num_count': 707, 'sum_payoffs': 139.98028169118354, 'action': [1.0, 0.0]}, {'num_count': 662, 'sum_payoffs': 127.61574864893808, 'action': [0.0, 1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 137.23350096428413, 'action': [1.0, 1.5707963267948966]}, {'num_count': 668, 'sum_payoffs': 129.32880033708224, 'action': [0.0, 0.0]}, {'num_count': 669, 'sum_payoffs': 129.56290075005697, 'action': [0.0, -1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 137.22081282167588, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1723969763472324, 0.16142404291636187, 0.16995854669592783, 0.1628871007071446, 0.16313094367227504, 0.16995854669592783]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.96963119506836 s
