Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 459, 'sum_payoffs': 99.8847230955322, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 94.9099928779787, 'action': [0.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 101.10965446071843, 'action': [2.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 98.64358621909082, 'action': [1.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 108.90616657714038, 'action': [2.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 91.14805298346567, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 95.75501991859504, 'action': [0.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 95.79734281449754, 'action': [1.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 102.99311138654785, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1119239209948793, 0.10802243355279201, 0.11289929285540112, 0.11094854913435748, 0.11899536698366252, 0.10509631797122652, 0.10875396244818337, 0.10875396244818337, 0.11436235064618386]
Actions to choose Agent 1: dict_values([{'num_count': 672, 'sum_payoffs': 129.94336655605656, 'action': [0.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 139.4770382981128, 'action': [1.0, 0.0]}, {'num_count': 659, 'sum_payoffs': 126.38119544120369, 'action': [0.0, -1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 134.87533800663277, 'action': [1.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 140.00169043868007, 'action': [1.0, -1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 127.52213787839888, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16386247256766642, 0.1723969763472324, 0.16069251402097048, 0.16825164594001463, 0.1728846622774933, 0.1616678858814923]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.50709009170532 s
