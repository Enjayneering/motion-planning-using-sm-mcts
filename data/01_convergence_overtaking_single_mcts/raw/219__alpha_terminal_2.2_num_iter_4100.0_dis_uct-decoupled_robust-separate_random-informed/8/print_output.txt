Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 458, 'sum_payoffs': 99.09544672517441, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 102.23449678218631, 'action': [2.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 96.92532587909704, 'action': [0.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 91.28044609461784, 'action': [0.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 105.98767103306238, 'action': [2.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 101.84702399273117, 'action': [2.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 94.45673479034095, 'action': [0.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 93.17642177609872, 'action': [1.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 100.02257012409214, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11168007802974884, 0.1141185076810534, 0.10997317727383565, 0.10558400390148744, 0.11704462326261887, 0.11387466471592295, 0.10802243355279201, 0.10704706169227018, 0.11241160692514021]
Actions to choose Agent 1: dict_values([{'num_count': 686, 'sum_payoffs': 133.72235593932012, 'action': [1.0, -1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 140.06294573770046, 'action': [1.0, 0.0]}, {'num_count': 689, 'sum_payoffs': 134.5834426679073, 'action': [1.0, 1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 128.01724072267828, 'action': [0.0, 1.5707963267948966]}, {'num_count': 672, 'sum_payoffs': 129.8800963055527, 'action': [0.0, 0.0]}, {'num_count': 679, 'sum_payoffs': 131.7737139120546, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1672762740794928, 0.1728846622774933, 0.16800780297488419, 0.16215557181175322, 0.16386247256766642, 0.16556937332357963]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 117.5189745426178 s
