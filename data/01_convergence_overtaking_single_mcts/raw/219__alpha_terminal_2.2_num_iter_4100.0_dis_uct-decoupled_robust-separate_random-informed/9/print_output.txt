Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 444, 'sum_payoffs': 94.94450737519308, 'action': [0.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 94.31665954222096, 'action': [0.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 101.43369619040664, 'action': [2.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 97.71275794760496, 'action': [1.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 97.31969524033313, 'action': [1.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 98.9345686366767, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 108.38604146890916, 'action': [2.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 95.24599554684842, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 98.36822038717136, 'action': [1.0, 0.0]}])
Weights num count: [0.10826627651792246, 0.10777859058766155, 0.11338697878566203, 0.11046086320409657, 0.11021702023896611, 0.11143623506461839, 0.11875152401853206, 0.10851011948305292, 0.11094854913435748]
Actions to choose Agent 1: dict_values([{'num_count': 671, 'sum_payoffs': 129.75573571525368, 'action': [0.0, 0.0]}, {'num_count': 705, 'sum_payoffs': 138.9933599680874, 'action': [1.0, -1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 138.45727545955438, 'action': [1.0, 0.0]}, {'num_count': 672, 'sum_payoffs': 129.99658799544693, 'action': [0.0, -1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 126.45301251174777, 'action': [0.0, 1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 134.9343661563474, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16361862960253595, 0.17190929041697148, 0.17142160448671057, 0.16386247256766642, 0.16069251402097048, 0.16825164594001463]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 116.65922284126282 s
