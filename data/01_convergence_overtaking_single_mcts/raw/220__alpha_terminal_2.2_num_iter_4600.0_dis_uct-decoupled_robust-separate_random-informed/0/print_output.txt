Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 502, 'sum_payoffs': 106.96049478104801, 'action': [1.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 108.10242016407128, 'action': [1.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 108.15497472669692, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 106.95651096604708, 'action': [0.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 109.9899899011004, 'action': [2.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 102.37567936972, 'action': [0.0, 1.5707963267948966]}, {'num_count': 558, 'sum_payoffs': 124.10680330575644, 'action': [2.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 111.20495422527353, 'action': [2.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 109.65142482609906, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10910671593131928, 0.10997609215387959, 0.10997609215387959, 0.10910671593131928, 0.11128015648772006, 0.1058465550967181, 0.12127798304716365, 0.11214953271028037, 0.11106281243207998]
Actions to choose Agent 1: dict_values([{'num_count': 785, 'sum_payoffs': 152.95325419497922, 'action': [1.0, 1.5707963267948966]}, {'num_count': 741, 'sum_payoffs': 141.27369810090389, 'action': [0.0, -1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 140.13572299228716, 'action': [0.0, 1.5707963267948966]}, {'num_count': 781, 'sum_payoffs': 151.88935184025482, 'action': [1.0, -1.5707963267948966]}, {'num_count': 807, 'sum_payoffs': 158.93430023677973, 'action': [1.0, 0.0]}, {'num_count': 749, 'sum_payoffs': 143.29629671072024, 'action': [0.0, 0.0]}])
Weights num count: [0.17061508367746142, 0.16105194522929797, 0.16018256900673766, 0.1697457074549011, 0.17539665290154313, 0.16279069767441862]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 135.29027152061462 s
