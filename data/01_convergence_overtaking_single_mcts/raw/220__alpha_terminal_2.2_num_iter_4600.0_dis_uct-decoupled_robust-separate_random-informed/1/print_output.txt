Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 549, 'sum_payoffs': 120.92949127974103, 'action': [2.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 107.97198900741044, 'action': [1.0, 1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 101.66304829540037, 'action': [0.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 108.5867773006084, 'action': [0.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 107.67457425057451, 'action': [1.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 114.7153486256423, 'action': [2.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 105.59815526633052, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 106.52873837290262, 'action': [1.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 109.84170651572994, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11932188654640295, 0.11019343620951967, 0.10562921104107803, 0.11062812432079983, 0.10997609215387959, 0.1149750054336014, 0.10845468376439904, 0.10910671593131928, 0.11149750054336013]
Actions to choose Agent 1: dict_values([{'num_count': 741, 'sum_payoffs': 141.24600319917909, 'action': [0.0, -1.5707963267948966]}, {'num_count': 788, 'sum_payoffs': 153.9008563148143, 'action': [1.0, -1.5707963267948966]}, {'num_count': 741, 'sum_payoffs': 141.27442435728784, 'action': [0.0, 1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 157.0609324163164, 'action': [1.0, 0.0]}, {'num_count': 779, 'sum_payoffs': 151.37342457751726, 'action': [1.0, 1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 143.89865164807074, 'action': [0.0, 0.0]}])
Weights num count: [0.16105194522929797, 0.17126711584438165, 0.16105194522929797, 0.1738752445120626, 0.16931101934362094, 0.16322538578569876]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 130.7715208530426 s
