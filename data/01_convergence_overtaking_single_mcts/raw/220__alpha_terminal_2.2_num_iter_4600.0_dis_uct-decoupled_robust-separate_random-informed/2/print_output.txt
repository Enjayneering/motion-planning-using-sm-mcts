Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 497, 'sum_payoffs': 105.05456625030439, 'action': [0.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 110.2381284235754, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 106.27751267435887, 'action': [0.0, -1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 111.99509139975534, 'action': [2.0, -1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 112.0915176678065, 'action': [2.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 107.21179694962878, 'action': [1.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 105.0909062854652, 'action': [1.0, 0.0]}, {'num_count': 554, 'sum_payoffs': 122.4820459104852, 'action': [2.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 103.79738510075155, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10801999565311889, 0.11171484459900022, 0.1088893718756792, 0.11301890893284068, 0.11301890893284068, 0.10954140404259943, 0.10801999565311889, 0.12040860682460335, 0.10715061943055858]
Actions to choose Agent 1: dict_values([{'num_count': 749, 'sum_payoffs': 143.47337305296927, 'action': [0.0, 0.0]}, {'num_count': 746, 'sum_payoffs': 142.65241885794882, 'action': [0.0, 1.5707963267948966]}, {'num_count': 783, 'sum_payoffs': 152.53994689337782, 'action': [1.0, -1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 155.24842854708973, 'action': [1.0, 1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 140.27654396063932, 'action': [0.0, -1.5707963267948966]}, {'num_count': 792, 'sum_payoffs': 154.89502764174722, 'action': [1.0, 0.0]}])
Weights num count: [0.16279069767441862, 0.16213866550749836, 0.17018039556618125, 0.17235383612258204, 0.16018256900673766, 0.17213649206694198]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 130.73654627799988 s
