Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 509, 'sum_payoffs': 108.56377075608852, 'action': [1.0, -1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 108.19529626161099, 'action': [1.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 114.35569731045244, 'action': [2.0, 1.5707963267948966]}, {'num_count': 547, 'sum_payoffs': 120.10199576107253, 'action': [2.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 103.38338401533909, 'action': [0.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 112.20887443233755, 'action': [2.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 103.12440960377786, 'action': [0.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 108.5448734563972, 'action': [1.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 104.3162203332775, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11062812432079983, 0.11041078026515974, 0.11475766137796131, 0.1188871984351228, 0.1069332753749185, 0.11323625298848077, 0.10671593131927842, 0.11062812432079983, 0.10758530754183873]
Actions to choose Agent 1: dict_values([{'num_count': 745, 'sum_payoffs': 142.8446226317153, 'action': [0.0, 0.0]}, {'num_count': 734, 'sum_payoffs': 139.81522464702215, 'action': [0.0, 1.5707963267948966]}, {'num_count': 742, 'sum_payoffs': 141.9703077162723, 'action': [0.0, -1.5707963267948966]}, {'num_count': 788, 'sum_payoffs': 154.38501859473527, 'action': [1.0, 0.0]}, {'num_count': 802, 'sum_payoffs': 158.07086651539478, 'action': [1.0, 1.5707963267948966]}, {'num_count': 789, 'sum_payoffs': 154.6598267036073, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16192132145185828, 0.15953053683981744, 0.16126928928493806, 0.17126711584438165, 0.17430993262334274, 0.17148445990002173]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 130.37816214561462 s
