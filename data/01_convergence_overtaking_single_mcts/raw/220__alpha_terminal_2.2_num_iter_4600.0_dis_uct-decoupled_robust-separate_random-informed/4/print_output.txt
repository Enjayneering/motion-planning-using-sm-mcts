Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 491, 'sum_payoffs': 103.40164234078975, 'action': [0.0, 1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 107.40309046811493, 'action': [0.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 114.39812058616192, 'action': [2.0, -1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 116.8246342832508, 'action': [2.0, 0.0]}, {'num_count': 518, 'sum_payoffs': 111.6218488778529, 'action': [1.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 109.48496933727296, 'action': [2.0, 1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 108.59076744153786, 'action': [1.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 106.13841368442901, 'action': [0.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 107.98652721516807, 'action': [1.0, 0.0]}])
Weights num count: [0.10671593131927842, 0.10954140404259943, 0.11454031732232123, 0.11627906976744186, 0.11258422082156053, 0.11106281243207998, 0.11041078026515974, 0.10867202782003912, 0.10997609215387959]
Actions to choose Agent 1: dict_values([{'num_count': 774, 'sum_payoffs': 150.31287296713296, 'action': [1.0, 1.5707963267948966]}, {'num_count': 743, 'sum_payoffs': 141.92753404384723, 'action': [0.0, 1.5707963267948966]}, {'num_count': 758, 'sum_payoffs': 145.97394671805895, 'action': [0.0, 0.0]}, {'num_count': 739, 'sum_payoffs': 140.93394354020782, 'action': [0.0, -1.5707963267948966]}, {'num_count': 806, 'sum_payoffs': 158.86060385252276, 'action': [1.0, 0.0]}, {'num_count': 780, 'sum_payoffs': 151.87320263011787, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16822429906542055, 0.16148663334057814, 0.16474679417517932, 0.16061725711801783, 0.17517930884590308, 0.16952836339926103]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 130.6846661567688 s
