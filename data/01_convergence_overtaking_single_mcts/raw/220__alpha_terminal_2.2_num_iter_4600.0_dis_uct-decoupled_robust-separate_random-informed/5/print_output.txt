Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 506, 'sum_payoffs': 107.96222955371923, 'action': [1.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 105.84481661354488, 'action': [0.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 110.70988203390765, 'action': [1.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 102.56055993032167, 'action': [0.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 103.71555708706151, 'action': [0.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 108.31094458981208, 'action': [1.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 118.12160235729915, 'action': [2.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 110.14807090492735, 'action': [2.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 118.70766319291005, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10997609215387959, 0.10845468376439904, 0.1119321886546403, 0.10606389915235818, 0.1069332753749185, 0.11019343620951967, 0.11714844599000217, 0.11149750054336013, 0.11758313410128234]
Actions to choose Agent 1: dict_values([{'num_count': 740, 'sum_payoffs': 140.83947308327308, 'action': [0.0, 1.5707963267948966]}, {'num_count': 765, 'sum_payoffs': 147.4964455760936, 'action': [0.0, 0.0]}, {'num_count': 788, 'sum_payoffs': 153.6336618404562, 'action': [1.0, 0.0]}, {'num_count': 787, 'sum_payoffs': 153.35970748294255, 'action': [1.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 139.79176131554544, 'action': [0.0, -1.5707963267948966]}, {'num_count': 784, 'sum_payoffs': 152.57993964912345, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1608346011736579, 0.16626820256465985, 0.17126711584438165, 0.1710497717887416, 0.15996522495109758, 0.17039773962182134]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 129.82891821861267 s
