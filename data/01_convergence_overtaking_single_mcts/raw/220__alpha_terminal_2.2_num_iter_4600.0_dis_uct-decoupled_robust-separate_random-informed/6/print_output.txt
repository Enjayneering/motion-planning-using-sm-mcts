Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 500, 'sum_payoffs': 106.37795454495237, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 101.80365718843078, 'action': [0.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 107.23544055841117, 'action': [1.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 103.3234347742511, 'action': [0.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 113.32236013402134, 'action': [1.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 110.88818262378561, 'action': [2.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 111.24230203168521, 'action': [2.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 107.81438623960798, 'action': [1.0, -1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 125.62955720395965, 'action': [2.0, 0.0]}])
Weights num count: [0.10867202782003912, 0.10541186698543795, 0.10932405998695936, 0.10649858726363834, 0.11367094109976092, 0.1119321886546403, 0.11214953271028037, 0.10975874809823952, 0.12236470332536405]
Actions to choose Agent 1: dict_values([{'num_count': 749, 'sum_payoffs': 143.07327736532665, 'action': [0.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 139.67953632962008, 'action': [0.0, -1.5707963267948966]}, {'num_count': 782, 'sum_payoffs': 151.93781304981098, 'action': [1.0, -1.5707963267948966]}, {'num_count': 782, 'sum_payoffs': 151.95349134237517, 'action': [1.0, 1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 154.87207050411632, 'action': [1.0, 0.0]}, {'num_count': 758, 'sum_payoffs': 145.40377006646298, 'action': [0.0, 0.0]}])
Weights num count: [0.16279069767441862, 0.15996522495109758, 0.1699630515105412, 0.1699630515105412, 0.17235383612258204, 0.16474679417517932]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 130.1355242729187 s
