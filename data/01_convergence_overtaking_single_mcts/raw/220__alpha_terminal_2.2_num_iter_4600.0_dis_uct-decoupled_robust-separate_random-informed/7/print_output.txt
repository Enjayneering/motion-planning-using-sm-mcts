Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 507, 'sum_payoffs': 108.63581847542692, 'action': [1.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 111.7882656544394, 'action': [2.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 109.62808661541271, 'action': [1.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 109.3106953491915, 'action': [1.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 103.47865895858978, 'action': [0.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 125.57989328512126, 'action': [2.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 102.31573016490158, 'action': [0.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 114.84852538860257, 'action': [2.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 104.13848001532354, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11019343620951967, 0.11236687676592046, 0.11084546837643991, 0.11062812432079983, 0.10649858726363834, 0.12214735926972398, 0.10562921104107803, 0.11454031732232123, 0.1069332753749185]
Actions to choose Agent 1: dict_values([{'num_count': 744, 'sum_payoffs': 141.7490843981485, 'action': [0.0, -1.5707963267948966]}, {'num_count': 758, 'sum_payoffs': 145.46298031195101, 'action': [0.0, 0.0]}, {'num_count': 777, 'sum_payoffs': 150.50736449352974, 'action': [1.0, -1.5707963267948966]}, {'num_count': 801, 'sum_payoffs': 156.99454575483642, 'action': [1.0, 0.0]}, {'num_count': 754, 'sum_payoffs': 144.4020506043221, 'action': [0.0, 1.5707963267948966]}, {'num_count': 766, 'sum_payoffs': 147.5621275349816, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16170397739621822, 0.16474679417517932, 0.1688763312323408, 0.17409258856770268, 0.163877417952619, 0.16648554662029993]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 130.357901096344 s
