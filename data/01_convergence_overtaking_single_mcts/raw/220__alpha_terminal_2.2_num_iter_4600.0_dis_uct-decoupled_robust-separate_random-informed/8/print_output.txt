Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 490, 'sum_payoffs': 103.38194196472524, 'action': [0.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 110.71903826512873, 'action': [1.0, 0.0]}, {'num_count': 551, 'sum_payoffs': 122.11759461807252, 'action': [2.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 104.6108144954814, 'action': [1.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 113.47811176421841, 'action': [2.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 105.4873039839156, 'action': [0.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 110.41928974321112, 'action': [1.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 106.0614325820098, 'action': [0.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 112.26963333156594, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10649858726363834, 0.11171484459900022, 0.11975657465768311, 0.10736796348619865, 0.11367094109976092, 0.10801999565311889, 0.11149750054336013, 0.10845468376439904, 0.11280156487720061]
Actions to choose Agent 1: dict_values([{'num_count': 779, 'sum_payoffs': 151.0770484649054, 'action': [1.0, -1.5707963267948966]}, {'num_count': 742, 'sum_payoffs': 141.25610223736797, 'action': [0.0, -1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 141.743903795595, 'action': [0.0, 1.5707963267948966]}, {'num_count': 787, 'sum_payoffs': 153.19535320553038, 'action': [1.0, 1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 156.69487042983144, 'action': [1.0, 0.0]}, {'num_count': 748, 'sum_payoffs': 142.76030256448618, 'action': [0.0, 0.0]}])
Weights num count: [0.16931101934362094, 0.16126928928493806, 0.16170397739621822, 0.1710497717887416, 0.1738752445120626, 0.16257335361877853]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 140.00637936592102 s
