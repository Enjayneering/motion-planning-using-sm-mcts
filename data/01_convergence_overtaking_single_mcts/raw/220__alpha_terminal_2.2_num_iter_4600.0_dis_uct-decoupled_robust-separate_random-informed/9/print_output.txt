Searching game tree in timestep 0...
Max timehorizon: 15
Actions to choose Agent 0: dict_values([{'num_count': 566, 'sum_payoffs': 126.72693798286849, 'action': [2.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 104.31502790683224, 'action': [0.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 102.54018443981796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 110.09901463962709, 'action': [1.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 109.17225311360912, 'action': [1.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 110.43703306596807, 'action': [1.0, -1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 113.73693005696514, 'action': [2.0, -1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 100.08666313492591, 'action': [0.0, 0.0]}, {'num_count': 517, 'sum_payoffs': 111.57157814829942, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12301673549228428, 0.10715061943055858, 0.1058465550967181, 0.11128015648772006, 0.11062812432079983, 0.11149750054336013, 0.113888285155401, 0.10410780265159748, 0.11236687676592046]
Actions to choose Agent 1: dict_values([{'num_count': 783, 'sum_payoffs': 152.02894263001053, 'action': [1.0, -1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 143.47424730145713, 'action': [0.0, -1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 154.68170081144635, 'action': [1.0, 0.0]}, {'num_count': 745, 'sum_payoffs': 141.90855252914344, 'action': [0.0, 0.0]}, {'num_count': 777, 'sum_payoffs': 150.3628877142665, 'action': [1.0, 1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 143.45761103817676, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17018039556618125, 0.16322538578569876, 0.17235383612258204, 0.16192132145185828, 0.1688763312323408, 0.16322538578569876]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 137.84685277938843 s
