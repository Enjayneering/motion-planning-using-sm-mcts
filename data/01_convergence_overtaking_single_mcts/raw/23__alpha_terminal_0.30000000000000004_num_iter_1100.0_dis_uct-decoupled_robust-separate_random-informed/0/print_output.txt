Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 134, 'sum_payoffs': 33.63216941699736, 'action': [1.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 27.514286330558285, 'action': [1.0, 1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 26.743584724960613, 'action': [1.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 27.655375206396386, 'action': [0.0, 0.0]}, {'num_count': 127, 'sum_payoffs': 30.897811958455495, 'action': [2.0, -1.5707963267948966]}, {'num_count': 108, 'sum_payoffs': 23.107793925321836, 'action': [0.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.219390299666056, 'action': [2.0, 1.5707963267948966]}, {'num_count': 141, 'sum_payoffs': 36.563638464553584, 'action': [2.0, 0.0]}, {'num_count': 107, 'sum_payoffs': 22.832496791318793, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12170753860127158, 0.1080835603996367, 0.10626702997275204, 0.1080835603996367, 0.11534968210717529, 0.09809264305177112, 0.11625794732061762, 0.12806539509536785, 0.0971843778383288]
Actions to choose Agent 1: dict_values([{'num_count': 184, 'sum_payoffs': 52.144869585587024, 'action': [0.0, 0.0]}, {'num_count': 165, 'sum_payoffs': 44.09143253729215, 'action': [0.0, -1.5707963267948966]}, {'num_count': 204, 'sum_payoffs': 60.52531704163069, 'action': [1.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 54.643004575633235, 'action': [1.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 55.975707789025314, 'action': [1.0, -1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 43.79310344102772, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16712079927338783, 0.14986376021798364, 0.18528610354223432, 0.17257039055404177, 0.17529518619436876, 0.14895549500454133]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.9194555282592773 s
