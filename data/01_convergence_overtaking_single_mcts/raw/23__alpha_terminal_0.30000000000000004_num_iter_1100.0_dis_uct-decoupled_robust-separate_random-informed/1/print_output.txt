Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 108, 'sum_payoffs': 23.426547591920894, 'action': [0.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 26.94066010029082, 'action': [1.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 27.680435140188592, 'action': [0.0, 0.0]}, {'num_count': 129, 'sum_payoffs': 31.778675874191347, 'action': [2.0, -1.5707963267948966]}, {'num_count': 107, 'sum_payoffs': 22.91940985651083, 'action': [0.0, 1.5707963267948966]}, {'num_count': 144, 'sum_payoffs': 38.057746482776004, 'action': [2.0, 0.0]}, {'num_count': 130, 'sum_payoffs': 32.22598844973236, 'action': [1.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 27.383916732770217, 'action': [1.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.399083061957842, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.09809264305177112, 0.10626702997275204, 0.1080835603996367, 0.11716621253405994, 0.0971843778383288, 0.1307901907356948, 0.11807447774750227, 0.10717529518619437, 0.11625794732061762]
Actions to choose Agent 1: dict_values([{'num_count': 190, 'sum_payoffs': 54.22853789598056, 'action': [1.0, -1.5707963267948966]}, {'num_count': 203, 'sum_payoffs': 59.82313190242126, 'action': [1.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 50.418920965957554, 'action': [0.0, 0.0]}, {'num_count': 194, 'sum_payoffs': 55.89455271439863, 'action': [1.0, 1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 44.63616017339053, 'action': [0.0, -1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 43.9117397750004, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17257039055404177, 0.184377838328792, 0.16439600363306087, 0.17620345140781107, 0.1516802906448683, 0.14986376021798364]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.9102973937988281 s
