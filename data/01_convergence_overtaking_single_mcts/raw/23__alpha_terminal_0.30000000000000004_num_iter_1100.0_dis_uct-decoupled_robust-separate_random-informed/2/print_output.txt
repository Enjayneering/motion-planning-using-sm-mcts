Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 144, 'sum_payoffs': 38.011247992883774, 'action': [2.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 27.19270798934773, 'action': [1.0, -1.5707963267948966]}, {'num_count': 107, 'sum_payoffs': 22.925276488418547, 'action': [0.0, 1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 26.84788040319106, 'action': [1.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 27.655375206396393, 'action': [0.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 31.306303364858113, 'action': [2.0, 1.5707963267948966]}, {'num_count': 109, 'sum_payoffs': 23.62644764186259, 'action': [0.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 30.897811958455506, 'action': [2.0, -1.5707963267948966]}, {'num_count': 131, 'sum_payoffs': 32.600619974481184, 'action': [1.0, 0.0]}])
Weights num count: [0.1307901907356948, 0.10717529518619437, 0.0971843778383288, 0.10626702997275204, 0.1080835603996367, 0.11625794732061762, 0.09900090826521345, 0.11534968210717529, 0.11898274296094459]
Actions to choose Agent 1: dict_values([{'num_count': 206, 'sum_payoffs': 61.15290179985951, 'action': [1.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 49.74483771783651, 'action': [0.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 45.10266605081607, 'action': [0.0, 1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 55.987332411498365, 'action': [1.0, -1.5707963267948966]}, {'num_count': 163, 'sum_payoffs': 43.028159825995374, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 54.38498141332622, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18710263396911897, 0.16257947320617622, 0.15258855585831063, 0.17620345140781107, 0.148047229791099, 0.17257039055404177]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.8909635543823242 s
