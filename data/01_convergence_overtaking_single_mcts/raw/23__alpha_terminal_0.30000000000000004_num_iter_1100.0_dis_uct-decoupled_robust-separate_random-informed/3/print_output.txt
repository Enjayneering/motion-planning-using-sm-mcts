Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 108, 'sum_payoffs': 23.212089603552286, 'action': [0.0, -1.5707963267948966]}, {'num_count': 146, 'sum_payoffs': 38.80813215938144, 'action': [2.0, 0.0]}, {'num_count': 132, 'sum_payoffs': 32.969602150007034, 'action': [1.0, 0.0]}, {'num_count': 129, 'sum_payoffs': 31.651130951014768, 'action': [2.0, -1.5707963267948966]}, {'num_count': 107, 'sum_payoffs': 22.884644630434014, 'action': [0.0, 1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 26.830497790152652, 'action': [1.0, 1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 26.760967337999023, 'action': [1.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 30.903678590363196, 'action': [2.0, 1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 26.808371172314335, 'action': [0.0, 0.0]}])
Weights num count: [0.09809264305177112, 0.13260672116257946, 0.11989100817438691, 0.11716621253405994, 0.0971843778383288, 0.10626702997275204, 0.10626702997275204, 0.11534968210717529, 0.10626702997275204]
Actions to choose Agent 1: dict_values([{'num_count': 192, 'sum_payoffs': 55.23108010298523, 'action': [1.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 50.6893654538183, 'action': [0.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 53.69261020774372, 'action': [1.0, 1.5707963267948966]}, {'num_count': 210, 'sum_payoffs': 62.98799874386173, 'action': [1.0, 0.0]}, {'num_count': 162, 'sum_payoffs': 42.825326460099824, 'action': [0.0, 1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 44.81585293568233, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17438692098092642, 0.16439600363306087, 0.17075386012715713, 0.1907356948228883, 0.14713896457765668, 0.1516802906448683]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.8772315979003906 s
