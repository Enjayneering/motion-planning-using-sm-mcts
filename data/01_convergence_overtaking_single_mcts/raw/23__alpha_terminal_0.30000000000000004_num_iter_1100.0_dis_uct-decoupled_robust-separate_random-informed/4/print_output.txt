Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 127, 'sum_payoffs': 30.9036785903632, 'action': [2.0, 1.5707963267948966]}, {'num_count': 129, 'sum_payoffs': 31.63374833797636, 'action': [2.0, -1.5707963267948966]}, {'num_count': 109, 'sum_payoffs': 23.609065028824176, 'action': [0.0, -1.5707963267948966]}, {'num_count': 108, 'sum_payoffs': 23.084544680375714, 'action': [0.0, 1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 27.257494436701446, 'action': [0.0, 0.0]}, {'num_count': 132, 'sum_payoffs': 32.98901273457625, 'action': [1.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 27.157942763270917, 'action': [1.0, -1.5707963267948966]}, {'num_count': 142, 'sum_payoffs': 37.03104968640215, 'action': [2.0, 0.0]}, {'num_count': 117, 'sum_payoffs': 26.726202111922216, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11534968210717529, 0.11716621253405994, 0.09900090826521345, 0.09809264305177112, 0.10717529518619437, 0.11989100817438691, 0.10717529518619437, 0.12897366030881016, 0.10626702997275204]
Actions to choose Agent 1: dict_values([{'num_count': 206, 'sum_payoffs': 61.22829888392085, 'action': [1.0, 0.0]}, {'num_count': 162, 'sum_payoffs': 42.70364816883098, 'action': [0.0, -1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 43.87697454892353, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 51.01094379502886, 'action': [0.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 54.79053950377499, 'action': [1.0, 1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 56.0595788969211, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18710263396911897, 0.14713896457765668, 0.14986376021798364, 0.16530426884650318, 0.1734786557674841, 0.17620345140781107]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.8885760307312012 s
