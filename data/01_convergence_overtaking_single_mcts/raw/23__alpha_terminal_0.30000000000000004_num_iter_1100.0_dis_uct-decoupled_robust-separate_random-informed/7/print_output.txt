Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 108, 'sum_payoffs': 23.31638528178273, 'action': [0.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 27.403327317339464, 'action': [0.0, 0.0]}, {'num_count': 107, 'sum_payoffs': 22.902027243472432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.45123090107307, 'action': [2.0, -1.5707963267948966]}, {'num_count': 132, 'sum_payoffs': 33.12423497850661, 'action': [1.0, 0.0]}, {'num_count': 117, 'sum_payoffs': 27.010190552444453, 'action': [1.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 27.62444864069645, 'action': [1.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.39908306195785, 'action': [2.0, 1.5707963267948966]}, {'num_count': 143, 'sum_payoffs': 37.591240605350485, 'action': [2.0, 0.0]}])
Weights num count: [0.09809264305177112, 0.10717529518619437, 0.0971843778383288, 0.11625794732061762, 0.11989100817438691, 0.10626702997275204, 0.1080835603996367, 0.11625794732061762, 0.1298819255222525]
Actions to choose Agent 1: dict_values([{'num_count': 182, 'sum_payoffs': 51.15406064239773, 'action': [0.0, 0.0]}, {'num_count': 164, 'sum_payoffs': 43.62785997582048, 'action': [0.0, 1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 54.822588696582656, 'action': [1.0, 1.5707963267948966]}, {'num_count': 209, 'sum_payoffs': 62.58718465821291, 'action': [1.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 53.74182473090514, 'action': [1.0, -1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 44.40736152927973, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16530426884650318, 0.14895549500454133, 0.1734786557674841, 0.18982742960944596, 0.17075386012715713, 0.15077202543142598]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.8837933540344238 s
