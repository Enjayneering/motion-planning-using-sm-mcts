Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 128, 'sum_payoffs': 31.32955260980422, 'action': [2.0, 1.5707963267948966]}, {'num_count': 106, 'sum_payoffs': 22.441387997954614, 'action': [0.0, 1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 27.383916732770217, 'action': [1.0, -1.5707963267948966]}, {'num_count': 142, 'sum_payoffs': 37.16446124148644, 'action': [2.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 28.052350631668276, 'action': [0.0, 0.0]}, {'num_count': 132, 'sum_payoffs': 33.12220700697579, 'action': [1.0, 0.0]}, {'num_count': 108, 'sum_payoffs': 23.246854829629093, 'action': [0.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 27.31438628061658, 'action': [1.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.3006540156351, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11625794732061762, 0.09627611262488647, 0.10717529518619437, 0.12897366030881016, 0.10899182561307902, 0.11989100817438691, 0.09809264305177112, 0.10717529518619437, 0.11625794732061762]
Actions to choose Agent 1: dict_values([{'num_count': 184, 'sum_payoffs': 51.857042484687874, 'action': [0.0, 0.0]}, {'num_count': 165, 'sum_payoffs': 43.929122388038785, 'action': [0.0, 1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 44.3260978133107, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 54.477761110425966, 'action': [1.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 54.483627742333695, 'action': [1.0, -1.5707963267948966]}, {'num_count': 205, 'sum_payoffs': 60.76563166687223, 'action': [1.0, 0.0]}])
Weights num count: [0.16712079927338783, 0.14986376021798364, 0.15077202543142598, 0.17257039055404177, 0.17257039055404177, 0.18619436875567666]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.8387153148651123 s
