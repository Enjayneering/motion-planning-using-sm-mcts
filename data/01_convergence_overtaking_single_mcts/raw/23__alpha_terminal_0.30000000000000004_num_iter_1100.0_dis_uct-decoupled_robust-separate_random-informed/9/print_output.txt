Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 108, 'sum_payoffs': 23.35115050785955, 'action': [0.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 31.312169996765817, 'action': [2.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 27.314386280616596, 'action': [1.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 27.352990167070327, 'action': [0.0, 0.0]}, {'num_count': 145, 'sum_payoffs': 38.38519145589426, 'action': [2.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 31.294787383727396, 'action': [2.0, 1.5707963267948966]}, {'num_count': 106, 'sum_payoffs': 22.441387997954607, 'action': [0.0, -1.5707963267948966]}, {'num_count': 131, 'sum_payoffs': 32.7001716479117, 'action': [1.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 27.24485582846296, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.09809264305177112, 0.11625794732061762, 0.10717529518619437, 0.10717529518619437, 0.13169845594913715, 0.11625794732061762, 0.09627611262488647, 0.11898274296094459, 0.10717529518619437]
Actions to choose Agent 1: dict_values([{'num_count': 191, 'sum_payoffs': 54.67766116036766, 'action': [1.0, -1.5707963267948966]}, {'num_count': 207, 'sum_payoffs': 61.48802409373153, 'action': [1.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 54.81378874872109, 'action': [1.0, 1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 43.42220193531345, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 50.1226198412239, 'action': [0.0, 0.0]}, {'num_count': 167, 'sum_payoffs': 44.798253039959214, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1734786557674841, 0.1880108991825613, 0.1734786557674841, 0.14895549500454133, 0.16348773841961853, 0.1516802906448683]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 1.9276862144470215 s
