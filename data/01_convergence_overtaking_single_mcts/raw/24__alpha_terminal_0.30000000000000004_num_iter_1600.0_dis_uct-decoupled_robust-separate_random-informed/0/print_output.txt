Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 174, 'sum_payoffs': 40.26555562133856, 'action': [0.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 38.0509745064452, 'action': [1.0, -1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.10612084730964, 'action': [0.0, -1.5707963267948966]}, {'num_count': 214, 'sum_payoffs': 55.730902655352615, 'action': [2.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 45.97440409466311, 'action': [2.0, -1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.14088607338644, 'action': [0.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 47.52370190929705, 'action': [1.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 38.40731807373256, 'action': [1.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 45.97440409466311, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10868207370393504, 0.10493441599000625, 0.09494066208619613, 0.13366645846346034, 0.11805121798875702, 0.09494066208619613, 0.12054965646470955, 0.10555902560899438, 0.11805121798875702]
Actions to choose Agent 1: dict_values([{'num_count': 279, 'sum_payoffs': 79.3291397647847, 'action': [1.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 64.246680996669, 'action': [0.0, 1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 70.86956520564509, 'action': [0.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 62.24463853995576, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 78.88294981635146, 'action': [1.0, 1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 90.9507564907532, 'action': [1.0, 0.0]}])
Weights num count: [0.17426608369768895, 0.15053091817613992, 0.16114928169893816, 0.14740787008119924, 0.1736414740787008, 0.19237976264834478]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.7786145210266113 s
