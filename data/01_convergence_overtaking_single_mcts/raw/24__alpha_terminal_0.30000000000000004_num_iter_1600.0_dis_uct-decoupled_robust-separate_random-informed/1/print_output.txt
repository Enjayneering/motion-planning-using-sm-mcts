Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 153, 'sum_payoffs': 32.503096272581544, 'action': [0.0, -1.5707963267948966]}, {'num_count': 214, 'sum_payoffs': 55.8257103239832, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 39.192686259099546, 'action': [0.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 38.4653325447555, 'action': [1.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 44.63007625879797, 'action': [2.0, -1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.210416525540104, 'action': [0.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 45.739738818644604, 'action': [2.0, 1.5707963267948966]}, {'num_count': 199, 'sum_payoffs': 49.9303971120359, 'action': [1.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 38.4653325447555, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.09556527170518427, 0.13366645846346034, 0.10680824484697064, 0.10555902560899438, 0.1155527795128045, 0.09494066208619613, 0.1174266083697689, 0.12429731417863835, 0.10555902560899438]
Actions to choose Agent 1: dict_values([{'num_count': 240, 'sum_payoffs': 64.05536361190414, 'action': [0.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 81.37453011275153, 'action': [1.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 72.22077365918337, 'action': [0.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 62.78654150144268, 'action': [0.0, 1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 87.49827983109833, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 79.40160353289221, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14990630855715179, 0.1773891317926296, 0.16302311055590257, 0.14803247970018737, 0.1867582760774516, 0.17426608369768895]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.7195396423339844 s
