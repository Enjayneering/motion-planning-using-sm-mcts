Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 172, 'sum_payoffs': 39.51516994473315, 'action': [0.0, 0.0]}, {'num_count': 167, 'sum_payoffs': 37.64248310004262, 'action': [1.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 47.22718350187871, 'action': [1.0, 0.0]}, {'num_count': 214, 'sum_payoffs': 55.6846214481451, 'action': [2.0, 0.0]}, {'num_count': 152, 'sum_payoffs': 32.12937009225575, 'action': [0.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 46.348130274988904, 'action': [2.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.821676112042866, 'action': [1.0, -1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 32.4161832073895, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 46.330747661950504, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10743285446595878, 0.10430980637101811, 0.11992504684572143, 0.13366645846346034, 0.09494066208619613, 0.11867582760774516, 0.1061836352279825, 0.09556527170518427, 0.11867582760774516]
Actions to choose Agent 1: dict_values([{'num_count': 308, 'sum_payoffs': 91.05505216898364, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 79.29730785466174, 'action': [1.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 62.31123567615557, 'action': [0.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 79.71731524219508, 'action': [1.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 63.1573343658146, 'action': [0.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 71.45188274243174, 'action': [0.0, 0.0]}])
Weights num count: [0.19237976264834478, 0.17426608369768895, 0.14740787008119924, 0.1748906933166771, 0.1486570893191755, 0.1617738913179263]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.6939539909362793 s
