Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 218, 'sum_payoffs': 57.22399668780967, 'action': [2.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 38.085739732522015, 'action': [1.0, 1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.123503460348054, 'action': [0.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 46.45829258512704, 'action': [2.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 47.066901324156106, 'action': [1.0, 0.0]}, {'num_count': 187, 'sum_payoffs': 45.27323294121907, 'action': [2.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 38.40731807373256, 'action': [1.0, -1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.05397300819441, 'action': [0.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 39.6079496418329, 'action': [0.0, 0.0]}])
Weights num count: [0.13616489693941286, 0.10493441599000625, 0.09494066208619613, 0.11867582760774516, 0.11992504684572143, 0.11680199875078076, 0.10555902560899438, 0.09494066208619613, 0.10743285446595878]
Actions to choose Agent 1: dict_values([{'num_count': 239, 'sum_payoffs': 63.61797361133235, 'action': [0.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 62.05353843787571, 'action': [0.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 79.07415855977396, 'action': [1.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 80.28225016594332, 'action': [1.0, 1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 73.0321071227656, 'action': [0.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 89.4346304960348, 'action': [1.0, 0.0]}])
Weights num count: [0.14928169893816365, 0.14678326046221113, 0.1736414740787008, 0.1755153029356652, 0.16427232979387882, 0.18988132417239226]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.670454740524292 s
