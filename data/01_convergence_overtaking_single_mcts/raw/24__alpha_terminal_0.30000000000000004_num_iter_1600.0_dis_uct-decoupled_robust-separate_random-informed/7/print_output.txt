Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 153, 'sum_payoffs': 32.43943245233561, 'action': [0.0, -1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 45.221085102103864, 'action': [2.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.815809480135165, 'action': [1.0, 1.5707963267948966]}, {'num_count': 152, 'sum_payoffs': 32.09460486617894, 'action': [0.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 47.53905655080465, 'action': [1.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 38.137887571637236, 'action': [1.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 39.89182944101276, 'action': [0.0, 0.0]}, {'num_count': 215, 'sum_payoffs': 56.167604594185946, 'action': [2.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 45.99178670770152, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.09556527170518427, 0.11680199875078076, 0.1061836352279825, 0.09494066208619613, 0.12054965646470955, 0.10493441599000625, 0.1080574640849469, 0.13429106808244848, 0.11805121798875702]
Actions to choose Agent 1: dict_values([{'num_count': 277, 'sum_payoffs': 78.6135193142561, 'action': [1.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 63.904786726466135, 'action': [0.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 71.49838123232395, 'action': [0.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 78.94096428737441, 'action': [1.0, -1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 90.65807674371175, 'action': [1.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 63.583208385255645, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17301686445971268, 0.14990630855715179, 0.1617738913179263, 0.1736414740787008, 0.19175515302935664, 0.14928169893816365]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.682589054107666 s
