Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 190, 'sum_payoffs': 46.38289550106572, 'action': [2.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 39.114355859084355, 'action': [0.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 46.361891510335184, 'action': [1.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 38.81580948013515, 'action': [1.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 46.04393454681675, 'action': [2.0, 1.5707963267948966]}, {'num_count': 151, 'sum_payoffs': 31.645481601791822, 'action': [0.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 38.821676112042866, 'action': [1.0, -1.5707963267948966]}, {'num_count': 154, 'sum_payoffs': 32.79577601962297, 'action': [0.0, 1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 55.9937784638019, 'action': [2.0, 0.0]}])
Weights num count: [0.11867582760774516, 0.10680824484697064, 0.11867582760774516, 0.1061836352279825, 0.11805121798875702, 0.09431605246720799, 0.1061836352279825, 0.09618988132417239, 0.13429106808244848]
Actions to choose Agent 1: dict_values([{'num_count': 261, 'sum_payoffs': 72.27473218714469, 'action': [0.0, 0.0]}, {'num_count': 307, 'sum_payoffs': 90.65039942295793, 'action': [1.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 64.02646501773508, 'action': [0.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 62.386632760216926, 'action': [0.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 78.66273383741752, 'action': [1.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 79.45961800391517, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16302311055590257, 0.19175515302935664, 0.14990630855715179, 0.14740787008119924, 0.17301686445971268, 0.17426608369768895]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.643188953399658 s
