Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 152, 'sum_payoffs': 32.11198747921735, 'action': [0.0, -1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 32.59000933777358, 'action': [0.0, 1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 45.33124741224201, 'action': [2.0, -1.5707963267948966]}, {'num_count': 214, 'sum_payoffs': 55.80832771094472, 'action': [2.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 40.26939428171544, 'action': [0.0, 0.0]}, {'num_count': 192, 'sum_payoffs': 47.253148780093944, 'action': [1.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 39.270799376429984, 'action': [1.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 45.7108402244755, 'action': [2.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 38.465332544755505, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.09494066208619613, 0.09556527170518427, 0.11680199875078076, 0.13366645846346034, 0.10868207370393504, 0.11992504684572143, 0.10680824484697064, 0.1174266083697689, 0.10555902560899438]
Actions to choose Agent 1: dict_values([{'num_count': 279, 'sum_payoffs': 79.20159484160811, 'action': [1.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 62.244638539955794, 'action': [0.0, 1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 73.25301116343786, 'action': [0.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 63.70184471922828, 'action': [0.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 78.75540489317491, 'action': [1.0, -1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 88.8753449215095, 'action': [1.0, 0.0]}])
Weights num count: [0.17426608369768895, 0.14740787008119924, 0.16489693941286696, 0.14990630855715179, 0.1736414740787008, 0.18925671455340412]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.7102255821228027 s
