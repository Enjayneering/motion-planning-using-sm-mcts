Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 290, 'sum_payoffs': 75.4427858409078, 'action': [2.0, 0.0]}, {'num_count': 195, 'sum_payoffs': 40.85826651214826, 'action': [0.0, -1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 60.596006334575904, 'action': [2.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 62.7421796244235, 'action': [1.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 50.704720095325776, 'action': [0.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.62779479002511, 'action': [1.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 49.68580926104805, 'action': [1.0, -1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 60.58449035344519, 'action': [2.0, 1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 41.122047665020595, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.13802950975725845, 0.09281294621608757, 0.11899095668729176, 0.12184673964778676, 0.10613993336506425, 0.10471204188481675, 0.10471204188481675, 0.11899095668729176, 0.09328891004283675]
Actions to choose Agent 1: dict_values([{'num_count': 303, 'sum_payoffs': 79.51133127766084, 'action': [0.0, 1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 81.6873085061005, 'action': [0.0, -1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 104.95415334069503, 'action': [1.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 121.29877088428354, 'action': [1.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 104.1805184191435, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 92.71002178532468, 'action': [0.0, 0.0]}])
Weights num count: [0.14421703950499762, 0.14707282246549264, 0.17610661589719181, 0.19609709662065683, 0.17515468824369348, 0.16087577344121848]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.4760708808898926 s
