Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 222, 'sum_payoffs': 50.38111378258437, 'action': [1.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 74.25794347968454, 'action': [2.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.7553397132017, 'action': [0.0, 0.0]}, {'num_count': 224, 'sum_payoffs': 51.18658061425885, 'action': [1.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 41.576820278630706, 'action': [0.0, -1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 61.29717748801994, 'action': [2.0, 1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 41.26110856932786, 'action': [0.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 60.58449035344521, 'action': [2.0, -1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 61.341865289066185, 'action': [1.0, 0.0]}])
Weights num count: [0.10566396953831508, 0.13660161827701095, 0.10471204188481675, 0.10661589719181343, 0.09376487386958592, 0.1199428843407901, 0.09328891004283675, 0.11899095668729176, 0.1199428843407901]
Actions to choose Agent 1: dict_values([{'num_count': 311, 'sum_payoffs': 82.30993197684458, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 115.13641727948308, 'action': [1.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 92.19115078609927, 'action': [0.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 83.4342611164606, 'action': [0.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 104.38041846908523, 'action': [1.0, -1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 105.5334289201855, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14802475011899097, 0.18895763921941933, 0.1603998096144693, 0.14945264159923846, 0.17563065207044265, 0.17705854355069015]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.4770944118499756 s
