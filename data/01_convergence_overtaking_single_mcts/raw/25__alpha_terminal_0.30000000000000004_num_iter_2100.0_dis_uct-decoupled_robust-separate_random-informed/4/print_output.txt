Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 198, 'sum_payoffs': 41.90426525174897, 'action': [0.0, 1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 49.036785946719235, 'action': [1.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 75.03248374565912, 'action': [2.0, 0.0]}, {'num_count': 198, 'sum_payoffs': 41.82886816768762, 'action': [0.0, -1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 60.92345130769416, 'action': [2.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 60.18751492817331, 'action': [2.0, 1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 62.717119690631264, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.72057448712486, 'action': [1.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 50.04892480466622, 'action': [0.0, 0.0]}])
Weights num count: [0.09424083769633508, 0.10376011423131842, 0.13755354593050928, 0.09424083769633508, 0.11946692051404094, 0.1185149928605426, 0.12184673964778676, 0.10471204188481675, 0.10518800571156592]
Actions to choose Agent 1: dict_values([{'num_count': 310, 'sum_payoffs': 82.0086695646263, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 92.2608985209376, 'action': [0.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 82.08699996464148, 'action': [0.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 118.11572472672397, 'action': [1.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 106.44319143009048, 'action': [1.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 102.97536012892802, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1475487862922418, 0.1603998096144693, 0.1475487862922418, 0.19228938600666348, 0.17801047120418848, 0.173726796763446]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.4666881561279297 s
