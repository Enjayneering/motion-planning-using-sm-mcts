Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 251, 'sum_payoffs': 60.90606869465574, 'action': [2.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 76.88192134540357, 'action': [2.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 59.865936586962746, 'action': [2.0, -1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 60.596223617260755, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.668426648009635, 'action': [1.0, -1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 40.55407078397611, 'action': [0.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 49.73209046825556, 'action': [1.0, 1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 41.61745213661523, 'action': [0.0, -1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 51.87127449908756, 'action': [0.0, 0.0]}])
Weights num count: [0.11946692051404094, 0.1399333650642551, 0.11803902903379343, 0.11899095668729176, 0.10471204188481675, 0.0923369823893384, 0.10471204188481675, 0.09376487386958592, 0.10756782484531176]
Actions to choose Agent 1: dict_values([{'num_count': 344, 'sum_payoffs': 94.76479150154907, 'action': [0.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 103.57495163741073, 'action': [1.0, 1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 80.33700539698525, 'action': [0.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 103.05629792086998, 'action': [1.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 119.02063459048661, 'action': [1.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 81.86395931109608, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16373155640171347, 0.17467872441694432, 0.1456449309852451, 0.17420276059019515, 0.193717277486911, 0.1475487862922418]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.468379259109497 s
