Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 197, 'sum_payoffs': 41.43775937432343, 'action': [0.0, 1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 50.04215282833542, 'action': [1.0, -1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 60.56710774040679, 'action': [2.0, -1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 61.635450380530685, 'action': [1.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 75.3405181342082, 'action': [2.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 60.13536708905808, 'action': [2.0, 1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 41.197227466397216, 'action': [0.0, -1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 50.44387225840729, 'action': [0.0, 0.0]}, {'num_count': 222, 'sum_payoffs': 50.328965943469136, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.09376487386958592, 0.10518800571156592, 0.11899095668729176, 0.12041884816753927, 0.13802950975725845, 0.1185149928605426, 0.09328891004283675, 0.10566396953831508, 0.10566396953831508]
Actions to choose Agent 1: dict_values([{'num_count': 412, 'sum_payoffs': 121.21591376215315, 'action': [1.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 79.728613940641, 'action': [0.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 101.40212500760968, 'action': [1.0, 1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 81.34226363725907, 'action': [0.0, 1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 105.32201288911307, 'action': [1.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 94.93115759386416, 'action': [0.0, 0.0]}])
Weights num count: [0.19609709662065683, 0.14469300333174678, 0.1718229414564493, 0.14659685863874344, 0.17658257972394098, 0.16373155640171347]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.4897077083587646 s
