Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 256, 'sum_payoffs': 62.82706471808472, 'action': [1.0, 0.0]}, {'num_count': 224, 'sum_payoffs': 51.09108488389002, 'action': [0.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 74.24530487144607, 'action': [2.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 49.64517740306352, 'action': [1.0, 1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 59.79640613480914, 'action': [2.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 41.53053907142319, 'action': [0.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 60.262912012234644, 'action': [2.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 50.433261621699586, 'action': [1.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 41.54205505255388, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12184673964778676, 0.10661589719181343, 0.13660161827701095, 0.10471204188481675, 0.11803902903379343, 0.09376487386958592, 0.1185149928605426, 0.10566396953831508, 0.09376487386958592]
Actions to choose Agent 1: dict_values([{'num_count': 374, 'sum_payoffs': 106.37072766198295, 'action': [1.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 114.58785098300774, 'action': [1.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 82.81706971225466, 'action': [0.0, 1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 81.62636071912368, 'action': [0.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 93.81381771326366, 'action': [0.0, 0.0]}, {'num_count': 369, 'sum_payoffs': 104.50796339226177, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17801047120418848, 0.188005711565921, 0.14850071394574013, 0.14707282246549264, 0.16230366492146597, 0.17563065207044265]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.447641372680664 s
