Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 250, 'sum_payoffs': 60.59600633457588, 'action': [2.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 41.45514198736184, 'action': [0.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 49.749473081293964, 'action': [1.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 75.80792935605682, 'action': [2.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 59.84855397392435, 'action': [2.0, 1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 40.50192294486088, 'action': [0.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 62.80515538293101, 'action': [1.0, 0.0]}, {'num_count': 219, 'sum_payoffs': 49.37574690096819, 'action': [1.0, 1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 51.43681781446973, 'action': [0.0, 0.0]}])
Weights num count: [0.11899095668729176, 0.09376487386958592, 0.10471204188481675, 0.1385054735840076, 0.11803902903379343, 0.0923369823893384, 0.12184673964778676, 0.10423607805806759, 0.10709186101856259]
Actions to choose Agent 1: dict_values([{'num_count': 309, 'sum_payoffs': 81.81756946254622, 'action': [0.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 119.10291229222112, 'action': [1.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 105.49583901949721, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 103.92553721413275, 'action': [1.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 94.3247941090506, 'action': [0.0, 0.0]}, {'num_count': 305, 'sum_payoffs': 80.41544443834277, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14707282246549264, 0.19324131366016184, 0.17658257972394098, 0.17467872441694432, 0.16277962874821514, 0.14516896715849595]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.4855728149414062 s
