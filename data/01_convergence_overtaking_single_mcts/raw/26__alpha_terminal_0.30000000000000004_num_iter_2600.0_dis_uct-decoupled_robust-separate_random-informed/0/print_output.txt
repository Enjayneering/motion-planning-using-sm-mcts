Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 272, 'sum_payoffs': 61.34954260981999, 'action': [1.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 49.714707855217306, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.38430783589681, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 95.09879116831341, 'action': [2.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 75.39447666216932, 'action': [2.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 75.41185927520768, 'action': [2.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 50.45064423473818, 'action': [0.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 61.702047516730474, 'action': [0.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 76.04733863687512, 'action': [1.0, 0.0]}])
Weights num count: [0.10457516339869281, 0.0915032679738562, 0.10457516339869281, 0.14109957708573626, 0.11995386389850057, 0.11995386389850057, 0.0922722029988466, 0.104959630911188, 0.12072279892349097]
Actions to choose Agent 1: dict_values([{'num_count': 386, 'sum_payoffs': 102.45083978047963, 'action': [0.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 111.94851847578133, 'action': [0.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 102.44790646452564, 'action': [0.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 130.2687786324984, 'action': [1.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 144.90030344738042, 'action': [1.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 127.93385913612877, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14840445982314496, 0.15840061514801998, 0.14840445982314496, 0.1772395232602845, 0.1922337562475971, 0.17493271818531334]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.1707682609558105 s
