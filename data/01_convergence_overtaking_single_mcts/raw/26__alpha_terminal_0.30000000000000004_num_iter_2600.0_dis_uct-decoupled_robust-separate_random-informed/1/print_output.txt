Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 271, 'sum_payoffs': 61.11487733380147, 'action': [1.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 60.80481497372162, 'action': [1.0, 1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 77.24029288422159, 'action': [1.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 75.14242877311241, 'action': [2.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 75.83773329464873, 'action': [2.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 50.187080364550546, 'action': [0.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 95.61067290852341, 'action': [2.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 50.13493252543533, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.480926193373485, 'action': [0.0, 0.0]}])
Weights num count: [0.10419069588619762, 0.10380622837370242, 0.12187620146097655, 0.11956939638600539, 0.12033833141099577, 0.0918877354863514, 0.14148404459823144, 0.0918877354863514, 0.10457516339869281]
Actions to choose Agent 1: dict_values([{'num_count': 379, 'sum_payoffs': 99.96795079059865, 'action': [0.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 126.29706883590686, 'action': [1.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 130.01108139421848, 'action': [1.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 148.79806471104544, 'action': [1.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 114.33196443357409, 'action': [0.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 101.46285551190176, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14571318723567858, 0.17301038062283736, 0.1768550557477893, 0.19607843137254902, 0.16070742022299117, 0.14725105728565935]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.134095907211304 s
