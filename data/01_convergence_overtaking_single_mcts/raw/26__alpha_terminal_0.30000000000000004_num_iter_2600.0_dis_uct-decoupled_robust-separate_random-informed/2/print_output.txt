Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 270, 'sum_payoffs': 60.648371456375955, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 77.03351221660685, 'action': [1.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 61.982088655533474, 'action': [0.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 61.636355724953724, 'action': [1.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 49.06003519166552, 'action': [0.0, 1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 74.99750123689742, 'action': [2.0, -1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 75.75082022945668, 'action': [2.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 95.37589899116256, 'action': [2.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 49.737957100163435, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10380622837370242, 0.12187620146097655, 0.1053440984236832, 0.104959630911188, 0.09073433294886582, 0.11956939638600539, 0.12033833141099577, 0.14148404459823144, 0.0915032679738562]
Actions to choose Agent 1: dict_values([{'num_count': 376, 'sum_payoffs': 98.7596419017445, 'action': [0.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 130.14145099200653, 'action': [1.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 128.3741824526543, 'action': [1.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 99.87223777754495, 'action': [0.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 149.47395864801257, 'action': [1.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 113.46667244203064, 'action': [0.0, 0.0]}])
Weights num count: [0.144559784698193, 0.1772395232602845, 0.17531718569780855, 0.14571318723567858, 0.19684736639753941, 0.15993848519800077]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.09892201423645 s
