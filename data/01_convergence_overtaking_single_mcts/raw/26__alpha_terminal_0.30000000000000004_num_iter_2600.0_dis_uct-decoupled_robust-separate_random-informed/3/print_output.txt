Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 316, 'sum_payoffs': 76.85686141161125, 'action': [1.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 74.786085205825, 'action': [2.0, -1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 75.10766354703559, 'action': [2.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 62.051619107687095, 'action': [0.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 61.75803401622258, 'action': [1.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 95.51405455104678, 'action': [2.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 49.80748755231706, 'action': [0.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 49.80162092040934, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.41907306197363, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12149173394848135, 0.11918492887351019, 0.11956939638600539, 0.1053440984236832, 0.104959630911188, 0.14148404459823144, 0.0915032679738562, 0.0915032679738562, 0.10457516339869281]
Actions to choose Agent 1: dict_values([{'num_count': 506, 'sum_payoffs': 147.276289367104, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 130.34710903251354, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 102.85639787092826, 'action': [0.0, 1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 98.8901201408749, 'action': [0.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 112.8235157596095, 'action': [0.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 128.42926360772339, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.19454056132256825, 0.1772395232602845, 0.14878892733564014, 0.144559784698193, 0.15916955017301038, 0.17531718569780855]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.137575149536133 s
