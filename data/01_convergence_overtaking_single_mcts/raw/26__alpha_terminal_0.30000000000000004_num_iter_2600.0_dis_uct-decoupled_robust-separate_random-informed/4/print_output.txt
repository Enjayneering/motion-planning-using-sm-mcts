Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 272, 'sum_payoffs': 61.308910751835455, 'action': [1.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 76.48088997706985, 'action': [2.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 62.690031785308264, 'action': [0.0, 0.0]}, {'num_count': 363, 'sum_payoffs': 93.62217440447175, 'action': [2.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 76.03763334459043, 'action': [2.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 49.088933785834634, 'action': [0.0, 1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 49.34098167489153, 'action': [0.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 61.00471502366332, 'action': [1.0, -1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 76.80652426134213, 'action': [1.0, 0.0]}])
Weights num count: [0.10457516339869281, 0.12110726643598616, 0.1061130334486736, 0.1395617070357555, 0.12072279892349097, 0.09073433294886582, 0.09111880046136102, 0.10419069588619762, 0.12149173394848135]
Actions to choose Agent 1: dict_values([{'num_count': 503, 'sum_payoffs': 146.038864601396, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 130.158833605045, 'action': [1.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 129.82280596674988, 'action': [1.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 99.06383762991659, 'action': [0.0, -1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 99.1480346618395, 'action': [0.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 115.71105749551612, 'action': [0.0, 0.0]}])
Weights num count: [0.19338715878508267, 0.1772395232602845, 0.1768550557477893, 0.14494425221068818, 0.14494425221068818, 0.16224529027297194]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.176697254180908 s
