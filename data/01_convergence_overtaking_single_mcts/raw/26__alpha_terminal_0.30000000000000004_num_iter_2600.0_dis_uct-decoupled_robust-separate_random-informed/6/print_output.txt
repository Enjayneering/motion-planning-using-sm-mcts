Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 237, 'sum_payoffs': 49.37574690096836, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.42493969388134, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 95.04754867362122, 'action': [2.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 74.99750123689742, 'action': [2.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 61.016448287478745, 'action': [1.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 75.35384480418479, 'action': [2.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 61.760967332176456, 'action': [0.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 49.784238307370934, 'action': [0.0, -1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 77.80511916662756, 'action': [1.0, 0.0]}])
Weights num count: [0.09111880046136102, 0.10457516339869281, 0.14109957708573626, 0.11956939638600539, 0.10419069588619762, 0.11995386389850057, 0.104959630911188, 0.0915032679738562, 0.12264513648596694]
Actions to choose Agent 1: dict_values([{'num_count': 383, 'sum_payoffs': 101.4657888278556, 'action': [0.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 150.06236009939153, 'action': [1.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 99.63192315230341, 'action': [0.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 131.57551656767512, 'action': [1.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 110.68929301626221, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 127.733959086187, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14725105728565935, 0.1972318339100346, 0.1453287197231834, 0.17839292579777008, 0.1568627450980392, 0.17454825067281815]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.20325231552124 s
