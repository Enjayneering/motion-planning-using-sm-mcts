Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 321, 'sum_payoffs': 78.59907001717137, 'action': [1.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 49.46265996616038, 'action': [0.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 94.04895376833576, 'action': [2.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 62.457394480820554, 'action': [0.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 60.68313668245277, 'action': [1.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 61.34367597791228, 'action': [1.0, -1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 75.04964907601264, 'action': [2.0, -1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 74.699172140633, 'action': [2.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 50.43326162169976, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12341407151095732, 0.09111880046136102, 0.13994617454825067, 0.1057285659361784, 0.10380622837370242, 0.10457516339869281, 0.11956939638600539, 0.11918492887351019, 0.0922722029988466]
Actions to choose Agent 1: dict_values([{'num_count': 423, 'sum_payoffs': 116.0847836758419, 'action': [0.0, 0.0]}, {'num_count': 384, 'sum_payoffs': 101.76705124007387, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 98.40623165041103, 'action': [0.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 128.7971231561414, 'action': [1.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 128.81743908513369, 'action': [1.0, -1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 146.46111724314505, 'action': [1.0, 0.0]}])
Weights num count: [0.16262975778546712, 0.14763552479815456, 0.14417531718569782, 0.17570165321030373, 0.17570165321030373, 0.19377162629757785]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.262517213821411 s
