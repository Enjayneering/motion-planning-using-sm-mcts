Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 277, 'sum_payoffs': 63.1662429550184, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 61.43645567501204, 'action': [1.0, 1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 75.04964907601263, 'action': [2.0, 1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 49.393129514006766, 'action': [0.0, -1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 75.37709404913092, 'action': [2.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 76.4443140621471, 'action': [1.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 95.08434187122884, 'action': [2.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 60.987332410624916, 'action': [1.0, -1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 49.80748755231707, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10649750096116878, 0.10457516339869281, 0.11956939638600539, 0.09111880046136102, 0.11995386389850057, 0.12110726643598616, 0.14109957708573626, 0.10419069588619762, 0.0915032679738562]
Actions to choose Agent 1: dict_values([{'num_count': 502, 'sum_payoffs': 145.70012092983188, 'action': [1.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 126.58659798430972, 'action': [1.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 98.03837210199295, 'action': [0.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 117.21566750910382, 'action': [0.0, 0.0]}, {'num_count': 384, 'sum_payoffs': 101.78150053715846, 'action': [0.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 131.07717878012673, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.19300269127258746, 0.17339484813533257, 0.1437908496732026, 0.1637831603229527, 0.14763552479815456, 0.1780084582852749]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.197679758071899 s
