Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 322, 'sum_payoffs': 72.50918018047831, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 114.33073316512409, 'action': [2.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 91.29087628579406, 'action': [2.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 73.1175716368226, 'action': [1.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 92.94932242480081, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 58.12506789119549, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 90.59557176425773, 'action': [2.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.09391679774494, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 57.47604457686666, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1038374717832957, 0.14285714285714285, 0.12157368590777169, 0.10448242502418574, 0.12318606900999678, 0.08997097710415995, 0.12092873266688164, 0.10351499516285069, 0.08932602386326992]
Actions to choose Agent 1: dict_values([{'num_count': 445, 'sum_payoffs': 116.78552026391655, 'action': [0.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 151.28153312132235, 'action': [1.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 156.10814155543449, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 137.04053767940854, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 119.03961060968662, 'action': [0.0, 1.5707963267948966]}, {'num_count': 610, 'sum_payoffs': 177.07189880378178, 'action': [1.0, 0.0]}])
Weights num count: [0.14350209609803288, 0.17413737504030957, 0.17832957110609482, 0.16156078684295389, 0.145436955820703, 0.1967107384714608]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.653172969818115 s
