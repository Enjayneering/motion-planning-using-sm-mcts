Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 372, 'sum_payoffs': 89.56130628847241, 'action': [2.0, -1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 74.53345789637228, 'action': [0.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 72.1469699812832, 'action': [1.0, 1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 90.90675675144546, 'action': [1.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 116.3895588244101, 'action': [2.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 58.46402884544445, 'action': [0.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 71.72109596184222, 'action': [1.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 90.89976749242986, 'action': [2.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 57.832388144154024, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1199613028055466, 0.10577233150596582, 0.10351499516285069, 0.12125120928732668, 0.14479200257981295, 0.09029345372460497, 0.10319251854240567, 0.12125120928732668, 0.08964850048371494]
Actions to choose Agent 1: dict_values([{'num_count': 548, 'sum_payoffs': 154.11783236253675, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 117.48375810140678, 'action': [0.0, -1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 176.31293046199954, 'action': [1.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 136.67448881983654, 'action': [0.0, 0.0]}, {'num_count': 551, 'sum_payoffs': 155.2216282904756, 'action': [1.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 117.17662905728075, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1767171880038697, 0.14414704933892292, 0.19606578523057078, 0.16123831022250887, 0.17768461786520479, 0.1438245727184779]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.604242563247681 s
