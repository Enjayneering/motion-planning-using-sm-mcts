Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 319, 'sum_payoffs': 71.43428284670846, 'action': [1.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 90.93453271850669, 'action': [2.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.217847588295236, 'action': [0.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 113.66048857737998, 'action': [2.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 92.37660153895635, 'action': [1.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 71.79062641399587, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.19459834334911, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 90.56080653818094, 'action': [2.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 75.45575037317325, 'action': [0.0, 0.0]}])
Weights num count: [0.10287004192196066, 0.12125120928732668, 0.08997097710415995, 0.14221218961625282, 0.12254111576910674, 0.10319251854240567, 0.08997097710415995, 0.12092873266688164, 0.10673976136730087]
Actions to choose Agent 1: dict_values([{'num_count': 450, 'sum_payoffs': 118.6774004104916, 'action': [0.0, -1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 179.7557018293697, 'action': [1.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 151.97977095881257, 'action': [1.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 152.0463680950123, 'action': [1.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 137.42194118048823, 'action': [0.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 117.55035523760658, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14511447920025797, 0.19896807481457596, 0.1747823282811996, 0.1747823282811996, 0.1618832634633989, 0.14414704933892292]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.923835754394531 s
