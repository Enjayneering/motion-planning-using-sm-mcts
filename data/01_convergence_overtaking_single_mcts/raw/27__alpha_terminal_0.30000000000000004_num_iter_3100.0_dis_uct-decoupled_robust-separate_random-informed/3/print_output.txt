Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 321, 'sum_payoffs': 72.13545400015254, 'action': [1.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 90.60143839616543, 'action': [2.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 89.5613062884724, 'action': [2.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 116.8212994757589, 'action': [2.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 72.89080096424259, 'action': [0.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 58.46989547735214, 'action': [0.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 72.82489188978118, 'action': [1.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 91.60296661740486, 'action': [1.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 57.84977075719243, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10351499516285069, 0.12092873266688164, 0.1199613028055466, 0.14511447920025797, 0.10415994840374072, 0.09029345372460497, 0.10415994840374072, 0.12189616252821671, 0.08964850048371494]
Actions to choose Agent 1: dict_values([{'num_count': 553, 'sum_payoffs': 155.88216758593512, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 116.33933031548331, 'action': [0.0, 1.5707963267948966]}, {'num_count': 610, 'sum_payoffs': 176.9308099279435, 'action': [1.0, 0.0]}, {'num_count': 545, 'sum_payoffs': 152.96188859548255, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 118.10366553888177, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 136.15583510329583, 'action': [0.0, 0.0]}])
Weights num count: [0.17832957110609482, 0.14317961947758787, 0.1967107384714608, 0.17574975814253466, 0.14479200257981295, 0.16091583360206385]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.8669562339782715 s
