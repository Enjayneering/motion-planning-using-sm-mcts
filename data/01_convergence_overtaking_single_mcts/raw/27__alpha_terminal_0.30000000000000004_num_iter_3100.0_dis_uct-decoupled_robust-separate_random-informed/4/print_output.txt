Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 373, 'sum_payoffs': 89.85963538473686, 'action': [2.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 115.02897100261434, 'action': [2.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 89.90026724272137, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 58.44077960049831, 'action': [0.0, -1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.12958736824481, 'action': [1.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 58.48141145848283, 'action': [0.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 91.60499458893561, 'action': [1.0, 0.0]}, {'num_count': 324, 'sum_payoffs': 73.08573972669959, 'action': [0.0, 0.0]}, {'num_count': 326, 'sum_payoffs': 73.8302587713974, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12028377942599161, 0.14350209609803288, 0.12028377942599161, 0.09029345372460497, 0.10351499516285069, 0.09029345372460497, 0.12189616252821671, 0.10448242502418574, 0.10512737826507579]
Actions to choose Agent 1: dict_values([{'num_count': 461, 'sum_payoffs': 122.62042889559885, 'action': [0.0, -1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 152.7387393005948, 'action': [1.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 133.80635766849926, 'action': [0.0, 0.0]}, {'num_count': 612, 'sum_payoffs': 177.84260040937946, 'action': [1.0, 0.0]}, {'num_count': 539, 'sum_payoffs': 150.8556591018814, 'action': [1.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 119.33500638999726, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14866172202515318, 0.17542728152208964, 0.15865849725894873, 0.19735569171235084, 0.17381489841986456, 0.14575943244114803]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.561685800552368 s
