Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 381, 'sum_payoffs': 92.50019916041362, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 113.48170115951125, 'action': [2.0, 0.0]}, {'num_count': 325, 'sum_payoffs': 73.4450166099409, 'action': [0.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 90.83023704027623, 'action': [2.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.14245050423387, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.10181864624934, 'action': [0.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 71.75586118791904, 'action': [1.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 90.4158790019659, 'action': [2.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 73.03065857163057, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12286359238955176, 0.14221218961625282, 0.10480490164463076, 0.12125120928732668, 0.08997097710415995, 0.08997097710415995, 0.10319251854240567, 0.12092873266688164, 0.10448242502418574]
Actions to choose Agent 1: dict_values([{'num_count': 541, 'sum_payoffs': 151.69002452772503, 'action': [1.0, 1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 179.75570182936974, 'action': [1.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 151.40027809663732, 'action': [1.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 121.27045171051066, 'action': [0.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 118.02837709616276, 'action': [0.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 135.7117817677358, 'action': [0.0, 0.0]}])
Weights num count: [0.1744598516607546, 0.19896807481457596, 0.17413737504030957, 0.1473718155433731, 0.14446952595936793, 0.16027088036117382]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.663650035858154 s
