Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 447, 'sum_payoffs': 115.7716793584659, 'action': [2.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 91.59619464107409, 'action': [1.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 72.75536143762754, 'action': [1.0, -1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 89.86550201664454, 'action': [2.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.14245050423387, 'action': [0.0, -1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.07743952912962, 'action': [1.0, 1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 89.91178322385208, 'action': [2.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 59.13043477281166, 'action': [0.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 73.1214102971995, 'action': [0.0, 0.0]}])
Weights num count: [0.14414704933892292, 0.12189616252821671, 0.10415994840374072, 0.12028377942599161, 0.08997097710415995, 0.10351499516285069, 0.12028377942599161, 0.090938406965495, 0.10448242502418574]
Actions to choose Agent 1: dict_values([{'num_count': 607, 'sum_payoffs': 175.65702652999752, 'action': [1.0, 0.0]}, {'num_count': 546, 'sum_payoffs': 153.16472196137812, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 118.0630336808972, 'action': [0.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 118.69174106623382, 'action': [0.0, -1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 154.2394020124632, 'action': [1.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 135.60422684952437, 'action': [0.0, 0.0]}])
Weights num count: [0.19574330861012576, 0.17607223476297967, 0.14479200257981295, 0.145436955820703, 0.17703966462431472, 0.16059335698161883]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.664953947067261 s
