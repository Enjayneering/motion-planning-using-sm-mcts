Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 379, 'sum_payoffs': 91.90535165673084, 'action': [1.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 57.476044576866656, 'action': [0.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 72.46289897327082, 'action': [1.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 59.46939572706061, 'action': [0.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 90.59557176425774, 'action': [2.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 115.67393837388154, 'action': [2.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 71.8486408850188, 'action': [1.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 73.1909407493531, 'action': [0.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 89.90026724272137, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.12221863914866173, 0.08932602386326992, 0.1038374717832957, 0.09126088358594002, 0.12092873266688164, 0.14414704933892292, 0.10319251854240567, 0.10448242502418574, 0.12028377942599161]
Actions to choose Agent 1: dict_values([{'num_count': 546, 'sum_payoffs': 153.33268145985448, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 118.15874669395082, 'action': [0.0, -1.5707963267948966]}, {'num_count': 550, 'sum_payoffs': 154.77250502608842, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 117.44019292746837, 'action': [0.0, 1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 176.13888704893074, 'action': [1.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 136.5722211131368, 'action': [0.0, 0.0]}])
Weights num count: [0.17607223476297967, 0.14479200257981295, 0.17736214124475974, 0.14414704933892292, 0.19606578523057078, 0.16123831022250887]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.670316696166992 s
