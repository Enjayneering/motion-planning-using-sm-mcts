Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 278, 'sum_payoffs': 57.72809246592357, 'action': [0.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 92.51352583039039, 'action': [2.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 72.10068877407573, 'action': [1.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 92.26937254477203, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 113.58013020583394, 'action': [2.0, 0.0]}, {'num_count': 327, 'sum_payoffs': 74.15295973971567, 'action': [0.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 89.06003518497008, 'action': [2.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 72.37598590807876, 'action': [1.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.08443603321095, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.08964850048371494, 0.12286359238955176, 0.10351499516285069, 0.12254111576910674, 0.14221218961625282, 0.1054498548855208, 0.11963882618510158, 0.1038374717832957, 0.08997097710415995]
Actions to choose Agent 1: dict_values([{'num_count': 448, 'sum_payoffs': 117.92408141793233, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 136.74604724352096, 'action': [0.0, 0.0]}, {'num_count': 610, 'sum_payoffs': 177.14505063362768, 'action': [1.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 154.56402231096996, 'action': [1.0, 1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 153.18514653171272, 'action': [1.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 117.96471327591684, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14446952595936793, 0.16123831022250887, 0.1967107384714608, 0.17703966462431472, 0.17574975814253466, 0.14446952595936793]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.6595611572265625 s
