Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 322, 'sum_payoffs': 72.56719465150124, 'action': [1.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 58.23523020133362, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 114.77015113722652, 'action': [2.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 90.96929794458352, 'action': [2.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 92.0444125610381, 'action': [1.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 89.91764985575978, 'action': [2.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 57.528192415981884, 'action': [0.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 72.80750927674275, 'action': [1.0, 1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 74.22339553629233, 'action': [0.0, 0.0]}])
Weights num count: [0.1038374717832957, 0.08997097710415995, 0.14317961947758787, 0.12125120928732668, 0.12221863914866173, 0.12028377942599161, 0.08932602386326992, 0.10415994840374072, 0.1054498548855208]
Actions to choose Agent 1: dict_values([{'num_count': 553, 'sum_payoffs': 155.9255154771888, 'action': [1.0, 1.5707963267948966]}, {'num_count': 610, 'sum_payoffs': 177.00055766278194, 'action': [1.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 117.79946981070958, 'action': [0.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 116.79996956100112, 'action': [0.0, -1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 152.60261171224138, 'action': [1.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 136.53542791552937, 'action': [0.0, 0.0]}])
Weights num count: [0.17832957110609482, 0.1967107384714608, 0.14446952595936793, 0.14350209609803288, 0.17542728152208964, 0.16123831022250887]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.61590576171875 s
