Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 375, 'sum_payoffs': 84.62880876996834, 'action': [0.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 83.31247418384935, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 107.34089475215299, 'action': [2.0, 1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 66.32966124532417, 'action': [0.0, 1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 105.26649716867466, 'action': [2.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 105.28319171997474, 'action': [1.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 83.86871780107843, 'action': [1.0, 1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 135.77015837647014, 'action': [2.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 66.3759424525317, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10413773951680089, 0.10302693696195502, 0.12302138294918079, 0.08858650374895863, 0.12135517911691197, 0.12135517911691197, 0.10358233823937794, 0.1460705359622327, 0.08858650374895863]
Actions to choose Agent 1: dict_values([{'num_count': 646, 'sum_payoffs': 182.02865955299708, 'action': [1.0, -1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 156.49349235823317, 'action': [0.0, 0.0]}, {'num_count': 643, 'sum_payoffs': 180.91313036124285, 'action': [1.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 135.30951913095205, 'action': [0.0, 1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 203.7889750438151, 'action': [1.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 135.22282334844476, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.179394612607609, 0.1596778672590947, 0.17856151069147458, 0.14301582893640655, 0.19605665093029714, 0.14301582893640655]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.9492528438568115 s
