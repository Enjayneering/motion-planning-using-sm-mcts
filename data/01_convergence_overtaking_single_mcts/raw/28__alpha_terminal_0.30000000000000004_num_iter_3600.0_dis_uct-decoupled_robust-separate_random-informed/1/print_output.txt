Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 371, 'sum_payoffs': 83.32985679688774, 'action': [1.0, -1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 82.73884795358181, 'action': [1.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 106.45720616180567, 'action': [2.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 108.82450077501025, 'action': [1.0, 0.0]}, {'num_count': 436, 'sum_payoffs': 105.06659711873296, 'action': [2.0, -1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 66.43982355546233, 'action': [0.0, 1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 83.99242406387792, 'action': [0.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 67.01344978572983, 'action': [0.0, -1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 135.15611757090275, 'action': [2.0, 0.0]}])
Weights num count: [0.10302693696195502, 0.10247153568453207, 0.12218828103304638, 0.12413218550402666, 0.1210774784782005, 0.08858650374895863, 0.10358233823937794, 0.08914190502638156, 0.14551513468480978]
Actions to choose Agent 1: dict_values([{'num_count': 513, 'sum_payoffs': 134.44038847903167, 'action': [0.0, -1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 177.51189619628028, 'action': [1.0, -1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 205.45466393820598, 'action': [1.0, 0.0]}, {'num_count': 587, 'sum_payoffs': 160.65764216669743, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 180.32223015927934, 'action': [1.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 134.38824063991652, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14246042765898362, 0.17606220494307137, 0.1974451541238545, 0.16301027492363232, 0.17828381005276311, 0.14246042765898362]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.672621011734009 s
