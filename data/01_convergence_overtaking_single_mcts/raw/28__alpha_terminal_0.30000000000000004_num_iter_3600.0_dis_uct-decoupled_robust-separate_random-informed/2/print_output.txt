Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 317, 'sum_payoffs': 65.6806379309954, 'action': [0.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 106.26599741838317, 'action': [2.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 103.99169978496315, 'action': [2.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 137.8978264261714, 'action': [2.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 83.1907958925805, 'action': [1.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 107.62205851806385, 'action': [1.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 82.93874800352356, 'action': [1.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 84.93209915371746, 'action': [0.0, 0.0]}, {'num_count': 317, 'sum_payoffs': 65.7327857701106, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.08803110247153569, 0.12218828103304638, 0.12024437656206609, 0.14773673979450153, 0.10302693696195502, 0.12329908358789225, 0.10274923632324354, 0.10441544015551235, 0.08803110247153569]
Actions to choose Agent 1: dict_values([{'num_count': 517, 'sum_payoffs': 135.89194530908122, 'action': [0.0, -1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 209.2269082502255, 'action': [1.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 181.9155639269052, 'action': [1.0, 1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 175.7938421800894, 'action': [1.0, -1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 156.8557025574283, 'action': [0.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 133.7741998343493, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1435712302138295, 0.20022216051096917, 0.179394612607609, 0.17467370174951402, 0.15995556789780616, 0.1419050263815607]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.065331220626831 s
