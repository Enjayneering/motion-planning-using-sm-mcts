Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 440, 'sum_payoffs': 106.35291048357523, 'action': [2.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 105.35341023386673, 'action': [2.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 66.98433390887598, 'action': [0.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 85.57525583613852, 'action': [0.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 83.24294373169575, 'action': [1.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 108.11090829601238, 'action': [1.0, 0.0]}, {'num_count': 318, 'sum_payoffs': 66.0949959693057, 'action': [0.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 81.95076373494572, 'action': [1.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 134.84605521082295, 'action': [2.0, 0.0]}])
Weights num count: [0.12218828103304638, 0.12135517911691197, 0.08914190502638156, 0.1049708414329353, 0.10302693696195502, 0.12357678422660372, 0.08830880311024715, 0.10191613440710913, 0.14523743404609832]
Actions to choose Agent 1: dict_values([{'num_count': 635, 'sum_payoffs': 177.9610194606674, 'action': [1.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 136.9058948558742, 'action': [0.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 158.2192236951779, 'action': [0.0, 0.0]}, {'num_count': 517, 'sum_payoffs': 135.85131345109662, 'action': [0.0, -1.5707963267948966]}, {'num_count': 630, 'sum_payoffs': 176.14453639815386, 'action': [1.0, -1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 208.07661383239412, 'action': [1.0, 0.0]}])
Weights num count: [0.17633990558178284, 0.1444043321299639, 0.16106637045265204, 0.1435712302138295, 0.17495140238822549, 0.19938905859483477]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.883638858795166 s
