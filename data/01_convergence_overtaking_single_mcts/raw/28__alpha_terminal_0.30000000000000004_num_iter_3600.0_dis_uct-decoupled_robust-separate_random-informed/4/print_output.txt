Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 449, 'sum_payoffs': 109.22136755893973, 'action': [1.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 135.2650486125908, 'action': [2.0, 0.0]}, {'num_count': 369, 'sum_payoffs': 82.50700735217481, 'action': [1.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 67.45953109282073, 'action': [0.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 104.85800576227211, 'action': [2.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 83.38866797099125, 'action': [0.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 83.0980161954807, 'action': [1.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 104.85213913036436, 'action': [2.0, -1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 66.27164677430125, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1246875867814496, 0.14579283532352125, 0.10247153568453207, 0.0896973063038045, 0.1210774784782005, 0.10330463760066648, 0.10302693696195502, 0.1210774784782005, 0.08858650374895863]
Actions to choose Agent 1: dict_values([{'num_count': 587, 'sum_payoffs': 160.88948276810441, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 132.81511415994058, 'action': [0.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 175.30441298174497, 'action': [1.0, 1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 208.3084544338011, 'action': [1.0, 0.0]}, {'num_count': 639, 'sum_payoffs': 179.6241009631315, 'action': [1.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 137.44779781736102, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16301027492363232, 0.14107192446542627, 0.1741183004720911, 0.19938905859483477, 0.17745070813662872, 0.14468203276867536]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.853433132171631 s
