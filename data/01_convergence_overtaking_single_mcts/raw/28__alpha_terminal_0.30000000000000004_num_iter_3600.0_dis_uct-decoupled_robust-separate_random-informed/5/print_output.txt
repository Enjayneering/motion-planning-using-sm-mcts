Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 371, 'sum_payoffs': 83.27770895777256, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 65.73278577011061, 'action': [0.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 136.53024934536006, 'action': [2.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 66.93218606976076, 'action': [0.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 85.26722144758949, 'action': [0.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 106.69592738088585, 'action': [1.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 83.24294373169569, 'action': [1.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.69823782002341, 'action': [2.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 104.94491882746416, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10302693696195502, 0.08803110247153569, 0.14662593723965564, 0.08914190502638156, 0.10469314079422383, 0.12246598167175785, 0.10302693696195502, 0.12163287975562344, 0.1210774784782005]
Actions to choose Agent 1: dict_values([{'num_count': 631, 'sum_payoffs': 176.4313495132875, 'action': [1.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 134.38259129069363, 'action': [0.0, -1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 204.75935941666967, 'action': [1.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 135.45727134177855, 'action': [0.0, 1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 162.10423770926224, 'action': [0.0, 0.0]}, {'num_count': 640, 'sum_payoffs': 179.65289091595824, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17522910302693695, 0.14246042765898362, 0.19688975284643154, 0.143293529575118, 0.1641210774784782, 0.17772840877534019]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.769645929336548 s
