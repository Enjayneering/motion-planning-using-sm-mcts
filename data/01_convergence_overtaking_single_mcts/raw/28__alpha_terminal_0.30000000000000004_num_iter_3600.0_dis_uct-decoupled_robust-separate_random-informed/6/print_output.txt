Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 438, 'sum_payoffs': 105.6749885750773, 'action': [2.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 83.29509157081095, 'action': [1.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 83.23707709978797, 'action': [1.0, 1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 66.00808290411365, 'action': [0.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 108.39478809519227, 'action': [1.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 136.58735847196, 'action': [2.0, 0.0]}, {'num_count': 315, 'sum_payoffs': 65.05486386161272, 'action': [0.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 83.62546985988288, 'action': [0.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 106.72663666390098, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12163287975562344, 0.10302693696195502, 0.10302693696195502, 0.08830880311024715, 0.1238544848653152, 0.14662593723965564, 0.08747570119411274, 0.10330463760066648, 0.12246598167175785]
Actions to choose Agent 1: dict_values([{'num_count': 634, 'sum_payoffs': 177.72048755274102, 'action': [1.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 134.171175259621, 'action': [0.0, 1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 180.9449622713659, 'action': [1.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 133.52508526124618, 'action': [0.0, -1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 207.84013786752973, 'action': [1.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 159.74708295371173, 'action': [0.0, 0.0]}])
Weights num count: [0.17606220494307137, 0.14218272702027215, 0.17856151069147458, 0.1416273257428492, 0.1991113579561233, 0.16217717300749793]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.762866735458374 s
