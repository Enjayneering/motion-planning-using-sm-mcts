Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 371, 'sum_payoffs': 83.28922493890322, 'action': [1.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 106.01394952932625, 'action': [2.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 108.04814982018952, 'action': [1.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 135.4747625961595, 'action': [2.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 65.44597265497688, 'action': [0.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 105.38817545994355, 'action': [2.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 83.62818589315224, 'action': [1.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 84.60758749655302, 'action': [0.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 66.65123958653474, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10302693696195502, 0.1219105803943349, 0.12357678422660372, 0.14579283532352125, 0.08775340183282422, 0.12135517911691197, 0.10330463760066648, 0.10413773951680089, 0.08886420438767009]
Actions to choose Agent 1: dict_values([{'num_count': 706, 'sum_payoffs': 203.52801856555408, 'action': [1.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 134.3360928008013, 'action': [0.0, 1.5707963267948966]}, {'num_count': 646, 'sum_payoffs': 181.86634940374398, 'action': [1.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 136.52630204364058, 'action': [0.0, -1.5707963267948966]}, {'num_count': 578, 'sum_payoffs': 157.3792089201114, 'action': [0.0, 0.0]}, {'num_count': 638, 'sum_payoffs': 178.96910237555255, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.19605665093029714, 0.14246042765898362, 0.179394612607609, 0.14412663149125243, 0.1605109691752291, 0.17717300749791726]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.840418815612793 s
