Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 320, 'sum_payoffs': 66.65123958653476, 'action': [0.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 84.26772119788102, 'action': [0.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 102.63585596796727, 'action': [2.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 105.61697410405434, 'action': [2.0, 1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 66.00808290411365, 'action': [0.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 107.64418513590215, 'action': [1.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 84.29459182051944, 'action': [1.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 137.84274527110225, 'action': [2.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 83.27770895777252, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.08886420438767009, 0.10386003887808942, 0.11913357400722022, 0.12163287975562344, 0.08830880311024715, 0.12329908358789225, 0.10386003887808942, 0.14773673979450153, 0.10302693696195502]
Actions to choose Agent 1: dict_values([{'num_count': 578, 'sum_payoffs': 157.55303505049557, 'action': [0.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 174.7769593173425, 'action': [1.0, 1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 179.8759315695038, 'action': [1.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 135.16480887742193, 'action': [0.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 135.97299174236562, 'action': [0.0, 1.5707963267948966]}, {'num_count': 724, 'sum_payoffs': 210.38487998881024, 'action': [1.0, 0.0]}])
Weights num count: [0.1605109691752291, 0.17384059983337963, 0.17772840877534019, 0.14301582893640655, 0.1435712302138295, 0.2010552624271036]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.846933603286743 s
