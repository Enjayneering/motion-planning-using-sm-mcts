Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 502, 'sum_payoffs': 120.98320403000763, 'action': [2.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 93.41459703377635, 'action': [1.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 94.7715634778797, 'action': [0.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 95.0109727586984, 'action': [1.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 73.58103555695187, 'action': [0.0, -1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 156.70378576219818, 'action': [2.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 74.60378505160656, 'action': [0.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 121.333898248072, 'action': [2.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 123.37465323258131, 'action': [1.0, 0.0]}])
Weights num count: [0.1224091684954889, 0.1019263594245306, 0.10290173128505242, 0.10314557425018288, 0.08680809558644233, 0.14825652279931725, 0.0875396244818337, 0.12265301146061935, 0.1241160692514021]
Actions to choose Agent 1: dict_values([{'num_count': 583, 'sum_payoffs': 153.05151769394368, 'action': [0.0, -1.5707963267948966]}, {'num_count': 733, 'sum_payoffs': 205.82534816123552, 'action': [1.0, -1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 151.65797533491718, 'action': [0.0, 1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 203.36570841630095, 'action': [1.0, 1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 181.01289931735803, 'action': [0.0, 0.0]}, {'num_count': 816, 'sum_payoffs': 235.6203781775174, 'action': [1.0, 0.0]}])
Weights num count: [0.14216044867105584, 0.17873689344062424, 0.14118507681053402, 0.17702999268471104, 0.1616678858814923, 0.19897585954645208]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.999958038330078 s
