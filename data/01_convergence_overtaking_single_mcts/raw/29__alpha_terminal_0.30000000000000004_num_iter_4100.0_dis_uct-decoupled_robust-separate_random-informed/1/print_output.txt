Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 357, 'sum_payoffs': 73.88523128512409, 'action': [0.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 95.66383473340385, 'action': [0.0, 0.0]}, {'num_count': 512, 'sum_payoffs': 124.18881137077501, 'action': [1.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 120.9484388039308, 'action': [2.0, -1.5707963267948966]}, {'num_count': 604, 'sum_payoffs': 155.21250241858718, 'action': [2.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 73.9084805300701, 'action': [0.0, 1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 93.97084065100535, 'action': [1.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 119.2362514196476, 'action': [2.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 95.94680918816097, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.08705193855157278, 0.10363326018044379, 0.12484759814679347, 0.1224091684954889, 0.1472811509387954, 0.08705193855157278, 0.10241404535479151, 0.12118995366983662, 0.10387710314557425]
Actions to choose Agent 1: dict_values([{'num_count': 725, 'sum_payoffs': 202.94841706203684, 'action': [1.0, 1.5707963267948966]}, {'num_count': 826, 'sum_payoffs': 239.14904862431408, 'action': [1.0, 0.0]}, {'num_count': 579, 'sum_payoffs': 151.57106226972527, 'action': [0.0, -1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 201.90556892107466, 'action': [1.0, -1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 153.35571342211577, 'action': [0.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 181.37105357349142, 'action': [0.0, 0.0]}])
Weights num count: [0.1767861497195806, 0.20141428919775664, 0.14118507681053402, 0.1760546208241892, 0.1424042916361863, 0.16191172884662278]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.0129759311676025 s
