Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 427, 'sum_payoffs': 96.33205134961722, 'action': [0.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 119.93155594118392, 'action': [2.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 74.29958932343435, 'action': [0.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 73.61580078302873, 'action': [0.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 120.31114875341741, 'action': [2.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 122.61072360331444, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 94.09838557418198, 'action': [1.0, 1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 157.97858314590962, 'action': [2.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 94.34456683133119, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1041209461107047, 0.12167763960009753, 0.08729578151670324, 0.08680809558644233, 0.12192148256522799, 0.12362838332114119, 0.10241404535479151, 0.14923189465983908, 0.10265788831992197]
Actions to choose Agent 1: dict_values([{'num_count': 575, 'sum_payoffs': 150.22380111790613, 'action': [0.0, -1.5707963267948966]}, {'num_count': 815, 'sum_payoffs': 235.0324112915077, 'action': [1.0, 0.0]}, {'num_count': 660, 'sum_payoffs': 179.8664435599037, 'action': [0.0, 0.0]}, {'num_count': 734, 'sum_payoffs': 206.13834383726936, 'action': [1.0, 1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 206.07446273433848, 'action': [1.0, -1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 152.6457423208102, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1402097049500122, 0.19873201658132164, 0.16093635698610095, 0.17898073640575468, 0.17898073640575468, 0.14191660570592537]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.955388784408569 s
