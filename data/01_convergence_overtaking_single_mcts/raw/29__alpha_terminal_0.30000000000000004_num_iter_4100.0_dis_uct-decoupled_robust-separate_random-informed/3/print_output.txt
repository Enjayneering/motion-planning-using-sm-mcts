Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 510, 'sum_payoffs': 123.59284123998452, 'action': [1.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 74.29958932343438, 'action': [0.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 120.95430543583852, 'action': [2.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 74.55163721249143, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 120.03020227019138, 'action': [2.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 95.0516046166829, 'action': [1.0, 1.5707963267948966]}, {'num_count': 604, 'sum_payoffs': 155.27819421036406, 'action': [2.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 93.36244919466104, 'action': [1.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 96.29344746316352, 'action': [0.0, 0.0]}])
Weights num count: [0.12435991221653256, 0.08729578151670324, 0.1224091684954889, 0.0875396244818337, 0.12167763960009753, 0.10314557425018288, 0.1472811509387954, 0.1019263594245306, 0.1041209461107047]
Actions to choose Agent 1: dict_values([{'num_count': 587, 'sum_payoffs': 154.34934703991652, 'action': [0.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 176.36869967880693, 'action': [0.0, 0.0]}, {'num_count': 723, 'sum_payoffs': 202.15175017822378, 'action': [1.0, -1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 154.73480648405777, 'action': [0.0, 1.5707963267948966]}, {'num_count': 821, 'sum_payoffs': 237.28425638306234, 'action': [1.0, 0.0]}, {'num_count': 731, 'sum_payoffs': 205.06909585272257, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14313582053157767, 0.1584979273347964, 0.17629846378931968, 0.1433796634967081, 0.20019507437210438, 0.17824920751036333]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.019700050354004 s
