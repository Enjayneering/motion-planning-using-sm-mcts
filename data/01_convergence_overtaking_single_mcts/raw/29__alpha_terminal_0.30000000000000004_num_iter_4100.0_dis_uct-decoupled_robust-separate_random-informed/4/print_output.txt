Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 610, 'sum_payoffs': 157.2580014078964, 'action': [2.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 94.99945677756767, 'action': [1.0, -1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 95.96893580599908, 'action': [0.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 120.65010970766636, 'action': [2.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 120.23575166935606, 'action': [2.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 74.54577058058356, 'action': [0.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 92.79468959630131, 'action': [1.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 74.23005887128068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 122.57979703761447, 'action': [1.0, 0.0]}])
Weights num count: [0.14874420872957816, 0.10314557425018288, 0.10387710314557425, 0.12216532553035844, 0.12192148256522799, 0.0875396244818337, 0.10143867349426969, 0.08729578151670324, 0.12362838332114119]
Actions to choose Agent 1: dict_values([{'num_count': 825, 'sum_payoffs': 238.56854177637373, 'action': [1.0, 0.0]}, {'num_count': 577, 'sum_payoffs': 150.82925925829656, 'action': [0.0, -1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 153.22230186703158, 'action': [0.0, 1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 203.48727806622736, 'action': [1.0, 1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 181.8306788332439, 'action': [0.0, 0.0]}, {'num_count': 721, 'sum_payoffs': 201.2884861582111, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.2011704462326262, 0.1406973908802731, 0.1424042916361863, 0.1772738356498415, 0.1623994147768837, 0.17581077785905877]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.931429862976074 s
