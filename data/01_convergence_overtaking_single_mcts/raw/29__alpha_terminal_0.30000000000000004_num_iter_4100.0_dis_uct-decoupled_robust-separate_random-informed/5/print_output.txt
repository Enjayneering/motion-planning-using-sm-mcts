Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 424, 'sum_payoffs': 95.26302064775527, 'action': [1.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 74.15466178721938, 'action': [0.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 123.53685474049233, 'action': [1.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 119.93155594118393, 'action': [2.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 94.35043346323883, 'action': [1.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 95.01006741427506, 'action': [0.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 73.54627033087498, 'action': [0.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 119.55782976085813, 'action': [2.0, -1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 157.60373433847607, 'action': [2.0, 0.0]}])
Weights num count: [0.10338941721531333, 0.08729578151670324, 0.12435991221653256, 0.12167763960009753, 0.10265788831992197, 0.10314557425018288, 0.08680809558644233, 0.12143379663496708, 0.1489880516947086]
Actions to choose Agent 1: dict_values([{'num_count': 727, 'sum_payoffs': 203.71911866763455, 'action': [1.0, -1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 150.26443297589066, 'action': [0.0, -1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 153.66555849951095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 826, 'sum_payoffs': 239.16078188812958, 'action': [1.0, 0.0]}, {'num_count': 654, 'sum_payoffs': 177.81529522137163, 'action': [0.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 205.85446403808945, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1772738356498415, 0.1402097049500122, 0.14264813460131676, 0.20141428919775664, 0.15947329919531822, 0.17873689344062424]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.980526924133301 s
