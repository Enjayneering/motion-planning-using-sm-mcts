Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 359, 'sum_payoffs': 74.5516372124913, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 120.30528212150975, 'action': [2.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 120.96897201560789, 'action': [1.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 93.67251155474091, 'action': [1.0, -1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 121.95380568554704, 'action': [2.0, -1.5707963267948966]}, {'num_count': 610, 'sum_payoffs': 157.19908159245037, 'action': [2.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 74.23005887128069, 'action': [0.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 95.64464143151935, 'action': [0.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 94.67201180444947, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.0875396244818337, 0.12192148256522799, 0.1224091684954889, 0.10217020238966106, 0.12314069739088028, 0.14874420872957816, 0.08729578151670324, 0.10363326018044379, 0.10290173128505242]
Actions to choose Agent 1: dict_values([{'num_count': 658, 'sum_payoffs': 179.17294972721348, 'action': [0.0, 0.0]}, {'num_count': 580, 'sum_payoffs': 151.9389218181432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 151.90708990802023, 'action': [0.0, -1.5707963267948966]}, {'num_count': 834, 'sum_payoffs': 241.97089856844423, 'action': [1.0, 0.0]}, {'num_count': 719, 'sum_payoffs': 200.80177299313556, 'action': [1.0, -1.5707963267948966]}, {'num_count': 729, 'sum_payoffs': 204.27829560081716, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16044867105584004, 0.14142891977566446, 0.14142891977566446, 0.2033650329188003, 0.17532309192879786, 0.17776152158010242]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.834657907485962 s
