Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 502, 'sum_payoffs': 120.89629096481559, 'action': [2.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 96.54549535222046, 'action': [0.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 94.26352039804675, 'action': [1.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 93.04087085345047, 'action': [1.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 74.49362274146827, 'action': [0.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 75.10201419781261, 'action': [0.0, -1.5707963267948966]}, {'num_count': 605, 'sum_payoffs': 155.43362374194425, 'action': [2.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 120.52256478448983, 'action': [2.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 122.194554876158, 'action': [1.0, 0.0]}])
Weights num count: [0.1224091684954889, 0.10436478907583516, 0.10265788831992197, 0.10168251645940014, 0.0875396244818337, 0.0880273104120946, 0.14752499390392587, 0.12216532553035844, 0.12338454035601074]
Actions to choose Agent 1: dict_values([{'num_count': 727, 'sum_payoffs': 203.68435344155753, 'action': [1.0, -1.5707963267948966]}, {'num_count': 654, 'sum_payoffs': 177.94102945570202, 'action': [0.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 153.4744583974308, 'action': [0.0, -1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 201.27099490383029, 'action': [1.0, 1.5707963267948966]}, {'num_count': 594, 'sum_payoffs': 156.9076331138587, 'action': [0.0, 1.5707963267948966]}, {'num_count': 821, 'sum_payoffs': 237.3731974197851, 'action': [1.0, 0.0]}])
Weights num count: [0.1772738356498415, 0.15947329919531822, 0.1424042916361863, 0.1755669348939283, 0.14484272128749084, 0.20019507437210438]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.003018617630005 s
