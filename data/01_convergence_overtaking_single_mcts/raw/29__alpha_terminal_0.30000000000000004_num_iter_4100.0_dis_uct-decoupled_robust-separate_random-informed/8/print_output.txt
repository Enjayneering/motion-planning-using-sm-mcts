Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 506, 'sum_payoffs': 122.37131432249593, 'action': [1.0, 0.0]}, {'num_count': 607, 'sum_payoffs': 156.34541422337992, 'action': [2.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 74.60378505160655, 'action': [0.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 120.98320403000764, 'action': [2.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 95.43684677813941, 'action': [1.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 96.99574124371529, 'action': [0.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 93.8230884401789, 'action': [1.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 120.32853136645582, 'action': [2.0, 1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 73.02479193972282, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12338454035601074, 0.14801267983418678, 0.0875396244818337, 0.1224091684954889, 0.10338941721531333, 0.10460863204096561, 0.10217020238966106, 0.12192148256522799, 0.08632040965618142]
Actions to choose Agent 1: dict_values([{'num_count': 723, 'sum_payoffs': 202.31992695938482, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 199.83117133759617, 'action': [1.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 154.0654672407366, 'action': [0.0, -1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 179.66473282111588, 'action': [0.0, 0.0]}, {'num_count': 829, 'sum_payoffs': 240.28760977833014, 'action': [1.0, 0.0]}, {'num_count': 587, 'sum_payoffs': 154.39584552980884, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17629846378931968, 0.17459156303340648, 0.1428919775664472, 0.16069251402097048, 0.20214581809314802, 0.14313582053157767]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.928117752075195 s
