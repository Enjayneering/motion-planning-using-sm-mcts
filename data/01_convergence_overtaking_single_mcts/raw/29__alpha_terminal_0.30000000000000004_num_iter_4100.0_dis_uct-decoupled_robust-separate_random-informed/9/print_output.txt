Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 360, 'sum_payoffs': 74.94274600585537, 'action': [0.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 74.58053580666032, 'action': [0.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 93.4841274859299, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 120.67900830183548, 'action': [2.0, 1.5707963267948966]}, {'num_count': 607, 'sum_payoffs': 156.34744219491088, 'action': [2.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 96.33114600519424, 'action': [0.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 121.07598372710737, 'action': [2.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 94.47797838641539, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 121.98878819430848, 'action': [1.0, 0.0]}])
Weights num count: [0.08778346744696415, 0.0875396244818337, 0.1019263594245306, 0.12216532553035844, 0.14801267983418678, 0.1041209461107047, 0.1224091684954889, 0.10265788831992197, 0.12314069739088028]
Actions to choose Agent 1: dict_values([{'num_count': 819, 'sum_payoffs': 236.5291267016569, 'action': [1.0, 0.0]}, {'num_count': 717, 'sum_payoffs': 200.06290329766102, 'action': [1.0, 1.5707963267948966]}, {'num_count': 725, 'sum_payoffs': 202.90506917078318, 'action': [1.0, -1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 154.72893985215006, 'action': [0.0, -1.5707963267948966]}, {'num_count': 589, 'sum_payoffs': 155.01281965132992, 'action': [0.0, 1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 180.54436546840162, 'action': [0.0, 0.0]}])
Weights num count: [0.19970738844184346, 0.17483540599853695, 0.1767861497195806, 0.1433796634967081, 0.14362350646183858, 0.16142404291636187]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.9852330684661865 s
