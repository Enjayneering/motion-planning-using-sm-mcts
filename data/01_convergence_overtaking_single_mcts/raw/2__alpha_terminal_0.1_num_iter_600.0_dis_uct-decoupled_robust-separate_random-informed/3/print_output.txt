Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 73, 'sum_payoffs': 20.519740126515085, 'action': [2.0, 1.5707963267948966]}, {'num_count': 61, 'sum_payoffs': 14.582708643246722, 'action': [0.0, 1.5707963267948966]}, {'num_count': 61, 'sum_payoffs': 14.642678658229231, 'action': [0.0, -1.5707963267948966]}, {'num_count': 73, 'sum_payoffs': 20.559720136503426, 'action': [2.0, 0.0]}, {'num_count': 66, 'sum_payoffs': 17.15142428499751, 'action': [1.0, 0.0]}, {'num_count': 66, 'sum_payoffs': 17.171414289991684, 'action': [1.0, -1.5707963267948966]}, {'num_count': 67, 'sum_payoffs': 17.461269362407137, 'action': [1.0, 1.5707963267948966]}, {'num_count': 60, 'sum_payoffs': 14.3328335808196, 'action': [0.0, 0.0]}, {'num_count': 73, 'sum_payoffs': 20.379810091555896, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.12146422628951747, 0.10149750415973377, 0.10149750415973377, 0.12146422628951747, 0.10981697171381032, 0.10981697171381032, 0.11148086522462562, 0.09983361064891846, 0.12146422628951747]
Actions to choose Agent 1: dict_values([{'num_count': 106, 'sum_payoffs': 34.8825587148259, 'action': [1.0, -1.5707963267948966]}, {'num_count': 94, 'sum_payoffs': 28.885557216575073, 'action': [0.0, 1.5707963267948966]}, {'num_count': 94, 'sum_payoffs': 28.90554722156924, 'action': [0.0, 0.0]}, {'num_count': 107, 'sum_payoffs': 35.45227385715973, 'action': [1.0, 1.5707963267948966]}, {'num_count': 106, 'sum_payoffs': 34.98250873979675, 'action': [1.0, 0.0]}, {'num_count': 93, 'sum_payoffs': 28.435782104206254, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17637271214642264, 0.15640599001663893, 0.15640599001663893, 0.17803660565723795, 0.17637271214642264, 0.15474209650582363]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.15172624588012695 s
