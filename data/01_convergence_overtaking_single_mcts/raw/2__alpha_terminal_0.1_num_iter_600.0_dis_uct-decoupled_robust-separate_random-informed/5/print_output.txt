Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 60, 'sum_payoffs': 14.332833580819605, 'action': [0.0, 0.0]}, {'num_count': 73, 'sum_payoffs': 20.479760116526748, 'action': [2.0, -1.5707963267948966]}, {'num_count': 61, 'sum_payoffs': 14.602698648240892, 'action': [0.0, 1.5707963267948966]}, {'num_count': 73, 'sum_payoffs': 20.459770111532574, 'action': [2.0, 0.0]}, {'num_count': 67, 'sum_payoffs': 17.48125936740131, 'action': [1.0, 0.0]}, {'num_count': 66, 'sum_payoffs': 17.191404294985848, 'action': [1.0, -1.5707963267948966]}, {'num_count': 67, 'sum_payoffs': 17.461269362407137, 'action': [1.0, 1.5707963267948966]}, {'num_count': 60, 'sum_payoffs': 14.352823585813772, 'action': [0.0, -1.5707963267948966]}, {'num_count': 73, 'sum_payoffs': 20.459770111532574, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.09983361064891846, 0.12146422628951747, 0.10149750415973377, 0.12146422628951747, 0.11148086522462562, 0.10981697171381032, 0.11148086522462562, 0.09983361064891846, 0.12146422628951747]
Actions to choose Agent 1: dict_values([{'num_count': 93, 'sum_payoffs': 28.49575211918876, 'action': [0.0, -1.5707963267948966]}, {'num_count': 106, 'sum_payoffs': 34.882558714825905, 'action': [1.0, 0.0]}, {'num_count': 107, 'sum_payoffs': 35.37231383718305, 'action': [1.0, -1.5707963267948966]}, {'num_count': 94, 'sum_payoffs': 28.8655672115809, 'action': [0.0, 0.0]}, {'num_count': 94, 'sum_payoffs': 28.885557216575066, 'action': [0.0, 1.5707963267948966]}, {'num_count': 106, 'sum_payoffs': 35.02248874978508, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15474209650582363, 0.17637271214642264, 0.17803660565723795, 0.15640599001663893, 0.15640599001663893, 0.17637271214642264]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.1467280387878418 s
