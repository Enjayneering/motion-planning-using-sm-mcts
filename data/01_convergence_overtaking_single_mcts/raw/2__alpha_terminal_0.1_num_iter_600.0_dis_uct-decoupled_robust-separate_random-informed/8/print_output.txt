Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 67, 'sum_payoffs': 17.54122938238381, 'action': [1.0, -1.5707963267948966]}, {'num_count': 66, 'sum_payoffs': 17.21139429998002, 'action': [1.0, 0.0]}, {'num_count': 61, 'sum_payoffs': 14.682658668217568, 'action': [0.0, 1.5707963267948966]}, {'num_count': 61, 'sum_payoffs': 14.60269864824089, 'action': [0.0, 0.0]}, {'num_count': 72, 'sum_payoffs': 20.129935029128774, 'action': [2.0, 0.0]}, {'num_count': 60, 'sum_payoffs': 14.232883555848751, 'action': [0.0, -1.5707963267948966]}, {'num_count': 73, 'sum_payoffs': 20.459770111532578, 'action': [2.0, -1.5707963267948966]}, {'num_count': 73, 'sum_payoffs': 20.439780106538407, 'action': [2.0, 1.5707963267948966]}, {'num_count': 67, 'sum_payoffs': 17.521239377389644, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11148086522462562, 0.10981697171381032, 0.10149750415973377, 0.10149750415973377, 0.11980033277870217, 0.09983361064891846, 0.12146422628951747, 0.12146422628951747, 0.11148086522462562]
Actions to choose Agent 1: dict_values([{'num_count': 93, 'sum_payoffs': 28.395802094217924, 'action': [0.0, -1.5707963267948966]}, {'num_count': 94, 'sum_payoffs': 28.88555721657508, 'action': [0.0, 1.5707963267948966]}, {'num_count': 106, 'sum_payoffs': 34.88255871482591, 'action': [1.0, 1.5707963267948966]}, {'num_count': 108, 'sum_payoffs': 36.00199899449938, 'action': [1.0, 0.0]}, {'num_count': 94, 'sum_payoffs': 29.025487251534255, 'action': [0.0, 0.0]}, {'num_count': 105, 'sum_payoffs': 34.372813587474596, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15474209650582363, 0.15640599001663893, 0.17637271214642264, 0.17970049916805325, 0.15640599001663893, 0.17470881863560733]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.14442682266235352 s
