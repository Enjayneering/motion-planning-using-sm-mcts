Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 561, 'sum_payoffs': 134.6257305905468, 'action': [2.0, 1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 137.6416139527106, 'action': [2.0, -1.5707963267948966]}, {'num_count': 696, 'sum_payoffs': 179.4266996637745, 'action': [2.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 104.04124023215171, 'action': [1.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 82.8779088578888, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 104.3048041023393, 'action': [1.0, 1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 105.60466141984284, 'action': [0.0, 0.0]}, {'num_count': 572, 'sum_payoffs': 138.2019135130014, 'action': [1.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 82.59696237466267, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12193001521408389, 0.1238861117148446, 0.15127146272549447, 0.10128232992827646, 0.08672027820039122, 0.10149967398391654, 0.10236905020647685, 0.12432079982612476, 0.08650293414475115]
Actions to choose Agent 1: dict_values([{'num_count': 936, 'sum_payoffs': 270.3467830851953, 'action': [1.0, 0.0]}, {'num_count': 737, 'sum_payoffs': 200.29745993233686, 'action': [0.0, 0.0]}, {'num_count': 810, 'sum_payoffs': 225.7312647648245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 171.92610213770095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 646, 'sum_payoffs': 168.85513762046807, 'action': [0.0, -1.5707963267948966]}, {'num_count': 816, 'sum_payoffs': 227.85194355551045, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.20343403607911323, 0.16018256900673766, 0.17604868506846338, 0.14236035644425124, 0.14040425994349054, 0.17735274940230383]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.974262475967407 s
