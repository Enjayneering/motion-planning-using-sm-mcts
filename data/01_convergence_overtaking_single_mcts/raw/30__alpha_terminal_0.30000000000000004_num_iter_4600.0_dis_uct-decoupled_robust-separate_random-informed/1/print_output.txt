Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 395, 'sum_payoffs': 81.66112594520023, 'action': [0.0, 1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 180.4986636815905, 'action': [2.0, 0.0]}, {'num_count': 567, 'sum_payoffs': 136.72743336203285, 'action': [1.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 81.98270428641074, 'action': [0.0, -1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 106.86410416204676, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 104.98859264274506, 'action': [1.0, -1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.7295265184857, 'action': [2.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 104.9653433977989, 'action': [1.0, 1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 136.40158179507586, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.0858509019778309, 0.1519234948924147, 0.12323407954792437, 0.08606824603347099, 0.10323842642903716, 0.1019343620951967, 0.12258204738100413, 0.1019343620951967, 0.12301673549228428]
Actions to choose Agent 1: dict_values([{'num_count': 815, 'sum_payoffs': 227.62021159544557, 'action': [1.0, 1.5707963267948966]}, {'num_count': 936, 'sum_payoffs': 270.40682552774894, 'action': [1.0, 0.0]}, {'num_count': 728, 'sum_payoffs': 197.12737830704293, 'action': [0.0, 0.0]}, {'num_count': 827, 'sum_payoffs': 231.81800400287858, 'action': [1.0, -1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 171.35812525665642, 'action': [0.0, 1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 167.22399666946905, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17713540534666378, 0.20343403607911323, 0.15822647250597696, 0.1797435340143447, 0.1419256683329711, 0.13931753966529015]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.963829517364502 s
