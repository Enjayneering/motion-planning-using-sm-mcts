Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 466, 'sum_payoffs': 104.1455359103822, 'action': [1.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 82.6953914209855, 'action': [0.0, -1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 138.04557863699793, 'action': [1.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 82.94157267813488, 'action': [0.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.74690913152412, 'action': [2.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 107.31999940276462, 'action': [0.0, 0.0]}, {'num_count': 692, 'sum_payoffs': 178.23407134045502, 'action': [2.0, 0.0]}, {'num_count': 564, 'sum_payoffs': 135.7295265184857, 'action': [2.0, -1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 105.38556806801691, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10128232992827646, 0.08650293414475115, 0.12410345577048468, 0.08672027820039122, 0.12258204738100413, 0.10345577048467725, 0.15040208650293416, 0.12258204738100413, 0.10215170615083677]
Actions to choose Agent 1: dict_values([{'num_count': 657, 'sum_payoffs': 172.76340087949828, 'action': [0.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 173.838298213268, 'action': [0.0, -1.5707963267948966]}, {'num_count': 815, 'sum_payoffs': 227.6723594345608, 'action': [1.0, -1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 196.63660919890594, 'action': [0.0, 0.0]}, {'num_count': 920, 'sum_payoffs': 264.86944928925055, 'action': [1.0, 0.0]}, {'num_count': 822, 'sum_payoffs': 230.149381792534, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1427950445555314, 0.14344707672245163, 0.17713540534666378, 0.1577917843946968, 0.199956531188872, 0.1786568137361443]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.067966461181641 s
