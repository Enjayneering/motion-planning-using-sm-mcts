Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 569, 'sum_payoffs': 137.25072244203102, 'action': [1.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 135.06898722302623, 'action': [2.0, 1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 179.77536591030807, 'action': [2.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 82.32166524065974, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 104.30480410233936, 'action': [1.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 104.64963168849602, 'action': [1.0, -1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 136.37268320090683, 'action': [2.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 82.01746951248754, 'action': [0.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 107.88482568517047, 'action': [0.0, 0.0]}])
Weights num count: [0.12366876765920452, 0.12214735926972398, 0.15148880678113455, 0.08628559008911106, 0.10149967398391654, 0.10171701803955661, 0.12301673549228428, 0.08606824603347099, 0.1038904585959574]
Actions to choose Agent 1: dict_values([{'num_count': 658, 'sum_payoffs': 173.12267776273956, 'action': [0.0, -1.5707963267948966]}, {'num_count': 815, 'sum_payoffs': 227.70419134468378, 'action': [1.0, 1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 225.57199657286736, 'action': [1.0, -1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 169.9936987744838, 'action': [0.0, 1.5707963267948966]}, {'num_count': 732, 'sum_payoffs': 198.6936241693458, 'action': [0.0, 0.0]}, {'num_count': 937, 'sum_payoffs': 270.8191555945286, 'action': [1.0, 0.0]}])
Weights num count: [0.1430123886111715, 0.17713540534666378, 0.1758313410128233, 0.1410562921104108, 0.15909584872853727, 0.2036513801347533]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.984194755554199 s
