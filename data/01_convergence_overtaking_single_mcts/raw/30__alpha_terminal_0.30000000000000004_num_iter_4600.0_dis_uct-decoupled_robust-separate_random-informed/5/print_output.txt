Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 400, 'sum_payoffs': 83.124090115038, 'action': [0.0, 1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 137.8078714036833, 'action': [1.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 106.11846249024154, 'action': [1.0, -1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 105.22416326318616, 'action': [0.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 135.94094254955803, 'action': [2.0, -1.5707963267948966]}, {'num_count': 691, 'sum_payoffs': 177.69126303454513, 'action': [2.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 82.19412031748315, 'action': [0.0, -1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 105.22325791876357, 'action': [1.0, 1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 135.2804032540986, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.0869376222560313, 0.12410345577048468, 0.102803738317757, 0.10215170615083677, 0.12279939143664421, 0.15018474244729407, 0.08628559008911106, 0.10215170615083677, 0.12236470332536405]
Actions to choose Agent 1: dict_values([{'num_count': 917, 'sum_payoffs': 263.7191548714194, 'action': [1.0, 0.0]}, {'num_count': 817, 'sum_payoffs': 228.36766395609726, 'action': [1.0, 1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 228.67772631617677, 'action': [1.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 170.32679309682487, 'action': [0.0, -1.5707963267948966]}, {'num_count': 740, 'sum_payoffs': 201.44210500094496, 'action': [0.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 173.1226777627396, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.19930449902195174, 0.17757009345794392, 0.177787437513584, 0.14127363616605085, 0.1608346011736579, 0.1430123886111715]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.921255588531494 s
