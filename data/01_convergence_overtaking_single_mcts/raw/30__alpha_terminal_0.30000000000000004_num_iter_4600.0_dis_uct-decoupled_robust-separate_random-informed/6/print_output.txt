Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 468, 'sum_payoffs': 104.66701430153438, 'action': [1.0, 1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 179.82367508904647, 'action': [2.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 82.93005669700405, 'action': [0.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.7642917445625, 'action': [2.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 82.95330594195019, 'action': [0.0, -1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.74690913152415, 'action': [2.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 105.01749123691417, 'action': [1.0, -1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 137.33875813433096, 'action': [1.0, 0.0]}, {'num_count': 471, 'sum_payoffs': 105.61820537250424, 'action': [0.0, 0.0]}])
Weights num count: [0.10171701803955661, 0.15148880678113455, 0.08672027820039122, 0.12258204738100413, 0.08672027820039122, 0.12258204738100413, 0.1019343620951967, 0.12366876765920452, 0.10236905020647685]
Actions to choose Agent 1: dict_values([{'num_count': 741, 'sum_payoffs': 201.78874327594787, 'action': [0.0, 0.0]}, {'num_count': 650, 'sum_payoffs': 170.32994369546358, 'action': [0.0, -1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 172.10851093326175, 'action': [0.0, 1.5707963267948966]}, {'num_count': 823, 'sum_payoffs': 230.45357752070606, 'action': [1.0, 1.5707963267948966]}, {'num_count': 922, 'sum_payoffs': 265.53969387699465, 'action': [1.0, 0.0]}, {'num_count': 809, 'sum_payoffs': 225.56026330905183, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16105194522929797, 0.14127363616605085, 0.14236035644425124, 0.1788741577917844, 0.20039121930015213, 0.1758313410128233]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.9720940589904785 s
