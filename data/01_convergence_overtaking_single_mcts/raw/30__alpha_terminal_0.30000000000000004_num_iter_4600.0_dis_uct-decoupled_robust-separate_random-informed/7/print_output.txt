Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 568, 'sum_payoffs': 136.88872952552074, 'action': [1.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 107.15001193275741, 'action': [0.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 81.29891574600512, 'action': [0.0, 1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 136.25100490963794, 'action': [2.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 82.29841599571367, 'action': [0.0, -1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 178.71672856246894, 'action': [2.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 135.94680918146582, 'action': [2.0, -1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 105.52745364693564, 'action': [1.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 104.90167957755294, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12345142360356444, 0.10345577048467725, 0.08563355792219082, 0.12301673549228428, 0.08628559008911106, 0.1508367746142143, 0.12279939143664421, 0.10236905020647685, 0.1019343620951967]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 199.12152616031767, 'action': [0.0, 0.0]}, {'num_count': 935, 'sum_payoffs': 270.2166307700918, 'action': [1.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 169.08111158996738, 'action': [0.0, 1.5707963267948966]}, {'num_count': 816, 'sum_payoffs': 228.15331460907086, 'action': [1.0, -1.5707963267948966]}, {'num_count': 651, 'sum_payoffs': 170.74115113513528, 'action': [0.0, -1.5707963267948966]}, {'num_count': 819, 'sum_payoffs': 229.23679460801773, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15931319278417735, 0.20321669202347314, 0.14040425994349054, 0.17735274940230383, 0.14149098022169093, 0.17800478156922409]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.9539806842803955 s
