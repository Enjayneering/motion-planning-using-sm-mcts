Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 473, 'sum_payoffs': 106.20356486658716, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 82.64911021377786, 'action': [0.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 103.7137952590334, 'action': [1.0, 1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 138.02616805242872, 'action': [1.0, 0.0]}, {'num_count': 566, 'sum_payoffs': 136.3146687298838, 'action': [2.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.72952651848576, 'action': [2.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 83.22273644404555, 'action': [0.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 105.02922450072954, 'action': [1.0, -1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 178.8124415755226, 'action': [2.0, 0.0]}])
Weights num count: [0.102803738317757, 0.08650293414475115, 0.10106498587263639, 0.12410345577048468, 0.12301673549228428, 0.12258204738100413, 0.0869376222560313, 0.1019343620951967, 0.1508367746142143]
Actions to choose Agent 1: dict_values([{'num_count': 809, 'sum_payoffs': 225.44445164969076, 'action': [1.0, 1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 228.6515437552771, 'action': [1.0, -1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 171.01058163723076, 'action': [0.0, 1.5707963267948966]}, {'num_count': 937, 'sum_payoffs': 270.75527449159756, 'action': [1.0, 0.0]}, {'num_count': 737, 'sum_payoffs': 200.29383855464482, 'action': [0.0, 0.0]}, {'num_count': 647, 'sum_payoffs': 169.28101163990897, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1758313410128233, 0.177787437513584, 0.14170832427733102, 0.2036513801347533, 0.16018256900673766, 0.14062160399913062]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.01579213142395 s
