Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 469, 'sum_payoffs': 104.91815684616795, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 82.52156529060156, 'action': [0.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 81.8377767501958, 'action': [0.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 135.53245114315553, 'action': [2.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 105.83164937510782, 'action': [1.0, 1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 178.66660869488442, 'action': [2.0, 0.0]}, {'num_count': 572, 'sum_payoffs': 138.2145521212398, 'action': [1.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 137.19249068832355, 'action': [2.0, 1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 103.87893008289831, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1019343620951967, 0.08650293414475115, 0.08606824603347099, 0.12258204738100413, 0.10258639426211694, 0.1508367746142143, 0.12432079982612476, 0.12366876765920452, 0.10128232992827646]
Actions to choose Agent 1: dict_values([{'num_count': 810, 'sum_payoffs': 226.13399818066188, 'action': [1.0, -1.5707963267948966]}, {'num_count': 822, 'sum_payoffs': 230.34939048381807, 'action': [1.0, 1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 169.55055078334672, 'action': [0.0, 1.5707963267948966]}, {'num_count': 741, 'sum_payoffs': 202.02261184888567, 'action': [0.0, 0.0]}, {'num_count': 933, 'sum_payoffs': 269.6680644736166, 'action': [1.0, 0.0]}, {'num_count': 647, 'sum_payoffs': 169.5361014862622, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17604868506846338, 0.1786568137361443, 0.14062160399913062, 0.16105194522929797, 0.202782003912193, 0.14062160399913062]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.016420364379883 s
