Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 69, 'sum_payoffs': 25.571408032194338, 'action': [0.0, 1.5707963267948966]}, {'num_count': 61, 'sum_payoffs': 21.00034132917859, 'action': [0.0, 0.0]}, {'num_count': 72, 'sum_payoffs': 27.39905277397602, 'action': [2.0, 1.5707963267948966]}, {'num_count': 63, 'sum_payoffs': 22.186241908930583, 'action': [2.0, 0.0]}, {'num_count': 71, 'sum_payoffs': 26.850185709124528, 'action': [1.0, -1.5707963267948966]}, {'num_count': 67, 'sum_payoffs': 24.34592941896763, 'action': [0.0, -1.5707963267948966]}, {'num_count': 67, 'sum_payoffs': 24.35058272452106, 'action': [1.0, 1.5707963267948966]}, {'num_count': 63, 'sum_payoffs': 22.182410608521835, 'action': [1.0, 0.0]}, {'num_count': 67, 'sum_payoffs': 24.507079116128775, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11480865224625623, 0.10149750415973377, 0.11980033277870217, 0.1048252911813644, 0.11813643926788686, 0.11148086522462562, 0.11148086522462562, 0.1048252911813644, 0.11148086522462562]
Actions to choose Agent 1: dict_values([{'num_count': 105, 'sum_payoffs': 42.57433701009871, 'action': [1.0, 0.0]}, {'num_count': 95, 'sum_payoffs': 36.91484542503178, 'action': [0.0, 1.5707963267948966]}, {'num_count': 87, 'sum_payoffs': 32.37547328345589, 'action': [0.0, 0.0]}, {'num_count': 95, 'sum_payoffs': 36.87854750300498, 'action': [0.0, -1.5707963267948966]}, {'num_count': 110, 'sum_payoffs': 45.442379036848834, 'action': [1.0, 1.5707963267948966]}, {'num_count': 108, 'sum_payoffs': 44.2118608499512, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17470881863560733, 0.15806988352745424, 0.1447587354409318, 0.15806988352745424, 0.18302828618968386, 0.17970049916805325]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.3959269523620605 s
