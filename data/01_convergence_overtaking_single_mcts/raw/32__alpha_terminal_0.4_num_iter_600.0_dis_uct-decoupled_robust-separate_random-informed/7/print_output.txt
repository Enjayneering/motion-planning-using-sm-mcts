Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 60, 'sum_payoffs': 20.64717483422628, 'action': [1.0, 0.0]}, {'num_count': 68, 'sum_payoffs': 25.457549023937936, 'action': [2.0, 0.0]}, {'num_count': 64, 'sum_payoffs': 22.950815770047996, 'action': [0.0, 0.0]}, {'num_count': 67, 'sum_payoffs': 24.85699918699811, 'action': [0.0, -1.5707963267948966]}, {'num_count': 70, 'sum_payoffs': 26.60390141291288, 'action': [2.0, -1.5707963267948966]}, {'num_count': 60, 'sum_payoffs': 20.752519430539568, 'action': [0.0, 1.5707963267948966]}, {'num_count': 68, 'sum_payoffs': 25.29063664931591, 'action': [1.0, -1.5707963267948966]}, {'num_count': 69, 'sum_payoffs': 26.11185314196184, 'action': [1.0, 1.5707963267948966]}, {'num_count': 74, 'sum_payoffs': 29.14328038006312, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.09983361064891846, 0.11314475873544093, 0.1064891846921797, 0.11148086522462562, 0.11647254575707154, 0.09983361064891846, 0.11314475873544093, 0.11480865224625623, 0.12312811980033278]
Actions to choose Agent 1: dict_values([{'num_count': 107, 'sum_payoffs': 43.84685526937723, 'action': [1.0, -1.5707963267948966]}, {'num_count': 113, 'sum_payoffs': 47.18574168655846, 'action': [1.0, 1.5707963267948966]}, {'num_count': 91, 'sum_payoffs': 34.44897654414281, 'action': [0.0, 0.0]}, {'num_count': 110, 'sum_payoffs': 45.55938681267258, 'action': [1.0, 0.0]}, {'num_count': 90, 'sum_payoffs': 33.84617262830855, 'action': [0.0, -1.5707963267948966]}, {'num_count': 89, 'sum_payoffs': 33.40851712106973, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17803660565723795, 0.18801996672212978, 0.15141430948419302, 0.18302828618968386, 0.1497504159733777, 0.1480865224625624]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 2.3526155948638916 s
