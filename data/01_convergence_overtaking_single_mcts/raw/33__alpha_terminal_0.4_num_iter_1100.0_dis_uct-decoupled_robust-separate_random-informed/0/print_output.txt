Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 125, 'sum_payoffs': 46.75786598630459, 'action': [2.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 42.83173596986965, 'action': [2.0, 0.0]}, {'num_count': 111, 'sum_payoffs': 39.27669145764663, 'action': [0.0, 0.0]}, {'num_count': 129, 'sum_payoffs': 48.93240744304264, 'action': [0.0, -1.5707963267948966]}, {'num_count': 112, 'sum_payoffs': 39.79196162095279, 'action': [1.0, 0.0]}, {'num_count': 130, 'sum_payoffs': 49.46309936602307, 'action': [1.0, 1.5707963267948966]}, {'num_count': 138, 'sum_payoffs': 53.88755431749269, 'action': [1.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 42.42577667203728, 'action': [0.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 44.075694081901396, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11353315168029064, 0.10717529518619437, 0.1008174386920981, 0.11716621253405994, 0.10172570390554042, 0.11807447774750227, 0.12534059945504086, 0.10626702997275204, 0.10899182561307902]
Actions to choose Agent 1: dict_values([{'num_count': 171, 'sum_payoffs': 65.96257844527786, 'action': [0.0, -1.5707963267948966]}, {'num_count': 200, 'sum_payoffs': 81.43062847743401, 'action': [1.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 69.14589607810703, 'action': [0.0, 1.5707963267948966]}, {'num_count': 151, 'sum_payoffs': 55.501172633420246, 'action': [0.0, 0.0]}, {'num_count': 206, 'sum_payoffs': 84.61679476869136, 'action': [1.0, -1.5707963267948966]}, {'num_count': 195, 'sum_payoffs': 78.79715326472578, 'action': [1.0, 0.0]}])
Weights num count: [0.1553133514986376, 0.18165304268846502, 0.16076294277929154, 0.1371480472297911, 0.18710263396911897, 0.1771117166212534]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 4.183680295944214 s
