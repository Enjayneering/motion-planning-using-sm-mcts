Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 132, 'sum_payoffs': 49.75955384084362, 'action': [2.0, -1.5707963267948966]}, {'num_count': 114, 'sum_payoffs': 39.94461807070342, 'action': [0.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 41.12002833569421, 'action': [2.0, 0.0]}, {'num_count': 131, 'sum_payoffs': 49.18714108331833, 'action': [2.0, 1.5707963267948966]}, {'num_count': 105, 'sum_payoffs': 35.38882373180258, 'action': [1.0, 0.0]}, {'num_count': 134, 'sum_payoffs': 50.65264834659098, 'action': [1.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 41.70151870468629, 'action': [0.0, -1.5707963267948966]}, {'num_count': 130, 'sum_payoffs': 48.574336379635874, 'action': [1.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 43.77784971578159, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11989100817438691, 0.10354223433242507, 0.10535876475930972, 0.11898274296094459, 0.09536784741144415, 0.12170753860127158, 0.10626702997275204, 0.11807447774750227, 0.10990009082652134]
Actions to choose Agent 1: dict_values([{'num_count': 162, 'sum_payoffs': 62.03992220535539, 'action': [0.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 78.72793170987515, 'action': [1.0, 0.0]}, {'num_count': 209, 'sum_payoffs': 87.52649289180145, 'action': [1.0, 1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 91.32403540165014, 'action': [1.0, -1.5707963267948966]}, {'num_count': 159, 'sum_payoffs': 60.55841416487538, 'action': [0.0, 1.5707963267948966]}, {'num_count': 161, 'sum_payoffs': 61.548446397543835, 'action': [0.0, 0.0]}])
Weights num count: [0.14713896457765668, 0.17529518619436876, 0.18982742960944596, 0.19618528610354224, 0.1444141689373297, 0.14623069936421434]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 4.3611063957214355 s
