Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 124, 'sum_payoffs': 45.79931015358498, 'action': [1.0, 1.5707963267948966]}, {'num_count': 108, 'sum_payoffs': 37.31195153480854, 'action': [0.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 42.640871934810875, 'action': [1.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 41.42180334342167, 'action': [0.0, -1.5707963267948966]}, {'num_count': 130, 'sum_payoffs': 48.96576007542068, 'action': [1.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 46.97966061483515, 'action': [2.0, 0.0]}, {'num_count': 130, 'sum_payoffs': 49.140533329626415, 'action': [2.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 44.11499888269548, 'action': [0.0, 1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 47.29628019524574, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11262488646684832, 0.09809264305177112, 0.10717529518619437, 0.10535876475930972, 0.11807447774750227, 0.11444141689373297, 0.11807447774750227, 0.10990009082652134, 0.11534968210717529]
Actions to choose Agent 1: dict_values([{'num_count': 192, 'sum_payoffs': 77.34159592918986, 'action': [1.0, -1.5707963267948966]}, {'num_count': 209, 'sum_payoffs': 86.60576511374543, 'action': [1.0, 1.5707963267948966]}, {'num_count': 195, 'sum_payoffs': 78.97535060673829, 'action': [1.0, 0.0]}, {'num_count': 161, 'sum_payoffs': 60.89246996485357, 'action': [0.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 67.26449174593881, 'action': [0.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 65.67590780518894, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17438692098092642, 0.18982742960944596, 0.1771117166212534, 0.14623069936421434, 0.15712988192552224, 0.15440508628519528]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 4.211990118026733 s
