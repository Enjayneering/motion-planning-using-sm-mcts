Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 142, 'sum_payoffs': 55.68826611522519, 'action': [2.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 44.32536775325983, 'action': [2.0, 1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 45.48581015768446, 'action': [1.0, 1.5707963267948966]}, {'num_count': 111, 'sum_payoffs': 38.8742362543252, 'action': [1.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 44.785378695124436, 'action': [2.0, 0.0]}, {'num_count': 126, 'sum_payoffs': 47.08359349643236, 'action': [1.0, -1.5707963267948966]}, {'num_count': 109, 'sum_payoffs': 37.905775459226966, 'action': [0.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 43.13666747146779, 'action': [0.0, 1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 47.576633856563205, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12897366030881016, 0.10990009082652134, 0.11171662125340599, 0.1008174386920981, 0.11080835603996367, 0.11444141689373297, 0.09900090826521345, 0.1080835603996367, 0.11534968210717529]
Actions to choose Agent 1: dict_values([{'num_count': 200, 'sum_payoffs': 81.21819472570094, 'action': [1.0, 1.5707963267948966]}, {'num_count': 162, 'sum_payoffs': 60.9254939269506, 'action': [0.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 65.25921176293988, 'action': [0.0, -1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 63.014113089277956, 'action': [0.0, 1.5707963267948966]}, {'num_count': 208, 'sum_payoffs': 85.5270667613147, 'action': [1.0, 0.0]}, {'num_count': 194, 'sum_payoffs': 77.90858983866049, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18165304268846502, 0.14713896457765668, 0.15440508628519528, 0.15077202543142598, 0.18891916439600362, 0.17620345140781107]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.232909202575684 s
