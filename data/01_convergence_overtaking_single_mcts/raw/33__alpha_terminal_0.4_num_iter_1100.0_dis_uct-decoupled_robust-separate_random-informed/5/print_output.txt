Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 134, 'sum_payoffs': 51.31344974340302, 'action': [1.0, -1.5707963267948966]}, {'num_count': 132, 'sum_payoffs': 50.27986775218212, 'action': [2.0, -1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 45.94233172457124, 'action': [2.0, 1.5707963267948966]}, {'num_count': 106, 'sum_payoffs': 36.35730462942169, 'action': [2.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 42.64204505632491, 'action': [0.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 44.32610879933468, 'action': [0.0, -1.5707963267948966]}, {'num_count': 114, 'sum_payoffs': 40.56223466643725, 'action': [0.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 44.16808886569637, 'action': [1.0, 0.0]}, {'num_count': 130, 'sum_payoffs': 49.12488019002794, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12170753860127158, 0.11989100817438691, 0.11262488646684832, 0.09627611262488647, 0.10717529518619437, 0.10990009082652134, 0.10354223433242507, 0.10990009082652134, 0.11807447774750227]
Actions to choose Agent 1: dict_values([{'num_count': 164, 'sum_payoffs': 62.746877969524796, 'action': [0.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 67.48621870726616, 'action': [0.0, -1.5707963267948966]}, {'num_count': 160, 'sum_payoffs': 60.56655834675857, 'action': [0.0, 1.5707963267948966]}, {'num_count': 203, 'sum_payoffs': 83.69119186200304, 'action': [1.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 80.35663919759932, 'action': [1.0, 0.0]}, {'num_count': 203, 'sum_payoffs': 83.59082946132025, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14895549500454133, 0.15712988192552224, 0.14532243415077203, 0.184377838328792, 0.17892824704813806, 0.184377838328792]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 4.190606594085693 s
