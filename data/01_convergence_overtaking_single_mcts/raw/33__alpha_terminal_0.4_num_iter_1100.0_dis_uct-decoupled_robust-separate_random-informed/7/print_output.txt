Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 100, 'sum_payoffs': 33.360782272981545, 'action': [1.0, 0.0]}, {'num_count': 117, 'sum_payoffs': 42.420838562832586, 'action': [0.0, 1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 45.58612275976341, 'action': [2.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 44.62156481391324, 'action': [0.0, -1.5707963267948966]}, {'num_count': 130, 'sum_payoffs': 49.43438832186513, 'action': [1.0, 1.5707963267948966]}, {'num_count': 131, 'sum_payoffs': 49.80443007078285, 'action': [1.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 43.876275722835125, 'action': [0.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 45.653636298457336, 'action': [2.0, 0.0]}, {'num_count': 135, 'sum_payoffs': 52.12120613521253, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.09082652134423251, 0.10626702997275204, 0.11171662125340599, 0.10990009082652134, 0.11807447774750227, 0.11898274296094459, 0.10899182561307902, 0.11171662125340599, 0.1226158038147139]
Actions to choose Agent 1: dict_values([{'num_count': 201, 'sum_payoffs': 82.58059536006384, 'action': [1.0, 0.0]}, {'num_count': 205, 'sum_payoffs': 84.75121635496313, 'action': [1.0, 1.5707963267948966]}, {'num_count': 160, 'sum_payoffs': 60.43425327080126, 'action': [0.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 68.58848398983257, 'action': [0.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 66.88865729403523, 'action': [0.0, -1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 74.91233207345266, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18256130790190736, 0.18619436875567666, 0.14532243415077203, 0.1589464123524069, 0.15622161671207993, 0.16984559491371481]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.332942008972168 s
