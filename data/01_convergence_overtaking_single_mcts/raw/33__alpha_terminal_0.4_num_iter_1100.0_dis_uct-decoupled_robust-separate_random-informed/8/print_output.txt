Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 135, 'sum_payoffs': 52.2694937533669, 'action': [2.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 43.477507587554875, 'action': [0.0, -1.5707963267948966]}, {'num_count': 115, 'sum_payoffs': 41.264233317357366, 'action': [2.0, 0.0]}, {'num_count': 131, 'sum_payoffs': 50.02803304474324, 'action': [2.0, 1.5707963267948966]}, {'num_count': 130, 'sum_payoffs': 49.44380390394545, 'action': [1.0, 1.5707963267948966]}, {'num_count': 129, 'sum_payoffs': 48.74411784490113, 'action': [1.0, -1.5707963267948966]}, {'num_count': 112, 'sum_payoffs': 39.71899843964492, 'action': [0.0, 0.0]}, {'num_count': 113, 'sum_payoffs': 40.355876187671846, 'action': [1.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 41.71230521073802, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1226158038147139, 0.1080835603996367, 0.1044504995458674, 0.11898274296094459, 0.11807447774750227, 0.11716621253405994, 0.10172570390554042, 0.10263396911898275, 0.10535876475930972]
Actions to choose Agent 1: dict_values([{'num_count': 193, 'sum_payoffs': 78.04061223473789, 'action': [1.0, -1.5707963267948966]}, {'num_count': 195, 'sum_payoffs': 79.06197776992872, 'action': [1.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 65.22348571834978, 'action': [0.0, 1.5707963267948966]}, {'num_count': 210, 'sum_payoffs': 87.30337083951831, 'action': [1.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 66.78027421914848, 'action': [0.0, -1.5707963267948966]}, {'num_count': 161, 'sum_payoffs': 60.75810908876808, 'action': [0.0, 0.0]}])
Weights num count: [0.17529518619436876, 0.1771117166212534, 0.15349682107175294, 0.1907356948228883, 0.15622161671207993, 0.14623069936421434]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 4.205920934677124 s
