Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 119, 'sum_payoffs': 43.138800678976025, 'action': [0.0, -1.5707963267948966]}, {'num_count': 129, 'sum_payoffs': 48.4226649518979, 'action': [2.0, 1.5707963267948966]}, {'num_count': 112, 'sum_payoffs': 39.56700614111064, 'action': [0.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 44.813122220214424, 'action': [0.0, 1.5707963267948966]}, {'num_count': 133, 'sum_payoffs': 50.650436706407824, 'action': [1.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 47.05049271265831, 'action': [1.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 48.071515004145695, 'action': [2.0, -1.5707963267948966]}, {'num_count': 114, 'sum_payoffs': 40.57046130874207, 'action': [1.0, 0.0]}, {'num_count': 117, 'sum_payoffs': 42.06265241339885, 'action': [2.0, 0.0]}])
Weights num count: [0.1080835603996367, 0.11716621253405994, 0.10172570390554042, 0.11080835603996367, 0.12079927338782924, 0.11444141689373297, 0.11625794732061762, 0.10354223433242507, 0.10626702997275204]
Actions to choose Agent 1: dict_values([{'num_count': 218, 'sum_payoffs': 91.94075053426927, 'action': [1.0, -1.5707963267948966]}, {'num_count': 163, 'sum_payoffs': 62.408500024204365, 'action': [0.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 71.97397032475638, 'action': [0.0, -1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 78.95569600951447, 'action': [1.0, 1.5707963267948966]}, {'num_count': 158, 'sum_payoffs': 59.71755545122324, 'action': [0.0, 0.0]}, {'num_count': 186, 'sum_payoffs': 74.53922273558145, 'action': [1.0, 0.0]}])
Weights num count: [0.1980018165304269, 0.148047229791099, 0.16439600363306087, 0.17620345140781107, 0.14350590372388738, 0.16893732970027248]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 4.275967836380005 s
