Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 61.225528183522115, 'action': [0.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 71.38099791848614, 'action': [2.0, 1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 69.93350109603728, 'action': [1.0, 1.5707963267948966]}, {'num_count': 198, 'sum_payoffs': 75.59440659302508, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 63.800090537851915, 'action': [0.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 61.78149505254295, 'action': [1.0, 0.0]}, {'num_count': 166, 'sum_payoffs': 59.24435496819325, 'action': [2.0, 0.0]}, {'num_count': 157, 'sum_payoffs': 54.67169332142644, 'action': [0.0, 0.0]}, {'num_count': 186, 'sum_payoffs': 69.32304314740037, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1061836352279825, 0.11867582760774516, 0.11680199875078076, 0.12367270455965022, 0.10930668332292318, 0.10680824484697064, 0.10368519675202999, 0.09806371018113678, 0.11617738913179262]
Actions to choose Agent 1: dict_values([{'num_count': 311, 'sum_payoffs': 129.17438942806632, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 91.95420854779888, 'action': [0.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 114.18335422564245, 'action': [1.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 83.92177856577956, 'action': [0.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 124.05286576923778, 'action': [1.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 94.56098820153768, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1942535915053092, 0.14928169893816365, 0.17613991255465333, 0.13928794503435352, 0.18800749531542785, 0.1524047470331043]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 6.090026378631592 s
