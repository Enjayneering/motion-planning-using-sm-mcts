Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 195, 'sum_payoffs': 74.19206194267036, 'action': [1.0, -1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 72.1663279127854, 'action': [2.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 65.40049078844095, 'action': [2.0, 0.0]}, {'num_count': 161, 'sum_payoffs': 56.74639225910984, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 64.48748368559984, 'action': [0.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 66.01422796984549, 'action': [1.0, 1.5707963267948966]}, {'num_count': 157, 'sum_payoffs': 54.74367470296941, 'action': [0.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 61.307519324055704, 'action': [0.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 73.2002893979341, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12179887570268583, 0.1193004372267333, 0.11118051217988757, 0.10056214865708932, 0.1099312929419113, 0.11180512179887571, 0.09806371018113678, 0.1061836352279825, 0.12054965646470955]
Actions to choose Agent 1: dict_values([{'num_count': 266, 'sum_payoffs': 106.31549513628936, 'action': [0.0, 1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 126.12769681958402, 'action': [1.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 121.32609416325722, 'action': [1.0, -1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 106.31048734550407, 'action': [1.0, 0.0]}, {'num_count': 252, 'sum_payoffs': 98.99526810817775, 'action': [0.0, -1.5707963267948966]}, {'num_count': 217, 'sum_payoffs': 81.23878264235839, 'action': [0.0, 0.0]}])
Weights num count: [0.16614615865084323, 0.18988132417239226, 0.18425983760149905, 0.16614615865084323, 0.15740162398500937, 0.13554028732042472]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 6.1715216636657715 s
