Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 180, 'sum_payoffs': 66.14009219294131, 'action': [1.0, 1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 58.044602930232976, 'action': [0.0, 1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 73.27732851497689, 'action': [2.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 74.90552402995168, 'action': [2.0, 1.5707963267948966]}, {'num_count': 160, 'sum_payoffs': 55.994543860784894, 'action': [1.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 67.65710006102702, 'action': [0.0, -1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 59.54843183904155, 'action': [2.0, 0.0]}, {'num_count': 198, 'sum_payoffs': 75.48724310585841, 'action': [1.0, -1.5707963267948966]}, {'num_count': 157, 'sum_payoffs': 54.512665187621515, 'action': [0.0, 0.0]}])
Weights num count: [0.11242973141786383, 0.10243597751405371, 0.12117426608369769, 0.12304809494066209, 0.09993753903810118, 0.11430356027482823, 0.10430980637101811, 0.12367270455965022, 0.09806371018113678]
Actions to choose Agent 1: dict_values([{'num_count': 232, 'sum_payoffs': 89.04870367986818, 'action': [0.0, 1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 129.01771163796568, 'action': [1.0, 1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 84.3848101176182, 'action': [0.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 132.75818090548177, 'action': [1.0, -1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 96.78387186499081, 'action': [0.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 110.19270136460867, 'action': [1.0, 0.0]}])
Weights num count: [0.14490943160524672, 0.1930043722673329, 0.13928794503435352, 0.19737663960024984, 0.15427857589006871, 0.17051842598376016]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 6.09937858581543 s
