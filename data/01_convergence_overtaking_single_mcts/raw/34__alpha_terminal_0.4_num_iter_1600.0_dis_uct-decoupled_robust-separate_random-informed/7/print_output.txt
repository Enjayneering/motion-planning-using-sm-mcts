Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 195, 'sum_payoffs': 73.997042367292, 'action': [2.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 72.9956362990325, 'action': [1.0, 1.5707963267948966]}, {'num_count': 195, 'sum_payoffs': 74.002025075444, 'action': [2.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 72.41616753053425, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 64.2556882320003, 'action': [2.0, 0.0]}, {'num_count': 157, 'sum_payoffs': 54.70174694821006, 'action': [1.0, 0.0]}, {'num_count': 156, 'sum_payoffs': 54.04558684689262, 'action': [0.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 60.25360100852066, 'action': [0.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 60.20292434954422, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12179887570268583, 0.12054965646470955, 0.12179887570268583, 0.11992504684572143, 0.1099312929419113, 0.09806371018113678, 0.09743910056214866, 0.10493441599000625, 0.10493441599000625]
Actions to choose Agent 1: dict_values([{'num_count': 223, 'sum_payoffs': 83.3493018026389, 'action': [0.0, 1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 90.61072340527704, 'action': [0.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 96.25673482395536, 'action': [0.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 119.45169721536796, 'action': [1.0, -1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 128.37311870760973, 'action': [1.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 117.335496812753, 'action': [1.0, 0.0]}])
Weights num count: [0.13928794503435352, 0.14803247970018737, 0.15490318550905685, 0.1830106183635228, 0.19362898188632105, 0.18051217988757026]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 6.172736644744873 s
