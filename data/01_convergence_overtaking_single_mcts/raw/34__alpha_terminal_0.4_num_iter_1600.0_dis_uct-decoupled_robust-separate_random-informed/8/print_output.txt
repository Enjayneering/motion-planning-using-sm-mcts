Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 198, 'sum_payoffs': 75.5285100412951, 'action': [2.0, -1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 58.96624674432192, 'action': [2.0, 0.0]}, {'num_count': 192, 'sum_payoffs': 72.3130201475966, 'action': [1.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 72.94128194603283, 'action': [1.0, 1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 59.683412340829456, 'action': [0.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 66.70048036517177, 'action': [2.0, 1.5707963267948966]}, {'num_count': 161, 'sum_payoffs': 56.571706518367506, 'action': [1.0, 0.0]}, {'num_count': 161, 'sum_payoffs': 56.6637766598802, 'action': [0.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 66.8287010105606, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12367270455965022, 0.10368519675202999, 0.11992504684572143, 0.12054965646470955, 0.10430980637101811, 0.11305434103685197, 0.10056214865708932, 0.10056214865708932, 0.11305434103685197]
Actions to choose Agent 1: dict_values([{'num_count': 295, 'sum_payoffs': 120.6165004688247, 'action': [1.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 108.15481290740426, 'action': [1.0, 0.0]}, {'num_count': 244, 'sum_payoffs': 94.27249779166303, 'action': [0.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 88.17621316555243, 'action': [0.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 128.9544447412093, 'action': [1.0, -1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 95.89785145886381, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18425983760149905, 0.16926920674578388, 0.1524047470331043, 0.14490943160524672, 0.1942535915053092, 0.15427857589006871]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 6.106074094772339 s
