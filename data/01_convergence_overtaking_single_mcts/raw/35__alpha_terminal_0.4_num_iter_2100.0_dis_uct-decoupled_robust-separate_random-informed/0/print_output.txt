Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 227, 'sum_payoffs': 82.46362614131534, 'action': [2.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 91.81131894311282, 'action': [1.0, 1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 80.40284482869974, 'action': [0.0, 1.5707963267948966]}, {'num_count': 209, 'sum_payoffs': 73.59879041533523, 'action': [1.0, 0.0]}, {'num_count': 213, 'sum_payoffs': 75.47291720642893, 'action': [0.0, 0.0]}, {'num_count': 226, 'sum_payoffs': 81.98859735745353, 'action': [0.0, -1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 98.8299860860717, 'action': [1.0, -1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 95.64785211992825, 'action': [2.0, -1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 89.90445698460334, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10804378867206092, 0.1170871013802951, 0.10613993336506425, 0.09947643979057591, 0.10138029509757258, 0.10756782484531176, 0.12375059495478344, 0.12089481199428843, 0.11518324607329843]
Actions to choose Agent 1: dict_values([{'num_count': 332, 'sum_payoffs': 129.67294241113152, 'action': [0.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 159.12212038368995, 'action': [1.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 164.90135456232528, 'action': [1.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 109.4331159605379, 'action': [0.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 147.7666932643285, 'action': [1.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 121.75563738164843, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15801999048072346, 0.1861018562589243, 0.19133745835316515, 0.1385054735840076, 0.17515468824369348, 0.1504045692527368]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 8.035014867782593 s
