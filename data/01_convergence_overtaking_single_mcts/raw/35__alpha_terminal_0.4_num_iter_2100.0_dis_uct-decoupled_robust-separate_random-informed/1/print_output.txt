Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 225, 'sum_payoffs': 81.50766155310887, 'action': [0.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 99.43848753855846, 'action': [1.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 92.49348932436752, 'action': [2.0, 1.5707963267948966]}, {'num_count': 203, 'sum_payoffs': 70.82055295366607, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 79.07020558389122, 'action': [2.0, 0.0]}, {'num_count': 205, 'sum_payoffs': 71.79246295566911, 'action': [0.0, 0.0]}, {'num_count': 260, 'sum_payoffs': 98.8408788533705, 'action': [1.0, -1.5707963267948966]}, {'num_count': 262, 'sum_payoffs': 99.87893948887277, 'action': [2.0, -1.5707963267948966]}, {'num_count': 217, 'sum_payoffs': 77.64319382268387, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10709186101856259, 0.1242265587815326, 0.11756306520704426, 0.09662065683008092, 0.10471204188481675, 0.09757258448357925, 0.12375059495478344, 0.12470252260828177, 0.10328415040456926]
Actions to choose Agent 1: dict_values([{'num_count': 419, 'sum_payoffs': 174.25078807831008, 'action': [1.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 157.42073715460253, 'action': [1.0, 1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 119.71111557842796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 109.43691778465985, 'action': [0.0, 0.0]}, {'num_count': 313, 'sum_payoffs': 120.80646373662174, 'action': [0.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 154.99041767987472, 'action': [1.0, 0.0]}])
Weights num count: [0.199428843407901, 0.18372203712517848, 0.14802475011899097, 0.13802950975725845, 0.1489766777724893, 0.18134221799143266]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 7.897287368774414 s
