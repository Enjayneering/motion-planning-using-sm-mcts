Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 245, 'sum_payoffs': 91.3559725478261, 'action': [2.0, 1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 76.38188466098036, 'action': [0.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 83.40112807553348, 'action': [0.0, 1.5707963267948966]}, {'num_count': 199, 'sum_payoffs': 68.84830008183056, 'action': [1.0, 0.0]}, {'num_count': 244, 'sum_payoffs': 90.8097366025834, 'action': [1.0, 1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 102.19292222257674, 'action': [2.0, -1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 92.80111287530464, 'action': [1.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 84.99597978215385, 'action': [2.0, 0.0]}, {'num_count': 221, 'sum_payoffs': 79.47295844364248, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11661113755354593, 0.10233222275107091, 0.10899571632555925, 0.09471680152308425, 0.11613517372679677, 0.1270823417420276, 0.11803902903379343, 0.11042360780580676, 0.10518800571156592]
Actions to choose Agent 1: dict_values([{'num_count': 333, 'sum_payoffs': 130.66522319359575, 'action': [0.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 102.64083120823113, 'action': [0.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 169.2301488258156, 'action': [1.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 145.37942431803242, 'action': [1.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 167.76574009604963, 'action': [1.0, -1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 121.3955781123874, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15849595430747263, 0.1313660161827701, 0.19466920514040933, 0.17229890528319847, 0.19324131366016184, 0.14945264159923846]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 7.982010126113892 s
