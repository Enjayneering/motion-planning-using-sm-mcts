Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 227, 'sum_payoffs': 82.10234230497788, 'action': [0.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 89.13149013216733, 'action': [2.0, 1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 97.552704561688, 'action': [1.0, 1.5707963267948966]}, {'num_count': 206, 'sum_payoffs': 71.8667105725279, 'action': [1.0, 0.0]}, {'num_count': 256, 'sum_payoffs': 96.59091526680584, 'action': [2.0, -1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 95.09224687598366, 'action': [1.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 82.23593635700641, 'action': [0.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 86.14525768769873, 'action': [2.0, 0.0]}, {'num_count': 197, 'sum_payoffs': 67.50989211844524, 'action': [0.0, 0.0]}])
Weights num count: [0.10804378867206092, 0.11470728224654926, 0.12279866730128511, 0.09804854831032842, 0.12184673964778676, 0.12041884816753927, 0.10804378867206092, 0.11185149928605426, 0.09376487386958592]
Actions to choose Agent 1: dict_values([{'num_count': 400, 'sum_payoffs': 164.78674246147074, 'action': [1.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 142.43974689963375, 'action': [1.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 112.96317490735669, 'action': [0.0, 0.0]}, {'num_count': 315, 'sum_payoffs': 121.93384984632151, 'action': [0.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 168.33450990867624, 'action': [1.0, 1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 126.85841376490858, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.19038553069966682, 0.16944312232270348, 0.14136125654450263, 0.14992860542598763, 0.193717277486911, 0.1546882436934793]
Selected final action: [1.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 8.098019123077393 s
