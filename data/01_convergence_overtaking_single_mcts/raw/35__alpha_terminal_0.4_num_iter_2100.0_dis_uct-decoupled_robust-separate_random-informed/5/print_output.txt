Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 229, 'sum_payoffs': 82.95879491267752, 'action': [0.0, -1.5707963267948966]}, {'num_count': 214, 'sum_payoffs': 75.61266335285019, 'action': [1.0, 0.0]}, {'num_count': 263, 'sum_payoffs': 99.70191160653448, 'action': [2.0, -1.5707963267948966]}, {'num_count': 200, 'sum_payoffs': 68.79415499261287, 'action': [0.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 82.88858038996675, 'action': [0.0, 1.5707963267948966]}, {'num_count': 217, 'sum_payoffs': 76.94699318354792, 'action': [2.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 92.1875726911106, 'action': [2.0, 1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 92.91120502810773, 'action': [1.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 93.71035037700561, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10899571632555925, 0.10185625892432175, 0.12517848643503093, 0.09519276534983341, 0.10899571632555925, 0.10328415040456926, 0.11803902903379343, 0.1185149928605426, 0.11946692051404094]
Actions to choose Agent 1: dict_values([{'num_count': 397, 'sum_payoffs': 162.6073271943728, 'action': [1.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 113.06836974195186, 'action': [0.0, -1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 120.79625610174038, 'action': [0.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 150.9136595149393, 'action': [1.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 118.55618773762956, 'action': [0.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 168.06380661494245, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18895763921941933, 0.1418372203712518, 0.14945264159923846, 0.17801047120418848, 0.14707282246549264, 0.19419324131366017]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 7.904119253158569 s
