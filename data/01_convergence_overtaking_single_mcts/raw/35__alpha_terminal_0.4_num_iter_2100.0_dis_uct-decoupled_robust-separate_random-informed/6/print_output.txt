Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 211, 'sum_payoffs': 74.8388142244234, 'action': [1.0, 0.0]}, {'num_count': 259, 'sum_payoffs': 98.62029630605275, 'action': [1.0, -1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 77.24605794364284, 'action': [0.0, 1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 85.1046366029988, 'action': [0.0, -1.5707963267948966]}, {'num_count': 209, 'sum_payoffs': 73.8834486461208, 'action': [0.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 92.23959383693997, 'action': [2.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 96.20432129520707, 'action': [2.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 81.75482625729657, 'action': [2.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 93.25237315533964, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10042836744407425, 0.12327463112803427, 0.10280818657782008, 0.11042360780580676, 0.09947643979057591, 0.1170871013802951, 0.12089481199428843, 0.10709186101856259, 0.11803902903379343]
Actions to choose Agent 1: dict_values([{'num_count': 385, 'sum_payoffs': 155.99391081913063, 'action': [1.0, -1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 122.89333599602298, 'action': [0.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 150.43860454971554, 'action': [1.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 121.4467623499557, 'action': [0.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 120.39669108016146, 'action': [0.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 159.58509404471482, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18324607329842932, 0.1518324607329843, 0.17801047120418848, 0.1504045692527368, 0.14945264159923846, 0.1865778200856735]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 7.895990610122681 s
