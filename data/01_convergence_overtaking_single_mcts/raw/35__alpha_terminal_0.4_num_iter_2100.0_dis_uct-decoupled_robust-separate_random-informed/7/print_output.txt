Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 204, 'sum_payoffs': 71.07623207030092, 'action': [1.0, 0.0]}, {'num_count': 202, 'sum_payoffs': 70.02124711454708, 'action': [0.0, 0.0]}, {'num_count': 263, 'sum_payoffs': 100.2537251329445, 'action': [1.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 80.82667945173738, 'action': [2.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 85.18140014379348, 'action': [0.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 91.21352014455024, 'action': [2.0, 1.5707963267948966]}, {'num_count': 213, 'sum_payoffs': 75.46966935442747, 'action': [0.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 93.7209208453377, 'action': [1.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 101.65707263744682, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.09709662065683008, 0.09614469300333174, 0.12517848643503093, 0.10661589719181343, 0.11089957163255593, 0.11661113755354593, 0.10138029509757258, 0.11899095668729176, 0.12660637791527843]
Actions to choose Agent 1: dict_values([{'num_count': 426, 'sum_payoffs': 177.19076132184057, 'action': [1.0, 1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 122.14373131399637, 'action': [0.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 143.2231757257374, 'action': [1.0, -1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 149.78610134404747, 'action': [1.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 118.18458575711657, 'action': [0.0, 0.0]}, {'num_count': 317, 'sum_payoffs': 122.08131619164887, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.20276059019514517, 0.15088053307948596, 0.17087101380295097, 0.17705854355069015, 0.14707282246549264, 0.15088053307948596]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 7.950954914093018 s
