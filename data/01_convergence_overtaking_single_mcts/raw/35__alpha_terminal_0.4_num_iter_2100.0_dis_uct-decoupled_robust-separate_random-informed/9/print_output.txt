Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 251, 'sum_payoffs': 94.08174657224508, 'action': [2.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 94.05417547500971, 'action': [1.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 81.21127247535743, 'action': [0.0, 1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 84.65457075512123, 'action': [0.0, -1.5707963267948966]}, {'num_count': 203, 'sum_payoffs': 70.36675456037173, 'action': [0.0, 0.0]}, {'num_count': 200, 'sum_payoffs': 68.9995008400854, 'action': [2.0, 0.0]}, {'num_count': 204, 'sum_payoffs': 70.93201280819282, 'action': [1.0, 0.0]}, {'num_count': 257, 'sum_payoffs': 97.04693335628076, 'action': [1.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 106.9755150259761, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11946692051404094, 0.11946692051404094, 0.10709186101856259, 0.11042360780580676, 0.09662065683008092, 0.09519276534983341, 0.09709662065683008, 0.12232270347453593, 0.13184198000951927]
Actions to choose Agent 1: dict_values([{'num_count': 316, 'sum_payoffs': 122.02518033321331, 'action': [0.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 156.5866038044855, 'action': [1.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 161.61329200272732, 'action': [1.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 126.97476775211238, 'action': [0.0, 1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 115.42599831568637, 'action': [0.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 151.3726685912347, 'action': [1.0, 0.0]}])
Weights num count: [0.1504045692527368, 0.18324607329842932, 0.188005711565921, 0.15516420752022847, 0.14421703950499762, 0.17848643503093764]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 7.95864200592041 s
