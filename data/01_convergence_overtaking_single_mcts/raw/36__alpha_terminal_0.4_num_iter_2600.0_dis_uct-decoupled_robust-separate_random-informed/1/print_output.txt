Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 107.17359288972352, 'action': [0.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 111.95628250691877, 'action': [2.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 88.20861940642635, 'action': [1.0, 0.0]}, {'num_count': 268, 'sum_payoffs': 94.73177003152449, 'action': [0.0, 1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 90.98397730546205, 'action': [0.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 109.05808081193392, 'action': [1.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 123.50545509148327, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 101.94975318964684, 'action': [2.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 115.33195629985843, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11303344867358708, 0.11687812379853903, 0.09765474817377931, 0.10303729334871203, 0.09996155324875049, 0.11457131872356786, 0.1261053440984237, 0.10880430603613994, 0.11956939638600539]
Actions to choose Agent 1: dict_values([{'num_count': 391, 'sum_payoffs': 150.66856921179703, 'action': [0.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 148.37524627303173, 'action': [0.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 184.98186163046267, 'action': [1.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 154.13500468982468, 'action': [0.0, 1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 191.00562851575307, 'action': [1.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 199.95577522269974, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1503267973856209, 0.14840445982314496, 0.1772395232602845, 0.15301806997308728, 0.18185313341022682, 0.18877354863514034]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.730971574783325 s
