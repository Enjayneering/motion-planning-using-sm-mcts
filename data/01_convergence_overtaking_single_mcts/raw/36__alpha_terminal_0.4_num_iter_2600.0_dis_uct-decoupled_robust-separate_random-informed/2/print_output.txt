Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 301, 'sum_payoffs': 109.5370477436588, 'action': [2.0, 1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 90.17410279183606, 'action': [1.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 119.20053995101216, 'action': [2.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 107.66063560381053, 'action': [0.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 103.38045663284909, 'action': [2.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 96.27252959684932, 'action': [0.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 109.19106417293779, 'action': [1.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 116.26725893314224, 'action': [1.0, 1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 83.20466635678645, 'action': [0.0, 0.0]}])
Weights num count: [0.11572472126105345, 0.09996155324875049, 0.12341407151095732, 0.11418685121107267, 0.11072664359861592, 0.104959630911188, 0.11534025374855825, 0.12110726643598616, 0.09419454056132257]
Actions to choose Agent 1: dict_values([{'num_count': 472, 'sum_payoffs': 189.04723512140652, 'action': [1.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 137.10674071407385, 'action': [0.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 156.83657103084153, 'action': [0.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 189.13421036129358, 'action': [1.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 194.0519607439981, 'action': [1.0, -1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 155.46021648472188, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18146866589773164, 0.14033064206074586, 0.15609381007304882, 0.18146866589773164, 0.18531334102268357, 0.15494040753556323]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.637738227844238 s
