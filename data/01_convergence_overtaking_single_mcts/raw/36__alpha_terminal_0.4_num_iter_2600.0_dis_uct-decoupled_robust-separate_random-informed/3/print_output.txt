Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 244, 'sum_payoffs': 83.65324443705308, 'action': [1.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 106.99803040208897, 'action': [0.0, -1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 113.86303703227115, 'action': [2.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 102.68610194486068, 'action': [2.0, 0.0]}, {'num_count': 262, 'sum_payoffs': 92.13031354648139, 'action': [0.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 117.0809273612553, 'action': [2.0, -1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 113.63216149709667, 'action': [1.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 97.85021491175974, 'action': [0.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 117.49805646777382, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.09381007304882738, 0.11264898116109189, 0.11803152633602461, 0.10918877354863514, 0.10073048827374087, 0.12072279892349097, 0.11803152633602461, 0.1053440984236832, 0.12110726643598616]
Actions to choose Agent 1: dict_values([{'num_count': 485, 'sum_payoffs': 195.19133119723418, 'action': [1.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 191.65472700381764, 'action': [1.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 148.2711434175494, 'action': [0.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 150.72350031219563, 'action': [0.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 178.0225992892924, 'action': [1.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 155.63328318765818, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18646674356016918, 0.1837754709727028, 0.14955786236063054, 0.1514801999231065, 0.17301038062283736, 0.15532487504805845]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 9.706287622451782 s
