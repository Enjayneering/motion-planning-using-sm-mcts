Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 304, 'sum_payoffs': 111.91636503819535, 'action': [1.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 106.65417820783013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 121.82409892334853, 'action': [2.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 107.10073106298374, 'action': [0.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 89.50317596455582, 'action': [0.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 108.82835181403638, 'action': [2.0, 1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 87.56918816596668, 'action': [1.0, 0.0]}, {'num_count': 295, 'sum_payoffs': 107.39015852706129, 'action': [1.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 100.81657342484755, 'action': [2.0, 0.0]}])
Weights num count: [0.11687812379853903, 0.11264898116109189, 0.1249519415609381, 0.11303344867358708, 0.09880815071126489, 0.11457131872356786, 0.09727028066128413, 0.11341791618608228, 0.10803537101114956]
Actions to choose Agent 1: dict_values([{'num_count': 483, 'sum_payoffs': 193.74716150167762, 'action': [1.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 158.1483509563498, 'action': [0.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 186.3095913998862, 'action': [1.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 146.6495265068435, 'action': [0.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 153.36260766423524, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 178.9416029399527, 'action': [1.0, 0.0]}])
Weights num count: [0.18569780853517878, 0.1576316801230296, 0.17993079584775087, 0.14840445982314496, 0.15378700499807765, 0.17416378316032297]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.643606424331665 s
