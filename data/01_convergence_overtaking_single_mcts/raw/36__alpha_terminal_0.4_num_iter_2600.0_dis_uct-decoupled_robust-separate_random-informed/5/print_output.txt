Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 279, 'sum_payoffs': 100.20899525889214, 'action': [2.0, 0.0]}, {'num_count': 318, 'sum_payoffs': 119.19262681986302, 'action': [1.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 99.86631746820738, 'action': [0.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 110.36396001559663, 'action': [1.0, 1.5707963267948966]}, {'num_count': 262, 'sum_payoffs': 92.2827416185075, 'action': [1.0, 0.0]}, {'num_count': 308, 'sum_payoffs': 114.28572035796769, 'action': [2.0, 1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 92.6432564345913, 'action': [0.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 105.55391204295259, 'action': [0.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 111.30251075714261, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10726643598615918, 0.12226066897347174, 0.10688196847366398, 0.11534025374855825, 0.10073048827374087, 0.1184159938485198, 0.10111495578623607, 0.1114955786236063, 0.11610918877354863]
Actions to choose Agent 1: dict_values([{'num_count': 405, 'sum_payoffs': 156.93719388511195, 'action': [0.0, 1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 135.2687662943669, 'action': [0.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 176.5762065291964, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 198.36251157574267, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 160.02995462904408, 'action': [0.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 198.71247474420716, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15570934256055363, 0.1384083044982699, 0.1710880430603614, 0.18800461361014995, 0.1580161476355248, 0.18838908112264513]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 9.619126796722412 s
