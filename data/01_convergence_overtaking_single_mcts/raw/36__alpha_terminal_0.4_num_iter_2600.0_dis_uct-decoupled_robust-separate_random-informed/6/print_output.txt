Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 288, 'sum_payoffs': 103.87006294620869, 'action': [0.0, -1.5707963267948966]}, {'num_count': 262, 'sum_payoffs': 91.66909846874732, 'action': [1.0, 0.0]}, {'num_count': 318, 'sum_payoffs': 118.36318060902721, 'action': [1.0, -1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 116.45464439490753, 'action': [2.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 120.34657381250621, 'action': [2.0, 1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 114.83244637961361, 'action': [1.0, 1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 92.5730854485641, 'action': [2.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 100.5149161769127, 'action': [0.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 81.27346379696529, 'action': [0.0, 0.0]}])
Weights num count: [0.11072664359861592, 0.10073048827374087, 0.12226066897347174, 0.12072279892349097, 0.12379853902345252, 0.11956939638600539, 0.10149942329873125, 0.10803537101114956, 0.0922722029988466]
Actions to choose Agent 1: dict_values([{'num_count': 476, 'sum_payoffs': 191.95439073709974, 'action': [1.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 196.43284991402965, 'action': [1.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 187.50942377474837, 'action': [1.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 147.4800036204472, 'action': [0.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 159.41897112306572, 'action': [0.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 143.65824756840368, 'action': [0.0, 0.0]}])
Weights num count: [0.1830065359477124, 0.18646674356016918, 0.17954632833525566, 0.14801999231064975, 0.1576316801230296, 0.14494425221068818]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.557236194610596 s
