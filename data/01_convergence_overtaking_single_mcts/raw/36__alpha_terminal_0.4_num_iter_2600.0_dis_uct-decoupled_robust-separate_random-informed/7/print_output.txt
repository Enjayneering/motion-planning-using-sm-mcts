Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 102.39876206455322, 'action': [0.0, -1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 115.6054199692707, 'action': [2.0, -1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 112.79641723784859, 'action': [2.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 113.20306075320565, 'action': [1.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 106.61761430311725, 'action': [0.0, 1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 111.35026731900157, 'action': [1.0, -1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 86.21554961804145, 'action': [0.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 98.57879656176522, 'action': [2.0, 0.0]}, {'num_count': 269, 'sum_payoffs': 95.23539671446137, 'action': [1.0, 0.0]}])
Weights num count: [0.10918877354863514, 0.11995386389850057, 0.11764705882352941, 0.11803152633602461, 0.11264898116109189, 0.11649365628604383, 0.09611687812379854, 0.1061130334486736, 0.10342176086120723]
Actions to choose Agent 1: dict_values([{'num_count': 481, 'sum_payoffs': 192.8389705012096, 'action': [1.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 147.15505882581908, 'action': [0.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 149.96697989725007, 'action': [0.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 138.50241346133714, 'action': [0.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 185.59845131471073, 'action': [1.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 204.2971507528276, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18492887351018839, 0.14878892733564014, 0.1510957324106113, 0.14186851211072665, 0.17916186082276048, 0.19377162629757785]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.591891288757324 s
