Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 245, 'sum_payoffs': 83.96689218079841, 'action': [1.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 114.37131017389477, 'action': [2.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 102.29805627380438, 'action': [0.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 113.64022080180928, 'action': [1.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 88.23536366552345, 'action': [0.0, 0.0]}, {'num_count': 322, 'sum_payoffs': 120.81454069064178, 'action': [2.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 98.69567174912143, 'action': [2.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 103.98481191979033, 'action': [0.0, -1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 117.91436690104159, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.09419454056132257, 0.11880046136101499, 0.10918877354863514, 0.11803152633602461, 0.09765474817377931, 0.12379853902345252, 0.1061130334486736, 0.11034217608612072, 0.12149173394848135]
Actions to choose Agent 1: dict_values([{'num_count': 478, 'sum_payoffs': 192.06339010529078, 'action': [1.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 196.99576448243823, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 162.87794008183738, 'action': [0.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 182.4214383943335, 'action': [1.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 141.07077970777758, 'action': [0.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 146.8198788976838, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1837754709727028, 0.18762014609765476, 0.16070742022299117, 0.17608612072279892, 0.14340638216070742, 0.14801999231064975]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.778486728668213 s
