Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 283, 'sum_payoffs': 102.14651556727532, 'action': [0.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 107.37986048540677, 'action': [2.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 103.62496268309346, 'action': [0.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 109.21400876814886, 'action': [1.0, 1.5707963267948966]}, {'num_count': 258, 'sum_payoffs': 90.23770747637869, 'action': [1.0, 0.0]}, {'num_count': 317, 'sum_payoffs': 118.33238337280534, 'action': [2.0, -1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 86.94024083248941, 'action': [0.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 116.16747976081868, 'action': [2.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 110.67295617468442, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10880430603613994, 0.11303344867358708, 0.10995770857362552, 0.11457131872356786, 0.09919261822376009, 0.12187620146097655, 0.09650134563629373, 0.11995386389850057, 0.11572472126105345]
Actions to choose Agent 1: dict_values([{'num_count': 487, 'sum_payoffs': 196.17881828619292, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 179.4311267921345, 'action': [1.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 130.20917178291467, 'action': [0.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 147.03894185778842, 'action': [0.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 165.47610228463338, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 202.13785233053213, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18723567858515955, 0.17416378316032297, 0.13494809688581316, 0.14840445982314496, 0.16301422529796233, 0.19184928873510187]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.727623224258423 s
