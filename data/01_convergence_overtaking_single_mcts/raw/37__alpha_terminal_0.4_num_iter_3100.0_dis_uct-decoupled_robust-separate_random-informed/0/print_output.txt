Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 369, 'sum_payoffs': 133.43558234642205, 'action': [1.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 105.392932941844, 'action': [0.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 128.28408444509446, 'action': [2.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 120.5980400842144, 'action': [1.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 119.50456116496798, 'action': [0.0, -1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 113.07320199253272, 'action': [1.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 120.14116362896809, 'action': [2.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 124.75728862005653, 'action': [0.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 133.82408206320127, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11899387294421154, 0.09932279909706546, 0.11544663011931634, 0.10996452757175104, 0.10931957433086101, 0.10480490164463076, 0.10964205095130602, 0.11286681715575621, 0.11931634956465656]
Actions to choose Agent 1: dict_values([{'num_count': 540, 'sum_payoffs': 209.0739406732195, 'action': [1.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 166.89139044886946, 'action': [0.0, 0.0]}, {'num_count': 487, 'sum_payoffs': 184.065940178895, 'action': [0.0, 1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 218.09715880038218, 'action': [1.0, -1.5707963267948966]}, {'num_count': 556, 'sum_payoffs': 216.82388565417634, 'action': [1.0, 1.5707963267948966]}, {'num_count': 508, 'sum_payoffs': 193.9527826693891, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17413737504030957, 0.14511447920025797, 0.15704611415672365, 0.18026443082876492, 0.17929700096742987, 0.163818123186069]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 11.243619918823242 s
