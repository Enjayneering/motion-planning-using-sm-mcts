Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 335, 'sum_payoffs': 116.84015729947983, 'action': [0.0, -1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 113.42597456649769, 'action': [1.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 130.61322138125314, 'action': [2.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 121.65913890697988, 'action': [1.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 114.85551667870043, 'action': [2.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 136.979377921935, 'action': [2.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 113.94186355025938, 'action': [0.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 127.36180991064133, 'action': [1.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 114.04797885150953, 'action': [0.0, 0.0]}])
Weights num count: [0.10802966784908094, 0.10577233150596582, 0.11770396646243148, 0.11157691067397614, 0.10673976136730087, 0.12221863914866173, 0.10609480812641084, 0.11544663011931634, 0.10609480812641084]
Actions to choose Agent 1: dict_values([{'num_count': 577, 'sum_payoffs': 225.8247610888374, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 172.97846663265827, 'action': [0.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 202.994797906432, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 182.45756572005607, 'action': [0.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 177.1052635963614, 'action': [0.0, 1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 222.34090964012532, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18606900999677523, 0.14995162850693325, 0.1705901322154144, 0.15640116091583361, 0.1528539180909384, 0.1838116736536601]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 11.190566301345825 s
