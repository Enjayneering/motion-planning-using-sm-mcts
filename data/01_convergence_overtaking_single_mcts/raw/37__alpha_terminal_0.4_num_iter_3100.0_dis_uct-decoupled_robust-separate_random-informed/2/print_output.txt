Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 367, 'sum_payoffs': 132.58537066510306, 'action': [1.0, 1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 109.54910887190344, 'action': [1.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 121.95018555573041, 'action': [0.0, -1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 114.47569274276573, 'action': [2.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 131.63394085387523, 'action': [2.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 126.96740511407138, 'action': [2.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 113.5701692225334, 'action': [0.0, 0.0]}, {'num_count': 364, 'sum_payoffs': 131.18059474843892, 'action': [1.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 117.24276215462139, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11834891970332151, 0.10222508868107062, 0.1109319574330861, 0.10577233150596582, 0.11770396646243148, 0.11447920025798129, 0.10512737826507579, 0.11738148984198646, 0.10770719122863592]
Actions to choose Agent 1: dict_values([{'num_count': 493, 'sum_payoffs': 186.84576782450273, 'action': [0.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 184.15870841499788, 'action': [0.0, -1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 205.34614893740294, 'action': [1.0, 0.0]}, {'num_count': 557, 'sum_payoffs': 216.9465000111454, 'action': [1.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 173.34508561204163, 'action': [0.0, 0.0]}, {'num_count': 567, 'sum_payoffs': 221.78790885862333, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15898097387939375, 0.15704611415672365, 0.17155756207674944, 0.1796194775878749, 0.14962915188648823, 0.18284424379232506]
Selected final action: [1.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 11.164241552352905 s
