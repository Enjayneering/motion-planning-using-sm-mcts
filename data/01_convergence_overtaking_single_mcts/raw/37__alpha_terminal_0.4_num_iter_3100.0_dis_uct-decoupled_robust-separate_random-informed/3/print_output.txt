Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 315, 'sum_payoffs': 108.34183257023582, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 118.83210969718061, 'action': [0.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 110.49375971329378, 'action': [0.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 120.65518472826729, 'action': [2.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 128.56121980362664, 'action': [2.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 131.34021153005568, 'action': [1.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 132.12243497545072, 'action': [1.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 109.63337496263458, 'action': [1.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 136.42427674135692, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10158013544018059, 0.10899709771041599, 0.10319251854240567, 0.11028700419219607, 0.11576910673976137, 0.11770396646243148, 0.11834891970332151, 0.10254756530151564, 0.12125120928732668]
Actions to choose Agent 1: dict_values([{'num_count': 532, 'sum_payoffs': 205.80365732063166, 'action': [1.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 186.39621036096827, 'action': [0.0, 1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 213.40743599052942, 'action': [1.0, -1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 180.76339408573514, 'action': [0.0, -1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 219.85029829108996, 'action': [1.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 185.02468645417943, 'action': [0.0, 0.0]}])
Weights num count: [0.17155756207674944, 0.15833602063850372, 0.1767171880038697, 0.15446630119316349, 0.18123186069009997, 0.15736859077716867]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 11.321112632751465 s
