Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 362, 'sum_payoffs': 129.59303848795707, 'action': [1.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 127.65547859745591, 'action': [2.0, -1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 107.38086542711312, 'action': [0.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 127.28663552823842, 'action': [2.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 126.5845962696528, 'action': [1.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 121.60748751540574, 'action': [0.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 121.20650916786067, 'action': [0.0, -1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 106.16214450954537, 'action': [1.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 125.43582726046814, 'action': [2.0, 0.0]}])
Weights num count: [0.11673653660109642, 0.11544663011931634, 0.10125765881973557, 0.11512415349887133, 0.11480167687842631, 0.11125443405353112, 0.1109319574330861, 0.10029022895840052, 0.11383424701709126]
Actions to choose Agent 1: dict_values([{'num_count': 537, 'sum_payoffs': 207.0513580479515, 'action': [1.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 189.27152707270383, 'action': [0.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 209.0141515440056, 'action': [1.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 193.91883406577136, 'action': [0.0, -1.5707963267948966]}, {'num_count': 558, 'sum_payoffs': 217.10174916954006, 'action': [1.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 169.08495157164106, 'action': [0.0, 0.0]}])
Weights num count: [0.17316994517897452, 0.16091583360206385, 0.1744598516607546, 0.16414059980651402, 0.1799419542083199, 0.1470493389229281]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 11.251058101654053 s
