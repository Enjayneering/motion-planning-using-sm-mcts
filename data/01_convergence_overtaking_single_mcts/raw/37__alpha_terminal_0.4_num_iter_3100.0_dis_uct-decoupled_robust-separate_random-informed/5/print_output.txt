Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 343, 'sum_payoffs': 120.22882206158931, 'action': [0.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 130.36446436910344, 'action': [1.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 128.55035845355837, 'action': [1.0, -1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 107.10616375138704, 'action': [1.0, 0.0]}, {'num_count': 352, 'sum_payoffs': 124.39242448843822, 'action': [2.0, 0.0]}, {'num_count': 347, 'sum_payoffs': 122.18146233532015, 'action': [0.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 125.31526888695896, 'action': [2.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 127.76393098751254, 'action': [2.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 102.89938820357241, 'action': [0.0, 0.0]}])
Weights num count: [0.11060948081264109, 0.11770396646243148, 0.11641405998065141, 0.10125765881973557, 0.11351177039664624, 0.11189938729442116, 0.11415672363753628, 0.11576910673976137, 0.0983553692357304]
Actions to choose Agent 1: dict_values([{'num_count': 502, 'sum_payoffs': 190.73689281283086, 'action': [0.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 185.69119172249344, 'action': [0.0, 1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 212.44025263724856, 'action': [1.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 181.80229375301784, 'action': [0.0, 0.0]}, {'num_count': 538, 'sum_payoffs': 207.84342874799032, 'action': [1.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 207.68941254211092, 'action': [1.0, 0.0]}])
Weights num count: [0.1618832634633989, 0.15833602063850372, 0.1767171880038697, 0.15575620767494355, 0.17349242179941954, 0.17349242179941954]
Selected final action: [1.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 11.121617555618286 s
