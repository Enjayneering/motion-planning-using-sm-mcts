Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 367, 'sum_payoffs': 131.36425403584963, 'action': [2.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 121.46383782388351, 'action': [0.0, -1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 113.3103019407298, 'action': [0.0, 1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 109.8190492229642, 'action': [1.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 127.29322262784474, 'action': [1.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 129.86701603434605, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 108.46500313274277, 'action': [0.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 120.18823384978302, 'action': [2.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 126.70084022387134, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11834891970332151, 0.11157691067397614, 0.10577233150596582, 0.10319251854240567, 0.11544663011931634, 0.11738148984198646, 0.10222508868107062, 0.11060948081264109, 0.11512415349887133]
Actions to choose Agent 1: dict_values([{'num_count': 471, 'sum_payoffs': 175.39614943961232, 'action': [0.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 184.1808694889583, 'action': [0.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 214.753957744114, 'action': [1.0, 1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 210.4016262096016, 'action': [1.0, 0.0]}, {'num_count': 556, 'sum_payoffs': 215.2887083931141, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 180.52153306988987, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15188648822960335, 0.1580135440180587, 0.17897452434698485, 0.17607223476297967, 0.17929700096742987, 0.15543373105449854]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 11.062462568283081 s
