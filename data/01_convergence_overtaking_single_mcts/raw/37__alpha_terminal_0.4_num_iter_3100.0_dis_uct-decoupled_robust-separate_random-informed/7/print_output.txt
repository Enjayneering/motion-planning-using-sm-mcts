Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 325, 'sum_payoffs': 112.37514906866004, 'action': [1.0, 0.0]}, {'num_count': 318, 'sum_payoffs': 108.99452599892462, 'action': [0.0, 0.0]}, {'num_count': 334, 'sum_payoffs': 116.33194801710904, 'action': [0.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 116.95833797702163, 'action': [0.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 130.32582679515866, 'action': [2.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 126.66544717053827, 'action': [1.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 136.40569083494196, 'action': [2.0, -1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 120.48146577452745, 'action': [2.0, 0.0]}, {'num_count': 348, 'sum_payoffs': 122.9595957840795, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10480490164463076, 0.10254756530151564, 0.10770719122863592, 0.10802966784908094, 0.11738148984198646, 0.11480167687842631, 0.12157368590777169, 0.11060948081264109, 0.11222186391486617]
Actions to choose Agent 1: dict_values([{'num_count': 562, 'sum_payoffs': 219.83369619662366, 'action': [1.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 177.1728532241629, 'action': [0.0, 0.0]}, {'num_count': 545, 'sum_payoffs': 211.719342594911, 'action': [1.0, 0.0]}, {'num_count': 560, 'sum_payoffs': 218.90751302731138, 'action': [1.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 186.70473262857706, 'action': [0.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 175.87744745524054, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18123186069009997, 0.15220896485004837, 0.17574975814253466, 0.18058690744920994, 0.15865849725894873, 0.15124153498871332]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 11.123182535171509 s
