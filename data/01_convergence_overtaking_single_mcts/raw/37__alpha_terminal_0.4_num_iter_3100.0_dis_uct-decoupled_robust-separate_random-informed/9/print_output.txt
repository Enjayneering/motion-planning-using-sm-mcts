Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 371, 'sum_payoffs': 133.90179536461469, 'action': [1.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 110.13174204365525, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 133.42672655803412, 'action': [2.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 127.24183223859222, 'action': [2.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 125.56175076658792, 'action': [0.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 126.45599019861432, 'action': [1.0, 1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 108.564613026278, 'action': [1.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 115.74573989685183, 'action': [2.0, 0.0]}, {'num_count': 326, 'sum_payoffs': 112.98297626435112, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11963882618510158, 0.10319251854240567, 0.11931634956465656, 0.11512415349887133, 0.11383424701709126, 0.11447920025798129, 0.1019026120606256, 0.10706223798774589, 0.10512737826507579]
Actions to choose Agent 1: dict_values([{'num_count': 498, 'sum_payoffs': 188.25693680929797, 'action': [0.0, -1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 205.50314880667423, 'action': [1.0, 0.0]}, {'num_count': 550, 'sum_payoffs': 212.5124481158188, 'action': [1.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 169.43781012125285, 'action': [0.0, 0.0]}, {'num_count': 504, 'sum_payoffs': 190.99536244081943, 'action': [0.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 215.04237499415146, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16059335698161883, 0.1725249919380845, 0.17736214124475974, 0.14769429216381813, 0.16252821670428894, 0.17897452434698485]
Selected final action: [1.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 11.076117753982544 s
