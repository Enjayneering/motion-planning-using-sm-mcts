Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 381, 'sum_payoffs': 126.71441610583148, 'action': [1.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 143.04358464750635, 'action': [2.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 121.46644667997917, 'action': [0.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 142.1352441053684, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 129.18945948592062, 'action': [0.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 139.02463919594084, 'action': [2.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 146.83155738293428, 'action': [2.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 126.1567801079276, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 140.8513693301122, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1058039433490697, 0.11607886698139405, 0.10247153568453207, 0.11552346570397112, 0.10747014718133852, 0.11357956123299083, 0.11857817272979727, 0.10552624271035824, 0.11469036378783672]
Actions to choose Agent 1: dict_values([{'num_count': 575, 'sum_payoffs': 208.69317049239572, 'action': [0.0, 1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 235.82676880958658, 'action': [1.0, -1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 196.81010505128717, 'action': [0.0, 0.0]}, {'num_count': 568, 'sum_payoffs': 205.6645461054006, 'action': [0.0, -1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 241.74210146639672, 'action': [1.0, 1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 231.8348994729299, 'action': [1.0, 0.0]}])
Weights num count: [0.1596778672590947, 0.17633990558178284, 0.15217995001388504, 0.1577339627881144, 0.17995001388503193, 0.17384059983337963]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.144181251525879 s
