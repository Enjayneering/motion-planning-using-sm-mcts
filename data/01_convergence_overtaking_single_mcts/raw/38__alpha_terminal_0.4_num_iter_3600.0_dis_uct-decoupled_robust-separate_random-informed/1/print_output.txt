Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 382, 'sum_payoffs': 126.61349552295829, 'action': [0.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 141.6988253953129, 'action': [2.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 143.6663713185759, 'action': [1.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 138.8172913744387, 'action': [2.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 132.64917589854582, 'action': [0.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 125.241515373746, 'action': [0.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 146.27966835337142, 'action': [2.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 119.99542606120264, 'action': [1.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 134.6512220899402, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10608164398778117, 0.11580116634268259, 0.11691196889752846, 0.11385726187170231, 0.10996945292974174, 0.10524854207164676, 0.11857817272979727, 0.10191613440710913, 0.11135795612329909]
Actions to choose Agent 1: dict_values([{'num_count': 551, 'sum_payoffs': 198.01071128065735, 'action': [0.0, 0.0]}, {'num_count': 631, 'sum_payoffs': 233.8576948040241, 'action': [1.0, 0.0]}, {'num_count': 568, 'sum_payoffs': 205.38354876179875, 'action': [0.0, -1.5707963267948966]}, {'num_count': 646, 'sum_payoffs': 240.5460295387041, 'action': [1.0, 1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 240.18844254629337, 'action': [1.0, -1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 201.56761748910824, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15301305193001943, 0.17522910302693695, 0.1577339627881144, 0.179394612607609, 0.17911691196889754, 0.1552346570397112]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.147267818450928 s
