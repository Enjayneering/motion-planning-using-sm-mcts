Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 414, 'sum_payoffs': 140.93737452403272, 'action': [2.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 142.4093203700327, 'action': [1.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 131.43068804558575, 'action': [0.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 119.25857346241824, 'action': [0.0, 0.0]}, {'num_count': 430, 'sum_payoffs': 147.98578397982408, 'action': [2.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 129.0691616389282, 'action': [0.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 136.21528165009684, 'action': [2.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 126.10501853624918, 'action': [1.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 140.48039737411062, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11496806442654818, 0.11580116634268259, 0.10885865037489587, 0.10108303249097472, 0.11941127464593168, 0.10747014718133852, 0.11191335740072202, 0.10552624271035824, 0.11469036378783672]
Actions to choose Agent 1: dict_values([{'num_count': 548, 'sum_payoffs': 196.02488593193686, 'action': [0.0, 0.0]}, {'num_count': 577, 'sum_payoffs': 209.0599304821508, 'action': [0.0, 1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 240.49499085177632, 'action': [1.0, -1.5707963267948966]}, {'num_count': 557, 'sum_payoffs': 200.198603354737, 'action': [0.0, -1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 234.2236492298201, 'action': [1.0, 1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 236.4229182513363, 'action': [1.0, 0.0]}])
Weights num count: [0.15217995001388504, 0.16023326853651762, 0.17967231324632046, 0.15467925576228825, 0.1757845043043599, 0.17717300749791726]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.228545188903809 s
