Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 409, 'sum_payoffs': 138.6230013116374, 'action': [1.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 145.29591454239952, 'action': [2.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 125.61710606900371, 'action': [1.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 128.13687035149883, 'action': [0.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 126.83002168782403, 'action': [0.0, -1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 146.19138202076667, 'action': [2.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 140.03702950053628, 'action': [1.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 126.27131992737323, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 135.68948217536195, 'action': [2.0, 0.0]}])
Weights num count: [0.11357956123299083, 0.11774507081366287, 0.10524854207164676, 0.10691474590391557, 0.10608164398778117, 0.11830047209108581, 0.11441266314912524, 0.1058039433490697, 0.11163565676201055]
Actions to choose Agent 1: dict_values([{'num_count': 647, 'sum_payoffs': 241.28303625690003, 'action': [1.0, -1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 238.0741061827169, 'action': [1.0, 0.0]}, {'num_count': 557, 'sum_payoffs': 200.82520927918813, 'action': [0.0, 1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 199.42210490815586, 'action': [0.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 204.27763159990766, 'action': [0.0, -1.5707963267948966]}, {'num_count': 637, 'sum_payoffs': 236.7132160914962, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17967231324632046, 0.17772840877534019, 0.15467925576228825, 0.15384615384615385, 0.15690086087198002, 0.17689530685920576]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.089778900146484 s
