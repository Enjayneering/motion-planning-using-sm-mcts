Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 430, 'sum_payoffs': 146.94597622159966, 'action': [2.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 133.31482104271998, 'action': [2.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 143.83164682325713, 'action': [1.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 116.9802898601293, 'action': [0.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 139.50621044450153, 'action': [2.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 132.59449924657935, 'action': [0.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 136.8214321736808, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 128.30432988683063, 'action': [1.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 126.37116559549445, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11941127464593168, 0.11080255484587614, 0.1174673701749514, 0.10024993057484032, 0.11469036378783672, 0.1102471535684532, 0.1130241599555679, 0.10747014718133852, 0.10635934462649264]
Actions to choose Agent 1: dict_values([{'num_count': 645, 'sum_payoffs': 242.2166044736253, 'action': [1.0, 1.5707963267948966]}, {'num_count': 567, 'sum_payoffs': 206.7625547495923, 'action': [0.0, 1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 209.55124074644723, 'action': [0.0, -1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 238.01931960791532, 'action': [1.0, 0.0]}, {'num_count': 554, 'sum_payoffs': 200.92005883688824, 'action': [0.0, 0.0]}, {'num_count': 625, 'sum_payoffs': 232.9540543268759, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17911691196889754, 0.15745626214940295, 0.15912246598167176, 0.1766176062204943, 0.15384615384615385, 0.17356289919466814]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.251932621002197 s
