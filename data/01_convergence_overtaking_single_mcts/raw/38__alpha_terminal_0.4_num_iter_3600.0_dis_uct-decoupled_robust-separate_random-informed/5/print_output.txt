Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 376, 'sum_payoffs': 123.51762914551902, 'action': [0.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 143.34669213152384, 'action': [2.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 144.96371886361143, 'action': [2.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 143.6307001988127, 'action': [1.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 143.28643724001012, 'action': [1.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 127.90389207267278, 'action': [2.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 124.9937823800242, 'action': [0.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 124.04116615566758, 'action': [1.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 131.04546573027608, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10441544015551235, 0.11691196889752846, 0.11802277145237434, 0.11718966953623994, 0.11691196889752846, 0.10719244654262705, 0.10524854207164676, 0.10469314079422383, 0.10913635101360733]
Actions to choose Agent 1: dict_values([{'num_count': 642, 'sum_payoffs': 239.5734757445188, 'action': [1.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 194.24321013127295, 'action': [0.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 206.50529438344412, 'action': [0.0, 1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 207.47255781711465, 'action': [0.0, -1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 235.40091375720831, 'action': [1.0, 0.0]}, {'num_count': 644, 'sum_payoffs': 240.55642325896173, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17828381005276311, 0.15023604554290476, 0.15801166342682588, 0.15856706470424883, 0.1757845043043599, 0.17883921133018607]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.146888017654419 s
