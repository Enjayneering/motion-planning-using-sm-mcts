Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 391, 'sum_payoffs': 130.93079518936932, 'action': [0.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 140.1309573457342, 'action': [2.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 143.10236652866712, 'action': [1.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 123.50552517568742, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 123.95618534658598, 'action': [0.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 142.11546991806202, 'action': [2.0, -1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 141.26798822124195, 'action': [1.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 122.51823252007159, 'action': [1.0, 0.0]}, {'num_count': 425, 'sum_payoffs': 145.67260234324505, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10858094973618439, 0.11441266314912524, 0.11635656762010553, 0.10386003887808942, 0.10413773951680089, 0.11580116634268259, 0.11524576506525964, 0.10330463760066648, 0.11802277145237434]
Actions to choose Agent 1: dict_values([{'num_count': 572, 'sum_payoffs': 207.87434480475645, 'action': [0.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 231.52028098761713, 'action': [1.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 206.36620971497763, 'action': [0.0, 1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 239.80237750089873, 'action': [1.0, 1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 196.15097864240317, 'action': [0.0, 0.0]}, {'num_count': 645, 'sum_payoffs': 240.7778578369492, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1588447653429603, 0.17356289919466814, 0.15801166342682588, 0.17856151069147458, 0.15162454873646208, 0.17911691196889754]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.147821187973022 s
