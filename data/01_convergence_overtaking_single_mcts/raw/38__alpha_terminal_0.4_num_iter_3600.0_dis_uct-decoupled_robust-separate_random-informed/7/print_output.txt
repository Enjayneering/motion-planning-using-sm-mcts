Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 415, 'sum_payoffs': 141.1405253188696, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 131.5172964609923, 'action': [0.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 137.14733917807422, 'action': [1.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 135.39382700333704, 'action': [2.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 130.88839394600643, 'action': [0.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 123.17550682704623, 'action': [0.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 123.66430940597984, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 145.0367426087427, 'action': [2.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 142.93221816548768, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11524576506525964, 0.10913635101360733, 0.11274645931685642, 0.11163565676201055, 0.10885865037489587, 0.10386003887808942, 0.10413773951680089, 0.11774507081366287, 0.11635656762010553]
Actions to choose Agent 1: dict_values([{'num_count': 553, 'sum_payoffs': 199.02918198393917, 'action': [0.0, 0.0]}, {'num_count': 572, 'sum_payoffs': 207.6062267865929, 'action': [0.0, 1.5707963267948966]}, {'num_count': 639, 'sum_payoffs': 237.5810891843244, 'action': [1.0, -1.5707963267948966]}, {'num_count': 642, 'sum_payoffs': 238.8887193918408, 'action': [1.0, 1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 230.39463962035512, 'action': [1.0, 0.0]}, {'num_count': 571, 'sum_payoffs': 206.9749674834074, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1535684532074424, 0.1588447653429603, 0.17745070813662872, 0.17828381005276311, 0.1730074979172452, 0.15856706470424883]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.292826414108276 s
