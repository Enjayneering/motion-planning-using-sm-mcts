Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 415, 'sum_payoffs': 142.12008858438904, 'action': [2.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 140.3102140055459, 'action': [1.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 146.40016012168917, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 133.34027603352453, 'action': [0.0, -1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 123.10477655984802, 'action': [1.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 132.92715702633149, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 141.1375937242151, 'action': [1.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 134.97000343799246, 'action': [2.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 124.88762929022897, 'action': [0.0, 0.0]}])
Weights num count: [0.11524576506525964, 0.11413496251041377, 0.11802277145237434, 0.10969175229103027, 0.10330463760066648, 0.1094140516523188, 0.11469036378783672, 0.11080255484587614, 0.10441544015551235]
Actions to choose Agent 1: dict_values([{'num_count': 552, 'sum_payoffs': 199.2157338333768, 'action': [0.0, 0.0]}, {'num_count': 637, 'sum_payoffs': 237.41977947274663, 'action': [1.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 211.75385567960825, 'action': [0.0, 1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 236.6157953920556, 'action': [1.0, -1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 209.40181969929924, 'action': [0.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 230.1530819361486, 'action': [1.0, 0.0]}])
Weights num count: [0.1532907525687309, 0.17689530685920576, 0.16106637045265204, 0.17633990558178284, 0.1596778672590947, 0.17245209663982228]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.111746311187744 s
