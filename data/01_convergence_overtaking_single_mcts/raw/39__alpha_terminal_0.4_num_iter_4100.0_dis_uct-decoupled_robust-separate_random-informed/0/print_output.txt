Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 469, 'sum_payoffs': 152.131611667758, 'action': [1.0, -1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 153.1436217078788, 'action': [2.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 133.17432180370068, 'action': [0.0, 0.0]}, {'num_count': 471, 'sum_payoffs': 153.0116357461136, 'action': [1.0, 1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 137.14399615948744, 'action': [0.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 157.74224790119857, 'action': [2.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 141.3828302009211, 'action': [0.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 153.54749294714446, 'action': [2.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 138.55484112366364, 'action': [1.0, 0.0]}])
Weights num count: [0.11436235064618386, 0.11485003657644477, 0.10314557425018288, 0.11485003657644477, 0.10558400390148744, 0.11753230919287978, 0.10802243355279201, 0.11509387954157523, 0.1063155327968788]
Actions to choose Agent 1: dict_values([{'num_count': 643, 'sum_payoffs': 222.11762980684313, 'action': [0.0, 1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 258.70125531072756, 'action': [1.0, 1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 263.4355687778343, 'action': [1.0, -1.5707963267948966]}, {'num_count': 644, 'sum_payoffs': 222.6091352740207, 'action': [0.0, -1.5707963267948966]}, {'num_count': 713, 'sum_payoffs': 252.18733377971006, 'action': [1.0, 0.0]}, {'num_count': 633, 'sum_payoffs': 217.88359117955886, 'action': [0.0, 0.0]}])
Weights num count: [0.1567910265788832, 0.17751767861497195, 0.18019995123140697, 0.15703486954401366, 0.17386003413801512, 0.15435259692757863]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.893649578094482 s
