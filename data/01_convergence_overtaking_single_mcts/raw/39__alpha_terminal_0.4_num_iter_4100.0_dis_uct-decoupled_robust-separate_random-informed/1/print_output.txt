Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 447, 'sum_payoffs': 142.77676468670657, 'action': [0.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 154.25487778455346, 'action': [1.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 154.10680349414054, 'action': [2.0, 1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 136.2859244408004, 'action': [0.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 132.89815487963332, 'action': [1.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 134.97422950185654, 'action': [0.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 150.81732618090996, 'action': [1.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 158.6450055900872, 'action': [2.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 153.29491126140485, 'action': [2.0, 0.0]}])
Weights num count: [0.10899780541331383, 0.11558156547183614, 0.11558156547183614, 0.10509631797122652, 0.10314557425018288, 0.10436478907583516, 0.11363082175079249, 0.11826383808827115, 0.11509387954157523]
Actions to choose Agent 1: dict_values([{'num_count': 647, 'sum_payoffs': 224.24894252243956, 'action': [0.0, 1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 255.95047096796506, 'action': [1.0, -1.5707963267948966]}, {'num_count': 724, 'sum_payoffs': 257.1747139916267, 'action': [1.0, 0.0]}, {'num_count': 627, 'sum_payoffs': 215.64879497766935, 'action': [0.0, 0.0]}, {'num_count': 650, 'sum_payoffs': 225.5737609823042, 'action': [0.0, -1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 260.24195156603787, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15776639843940501, 0.17581077785905877, 0.17654230675445012, 0.1528895391367959, 0.1584979273347964, 0.17824920751036333]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.916098833084106 s
