Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 469, 'sum_payoffs': 151.82865936390263, 'action': [2.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 134.2917749763388, 'action': [0.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 147.89546022871605, 'action': [1.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 158.45545337981156, 'action': [2.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 136.4218050866746, 'action': [0.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 150.92255191971034, 'action': [1.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 144.3563978560666, 'action': [0.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 157.61200851190887, 'action': [2.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 133.91988354125237, 'action': [1.0, 0.0]}])
Weights num count: [0.11436235064618386, 0.1041209461107047, 0.11216776396000976, 0.11826383808827115, 0.10534016093635698, 0.11387466471592295, 0.10997317727383565, 0.11777615215801024, 0.10387710314557425]
Actions to choose Agent 1: dict_values([{'num_count': 638, 'sum_payoffs': 221.54316985417327, 'action': [0.0, -1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 261.50549993998413, 'action': [1.0, -1.5707963267948966]}, {'num_count': 723, 'sum_payoffs': 258.0852029009017, 'action': [1.0, 0.0]}, {'num_count': 726, 'sum_payoffs': 259.46705575329895, 'action': [1.0, 1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 222.28249304537619, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 223.25276299190753, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15557181175323093, 0.17824920751036333, 0.17629846378931968, 0.17702999268471104, 0.15605949768349184, 0.15654718361375275]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.838967323303223 s
