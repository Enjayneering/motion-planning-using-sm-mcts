Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 438, 'sum_payoffs': 139.2901922980718, 'action': [0.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 157.51068770941112, 'action': [2.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 138.89554187784947, 'action': [0.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 153.78585488404795, 'action': [2.0, 0.0]}, {'num_count': 430, 'sum_payoffs': 135.82130358345128, 'action': [0.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 148.86469835683016, 'action': [1.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 152.53104333183504, 'action': [1.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 136.74407522425383, 'action': [1.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 155.41427610398983, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10680321872713973, 0.11753230919287978, 0.10655937576200927, 0.11533772250670568, 0.10485247500609607, 0.11241160692514021, 0.11460619361131431, 0.10534016093635698, 0.1163130943672275]
Actions to choose Agent 1: dict_values([{'num_count': 653, 'sum_payoffs': 227.1010018603781, 'action': [0.0, -1.5707963267948966]}, {'num_count': 656, 'sum_payoffs': 228.50333398790028, 'action': [0.0, 1.5707963267948966]}, {'num_count': 718, 'sum_payoffs': 255.14850152999435, 'action': [1.0, 0.0]}, {'num_count': 717, 'sum_payoffs': 254.53586870467697, 'action': [1.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 218.54291502499314, 'action': [0.0, 0.0]}, {'num_count': 723, 'sum_payoffs': 257.3055568221988, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15922945623018775, 0.15996098512557913, 0.1750792489636674, 0.17483540599853695, 0.15435259692757863, 0.17629846378931968]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.223814010620117 s
