Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 471, 'sum_payoffs': 153.6468911609051, 'action': [1.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 150.7084104807584, 'action': [2.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 139.39467127811525, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 134.02669664487138, 'action': [0.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 159.5547653904171, 'action': [2.0, -1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 138.9770146011381, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 142.14530113631596, 'action': [0.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 152.77794732667834, 'action': [1.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 153.14287996670714, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11485003657644477, 0.11314313582053158, 0.10655937576200927, 0.10338941721531333, 0.11826383808827115, 0.1063155327968788, 0.10826627651792246, 0.11436235064618386, 0.11460619361131431]
Actions to choose Agent 1: dict_values([{'num_count': 712, 'sum_payoffs': 251.980484419799, 'action': [1.0, -1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 230.54289065884274, 'action': [0.0, -1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 256.3716312641521, 'action': [1.0, 0.0]}, {'num_count': 731, 'sum_payoffs': 260.2791465937384, 'action': [1.0, 1.5707963267948966]}, {'num_count': 631, 'sum_payoffs': 217.37530383067158, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 222.11470234958134, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17361619117288465, 0.16142404291636187, 0.1760546208241892, 0.17824920751036333, 0.15386491099731772, 0.15654718361375275]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.09787392616272 s
