Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 461, 'sum_payoffs': 147.6636772617361, 'action': [1.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 148.08207370298024, 'action': [2.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 158.8998737597745, 'action': [2.0, -1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 136.00428952921098, 'action': [0.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 131.80010679943453, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 156.7041793563136, 'action': [2.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 133.37423715972238, 'action': [0.0, 1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 152.92074246088401, 'action': [1.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 142.66497220733336, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11241160692514021, 0.11265544989027067, 0.11899536698366252, 0.10558400390148744, 0.10314557425018288, 0.11777615215801024, 0.1041209461107047, 0.11558156547183614, 0.10948549134357474]
Actions to choose Agent 1: dict_values([{'num_count': 716, 'sum_payoffs': 256.21917382548503, 'action': [1.0, 0.0]}, {'num_count': 719, 'sum_payoffs': 257.6432728135075, 'action': [1.0, 1.5707963267948966]}, {'num_count': 737, 'sum_payoffs': 265.44899327241023, 'action': [1.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 223.7816455076985, 'action': [0.0, -1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 218.62053707766987, 'action': [0.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 231.20057060388672, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17459156303340648, 0.17532309192879786, 0.17971226530114606, 0.15630334064862228, 0.1533772250670568, 0.16044867105584004]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.331351041793823 s
