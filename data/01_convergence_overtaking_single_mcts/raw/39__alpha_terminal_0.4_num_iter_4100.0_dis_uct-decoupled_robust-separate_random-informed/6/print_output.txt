Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 491, 'sum_payoffs': 161.39516931882093, 'action': [2.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 133.26370919595476, 'action': [1.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 155.0153036111023, 'action': [2.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 152.6002378633602, 'action': [2.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 133.81845428080683, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 152.15088428112364, 'action': [1.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 140.08511509858306, 'action': [0.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 150.70869729903947, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 139.5809080793365, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11972689587905389, 0.10338941721531333, 0.11606925140209705, 0.11460619361131431, 0.10363326018044379, 0.11436235064618386, 0.10729090465740064, 0.11363082175079249, 0.10704706169227018]
Actions to choose Agent 1: dict_values([{'num_count': 744, 'sum_payoffs': 266.60051947123833, 'action': [1.0, 1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 221.01646503468757, 'action': [0.0, 0.0]}, {'num_count': 641, 'sum_payoffs': 222.13836949934384, 'action': [0.0, -1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 255.66606077758038, 'action': [1.0, 0.0]}, {'num_count': 644, 'sum_payoffs': 223.43004466354157, 'action': [0.0, 1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 253.5783141151072, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18141916605705927, 0.15557181175323093, 0.15630334064862228, 0.17532309192879786, 0.15703486954401366, 0.17410387710314557]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.15426254272461 s
