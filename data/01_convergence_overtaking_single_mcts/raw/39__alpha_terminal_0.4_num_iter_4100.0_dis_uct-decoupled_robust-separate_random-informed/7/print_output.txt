Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 470, 'sum_payoffs': 151.84321452024147, 'action': [1.0, -1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 133.6415974341132, 'action': [1.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 136.36717365657884, 'action': [0.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 156.7807563723922, 'action': [2.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 149.72492971938757, 'action': [2.0, 0.0]}, {'num_count': 430, 'sum_payoffs': 135.07455484656404, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 159.37106377394122, 'action': [2.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 147.49470297193218, 'action': [1.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 141.69358021995203, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11460619361131431, 0.10387710314557425, 0.10558400390148744, 0.11753230919287978, 0.11338697878566203, 0.10485247500609607, 0.11899536698366252, 0.11216776396000976, 0.10875396244818337]
Actions to choose Agent 1: dict_values([{'num_count': 648, 'sum_payoffs': 226.2062981301827, 'action': [0.0, 1.5707963267948966]}, {'num_count': 730, 'sum_payoffs': 261.5253684878728, 'action': [1.0, -1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 249.40211167282789, 'action': [1.0, 0.0]}, {'num_count': 636, 'sum_payoffs': 221.12876201701962, 'action': [0.0, 0.0]}, {'num_count': 725, 'sum_payoffs': 259.3825609921176, 'action': [1.0, 1.5707963267948966]}, {'num_count': 659, 'sum_payoffs': 230.93782174507214, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15801024140453548, 0.17800536454523286, 0.1711777615215801, 0.15508412582297001, 0.1767861497195806, 0.16069251402097048]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.049882411956787 s
