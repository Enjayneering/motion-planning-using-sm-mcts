Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 446, 'sum_payoffs': 141.79438333752228, 'action': [0.0, -1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 135.0894702355166, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 156.9971770083282, 'action': [2.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 128.89746988720606, 'action': [0.0, 0.0]}, {'num_count': 471, 'sum_payoffs': 152.02990829544743, 'action': [1.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 139.25199470894913, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 146.3528030094122, 'action': [2.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 156.27122508572137, 'action': [2.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 154.50063414313968, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10875396244818337, 0.10485247500609607, 0.11777615215801024, 0.10119483052913923, 0.11485003657644477, 0.10729090465740064, 0.11143623506461839, 0.11728846622774933, 0.1163130943672275]
Actions to choose Agent 1: dict_values([{'num_count': 716, 'sum_payoffs': 255.68660977100416, 'action': [1.0, 0.0]}, {'num_count': 636, 'sum_payoffs': 221.03397734361465, 'action': [0.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 261.2073986719667, 'action': [1.0, -1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 228.397969984919, 'action': [0.0, -1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 220.62218209858656, 'action': [0.0, 1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 262.01052112842655, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17459156303340648, 0.15508412582297001, 0.17776152158010242, 0.15922945623018775, 0.15484028285783955, 0.17824920751036333]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.953368902206421 s
