Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 479, 'sum_payoffs': 155.36295330951543, 'action': [1.0, -1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 135.04538554585704, 'action': [1.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 148.2981816335114, 'action': [2.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 135.35878777584784, 'action': [0.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 138.15797305885883, 'action': [0.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 156.500812523757, 'action': [2.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 152.41333090652344, 'action': [2.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 137.5434667103339, 'action': [0.0, -1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 151.58898421101472, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11680078029748842, 0.10485247500609607, 0.11265544989027067, 0.10509631797122652, 0.10680321872713973, 0.11753230919287978, 0.11509387954157523, 0.1063155327968788, 0.11460619361131431]
Actions to choose Agent 1: dict_values([{'num_count': 642, 'sum_payoffs': 224.37277899377577, 'action': [0.0, 1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 258.4932386878429, 'action': [1.0, 1.5707963267948966]}, {'num_count': 651, 'sum_payoffs': 228.1197187368025, 'action': [0.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 217.9245205105405, 'action': [0.0, 0.0]}, {'num_count': 730, 'sum_payoffs': 262.4790964269478, 'action': [1.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 261.8886182678117, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15654718361375275, 0.17581077785905877, 0.15874177029992684, 0.1528895391367959, 0.17800536454523286, 0.17776152158010242]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.037872314453125 s
