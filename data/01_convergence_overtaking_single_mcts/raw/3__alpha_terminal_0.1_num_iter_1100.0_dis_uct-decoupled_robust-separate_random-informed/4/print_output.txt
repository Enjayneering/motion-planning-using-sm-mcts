Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 109, 'sum_payoffs': 26.31684157482422, 'action': [0.0, -1.5707963267948966]}, {'num_count': 137, 'sum_payoffs': 38.310844571326044, 'action': [2.0, -1.5707963267948966]}, {'num_count': 137, 'sum_payoffs': 38.41079459629688, 'action': [2.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 31.27436281337829, 'action': [1.0, 0.0]}, {'num_count': 137, 'sum_payoffs': 38.41079459629686, 'action': [2.0, 1.5707963267948966]}, {'num_count': 109, 'sum_payoffs': 26.176911539865024, 'action': [0.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 31.314342823366623, 'action': [1.0, 1.5707963267948966]}, {'num_count': 107, 'sum_payoffs': 25.377311340098238, 'action': [0.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 31.704147920752938, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.09900090826521345, 0.12443233424159855, 0.12443233424159855, 0.10990009082652134, 0.12443233424159855, 0.09900090826521345, 0.10990009082652134, 0.0971843778383288, 0.11080835603996367]
Actions to choose Agent 1: dict_values([{'num_count': 196, 'sum_payoffs': 64.32783607123758, 'action': [1.0, -1.5707963267948966]}, {'num_count': 200, 'sum_payoffs': 66.1069465157187, 'action': [1.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 51.90404796736121, 'action': [0.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 51.94402797734954, 'action': [0.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 51.55422287996323, 'action': [0.0, -1.5707963267948966]}, {'num_count': 198, 'sum_payoffs': 65.14742627599854, 'action': [1.0, 0.0]}])
Weights num count: [0.17801998183469572, 0.18165304268846502, 0.15349682107175294, 0.15349682107175294, 0.15258855585831063, 0.17983651226158037]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.19162702560424805 s
