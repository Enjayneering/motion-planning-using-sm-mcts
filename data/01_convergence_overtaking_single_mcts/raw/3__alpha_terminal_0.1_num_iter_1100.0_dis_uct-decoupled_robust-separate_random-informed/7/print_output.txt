Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 137, 'sum_payoffs': 38.35082458131439, 'action': [2.0, 0.0]}, {'num_count': 108, 'sum_payoffs': 25.78710644247872, 'action': [0.0, 1.5707963267948966]}, {'num_count': 108, 'sum_payoffs': 25.807096447472887, 'action': [0.0, 0.0]}, {'num_count': 139, 'sum_payoffs': 39.170414786075305, 'action': [2.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 31.354322833354964, 'action': [1.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 31.374312838349137, 'action': [1.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 31.7241379257471, 'action': [1.0, -1.5707963267948966]}, {'num_count': 137, 'sum_payoffs': 38.31084457132602, 'action': [2.0, -1.5707963267948966]}, {'num_count': 107, 'sum_payoffs': 25.49725137006326, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12443233424159855, 0.09809264305177112, 0.09809264305177112, 0.1262488646684832, 0.10990009082652134, 0.10990009082652134, 0.11080835603996367, 0.12443233424159855, 0.0971843778383288]
Actions to choose Agent 1: dict_values([{'num_count': 168, 'sum_payoffs': 51.53423287496906, 'action': [0.0, 0.0]}, {'num_count': 200, 'sum_payoffs': 66.06696650573036, 'action': [1.0, 0.0]}, {'num_count': 201, 'sum_payoffs': 66.51674161809922, 'action': [1.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 64.67766115863556, 'action': [1.0, 1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 51.08445776260026, 'action': [0.0, -1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 51.10444776759442, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15258855585831063, 0.18165304268846502, 0.18256130790190736, 0.17892824704813806, 0.1516802906448683, 0.1516802906448683]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.20143651962280273 s
