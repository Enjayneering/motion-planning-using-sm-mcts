Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 527, 'sum_payoffs': 163.5061605397864, 'action': [1.0, 1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 150.12834035853678, 'action': [1.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 144.46100579970667, 'action': [0.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 150.86887057917139, 'action': [0.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 146.04480066115622, 'action': [0.0, 1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 164.07772866224306, 'action': [2.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 165.6018245812042, 'action': [2.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 166.85061688450898, 'action': [2.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 163.97071636572133, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11454031732232123, 0.10715061943055858, 0.10410780265159748, 0.10758530754183873, 0.10497717887415779, 0.11475766137796131, 0.11562703760052162, 0.11627906976744186, 0.11475766137796131]
Actions to choose Agent 1: dict_values([{'num_count': 710, 'sum_payoffs': 237.16804773036907, 'action': [0.0, 0.0]}, {'num_count': 712, 'sum_payoffs': 238.05309538130675, 'action': [0.0, 1.5707963267948966]}, {'num_count': 819, 'sum_payoffs': 282.23292710377535, 'action': [1.0, -1.5707963267948966]}, {'num_count': 816, 'sum_payoffs': 281.1021188467094, 'action': [1.0, 0.0]}, {'num_count': 727, 'sum_payoffs': 244.2475860953677, 'action': [0.0, -1.5707963267948966]}, {'num_count': 816, 'sum_payoffs': 280.9718510066799, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15431427950445556, 0.1547489676157357, 0.17800478156922409, 0.17735274940230383, 0.15800912845033688, 0.17735274940230383]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.78243613243103 s
