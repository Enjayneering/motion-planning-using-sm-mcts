Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 548, 'sum_payoffs': 172.2094899596019, 'action': [2.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 163.71333490864316, 'action': [1.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 150.4688492419876, 'action': [0.0, -1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 162.92713347599806, 'action': [2.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 147.06610991627915, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 150.1418010817929, 'action': [1.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 158.0638266087318, 'action': [1.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 165.8201245254544, 'action': [2.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 146.27554650066898, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11910454249076288, 0.11454031732232123, 0.10736796348619865, 0.11410562921104107, 0.10541186698543795, 0.10715061943055858, 0.11149750054336013, 0.11562703760052162, 0.10497717887415779]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 247.15718769715505, 'action': [0.0, 1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 242.22754264511767, 'action': [0.0, -1.5707963267948966]}, {'num_count': 808, 'sum_payoffs': 278.3199184847416, 'action': [1.0, 0.0]}, {'num_count': 813, 'sum_payoffs': 280.39488494343186, 'action': [1.0, -1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 234.5334358299547, 'action': [0.0, 0.0]}, {'num_count': 823, 'sum_payoffs': 284.7040067773119, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15931319278417735, 0.1567050641164964, 0.17561399695718322, 0.1767007172353836, 0.15257552705933491, 0.1788741577917844]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.80210828781128 s
