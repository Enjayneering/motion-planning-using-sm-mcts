Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 484, 'sum_payoffs': 146.59268252091564, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 146.89888128431022, 'action': [0.0, 0.0]}, {'num_count': 535, 'sum_payoffs': 166.94860294370918, 'action': [2.0, 1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 157.3074597269906, 'action': [1.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 165.7705812225424, 'action': [1.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 168.82476572513553, 'action': [2.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 150.17227510339904, 'action': [1.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 147.41721273894095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 166.59501444243236, 'action': [2.0, 0.0]}])
Weights num count: [0.10519452292979788, 0.10541186698543795, 0.11627906976744186, 0.11106281243207998, 0.11562703760052162, 0.11736579004564225, 0.10715061943055858, 0.10562921104107803, 0.11606172571180179]
Actions to choose Agent 1: dict_values([{'num_count': 825, 'sum_payoffs': 284.7828486370001, 'action': [1.0, 0.0]}, {'num_count': 800, 'sum_payoffs': 274.2289587340594, 'action': [1.0, -1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 242.1029888920684, 'action': [0.0, -1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 278.1164231284332, 'action': [1.0, 1.5707963267948966]}, {'num_count': 713, 'sum_payoffs': 238.22202419297884, 'action': [0.0, 0.0]}, {'num_count': 731, 'sum_payoffs': 245.6571766135571, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17930884590306456, 0.1738752445120626, 0.15692240817213648, 0.1758313410128233, 0.15496631167137578, 0.1588785046728972]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.993598461151123 s
