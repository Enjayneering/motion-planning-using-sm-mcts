Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 486, 'sum_payoffs': 146.94441613661937, 'action': [0.0, 1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 147.184582244339, 'action': [1.0, 0.0]}, {'num_count': 538, 'sum_payoffs': 167.77844461334882, 'action': [2.0, 1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 150.5562427164615, 'action': [0.0, -1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 142.6607238070272, 'action': [0.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 165.3876079695503, 'action': [1.0, -1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 156.55594073126423, 'action': [1.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 166.16872577705058, 'action': [2.0, 0.0]}, {'num_count': 544, 'sum_payoffs': 170.07131431214208, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10562921104107803, 0.10562921104107803, 0.1169311019343621, 0.10758530754183873, 0.10323842642903716, 0.11562703760052162, 0.11084546837643991, 0.11606172571180179, 0.11823516626820256]
Actions to choose Agent 1: dict_values([{'num_count': 805, 'sum_payoffs': 277.25386240261685, 'action': [1.0, -1.5707963267948966]}, {'num_count': 727, 'sum_payoffs': 244.9134644510161, 'action': [0.0, 0.0]}, {'num_count': 810, 'sum_payoffs': 279.4275673534648, 'action': [1.0, 0.0]}, {'num_count': 812, 'sum_payoffs': 280.22726923784717, 'action': [1.0, 1.5707963267948966]}, {'num_count': 729, 'sum_payoffs': 245.83577688416443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 240.77312719694487, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.174961964790263, 0.15800912845033688, 0.17604868506846338, 0.17648337317974352, 0.15844381656161705, 0.1558356878939361]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.722450971603394 s
