Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 489, 'sum_payoffs': 148.63858077720627, 'action': [0.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 159.68810510391165, 'action': [1.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 150.69160315050897, 'action': [1.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 158.5549068111245, 'action': [1.0, -1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 166.1672766034838, 'action': [2.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 150.92231896525277, 'action': [0.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 145.71860116327287, 'action': [0.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 166.56043804847707, 'action': [2.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 169.89104293499105, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10628124320799825, 0.11236687676592046, 0.10736796348619865, 0.11171484459900022, 0.1158443816561617, 0.10758530754183873, 0.10475983481851771, 0.11606172571180179, 0.1178004781569224]
Actions to choose Agent 1: dict_values([{'num_count': 810, 'sum_payoffs': 278.91454299351153, 'action': [1.0, 0.0]}, {'num_count': 727, 'sum_payoffs': 244.61357911470296, 'action': [0.0, -1.5707963267948966]}, {'num_count': 808, 'sum_payoffs': 278.14678444429677, 'action': [1.0, 1.5707963267948966]}, {'num_count': 828, 'sum_payoffs': 286.4716361728954, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 240.05597108410078, 'action': [0.0, 0.0]}, {'num_count': 711, 'sum_payoffs': 237.81366076180888, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17604868506846338, 0.15800912845033688, 0.17561399695718322, 0.1799608780699848, 0.15561834383829604, 0.15453162356009564]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.839095830917358 s
