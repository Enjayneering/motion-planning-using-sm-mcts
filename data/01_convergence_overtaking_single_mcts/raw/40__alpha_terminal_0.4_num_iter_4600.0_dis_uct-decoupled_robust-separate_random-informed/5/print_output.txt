Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 526, 'sum_payoffs': 163.65187174361526, 'action': [1.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 170.20797390253404, 'action': [2.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 144.48687821833846, 'action': [0.0, 0.0]}, {'num_count': 517, 'sum_payoffs': 160.0902639693279, 'action': [1.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 148.19315442973786, 'action': [1.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 149.67575580482378, 'action': [0.0, -1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 167.35204166718725, 'action': [2.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 166.8809099104975, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 149.3187004116263, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11432297326668116, 0.1178004781569224, 0.1038904585959574, 0.11236687676592046, 0.1058465550967181, 0.10671593131927842, 0.11627906976744186, 0.11606172571180179, 0.10649858726363834]
Actions to choose Agent 1: dict_values([{'num_count': 729, 'sum_payoffs': 244.03409668817952, 'action': [0.0, -1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 240.08885070418432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 280.98283769547305, 'action': [1.0, -1.5707963267948966]}, {'num_count': 817, 'sum_payoffs': 280.52838518335943, 'action': [1.0, 0.0]}, {'num_count': 814, 'sum_payoffs': 279.1572059489495, 'action': [1.0, 1.5707963267948966]}, {'num_count': 703, 'sum_payoffs': 233.49564542442926, 'action': [0.0, 0.0]}])
Weights num count: [0.15844381656161705, 0.15627037600521626, 0.177787437513584, 0.17757009345794392, 0.1769180612910237, 0.152792871114975]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 14.052862167358398 s
