Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 540, 'sum_payoffs': 168.06134356423468, 'action': [2.0, 0.0]}, {'num_count': 547, 'sum_payoffs': 170.87698194902794, 'action': [2.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 158.96239997686703, 'action': [1.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 145.52368740954952, 'action': [1.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 148.2596375427288, 'action': [0.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 144.29952728615382, 'action': [0.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 158.47979233366112, 'action': [1.0, -1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 171.75137945361544, 'action': [2.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 143.58024580175118, 'action': [0.0, 0.0]}])
Weights num count: [0.11736579004564225, 0.1188871984351228, 0.11236687676592046, 0.10497717887415779, 0.10649858726363834, 0.10432514670723755, 0.11214953271028037, 0.11932188654640295, 0.1038904585959574]
Actions to choose Agent 1: dict_values([{'num_count': 804, 'sum_payoffs': 276.69731149923365, 'action': [1.0, 1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 247.47691756123908, 'action': [0.0, -1.5707963267948966]}, {'num_count': 808, 'sum_payoffs': 278.3401734108982, 'action': [1.0, -1.5707963267948966]}, {'num_count': 829, 'sum_payoffs': 287.0338675857901, 'action': [1.0, 0.0]}, {'num_count': 719, 'sum_payoffs': 241.40237669040354, 'action': [0.0, 1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 236.0428045316373, 'action': [0.0, 0.0]}])
Weights num count: [0.1747446207346229, 0.15953053683981744, 0.17561399695718322, 0.18017822212562487, 0.15627037600521626, 0.15344490328189525]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.708608865737915 s
