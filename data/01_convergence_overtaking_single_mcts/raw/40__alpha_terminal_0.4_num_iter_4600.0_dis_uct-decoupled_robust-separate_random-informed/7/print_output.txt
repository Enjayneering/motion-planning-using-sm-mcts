Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 490, 'sum_payoffs': 148.8365848217282, 'action': [0.0, 1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 173.76214419451168, 'action': [2.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 163.3304220721033, 'action': [2.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 163.26605045461383, 'action': [1.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 148.0647195776756, 'action': [1.0, 0.0]}, {'num_count': 535, 'sum_payoffs': 166.8716151630866, 'action': [2.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 143.47397455029602, 'action': [0.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 159.29259719710242, 'action': [1.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 149.31066534707958, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10649858726363834, 0.11997391871332319, 0.11432297326668116, 0.11432297326668116, 0.10606389915235818, 0.11627906976744186, 0.10345577048467725, 0.11214953271028037, 0.10671593131927842]
Actions to choose Agent 1: dict_values([{'num_count': 730, 'sum_payoffs': 245.80607499591778, 'action': [0.0, 1.5707963267948966]}, {'num_count': 820, 'sum_payoffs': 283.09666446607963, 'action': [1.0, 1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 234.28274809963693, 'action': [0.0, 0.0]}, {'num_count': 831, 'sum_payoffs': 287.807546923664, 'action': [1.0, -1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 274.76683328778927, 'action': [1.0, 0.0]}, {'num_count': 717, 'sum_payoffs': 240.40697537156754, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15866116061725713, 0.17822212562486417, 0.15257552705933491, 0.180612910236905, 0.1738752445120626, 0.1558356878939361]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.687108278274536 s
