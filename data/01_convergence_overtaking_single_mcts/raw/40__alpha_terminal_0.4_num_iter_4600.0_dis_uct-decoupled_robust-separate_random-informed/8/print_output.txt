Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 494, 'sum_payoffs': 150.27105691446098, 'action': [0.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 158.58220950917686, 'action': [1.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 165.21046371887275, 'action': [2.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 140.6418353529276, 'action': [0.0, 0.0]}, {'num_count': 552, 'sum_payoffs': 173.3606420837028, 'action': [2.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 164.09608941393742, 'action': [2.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 147.78091337856762, 'action': [0.0, -1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 166.15827083484214, 'action': [1.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 146.99845463439152, 'action': [1.0, 0.0]}])
Weights num count: [0.10736796348619865, 0.1119321886546403, 0.11562703760052162, 0.10215170615083677, 0.11997391871332319, 0.1149750054336014, 0.10606389915235818, 0.11606172571180179, 0.10562921104107803]
Actions to choose Agent 1: dict_values([{'num_count': 720, 'sum_payoffs': 241.1371194141602, 'action': [0.0, 1.5707963267948966]}, {'num_count': 705, 'sum_payoffs': 234.99340433336025, 'action': [0.0, 0.0]}, {'num_count': 823, 'sum_payoffs': 283.6944963657999, 'action': [1.0, -1.5707963267948966]}, {'num_count': 818, 'sum_payoffs': 281.7473785645737, 'action': [1.0, 0.0]}, {'num_count': 817, 'sum_payoffs': 281.193897008009, 'action': [1.0, 1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 239.95585822567563, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15648772006085634, 0.15322755922625517, 0.1788741577917844, 0.177787437513584, 0.17757009345794392, 0.1558356878939361]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.634731769561768 s
