Searching game tree in timestep 0...
Max timehorizon: 3
Actions to choose Agent 0: dict_values([{'num_count': 530, 'sum_payoffs': 165.69189106450807, 'action': [2.0, 1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 142.79138219968576, 'action': [0.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 149.56770008118966, 'action': [1.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 169.69579974195238, 'action': [2.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 150.38982411604314, 'action': [0.0, -1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 161.50094035157704, 'action': [1.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 167.59738239177133, 'action': [2.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 165.67760460159002, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 149.660219398715, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11519234948924147, 0.102803738317757, 0.10649858726363834, 0.11736579004564225, 0.1069332753749185, 0.11301890893284068, 0.11627906976744186, 0.11519234948924147, 0.10649858726363834]
Actions to choose Agent 1: dict_values([{'num_count': 813, 'sum_payoffs': 279.13299588551564, 'action': [1.0, 1.5707963267948966]}, {'num_count': 813, 'sum_payoffs': 279.2074933373568, 'action': [1.0, -1.5707963267948966]}, {'num_count': 810, 'sum_payoffs': 277.86229615067896, 'action': [1.0, 0.0]}, {'num_count': 728, 'sum_payoffs': 243.90296081563574, 'action': [0.0, -1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 238.72137761403255, 'action': [0.0, 0.0]}, {'num_count': 721, 'sum_payoffs': 241.03755100697052, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1767007172353836, 0.1767007172353836, 0.17604868506846338, 0.15822647250597696, 0.15540099978265595, 0.1567050641164964]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.795431852340698 s
