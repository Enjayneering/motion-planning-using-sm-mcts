Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 117, 'sum_payoffs': 39.54441517507544, 'action': [0.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 43.07380119050727, 'action': [1.0, 1.5707963267948966]}, {'num_count': 115, 'sum_payoffs': 38.46345335338081, 'action': [0.0, 1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 43.72379856012552, 'action': [2.0, 1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 40.09857143899482, 'action': [1.0, 0.0]}, {'num_count': 131, 'sum_payoffs': 46.915822878575284, 'action': [2.0, 0.0]}, {'num_count': 117, 'sum_payoffs': 39.477784780920565, 'action': [0.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 44.25057779618594, 'action': [1.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 44.77767362758219, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10626702997275204, 0.11262488646684832, 0.1044504995458674, 0.11353315168029064, 0.10717529518619437, 0.11898274296094459, 0.10626702997275204, 0.11444141689373297, 0.11534968210717529]
Actions to choose Agent 1: dict_values([{'num_count': 202, 'sum_payoffs': 75.51260426766548, 'action': [1.0, -1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 70.43515059605605, 'action': [1.0, 1.5707963267948966]}, {'num_count': 163, 'sum_payoffs': 56.129318662459575, 'action': [0.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 58.3701974087134, 'action': [0.0, -1.5707963267948966]}, {'num_count': 207, 'sum_payoffs': 78.04152697179528, 'action': [1.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 58.63608037617812, 'action': [0.0, 0.0]}])
Weights num count: [0.18346957311534967, 0.17438692098092642, 0.148047229791099, 0.15258855585831063, 0.1880108991825613, 0.15258855585831063]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 6.6050333976745605 s
