Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 125, 'sum_payoffs': 43.18940882857111, 'action': [2.0, 1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 38.59194261451913, 'action': [0.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 41.777650292266735, 'action': [1.0, 1.5707963267948966]}, {'num_count': 109, 'sum_payoffs': 35.13599394609678, 'action': [0.0, 1.5707963267948966]}, {'num_count': 133, 'sum_payoffs': 47.3391135307374, 'action': [2.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 42.56118229664632, 'action': [1.0, -1.5707963267948966]}, {'num_count': 134, 'sum_payoffs': 47.913416306393714, 'action': [2.0, -1.5707963267948966]}, {'num_count': 113, 'sum_payoffs': 37.176062964599524, 'action': [0.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 42.71845751061802, 'action': [1.0, 0.0]}])
Weights num count: [0.11353315168029064, 0.10535876475930972, 0.11080835603996367, 0.09900090826521345, 0.12079927338782924, 0.11262488646684832, 0.12170753860127158, 0.10263396911898275, 0.11262488646684832]
Actions to choose Agent 1: dict_values([{'num_count': 159, 'sum_payoffs': 54.648834659584885, 'action': [0.0, 1.5707963267948966]}, {'num_count': 208, 'sum_payoffs': 79.07172707920606, 'action': [1.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 61.013930478029984, 'action': [0.0, 0.0]}, {'num_count': 205, 'sum_payoffs': 77.83080206250429, 'action': [1.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 69.21282802434047, 'action': [1.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 59.206367059399426, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1444141689373297, 0.18891916439600362, 0.15622161671207993, 0.18619436875567666, 0.17075386012715713, 0.15258855585831063]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 6.627974033355713 s
