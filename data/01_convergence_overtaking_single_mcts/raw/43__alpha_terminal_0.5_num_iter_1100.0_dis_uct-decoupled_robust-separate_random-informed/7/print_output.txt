Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 117, 'sum_payoffs': 39.833895132321395, 'action': [0.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 40.673161596153605, 'action': [0.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 41.842288432840334, 'action': [0.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 43.19043230039107, 'action': [2.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 42.35874318143096, 'action': [1.0, 0.0]}, {'num_count': 129, 'sum_payoffs': 46.082648654353896, 'action': [2.0, 1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 42.86252647324379, 'action': [1.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 42.84301460079669, 'action': [2.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 42.42002617020559, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10626702997275204, 0.1080835603996367, 0.10990009082652134, 0.11262488646684832, 0.11080835603996367, 0.11716621253405994, 0.11171662125340599, 0.11171662125340599, 0.11080835603996367]
Actions to choose Agent 1: dict_values([{'num_count': 167, 'sum_payoffs': 58.055932479353885, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 62.612662365780736, 'action': [0.0, 0.0]}, {'num_count': 204, 'sum_payoffs': 76.68598434183448, 'action': [1.0, 1.5707963267948966]}, {'num_count': 199, 'sum_payoffs': 74.21788786619582, 'action': [1.0, 0.0]}, {'num_count': 192, 'sum_payoffs': 70.52650272559771, 'action': [1.0, -1.5707963267948966]}, {'num_count': 162, 'sum_payoffs': 55.75372264411546, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1516802906448683, 0.15985467756584923, 0.18528610354223432, 0.1807447774750227, 0.17438692098092642, 0.14713896457765668]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 6.576756477355957 s
