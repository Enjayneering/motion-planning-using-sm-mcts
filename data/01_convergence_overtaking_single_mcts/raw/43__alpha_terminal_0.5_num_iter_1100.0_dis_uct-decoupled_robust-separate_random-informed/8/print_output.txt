Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 124, 'sum_payoffs': 43.003628135656584, 'action': [2.0, -1.5707963267948966]}, {'num_count': 114, 'sum_payoffs': 37.846315534092874, 'action': [0.0, 1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 44.541833155477434, 'action': [2.0, 1.5707963267948966]}, {'num_count': 136, 'sum_payoffs': 49.378585269889534, 'action': [2.0, 0.0]}, {'num_count': 126, 'sum_payoffs': 44.06923531221656, 'action': [1.0, 1.5707963267948966]}, {'num_count': 112, 'sum_payoffs': 36.9296130103996, 'action': [0.0, -1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 39.0133062909276, 'action': [1.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 39.98813607050803, 'action': [0.0, 0.0]}, {'num_count': 127, 'sum_payoffs': 44.55050082812828, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11262488646684832, 0.10354223433242507, 0.11534968210717529, 0.12352406902815623, 0.11444141689373297, 0.10172570390554042, 0.10535876475930972, 0.10717529518619437, 0.11534968210717529]
Actions to choose Agent 1: dict_values([{'num_count': 169, 'sum_payoffs': 58.92524310799315, 'action': [0.0, -1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 72.4134839940094, 'action': [1.0, -1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 70.34747553053853, 'action': [1.0, 0.0]}, {'num_count': 197, 'sum_payoffs': 72.95332278088098, 'action': [1.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 63.94054609445339, 'action': [0.0, 0.0]}, {'num_count': 167, 'sum_payoffs': 57.99235189255381, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15349682107175294, 0.17801998183469572, 0.17438692098092642, 0.17892824704813806, 0.16257947320617622, 0.1516802906448683]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 6.639609336853027 s
