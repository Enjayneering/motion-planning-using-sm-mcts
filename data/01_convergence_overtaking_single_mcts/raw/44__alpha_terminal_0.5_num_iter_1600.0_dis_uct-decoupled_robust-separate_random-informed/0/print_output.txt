Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 188, 'sum_payoffs': 65.96360364857279, 'action': [2.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 61.544791273782415, 'action': [2.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 61.05383083703892, 'action': [0.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 61.436560258367486, 'action': [1.0, 1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 55.72323754919443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 56.20156216481204, 'action': [0.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 58.38616568009262, 'action': [1.0, 0.0]}, {'num_count': 186, 'sum_payoffs': 64.95509840269939, 'action': [1.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 62.896875422873535, 'action': [2.0, 0.0]}])
Weights num count: [0.1174266083697689, 0.11180512179887571, 0.11118051217988757, 0.11180512179887571, 0.10430980637101811, 0.10493441599000625, 0.1080574640849469, 0.11617738913179262, 0.1136789506558401]
Actions to choose Agent 1: dict_values([{'num_count': 258, 'sum_payoffs': 93.06861019105835, 'action': [0.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 111.88749786767357, 'action': [1.0, -1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 87.14420412675608, 'action': [0.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 107.97620341163734, 'action': [1.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 101.12328603684216, 'action': [1.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 82.01272921672017, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16114928169893816, 0.18550905683947533, 0.15365396627108058, 0.18051217988757026, 0.1717676452217364, 0.14678326046221113]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.521843910217285 s
