Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 161, 'sum_payoffs': 52.92709570234398, 'action': [0.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 64.11198575996048, 'action': [2.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 58.17440843595596, 'action': [1.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 63.083550757691306, 'action': [2.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 57.24242642817298, 'action': [0.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 68.47923166207708, 'action': [2.0, -1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 66.43037441597633, 'action': [1.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 56.32206591569316, 'action': [0.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 62.626716955321065, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10056214865708932, 0.11492816989381636, 0.10743285446595878, 0.1136789506558401, 0.1061836352279825, 0.12054965646470955, 0.11805121798875702, 0.10493441599000625, 0.11305434103685197]
Actions to choose Agent 1: dict_values([{'num_count': 242, 'sum_payoffs': 84.70150872517388, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 101.5573865229871, 'action': [1.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 100.96499380680142, 'action': [1.0, 1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 111.14354525722233, 'action': [1.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 88.15725729048923, 'action': [0.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 92.81031478860345, 'action': [0.0, 0.0]}])
Weights num count: [0.15115552779512806, 0.17301686445971268, 0.17239225484072454, 0.18550905683947533, 0.15552779512804496, 0.1617738913179263]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.593087434768677 s
