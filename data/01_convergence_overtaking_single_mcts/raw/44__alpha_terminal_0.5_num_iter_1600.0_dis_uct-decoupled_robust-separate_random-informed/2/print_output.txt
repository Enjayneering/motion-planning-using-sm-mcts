Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 179, 'sum_payoffs': 62.25695284922243, 'action': [1.0, 1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 55.932251776301996, 'action': [0.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 66.80576838521495, 'action': [2.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 61.82696733561633, 'action': [1.0, -1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 56.41784361461563, 'action': [0.0, -1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 56.87409757642434, 'action': [0.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 60.83046846573101, 'action': [1.0, 0.0]}, {'num_count': 193, 'sum_payoffs': 69.2221881187701, 'action': [2.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 65.27538875386642, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11180512179887571, 0.10368519675202999, 0.1174266083697689, 0.11118051217988757, 0.10430980637101811, 0.10493441599000625, 0.1099312929419113, 0.12054965646470955, 0.1155527795128045]
Actions to choose Agent 1: dict_values([{'num_count': 243, 'sum_payoffs': 84.91160606710834, 'action': [0.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 90.98968951973582, 'action': [0.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 101.59480578917803, 'action': [1.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 106.88686099112819, 'action': [1.0, 0.0]}, {'num_count': 254, 'sum_payoffs': 90.04722544911128, 'action': [0.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 102.58519808032133, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15178013741411617, 0.1599000624609619, 0.1736414740787008, 0.18051217988757026, 0.15865084322298564, 0.1748906933166771]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.511491775512695 s
