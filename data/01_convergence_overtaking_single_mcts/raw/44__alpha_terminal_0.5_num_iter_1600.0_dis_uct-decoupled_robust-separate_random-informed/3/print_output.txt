Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 158, 'sum_payoffs': 51.80275711432047, 'action': [0.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 66.5501599038588, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 58.1746964231915, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 60.59550901326633, 'action': [1.0, 1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 67.86986762019819, 'action': [1.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 70.95218754267306, 'action': [2.0, -1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 65.9872319478055, 'action': [2.0, 1.5707963267948966]}, {'num_count': 159, 'sum_payoffs': 52.38967242859651, 'action': [1.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 59.10277684919917, 'action': [0.0, 0.0]}])
Weights num count: [0.09868831980012492, 0.1174266083697689, 0.10680824484697064, 0.1099312929419113, 0.1193004372267333, 0.12304809494066209, 0.11680199875078076, 0.09931292941911306, 0.1080574640849469]
Actions to choose Agent 1: dict_values([{'num_count': 236, 'sum_payoffs': 81.78377750468734, 'action': [0.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 107.65870415481412, 'action': [1.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 113.47664957523806, 'action': [1.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 109.11023391812114, 'action': [1.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 78.08346398895554, 'action': [0.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 88.82820373263488, 'action': [0.0, 0.0]}])
Weights num count: [0.14740787008119924, 0.1811367895065584, 0.18863210493441598, 0.1830106183635228, 0.1424109931292942, 0.15677701436602123]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.658191919326782 s
