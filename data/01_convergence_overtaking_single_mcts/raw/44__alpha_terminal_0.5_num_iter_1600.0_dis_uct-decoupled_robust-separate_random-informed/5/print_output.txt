Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 182, 'sum_payoffs': 62.752844012168936, 'action': [2.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 57.997814602460636, 'action': [0.0, -1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 66.13317779036409, 'action': [1.0, -1.5707963267948966]}, {'num_count': 160, 'sum_payoffs': 52.220653776341635, 'action': [0.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 59.311672711731326, 'action': [1.0, 1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 65.32838358487753, 'action': [2.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 59.389089474576224, 'action': [1.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 66.1730854198829, 'action': [2.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 57.44521612474496, 'action': [0.0, 0.0]}])
Weights num count: [0.1136789506558401, 0.10743285446595878, 0.11805121798875702, 0.09993753903810118, 0.10930668332292318, 0.11680199875078076, 0.10930668332292318, 0.11805121798875702, 0.10680824484697064]
Actions to choose Agent 1: dict_values([{'num_count': 245, 'sum_payoffs': 86.55268702895533, 'action': [0.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 87.9702074094061, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 112.696473254209, 'action': [1.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 109.75203101451775, 'action': [1.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 82.30462070110893, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 102.93741272421684, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15302935665209244, 0.15490318550905685, 0.1867582760774516, 0.1830106183635228, 0.14740787008119924, 0.17426608369768895]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 9.542287826538086 s
