Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 171, 'sum_payoffs': 57.70410797035695, 'action': [1.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 63.590350209532104, 'action': [1.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 62.56689094534921, 'action': [2.0, 0.0]}, {'num_count': 197, 'sum_payoffs': 70.52278018001644, 'action': [2.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 67.04900090982508, 'action': [2.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 58.129307306236605, 'action': [0.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 57.26642302304614, 'action': [0.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 57.80892191338532, 'action': [1.0, 1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 54.75751821566559, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10680824484697064, 0.11430356027482823, 0.11305434103685197, 0.12304809494066209, 0.11867582760774516, 0.10743285446595878, 0.1061836352279825, 0.10680824484697064, 0.10306058713304185]
Actions to choose Agent 1: dict_values([{'num_count': 238, 'sum_payoffs': 82.71289784099298, 'action': [0.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 100.01285488114893, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 83.30102219268865, 'action': [0.0, 0.0]}, {'num_count': 251, 'sum_payoffs': 89.00053864204644, 'action': [0.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 112.84148976342324, 'action': [1.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 110.98478738242106, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1486570893191755, 0.1711430356027483, 0.14928169893816365, 0.15677701436602123, 0.18800749531542785, 0.18550905683947533]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.485432386398315 s
