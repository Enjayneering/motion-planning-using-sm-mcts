Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 179, 'sum_payoffs': 61.91811832516111, 'action': [2.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 57.028163048831274, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 63.24235216741449, 'action': [1.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 59.43531951821321, 'action': [0.0, -1.5707963267948966]}, {'num_count': 162, 'sum_payoffs': 53.66216217704592, 'action': [0.0, 1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 70.20106074058093, 'action': [1.0, -1.5707963267948966]}, {'num_count': 195, 'sum_payoffs': 69.70104632096607, 'action': [2.0, -1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 55.470984972637936, 'action': [1.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 60.91180184372005, 'action': [2.0, 0.0]}])
Weights num count: [0.11180512179887571, 0.10555902560899438, 0.1136789506558401, 0.10868207370393504, 0.10118675827607745, 0.12242348532167395, 0.12179887570268583, 0.10368519675202999, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 289, 'sum_payoffs': 107.35551547750096, 'action': [1.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 85.75204980470184, 'action': [0.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 83.85234552706366, 'action': [0.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 111.58746071219893, 'action': [1.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 104.90673879668363, 'action': [1.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 86.23835745226745, 'action': [0.0, 0.0]}])
Weights num count: [0.18051217988757026, 0.1524047470331043, 0.14990630855715179, 0.18613366645846346, 0.1773891317926296, 0.15302935665209244]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 9.497335433959961 s
