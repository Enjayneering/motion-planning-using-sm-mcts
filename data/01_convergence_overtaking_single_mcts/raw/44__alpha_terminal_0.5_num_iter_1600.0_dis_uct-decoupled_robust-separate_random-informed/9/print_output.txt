Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 184, 'sum_payoffs': 64.12227494491196, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 57.77798003181862, 'action': [1.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 62.05045137234821, 'action': [1.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 57.72207724066844, 'action': [0.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 58.144301905779336, 'action': [0.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 62.68940578737673, 'action': [1.0, 1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 65.1689202095573, 'action': [2.0, 1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 54.91858872215494, 'action': [0.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 67.0814017026872, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11492816989381636, 0.10680824484697064, 0.11242973141786383, 0.10680824484697064, 0.10743285446595878, 0.11305434103685197, 0.11617738913179262, 0.10306058713304185, 0.11867582760774516]
Actions to choose Agent 1: dict_values([{'num_count': 241, 'sum_payoffs': 84.47067250315786, 'action': [0.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 103.72359408128266, 'action': [1.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 86.39973611060468, 'action': [0.0, 1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 89.79032299730798, 'action': [0.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 111.4528909232721, 'action': [1.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 105.18846737607053, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15053091817613992, 0.1755153029356652, 0.15302935665209244, 0.15740162398500937, 0.18550905683947533, 0.1773891317926296]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.51836347579956 s
