Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 248, 'sum_payoffs': 87.5050317524292, 'action': [1.0, -1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 88.38934325090034, 'action': [2.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 75.49899370564182, 'action': [0.0, 0.0]}, {'num_count': 209, 'sum_payoffs': 69.03082891499487, 'action': [0.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 85.51330559445395, 'action': [2.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 78.33666012332452, 'action': [1.0, 0.0]}, {'num_count': 251, 'sum_payoffs': 88.83879181778536, 'action': [2.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 77.96207830789432, 'action': [1.0, 1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 73.26808741280226, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11803902903379343, 0.11899095668729176, 0.10613993336506425, 0.09947643979057591, 0.11613517372679677, 0.10899571632555925, 0.11946692051404094, 0.10851975249881009, 0.10376011423131842]
Actions to choose Agent 1: dict_values([{'num_count': 329, 'sum_payoffs': 117.24922215683061, 'action': [0.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 145.72642945168892, 'action': [1.0, 0.0]}, {'num_count': 317, 'sum_payoffs': 111.70411393909708, 'action': [0.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 113.07760620694533, 'action': [0.0, 0.0]}, {'num_count': 364, 'sum_payoffs': 133.55616223357688, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 141.03477273658143, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15659209900047596, 0.18562589243217514, 0.15088053307948596, 0.15230842455973345, 0.17325083293669682, 0.1808662541646835]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.537218809127808 s
