Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 260, 'sum_payoffs': 92.48138529222908, 'action': [2.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 80.77307052363248, 'action': [2.0, 1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 88.75559826953103, 'action': [2.0, 0.0]}, {'num_count': 213, 'sum_payoffs': 70.44026080945484, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 80.67956726379359, 'action': [1.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 77.68963850177866, 'action': [1.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 83.11042746001743, 'action': [1.0, -1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 72.72367494192066, 'action': [0.0, 1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 72.7860581253327, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12375059495478344, 0.11185149928605426, 0.1199428843407901, 0.10138029509757258, 0.11185149928605426, 0.10899571632555925, 0.1142313184198001, 0.10376011423131842, 0.10376011423131842]
Actions to choose Agent 1: dict_values([{'num_count': 392, 'sum_payoffs': 147.01877931259975, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 143.13238916718242, 'action': [1.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 104.09781598480208, 'action': [0.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 144.21106141756383, 'action': [1.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 104.52569877800991, 'action': [0.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 121.17838415383238, 'action': [0.0, 0.0]}])
Weights num count: [0.1865778200856735, 0.18277010947168015, 0.14278914802475012, 0.18372203712517848, 0.1432651118514993, 0.1603998096144693]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.63301157951355 s
