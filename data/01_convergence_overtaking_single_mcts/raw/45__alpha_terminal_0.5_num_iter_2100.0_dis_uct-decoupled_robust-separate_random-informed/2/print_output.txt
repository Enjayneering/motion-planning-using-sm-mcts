Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 253, 'sum_payoffs': 89.57164866712075, 'action': [2.0, -1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 79.18208607988916, 'action': [0.0, 0.0]}, {'num_count': 224, 'sum_payoffs': 75.7633296977396, 'action': [1.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 85.78585942176848, 'action': [1.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 82.94917808717709, 'action': [2.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 77.08858183383245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 252, 'sum_payoffs': 89.01505561800305, 'action': [2.0, 0.0]}, {'num_count': 207, 'sum_payoffs': 67.89928252225303, 'action': [0.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 74.9129459398231, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12041884816753927, 0.1099476439790576, 0.10661589719181343, 0.11661113755354593, 0.11375535459305093, 0.10804378867206092, 0.1199428843407901, 0.09852451213707758, 0.10566396953831508]
Actions to choose Agent 1: dict_values([{'num_count': 319, 'sum_payoffs': 111.88829394048327, 'action': [0.0, 1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 110.82516745578539, 'action': [0.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 138.38278730839676, 'action': [1.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 144.46737920178091, 'action': [1.0, -1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 112.72187025163102, 'action': [0.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 139.23322073544097, 'action': [1.0, 0.0]}])
Weights num count: [0.1518324607329843, 0.15088053307948596, 0.1789623988576868, 0.18514992860542598, 0.15278438838648262, 0.17991432651118516]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.497544765472412 s
