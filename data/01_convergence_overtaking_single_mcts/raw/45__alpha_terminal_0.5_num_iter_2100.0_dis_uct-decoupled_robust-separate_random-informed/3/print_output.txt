Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 211, 'sum_payoffs': 70.01137729311992, 'action': [0.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 84.16287375438405, 'action': [1.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 83.20587949664808, 'action': [1.0, 1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 76.43278746441649, 'action': [0.0, 0.0]}, {'num_count': 250, 'sum_payoffs': 88.24948363758809, 'action': [2.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 78.36850300400185, 'action': [1.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 87.39514691494146, 'action': [2.0, -1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 82.27573415692156, 'action': [2.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 74.21132931874355, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10042836744407425, 0.11470728224654926, 0.11375535459305093, 0.10709186101856259, 0.11899095668729176, 0.10899571632555925, 0.11803902903379343, 0.1128034269395526, 0.10471204188481675]
Actions to choose Agent 1: dict_values([{'num_count': 308, 'sum_payoffs': 106.87106366676136, 'action': [0.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 119.77898713568513, 'action': [0.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 150.30691606285157, 'action': [1.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 142.27957718924907, 'action': [1.0, 0.0]}, {'num_count': 313, 'sum_payoffs': 109.216880999745, 'action': [0.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 130.06194455836766, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14659685863874344, 0.15992384578772012, 0.190861494526416, 0.18277010947168015, 0.1489766777724893, 0.1703950499762018]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.560285091400146 s
