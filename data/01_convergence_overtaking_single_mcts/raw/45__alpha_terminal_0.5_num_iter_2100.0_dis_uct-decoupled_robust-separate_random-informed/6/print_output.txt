Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 220, 'sum_payoffs': 74.03498781890967, 'action': [0.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 82.08225759337574, 'action': [1.0, -1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 73.99228414702326, 'action': [1.0, 0.0]}, {'num_count': 255, 'sum_payoffs': 90.70224902153433, 'action': [2.0, -1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 86.87896056545664, 'action': [2.0, 1.5707963267948966]}, {'num_count': 217, 'sum_payoffs': 72.67003879617424, 'action': [0.0, -1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 84.44574629618876, 'action': [2.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 75.48082784428797, 'action': [0.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 83.08368496204203, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10471204188481675, 0.1128034269395526, 0.10471204188481675, 0.1213707758210376, 0.11756306520704426, 0.10328415040456926, 0.11518324607329843, 0.10613993336506425, 0.11375535459305093]
Actions to choose Agent 1: dict_values([{'num_count': 321, 'sum_payoffs': 113.33379229795455, 'action': [0.0, 1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 108.76188142278622, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 135.98054993727138, 'action': [1.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 114.73798325547565, 'action': [0.0, -1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 151.29048604892606, 'action': [1.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 137.0177470529858, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15278438838648262, 0.14802475011899097, 0.17610661589719181, 0.15421227986673014, 0.19133745835316515, 0.17705854355069015]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.696377754211426 s
