Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 259, 'sum_payoffs': 92.19584239531352, 'action': [1.0, -1.5707963267948966]}, {'num_count': 210, 'sum_payoffs': 69.18419256047706, 'action': [0.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 74.73162893482893, 'action': [0.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 85.45788208684625, 'action': [2.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 75.05575385450274, 'action': [1.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 82.24763245651684, 'action': [2.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 80.79026847209077, 'action': [1.0, 1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 77.03668288916177, 'action': [0.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 83.72193992280654, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.12327463112803427, 0.09995240361732509, 0.10566396953831508, 0.11661113755354593, 0.10613993336506425, 0.11327939076630177, 0.11185149928605426, 0.10804378867206092, 0.11470728224654926]
Actions to choose Agent 1: dict_values([{'num_count': 389, 'sum_payoffs': 145.55857779031595, 'action': [1.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 110.23081229965263, 'action': [0.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 116.85514553226655, 'action': [0.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 142.0975584177762, 'action': [1.0, -1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 109.97370601514082, 'action': [0.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 138.4911388556065, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18514992860542598, 0.14945264159923846, 0.1561161351737268, 0.18181818181818182, 0.1489766777724893, 0.17801047120418848]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 12.462069988250732 s
