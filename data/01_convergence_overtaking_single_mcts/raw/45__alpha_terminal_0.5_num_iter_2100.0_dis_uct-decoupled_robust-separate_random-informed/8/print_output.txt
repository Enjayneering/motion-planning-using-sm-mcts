Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 215, 'sum_payoffs': 71.14384683621337, 'action': [0.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 80.36468413819742, 'action': [0.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 79.53863692370442, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 82.1785534553822, 'action': [1.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 80.95593176113637, 'action': [2.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 86.00539352123232, 'action': [2.0, -1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 86.15784707916947, 'action': [2.0, 0.0]}, {'num_count': 224, 'sum_payoffs': 75.32041650369129, 'action': [0.0, -1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 75.26461597547488, 'action': [1.0, 0.0]}])
Weights num count: [0.10233222275107091, 0.11185149928605426, 0.11089957163255593, 0.11375535459305093, 0.11232746311280342, 0.11756306520704426, 0.11756306520704426, 0.10661589719181343, 0.10661589719181343]
Actions to choose Agent 1: dict_values([{'num_count': 304, 'sum_payoffs': 106.52088646428629, 'action': [0.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 142.66815264353232, 'action': [1.0, 1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 118.6194564295555, 'action': [0.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 145.4768654972421, 'action': [1.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 113.75828864308839, 'action': [0.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 141.2521434200945, 'action': [1.0, 0.0]}])
Weights num count: [0.14469300333174678, 0.18134221799143266, 0.15706806282722513, 0.18419800095192765, 0.15230842455973345, 0.17991432651118516]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.53937315940857 s
