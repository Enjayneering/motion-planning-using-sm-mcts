Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 247, 'sum_payoffs': 86.66737964383951, 'action': [2.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 86.43766788618294, 'action': [2.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 85.23232741064461, 'action': [1.0, -1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 76.49906279099712, 'action': [0.0, 0.0]}, {'num_count': 225, 'sum_payoffs': 76.2328597652145, 'action': [1.0, 0.0]}, {'num_count': 244, 'sum_payoffs': 85.12924598450547, 'action': [2.0, 0.0]}, {'num_count': 211, 'sum_payoffs': 69.69458112070744, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 82.71813872301657, 'action': [1.0, 1.5707963267948966]}, {'num_count': 217, 'sum_payoffs': 72.47391614932364, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11756306520704426, 0.11756306520704426, 0.11613517372679677, 0.10756782484531176, 0.10709186101856259, 0.11613517372679677, 0.10042836744407425, 0.11375535459305093, 0.10328415040456926]
Actions to choose Agent 1: dict_values([{'num_count': 316, 'sum_payoffs': 111.24643510655066, 'action': [0.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 107.5791036143852, 'action': [0.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 142.96332076216865, 'action': [1.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 144.91877787944188, 'action': [1.0, 0.0]}, {'num_count': 325, 'sum_payoffs': 115.43800225926009, 'action': [0.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 140.71655453961398, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1504045692527368, 0.14659685863874344, 0.18277010947168015, 0.1846739647786768, 0.1546882436934793, 0.18039029033793433]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.44153094291687 s
