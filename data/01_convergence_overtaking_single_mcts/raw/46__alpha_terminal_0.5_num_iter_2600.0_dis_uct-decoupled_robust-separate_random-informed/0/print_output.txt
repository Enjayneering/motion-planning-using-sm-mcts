Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 263, 'sum_payoffs': 87.22879519773318, 'action': [0.0, 1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 108.83931742725599, 'action': [2.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 101.47950628662143, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 91.80189311133365, 'action': [1.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 110.64365749914887, 'action': [2.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 102.38906314688221, 'action': [1.0, 1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 107.84130848726927, 'action': [2.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 91.2781259774033, 'action': [0.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 90.4295341579074, 'action': [0.0, 0.0]}])
Weights num count: [0.10111495578623607, 0.11918492887351019, 0.11303344867358708, 0.104959630911188, 0.12072279892349097, 0.11380238369857747, 0.1184159938485198, 0.10457516339869281, 0.10380622837370242]
Actions to choose Agent 1: dict_values([{'num_count': 475, 'sum_payoffs': 176.04526959426806, 'action': [1.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 126.39959275370062, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 146.75025102903805, 'action': [0.0, 0.0]}, {'num_count': 487, 'sum_payoffs': 181.52407625184097, 'action': [1.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 141.33118533252684, 'action': [0.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 169.99417982331317, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18262206843521722, 0.14071510957324107, 0.1580161476355248, 0.18723567858515955, 0.15340253748558247, 0.1776239907727797]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.133759021759033 s
