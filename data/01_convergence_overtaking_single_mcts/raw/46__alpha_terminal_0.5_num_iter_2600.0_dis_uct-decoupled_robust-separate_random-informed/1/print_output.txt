Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 271, 'sum_payoffs': 91.26483636541184, 'action': [0.0, -1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 86.26676798758439, 'action': [0.0, 1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 112.20635568018822, 'action': [2.0, -1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 109.3616342759135, 'action': [2.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 99.1426795234168, 'action': [1.0, -1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 112.11951751393241, 'action': [2.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 94.11293471387103, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 96.93795023362375, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 94.97876127613988, 'action': [0.0, 0.0]}])
Weights num count: [0.10419069588619762, 0.09996155324875049, 0.12149173394848135, 0.11918492887351019, 0.11072664359861592, 0.12149173394848135, 0.10649750096116878, 0.10880430603613994, 0.10726643598615918]
Actions to choose Agent 1: dict_values([{'num_count': 393, 'sum_payoffs': 137.77945678280753, 'action': [0.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 139.05775877854438, 'action': [0.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 189.84455060367185, 'action': [1.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 134.9515226297694, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 163.09992216919338, 'action': [1.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 171.73251467544387, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1510957324106113, 0.1522491349480969, 0.19492502883506344, 0.14878892733564014, 0.17262591311034217, 0.17993079584775087]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.196951150894165 s
