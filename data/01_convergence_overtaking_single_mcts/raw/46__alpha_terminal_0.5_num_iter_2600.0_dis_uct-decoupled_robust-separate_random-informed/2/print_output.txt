Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 300, 'sum_payoffs': 104.02745795170144, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 91.67333632783402, 'action': [0.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 108.15331359874499, 'action': [2.0, 0.0]}, {'num_count': 269, 'sum_payoffs': 89.81266770871696, 'action': [0.0, 1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 110.49195823962692, 'action': [2.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 96.58869199049846, 'action': [1.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 91.20157022128033, 'action': [0.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 95.32788162460476, 'action': [1.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 103.01287488354293, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11534025374855825, 0.104959630911188, 0.11880046136101499, 0.10342176086120723, 0.12072279892349097, 0.10918877354863514, 0.10457516339869281, 0.10803537101114956, 0.11457131872356786]
Actions to choose Agent 1: dict_values([{'num_count': 472, 'sum_payoffs': 174.66615677843694, 'action': [1.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 173.24055994896852, 'action': [1.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 130.47329347584778, 'action': [0.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 142.7145073185803, 'action': [0.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 129.56653497989626, 'action': [0.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 191.90106929858322, 'action': [1.0, 0.0]}])
Weights num count: [0.18146866589773164, 0.18031526336024606, 0.14417531718569782, 0.15455594002306805, 0.14340638216070742, 0.19569396386005383]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.494721412658691 s
