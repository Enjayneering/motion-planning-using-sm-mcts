Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 262, 'sum_payoffs': 86.98567630760833, 'action': [0.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 98.44973905442164, 'action': [1.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 107.66947017059174, 'action': [2.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 93.2701145555789, 'action': [0.0, 0.0]}, {'num_count': 299, 'sum_payoffs': 103.89685155876388, 'action': [1.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 106.20690857010949, 'action': [2.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 96.62915180429923, 'action': [1.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 96.5914959779508, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 103.94925965283787, 'action': [2.0, 0.0]}])
Weights num count: [0.10073048827374087, 0.11034217608612072, 0.11803152633602461, 0.1061130334486736, 0.11495578623606305, 0.11687812379853903, 0.10880430603613994, 0.10880430603613994, 0.11495578623606305]
Actions to choose Agent 1: dict_values([{'num_count': 394, 'sum_payoffs': 139.16366433325138, 'action': [0.0, 0.0]}, {'num_count': 388, 'sum_payoffs': 136.39228552966915, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 187.88373591665032, 'action': [1.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 179.06357048076777, 'action': [1.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 132.76236182009077, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 167.9885613114061, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1514801999231065, 0.14917339484813533, 0.1922337562475971, 0.18492887351018839, 0.14609765474817377, 0.17570165321030373]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.480628728866577 s
