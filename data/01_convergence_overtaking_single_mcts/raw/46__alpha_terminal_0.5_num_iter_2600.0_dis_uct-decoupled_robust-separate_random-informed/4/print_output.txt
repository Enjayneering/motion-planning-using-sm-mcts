Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 305, 'sum_payoffs': 106.8265312203481, 'action': [2.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 102.62146432565524, 'action': [1.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 91.62974242132753, 'action': [0.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 94.81838174647174, 'action': [1.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 108.67397854098603, 'action': [2.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 101.74363115661131, 'action': [1.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 114.73423231895947, 'action': [2.0, 0.0]}, {'num_count': 261, 'sum_payoffs': 86.59079780333475, 'action': [0.0, 1.5707963267948966]}, {'num_count': 262, 'sum_payoffs': 86.9654998414833, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11726259131103421, 0.11380238369857747, 0.10457516339869281, 0.10726643598615918, 0.11880046136101499, 0.11303344867358708, 0.12379853902345252, 0.10034602076124567, 0.10073048827374087]
Actions to choose Agent 1: dict_values([{'num_count': 375, 'sum_payoffs': 130.78544022665878, 'action': [0.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 181.08531970440336, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 182.9863038690079, 'action': [1.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 175.13910276332416, 'action': [1.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 142.1285572644209, 'action': [0.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 132.5635578770619, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14417531718569782, 0.18646674356016918, 0.18800461361014995, 0.18146866589773164, 0.15378700499807765, 0.14571318723567858]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.425625562667847 s
