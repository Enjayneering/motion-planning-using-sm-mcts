Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 273, 'sum_payoffs': 91.65841440906902, 'action': [0.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 102.13288104733707, 'action': [1.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 97.53812966211623, 'action': [0.0, 0.0]}, {'num_count': 262, 'sum_payoffs': 86.6734223116152, 'action': [0.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 102.23766886226186, 'action': [2.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 95.27074757921663, 'action': [1.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 99.02694114653184, 'action': [1.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 109.65125342272707, 'action': [2.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 106.41310002531998, 'action': [2.0, 0.0]}])
Weights num count: [0.104959630911188, 0.11380238369857747, 0.10995770857362552, 0.10073048827374087, 0.11380238369857747, 0.10803537101114956, 0.1111111111111111, 0.11995386389850057, 0.11726259131103421]
Actions to choose Agent 1: dict_values([{'num_count': 389, 'sum_payoffs': 136.852560366343, 'action': [0.0, 0.0]}, {'num_count': 474, 'sum_payoffs': 175.8327706652974, 'action': [1.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 173.5420021417623, 'action': [1.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 133.72400252058563, 'action': [0.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 135.99020583248156, 'action': [0.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 187.3775203460927, 'action': [1.0, 0.0]}])
Weights num count: [0.14955786236063054, 0.18223760092272204, 0.18031526336024606, 0.14686658977316416, 0.14878892733564014, 0.19184928873510187]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.81000018119812 s
