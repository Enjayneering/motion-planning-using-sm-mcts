Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 289, 'sum_payoffs': 99.12691811024627, 'action': [1.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 96.53793894674523, 'action': [1.0, 0.0]}, {'num_count': 299, 'sum_payoffs': 103.78090064825068, 'action': [2.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 92.84414021873627, 'action': [0.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 106.07499101390847, 'action': [2.0, 0.0]}, {'num_count': 305, 'sum_payoffs': 106.68135540191162, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 91.87379933856742, 'action': [0.0, -1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 108.55460682008342, 'action': [2.0, -1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 87.33665038690539, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1111111111111111, 0.10880430603613994, 0.11495578623606305, 0.1057285659361784, 0.11687812379853903, 0.11726259131103421, 0.104959630911188, 0.11880046136101499, 0.10111495578623607]
Actions to choose Agent 1: dict_values([{'num_count': 390, 'sum_payoffs': 136.97290388383254, 'action': [0.0, 0.0]}, {'num_count': 493, 'sum_payoffs': 183.92314379412582, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 166.91062827413504, 'action': [1.0, -1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 177.49995876175052, 'action': [1.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 133.72337929566513, 'action': [0.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 140.95786335673478, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14994232987312572, 0.1895424836601307, 0.17531718569780855, 0.184159938485198, 0.14725105728565935, 0.15340253748558247]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.329975366592407 s
