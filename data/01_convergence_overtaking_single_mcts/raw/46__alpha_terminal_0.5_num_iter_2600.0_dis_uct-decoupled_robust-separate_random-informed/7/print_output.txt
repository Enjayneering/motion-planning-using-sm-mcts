Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 306, 'sum_payoffs': 107.79275856780531, 'action': [2.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 86.04428826182233, 'action': [0.0, 1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 111.5288979782742, 'action': [2.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 105.37333723139753, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 92.48659634164933, 'action': [0.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 104.9592325549887, 'action': [2.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 96.98718221421993, 'action': [0.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 94.26257784743663, 'action': [1.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 98.9608893460174, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11764705882352941, 0.09957708573625529, 0.12072279892349097, 0.11572472126105345, 0.104959630911188, 0.11534025374855825, 0.10880430603613994, 0.10649750096116878, 0.11034217608612072]
Actions to choose Agent 1: dict_values([{'num_count': 401, 'sum_payoffs': 141.88478841058281, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 181.62818080383403, 'action': [1.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 144.49535709332403, 'action': [0.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 135.4807151383297, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 160.52317633307422, 'action': [1.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 175.66028182660347, 'action': [1.0, 0.0]}])
Weights num count: [0.15417147251057287, 0.18762014609765476, 0.15647827758554403, 0.14878892733564014, 0.16993464052287582, 0.18262206843521722]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.518160820007324 s
