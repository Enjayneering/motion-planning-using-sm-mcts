Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 318, 'sum_payoffs': 112.96287562951757, 'action': [2.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 103.6412264326053, 'action': [2.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 97.24553735966917, 'action': [1.0, 1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 108.88564331020306, 'action': [2.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 94.44781725402642, 'action': [1.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 90.8056563691888, 'action': [0.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 91.3493806224216, 'action': [0.0, 0.0]}, {'num_count': 307, 'sum_payoffs': 107.94414795731282, 'action': [1.0, -1.5707963267948966]}, {'num_count': 265, 'sum_payoffs': 88.57494833496443, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12226066897347174, 0.11457131872356786, 0.10918877354863514, 0.11880046136101499, 0.10688196847366398, 0.10380622837370242, 0.10419069588619762, 0.11803152633602461, 0.10188389081122645]
Actions to choose Agent 1: dict_values([{'num_count': 398, 'sum_payoffs': 140.24821918772878, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 137.93027973155685, 'action': [0.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 137.5620761867226, 'action': [0.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 168.82258620180474, 'action': [1.0, -1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 178.16487113642057, 'action': [1.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 175.3703645966467, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15301806997308728, 0.1510957324106113, 0.15071126489811612, 0.1772395232602845, 0.18492887351018839, 0.18262206843521722]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.365612983703613 s
