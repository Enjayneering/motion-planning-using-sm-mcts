Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 343, 'sum_payoffs': 117.2199916646422, 'action': [1.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 103.2756500863795, 'action': [0.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 104.6386315449732, 'action': [0.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 130.36167042857477, 'action': [2.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 122.59281432750012, 'action': [1.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 128.6066276469432, 'action': [2.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 122.2007722726488, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 116.7423743239876, 'action': [1.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 115.47845604092316, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11060948081264109, 0.10061270557884554, 0.10158013544018059, 0.1199613028055466, 0.11447920025798129, 0.11867139632376653, 0.11415672363753628, 0.11028700419219607, 0.10931957433086101]
Actions to choose Agent 1: dict_values([{'num_count': 461, 'sum_payoffs': 161.3282705598085, 'action': [0.0, 0.0]}, {'num_count': 555, 'sum_payoffs': 203.35197049166905, 'action': [1.0, -1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 215.18261392561908, 'action': [1.0, 0.0]}, {'num_count': 596, 'sum_payoffs': 221.97512871147276, 'action': [1.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 161.2542508828391, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 154.69325183848085, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14866172202515318, 0.17897452434698485, 0.1873589164785553, 0.19219606578523057, 0.14866172202515318, 0.1438245727184779]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.152268171310425 s
