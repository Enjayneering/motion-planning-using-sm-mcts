Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 338, 'sum_payoffs': 114.8627943851433, 'action': [1.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 110.36285276676035, 'action': [0.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 124.9147571493832, 'action': [2.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 127.52388784600915, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 128.0174300612207, 'action': [2.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 111.35894822638657, 'action': [0.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 113.617565919192, 'action': [1.0, 0.0]}, {'num_count': 308, 'sum_payoffs': 101.56326718209112, 'action': [0.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 128.5889188248156, 'action': [2.0, 0.0]}])
Weights num count: [0.10899709771041599, 0.10577233150596582, 0.11609158336020639, 0.11802644308287649, 0.11834891970332151, 0.10641728474685586, 0.10802966784908094, 0.09932279909706546, 0.11867139632376653]
Actions to choose Agent 1: dict_values([{'num_count': 470, 'sum_payoffs': 165.77115944290918, 'action': [0.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 172.1747421709106, 'action': [0.0, 0.0]}, {'num_count': 577, 'sum_payoffs': 214.11458246359507, 'action': [1.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 162.8161060404222, 'action': [0.0, -1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 207.83588852841916, 'action': [1.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 198.7284950301837, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15156401160915833, 0.1560786842953886, 0.18606900999677523, 0.14930667526604322, 0.181554337310545, 0.17510480490164462]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.005725383758545 s
