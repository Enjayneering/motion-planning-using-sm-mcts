Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 332, 'sum_payoffs': 112.6953866889356, 'action': [0.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 115.79577149587965, 'action': [1.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 123.15420489633452, 'action': [2.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 108.19611088288302, 'action': [0.0, 1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 113.06289284812212, 'action': [0.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 114.48180730793428, 'action': [1.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 118.91789070713035, 'action': [1.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 125.33524609005981, 'action': [2.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 133.12795166623673, 'action': [2.0, 0.0]}])
Weights num count: [0.10706223798774589, 0.10931957433086101, 0.11447920025798129, 0.1038374717832957, 0.1073847146081909, 0.10835214446952596, 0.11157691067397614, 0.11609158336020639, 0.12157368590777169]
Actions to choose Agent 1: dict_values([{'num_count': 462, 'sum_payoffs': 162.97320687912932, 'action': [0.0, 1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 215.73788816485518, 'action': [1.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 196.6612976800751, 'action': [1.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 164.67211281806556, 'action': [0.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 179.05447914494536, 'action': [0.0, 0.0]}, {'num_count': 558, 'sum_payoffs': 206.05902985985435, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1489841986455982, 0.18671396323766526, 0.17316994517897452, 0.15027410512737827, 0.16059335698161883, 0.1799419542083199]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.83245277404785 s
