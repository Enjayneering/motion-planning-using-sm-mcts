Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 328, 'sum_payoffs': 109.8785322474184, 'action': [1.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 134.3123474700251, 'action': [2.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 117.11999267102641, 'action': [1.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 126.49143246910788, 'action': [2.0, 1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 106.35445256584359, 'action': [0.0, -1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 106.86681554232298, 'action': [0.0, 0.0]}, {'num_count': 347, 'sum_payoffs': 118.39785040717, 'action': [1.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 103.1514255568342, 'action': [0.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 133.29895849897858, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10577233150596582, 0.12318606900999678, 0.1109319574330861, 0.11770396646243148, 0.10319251854240567, 0.10351499516285069, 0.11189938729442116, 0.10093518219929055, 0.12254111576910674]
Actions to choose Agent 1: dict_values([{'num_count': 492, 'sum_payoffs': 175.92962743630525, 'action': [0.0, 1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 203.88652692453226, 'action': [1.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 161.5942976091226, 'action': [0.0, -1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 208.4493584729625, 'action': [1.0, -1.5707963267948966]}, {'num_count': 589, 'sum_payoffs': 219.84108335458492, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 153.21184838559833, 'action': [0.0, 0.0]}])
Weights num count: [0.15865849725894873, 0.17865204772653984, 0.14833924540470816, 0.18187681393099, 0.18993872944211546, 0.14221218961625282]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.03347635269165 s
