Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 366, 'sum_payoffs': 127.8605612938837, 'action': [2.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 125.22708319390495, 'action': [2.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 134.25854155453519, 'action': [2.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 116.0857506280828, 'action': [1.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 120.66165806941356, 'action': [1.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 104.89911900717274, 'action': [0.0, 1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 120.27062179526204, 'action': [1.0, 1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 105.37155459422924, 'action': [0.0, 0.0]}, {'num_count': 324, 'sum_payoffs': 108.99737634843392, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11802644308287649, 0.11609158336020639, 0.12254111576910674, 0.10964205095130602, 0.11286681715575621, 0.10158013544018059, 0.11254434053531119, 0.1019026120606256, 0.10448242502418574]
Actions to choose Agent 1: dict_values([{'num_count': 467, 'sum_payoffs': 164.7454663045448, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 160.80553447206012, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 164.66583303118952, 'action': [0.0, 1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 208.0355801238, 'action': [1.0, -1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 212.135091023631, 'action': [1.0, 0.0]}, {'num_count': 573, 'sum_payoffs': 212.5101243071421, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15059658174782328, 0.14769429216381813, 0.15059658174782328, 0.181554337310545, 0.18445662689455014, 0.18477910351499516]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.064648866653442 s
