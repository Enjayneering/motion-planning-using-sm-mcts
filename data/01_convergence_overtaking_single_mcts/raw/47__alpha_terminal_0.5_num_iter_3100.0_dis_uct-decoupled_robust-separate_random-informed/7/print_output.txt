Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 326, 'sum_payoffs': 109.4945942196127, 'action': [0.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 110.32515669444597, 'action': [1.0, 0.0]}, {'num_count': 326, 'sum_payoffs': 109.35419327575549, 'action': [0.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 122.37129090861755, 'action': [1.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 119.35329201399674, 'action': [1.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 121.45289479735217, 'action': [2.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 135.6162255961604, 'action': [2.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 108.13441338854963, 'action': [0.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 123.34098932943049, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10512737826507579, 0.10577233150596582, 0.10512737826507579, 0.11447920025798129, 0.11222186391486617, 0.11383424701709126, 0.12383102225088681, 0.10415994840374072, 0.11512415349887133]
Actions to choose Agent 1: dict_values([{'num_count': 456, 'sum_payoffs': 159.35691680627463, 'action': [0.0, 1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 191.9400464007408, 'action': [1.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 163.25711016202598, 'action': [0.0, 0.0]}, {'num_count': 500, 'sum_payoffs': 178.88122290986098, 'action': [0.0, -1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 210.06743410357998, 'action': [1.0, -1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 215.56343096206638, 'action': [1.0, 0.0]}])
Weights num count: [0.1470493389229281, 0.1705901322154144, 0.14995162850693325, 0.16123831022250887, 0.1834891970332151, 0.1873589164785553]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.225898504257202 s
