Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 324, 'sum_payoffs': 108.38337681924324, 'action': [0.0, 0.0]}, {'num_count': 369, 'sum_payoffs': 128.78358043994885, 'action': [2.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 129.11119936287434, 'action': [2.0, -1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 104.94754038095257, 'action': [0.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 117.35599273741144, 'action': [1.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 119.64987023656893, 'action': [1.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 110.66342050042597, 'action': [1.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 128.33383198753, 'action': [2.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 111.49946041814988, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10448242502418574, 0.11899387294421154, 0.11931634956465656, 0.1019026120606256, 0.1109319574330861, 0.11254434053531119, 0.10609480812641084, 0.11867139632376653, 0.10673976136730087]
Actions to choose Agent 1: dict_values([{'num_count': 457, 'sum_payoffs': 159.22735964138306, 'action': [0.0, 1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 198.48422282209543, 'action': [1.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 165.04609706052588, 'action': [0.0, -1.5707963267948966]}, {'num_count': 556, 'sum_payoffs': 203.51379725527786, 'action': [1.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 164.4711983514773, 'action': [0.0, 0.0]}, {'num_count': 603, 'sum_payoffs': 224.71039060905386, 'action': [1.0, 0.0]}])
Weights num count: [0.1473718155433731, 0.17574975814253466, 0.15156401160915833, 0.17929700096742987, 0.15124153498871332, 0.1944534021283457]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.03048276901245 s
