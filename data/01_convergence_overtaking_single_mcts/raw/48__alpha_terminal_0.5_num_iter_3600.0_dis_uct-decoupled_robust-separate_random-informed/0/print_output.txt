Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 361, 'sum_payoffs': 118.61894461555465, 'action': [0.0, 1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 151.1703045671248, 'action': [2.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 141.285677405959, 'action': [1.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 149.83411292415252, 'action': [2.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 134.6655112020476, 'action': [1.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 129.06175001116085, 'action': [0.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 132.17973486278046, 'action': [1.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 122.07637540517507, 'action': [0.0, -1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 142.23198329136616, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10024993057484032, 0.12079977783948903, 0.11469036378783672, 0.11996667592335462, 0.11052485420716468, 0.10691474590391557, 0.10885865037489587, 0.10247153568453207, 0.11524576506525964]
Actions to choose Agent 1: dict_values([{'num_count': 647, 'sum_payoffs': 236.5032296229317, 'action': [1.0, 1.5707963267948966]}, {'num_count': 688, 'sum_payoffs': 254.88427471790797, 'action': [1.0, 0.0]}, {'num_count': 651, 'sum_payoffs': 238.17180499963877, 'action': [1.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 189.31794802589843, 'action': [0.0, -1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 188.83889852921172, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 187.14938569313918, 'action': [0.0, 0.0]}])
Weights num count: [0.17967231324632046, 0.1910580394334907, 0.18078311580116635, 0.14995834490419327, 0.1496806442654818, 0.14856984171063595]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.77962374687195 s
