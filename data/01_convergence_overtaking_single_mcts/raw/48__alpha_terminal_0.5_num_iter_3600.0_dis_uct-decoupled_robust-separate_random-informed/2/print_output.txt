Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 423, 'sum_payoffs': 145.7262489617305, 'action': [2.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 127.72127392016361, 'action': [0.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 137.21618622026153, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 126.36163609108308, 'action': [0.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 149.30709025216393, 'action': [2.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 117.99472527010799, 'action': [0.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 139.08482563804046, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 132.45772778877978, 'action': [1.0, 1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 144.24581162298753, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1174673701749514, 0.10608164398778117, 0.1121910580394335, 0.10524854207164676, 0.11968897528464316, 0.09997222993612885, 0.11330186059427937, 0.10913635101360733, 0.116634268258817]
Actions to choose Agent 1: dict_values([{'num_count': 685, 'sum_payoffs': 254.33096729547924, 'action': [1.0, 0.0]}, {'num_count': 541, 'sum_payoffs': 190.36271723652342, 'action': [0.0, -1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 231.90125249264577, 'action': [1.0, -1.5707963267948966]}, {'num_count': 578, 'sum_payoffs': 206.71036375068624, 'action': [0.0, 0.0]}, {'num_count': 640, 'sum_payoffs': 234.2293732650713, 'action': [1.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 181.63760260544814, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.19022493751735628, 0.15023604554290476, 0.17633990558178284, 0.1605109691752291, 0.17772840877534019, 0.14468203276867536]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.720466136932373 s
