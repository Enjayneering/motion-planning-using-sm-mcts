Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 376, 'sum_payoffs': 125.0187723736875, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 140.34648806122598, 'action': [1.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 150.96797458374039, 'action': [2.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 118.48545784563656, 'action': [0.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 138.10505139056687, 'action': [1.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 133.70115800590088, 'action': [1.0, 0.0]}, {'num_count': 369, 'sum_payoffs': 121.95195528198103, 'action': [0.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 143.0403873417096, 'action': [2.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 148.28417930556304, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10441544015551235, 0.11413496251041377, 0.12079977783948903, 0.10024993057484032, 0.11274645931685642, 0.10996945292974174, 0.10247153568453207, 0.11580116634268259, 0.11913357400722022]
Actions to choose Agent 1: dict_values([{'num_count': 551, 'sum_payoffs': 194.6340846737236, 'action': [0.0, 1.5707963267948966]}, {'num_count': 642, 'sum_payoffs': 234.83385904426254, 'action': [1.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 202.95354782552533, 'action': [0.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 181.95281987800251, 'action': [0.0, -1.5707963267948966]}, {'num_count': 687, 'sum_payoffs': 255.12661383771973, 'action': [1.0, 0.0]}, {'num_count': 628, 'sum_payoffs': 228.6709792195295, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15301305193001943, 0.17828381005276311, 0.15828936406553734, 0.14495973340738683, 0.19078033879477924, 0.17439600111080256]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.94851851463318 s
