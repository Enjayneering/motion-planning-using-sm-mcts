Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 393, 'sum_payoffs': 132.74255026855633, 'action': [1.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 124.8072015829613, 'action': [0.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 150.52313276188664, 'action': [2.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 136.78280083508204, 'action': [1.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 148.16866725843119, 'action': [1.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 117.43034466778776, 'action': [0.0, 1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 150.48582540781882, 'action': [2.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 121.76631816364531, 'action': [0.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 140.2772205112469, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10913635101360733, 0.10413773951680089, 0.12024437656206609, 0.11163565676201055, 0.11885587336850875, 0.09941682865870592, 0.12024437656206609, 0.10219383504582061, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 649, 'sum_payoffs': 236.84844922305123, 'action': [1.0, -1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 189.2273332348392, 'action': [0.0, 1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 258.29325650659933, 'action': [1.0, 0.0]}, {'num_count': 547, 'sum_payoffs': 191.76429057941968, 'action': [0.0, 0.0]}, {'num_count': 636, 'sum_payoffs': 231.13744414280166, 'action': [1.0, 1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 184.53882531710167, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1802277145237434, 0.15023604554290476, 0.1935573451818939, 0.15190224937517358, 0.1766176062204943, 0.1471813385170786]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.8747079372406 s
