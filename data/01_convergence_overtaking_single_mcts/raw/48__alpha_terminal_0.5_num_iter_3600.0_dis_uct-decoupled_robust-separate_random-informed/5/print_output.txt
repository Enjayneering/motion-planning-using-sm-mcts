Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 424, 'sum_payoffs': 146.29595945391696, 'action': [2.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 149.8814861210812, 'action': [2.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 138.31866658954306, 'action': [2.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 139.98572063724092, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 143.5422204234954, 'action': [1.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 124.67098412250549, 'action': [0.0, -1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 130.73893707428556, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 134.79482527197052, 'action': [1.0, 1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 112.91142067336459, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11774507081366287, 0.11996667592335462, 0.11274645931685642, 0.11385726187170231, 0.11607886698139405, 0.10413773951680089, 0.10802554845876146, 0.11052485420716468, 0.09663982227159122]
Actions to choose Agent 1: dict_values([{'num_count': 654, 'sum_payoffs': 240.51002104524352, 'action': [1.0, -1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 196.1954157392175, 'action': [0.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 180.68471001579127, 'action': [0.0, -1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 193.88163059018578, 'action': [0.0, 1.5707963267948966]}, {'num_count': 678, 'sum_payoffs': 251.21105864691893, 'action': [1.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 236.8593134057534, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18161621771730074, 0.15384615384615385, 0.14412663149125243, 0.1524576506525965, 0.188281033046376, 0.179394612607609]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.111920595169067 s
