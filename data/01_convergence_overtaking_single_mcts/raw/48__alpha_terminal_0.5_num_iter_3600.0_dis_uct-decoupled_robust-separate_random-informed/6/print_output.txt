Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 395, 'sum_payoffs': 133.27951437554896, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 139.9270010441313, 'action': [1.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 132.82973818148577, 'action': [1.0, 1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 116.14929947476986, 'action': [0.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 148.8427336333782, 'action': [2.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 147.0344105337052, 'action': [2.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 157.18005522354062, 'action': [2.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 118.90990514251769, 'action': [0.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 125.91429171765917, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10969175229103027, 0.11385726187170231, 0.1094140516523188, 0.09886142738128298, 0.11941127464593168, 0.11830047209108581, 0.1246875867814496, 0.1005276312135518, 0.1049708414329353]
Actions to choose Agent 1: dict_values([{'num_count': 638, 'sum_payoffs': 232.57624843365602, 'action': [1.0, -1.5707963267948966]}, {'num_count': 639, 'sum_payoffs': 232.97897854564133, 'action': [1.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 181.81196643493664, 'action': [0.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 184.33734243238226, 'action': [0.0, 1.5707963267948966]}, {'num_count': 686, 'sum_payoffs': 253.85868222425722, 'action': [1.0, 0.0]}, {'num_count': 585, 'sum_payoffs': 208.83304206494407, 'action': [0.0, 0.0]}])
Weights num count: [0.17717300749791726, 0.17745070813662872, 0.14523743404609832, 0.14690363787836713, 0.19050263815606777, 0.1624548736462094]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.942827463150024 s
