Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 353, 'sum_payoffs': 115.0190365142292, 'action': [0.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 121.56400682017023, 'action': [0.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 148.3963881941839, 'action': [2.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 151.85414266712183, 'action': [2.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 142.49289330919137, 'action': [1.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 134.96674717439933, 'action': [1.0, 1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 142.58932692419572, 'action': [2.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 125.8441752377966, 'action': [0.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 137.3177072076944, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.09802832546514857, 0.10219383504582061, 0.11913357400722022, 0.12135517911691197, 0.11552346570397112, 0.11080255484587614, 0.11552346570397112, 0.1049708414329353, 0.1121910580394335]
Actions to choose Agent 1: dict_values([{'num_count': 703, 'sum_payoffs': 261.65241516108097, 'action': [1.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 236.13110405849685, 'action': [1.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 183.55928547638678, 'action': [0.0, -1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 233.96881431338403, 'action': [1.0, 1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 192.7494441447883, 'action': [0.0, 0.0]}, {'num_count': 535, 'sum_payoffs': 187.10483616891813, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.19522354901416272, 0.179394612607609, 0.14634823660094418, 0.17800610941405165, 0.15217995001388504, 0.14856984171063595]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.903974056243896 s
