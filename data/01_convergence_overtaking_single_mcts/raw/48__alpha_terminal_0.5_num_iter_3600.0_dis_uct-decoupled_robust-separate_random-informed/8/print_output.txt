Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 423, 'sum_payoffs': 145.6910444550077, 'action': [2.0, -1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 166.54489687068147, 'action': [2.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 133.67280764748716, 'action': [1.0, 0.0]}, {'num_count': 345, 'sum_payoffs': 111.5783303948769, 'action': [0.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 125.33502000687326, 'action': [0.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 142.52838168432604, 'action': [1.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 137.6913939146246, 'action': [2.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 124.17487057828028, 'action': [0.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 132.85459460484049, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1174673701749514, 0.13051930019439045, 0.10996945292974174, 0.09580672035545681, 0.10469314079422383, 0.11552346570397112, 0.11246875867814496, 0.10386003887808942, 0.1094140516523188]
Actions to choose Agent 1: dict_values([{'num_count': 562, 'sum_payoffs': 198.47005414086854, 'action': [0.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 186.2309755304315, 'action': [0.0, 1.5707963267948966]}, {'num_count': 692, 'sum_payoffs': 256.0653348307117, 'action': [1.0, 0.0]}, {'num_count': 632, 'sum_payoffs': 229.34644078393794, 'action': [1.0, -1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 236.85377152882307, 'action': [1.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 184.9078892936208, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1560677589558456, 0.14829214107192445, 0.1921688419883366, 0.17550680366564844, 0.1802277145237434, 0.14745903915579006]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.95039653778076 s
