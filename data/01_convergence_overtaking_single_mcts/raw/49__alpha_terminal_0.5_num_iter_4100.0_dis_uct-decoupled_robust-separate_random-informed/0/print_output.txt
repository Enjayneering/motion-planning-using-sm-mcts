Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 452, 'sum_payoffs': 152.09686718624656, 'action': [2.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 151.27763873137667, 'action': [1.0, 0.0]}, {'num_count': 503, 'sum_payoffs': 174.40065454867468, 'action': [2.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 179.14635795945907, 'action': [2.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 154.29840733471875, 'action': [1.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 159.9853316019094, 'action': [1.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 134.42678399273026, 'action': [0.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 134.55080477382486, 'action': [0.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 143.5601435111616, 'action': [0.0, 0.0]}])
Weights num count: [0.11021702023896611, 0.1097293343087052, 0.12265301146061935, 0.12533528407705438, 0.11143623506461839, 0.11460619361131431, 0.10021945866861741, 0.10021945866861741, 0.10534016093635698]
Actions to choose Agent 1: dict_values([{'num_count': 610, 'sum_payoffs': 212.0813239237369, 'action': [0.0, -1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 259.814899333986, 'action': [1.0, -1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 270.7947678500224, 'action': [1.0, 1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 214.28995280363876, 'action': [0.0, 1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 219.38409033170595, 'action': [0.0, 0.0]}, {'num_count': 783, 'sum_payoffs': 287.4248379655477, 'action': [1.0, 0.0]}])
Weights num count: [0.14874420872957816, 0.1755669348939283, 0.1816630090221897, 0.14996342355523043, 0.1528895391367959, 0.19092904169714703]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.861963748931885 s
