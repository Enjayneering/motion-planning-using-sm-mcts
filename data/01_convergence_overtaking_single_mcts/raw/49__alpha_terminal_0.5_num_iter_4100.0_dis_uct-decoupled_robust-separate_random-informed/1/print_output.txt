Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 462, 'sum_payoffs': 156.7481658766631, 'action': [2.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 147.25999003159325, 'action': [1.0, 1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 171.20503426741288, 'action': [2.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 154.2188189769854, 'action': [1.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 153.34651327081323, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 140.38414460787837, 'action': [0.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 135.68920233070236, 'action': [0.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 142.53506547321922, 'action': [0.0, 0.0]}, {'num_count': 527, 'sum_payoffs': 185.1430685988406, 'action': [2.0, 0.0]}])
Weights num count: [0.11265544989027067, 0.10729090465740064, 0.12070226773957571, 0.11119239209948793, 0.11070470616922702, 0.10338941721531333, 0.10070714459887832, 0.10460863204096561, 0.1285052426237503]
Actions to choose Agent 1: dict_values([{'num_count': 744, 'sum_payoffs': 270.76510370109844, 'action': [1.0, -1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 252.04757412575674, 'action': [1.0, 1.5707963267948966]}, {'num_count': 606, 'sum_payoffs': 210.8400325659169, 'action': [0.0, -1.5707963267948966]}, {'num_count': 605, 'sum_payoffs': 210.3057424546484, 'action': [0.0, 1.5707963267948966]}, {'num_count': 781, 'sum_payoffs': 287.0908717599877, 'action': [1.0, 0.0]}, {'num_count': 663, 'sum_payoffs': 235.46525993172955, 'action': [0.0, 0.0]}])
Weights num count: [0.18141916605705927, 0.17093391855644965, 0.14776883686905634, 0.14752499390392587, 0.19044135576688612, 0.1616678858814923]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.824214935302734 s
