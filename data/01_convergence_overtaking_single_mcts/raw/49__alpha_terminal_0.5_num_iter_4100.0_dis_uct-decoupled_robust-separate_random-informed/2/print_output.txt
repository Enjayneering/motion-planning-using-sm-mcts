Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 448, 'sum_payoffs': 150.75994063081873, 'action': [1.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 128.865264567659, 'action': [0.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 151.3026887474241, 'action': [1.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 136.96442907340258, 'action': [0.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 171.27934087406382, 'action': [2.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 155.07273269458864, 'action': [1.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 178.33614582253531, 'action': [2.0, 0.0]}, {'num_count': 494, 'sum_payoffs': 170.80310424797355, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 143.8982664832553, 'action': [0.0, 0.0]}])
Weights num count: [0.10924164837844429, 0.09680565715679103, 0.10948549134357474, 0.10143867349426969, 0.12070226773957571, 0.11168007802974884, 0.12460375518166301, 0.12045842477444525, 0.10534016093635698]
Actions to choose Agent 1: dict_values([{'num_count': 723, 'sum_payoffs': 261.2167670534106, 'action': [1.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 239.34587623695276, 'action': [0.0, 0.0]}, {'num_count': 605, 'sum_payoffs': 210.0372567398255, 'action': [0.0, -1.5707963267948966]}, {'num_count': 725, 'sum_payoffs': 262.025723469712, 'action': [1.0, 1.5707963267948966]}, {'num_count': 587, 'sum_payoffs': 202.2295673832453, 'action': [0.0, 1.5707963267948966]}, {'num_count': 787, 'sum_payoffs': 289.2891000826154, 'action': [1.0, 0.0]}])
Weights num count: [0.17629846378931968, 0.1641063155327969, 0.14752499390392587, 0.1767861497195806, 0.14313582053157767, 0.19190441355766885]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.84609317779541 s
