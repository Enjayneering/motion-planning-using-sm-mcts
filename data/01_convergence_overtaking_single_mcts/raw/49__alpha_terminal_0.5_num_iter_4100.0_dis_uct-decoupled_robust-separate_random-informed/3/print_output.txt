Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 538, 'sum_payoffs': 190.28684890086302, 'action': [2.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 142.72832497683672, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 155.32752164880583, 'action': [2.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 150.94432700707813, 'action': [1.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 159.2080098953494, 'action': [1.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 128.56489571895534, 'action': [0.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 137.93943360963274, 'action': [0.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 156.5183795555081, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 167.01429181670795, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.13118751524018532, 0.10460863204096561, 0.11168007802974884, 0.10924164837844429, 0.11387466471592295, 0.09656181419166057, 0.1019263594245306, 0.11241160692514021, 0.11826383808827115]
Actions to choose Agent 1: dict_values([{'num_count': 711, 'sum_payoffs': 255.76830532532446, 'action': [1.0, -1.5707963267948966]}, {'num_count': 744, 'sum_payoffs': 270.11801276945914, 'action': [1.0, 1.5707963267948966]}, {'num_count': 593, 'sum_payoffs': 204.6938477944537, 'action': [0.0, -1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 224.0547666169671, 'action': [0.0, 0.0]}, {'num_count': 601, 'sum_payoffs': 208.07400548022437, 'action': [0.0, 1.5707963267948966]}, {'num_count': 813, 'sum_payoffs': 300.49811067633203, 'action': [1.0, 0.0]}])
Weights num count: [0.1733723482077542, 0.18141916605705927, 0.1445988783223604, 0.15557181175323093, 0.14654962204340405, 0.19824433065106073]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.97000002861023 s
