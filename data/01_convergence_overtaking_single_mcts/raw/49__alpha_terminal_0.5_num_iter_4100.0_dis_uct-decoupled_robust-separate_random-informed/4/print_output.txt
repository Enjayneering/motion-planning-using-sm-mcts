Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 508, 'sum_payoffs': 177.21449042603004, 'action': [2.0, 0.0]}, {'num_count': 478, 'sum_payoffs': 164.00567788037006, 'action': [2.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 164.9119071528445, 'action': [2.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 159.31130969338196, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 138.05161703175642, 'action': [0.0, 1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 158.80349146977366, 'action': [1.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 137.66813495056996, 'action': [0.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 147.45937231896573, 'action': [1.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 141.43014820860904, 'action': [0.0, 0.0]}])
Weights num count: [0.12387222628627165, 0.11655693733235796, 0.11704462326261887, 0.11387466471592295, 0.1019263594245306, 0.11363082175079249, 0.10168251645940014, 0.10729090465740064, 0.10387710314557425]
Actions to choose Agent 1: dict_values([{'num_count': 816, 'sum_payoffs': 301.3244150039817, 'action': [1.0, 0.0]}, {'num_count': 616, 'sum_payoffs': 214.35013811359764, 'action': [0.0, 0.0]}, {'num_count': 726, 'sum_payoffs': 262.0400212358085, 'action': [1.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 266.37304286003547, 'action': [1.0, -1.5707963267948966]}, {'num_count': 604, 'sum_payoffs': 209.1998616620275, 'action': [0.0, -1.5707963267948966]}, {'num_count': 602, 'sum_payoffs': 208.2548974636428, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.19897585954645208, 0.1502072665203609, 0.17702999268471104, 0.1794684223360156, 0.1472811509387954, 0.1467934650085345]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.784340620040894 s
