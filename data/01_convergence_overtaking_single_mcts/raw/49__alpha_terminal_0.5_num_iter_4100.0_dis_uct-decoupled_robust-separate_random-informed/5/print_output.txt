Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 431, 'sum_payoffs': 142.69269265359395, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 176.02763389850475, 'action': [2.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 185.60704314142856, 'action': [2.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 131.51609603894013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 158.66599696402136, 'action': [1.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 162.45004974771655, 'action': [2.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 126.39837883980415, 'action': [0.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 149.41371202324473, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 146.952140881228, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10509631797122652, 0.12387222628627165, 0.12923677151914167, 0.09875640087783467, 0.1141185076810534, 0.1163130943672275, 0.0958302852962692, 0.10899780541331383, 0.1075347476225311]
Actions to choose Agent 1: dict_values([{'num_count': 605, 'sum_payoffs': 210.0693989380622, 'action': [0.0, -1.5707963267948966]}, {'num_count': 604, 'sum_payoffs': 209.5477357738098, 'action': [0.0, 1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 228.18212518997277, 'action': [0.0, 0.0]}, {'num_count': 769, 'sum_payoffs': 281.42267920318415, 'action': [1.0, 0.0]}, {'num_count': 769, 'sum_payoffs': 281.2932350796896, 'action': [1.0, -1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 253.7728823637006, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14752499390392587, 0.1472811509387954, 0.15776639843940501, 0.18751524018532065, 0.18751524018532065, 0.17215313338210192]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.751598119735718 s
