Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 466, 'sum_payoffs': 158.36087216065894, 'action': [2.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 139.78771352274077, 'action': [0.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 182.40718864555018, 'action': [2.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 158.8241126139295, 'action': [1.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 156.6525378652839, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 150.09380785146857, 'action': [1.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 139.36187803544007, 'action': [0.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 133.3723634939389, 'action': [0.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 166.2283551018186, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11363082175079249, 0.10314557425018288, 0.12704218483296756, 0.11387466471592295, 0.11265544989027067, 0.10899780541331383, 0.10290173128505242, 0.09948792977322604, 0.1180199951231407]
Actions to choose Agent 1: dict_values([{'num_count': 742, 'sum_payoffs': 270.8060594038441, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 259.480099883647, 'action': [1.0, 1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 218.38600988899577, 'action': [0.0, -1.5707963267948966]}, {'num_count': 595, 'sum_payoffs': 206.8128182360666, 'action': [0.0, 1.5707963267948966]}, {'num_count': 807, 'sum_payoffs': 299.5758291524468, 'action': [1.0, 0.0]}, {'num_count': 618, 'sum_payoffs': 216.7521819483443, 'action': [0.0, 0.0]}])
Weights num count: [0.18093148012679836, 0.17459156303340648, 0.15167032431114363, 0.14508656425262131, 0.196781272860278, 0.1506949524506218]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.004474878311157 s
