Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 508, 'sum_payoffs': 177.18300653000574, 'action': [2.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 156.6957469770911, 'action': [2.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 170.97572236366446, 'action': [2.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 132.9091577407476, 'action': [0.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 148.90157836560462, 'action': [1.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 153.63928849597815, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 157.06404386688538, 'action': [1.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 157.49679654211224, 'action': [1.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 134.1924167538168, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12387222628627165, 0.11241160692514021, 0.12045842477444525, 0.09900024384296513, 0.10802243355279201, 0.11070470616922702, 0.11265544989027067, 0.11289929285540112, 0.0997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 645, 'sum_payoffs': 227.11629507357833, 'action': [0.0, 0.0]}, {'num_count': 590, 'sum_payoffs': 203.48438653688888, 'action': [0.0, -1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 258.5435522308748, 'action': [1.0, -1.5707963267948966]}, {'num_count': 798, 'sum_payoffs': 294.11099869964056, 'action': [1.0, 0.0]}, {'num_count': 737, 'sum_payoffs': 267.31221988472487, 'action': [1.0, 1.5707963267948966]}, {'num_count': 613, 'sum_payoffs': 213.3599581700873, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1572787125091441, 0.14386734942696902, 0.17483540599853695, 0.19458668617410388, 0.17971226530114606, 0.14947573762496952]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.172283411026 s
