Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 463, 'sum_payoffs': 157.1838674127277, 'action': [1.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 181.9725479441412, 'action': [2.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 141.6080367607132, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 157.59811212894272, 'action': [2.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 161.04429468534366, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 152.7190450312814, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 132.21496843626252, 'action': [0.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 135.1439281588294, 'action': [0.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 166.34307589737196, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11289929285540112, 0.12679834186783712, 0.1041209461107047, 0.11314313582053158, 0.11509387954157523, 0.11046086320409657, 0.09875640087783467, 0.10046330163374786, 0.1180199951231407]
Actions to choose Agent 1: dict_values([{'num_count': 747, 'sum_payoffs': 271.8428338894679, 'action': [1.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 217.1165104598326, 'action': [0.0, 1.5707963267948966]}, {'num_count': 595, 'sum_payoffs': 205.932711866364, 'action': [0.0, -1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 250.09585918665658, 'action': [1.0, 1.5707963267948966]}, {'num_count': 782, 'sum_payoffs': 287.34633799350627, 'action': [1.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 232.92532187825702, 'action': [0.0, 0.0]}])
Weights num count: [0.18215069495245062, 0.15142648134601316, 0.14508656425262131, 0.16995854669592783, 0.1906851987320166, 0.16044867105584004]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.97772765159607 s
