Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 480, 'sum_payoffs': 164.5309624757775, 'action': [2.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 180.68603560422224, 'action': [2.0, 0.0]}, {'num_count': 416, 'sum_payoffs': 136.67043852650323, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 148.0036661954266, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 153.66686683363287, 'action': [1.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 144.40090857061224, 'action': [1.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 138.09737576032973, 'action': [0.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 161.05263941405227, 'action': [2.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 157.98016919531952, 'action': [1.0, 0.0]}])
Weights num count: [0.11704462326261887, 0.12606681297244574, 0.10143867349426969, 0.10777859058766155, 0.11094854913435748, 0.10582784686661789, 0.10217020238966106, 0.11509387954157523, 0.11338697878566203]
Actions to choose Agent 1: dict_values([{'num_count': 709, 'sum_payoffs': 255.7532922386415, 'action': [1.0, -1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 265.3707036369624, 'action': [1.0, 1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 202.417414097628, 'action': [0.0, -1.5707963267948966]}, {'num_count': 677, 'sum_payoffs': 241.77267369559766, 'action': [0.0, 0.0]}, {'num_count': 793, 'sum_payoffs': 292.6820683299129, 'action': [1.0, 0.0]}, {'num_count': 604, 'sum_payoffs': 210.08889974887094, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1728846622774933, 0.17824920751036333, 0.1428919775664472, 0.16508168739331872, 0.1933674713484516, 0.1472811509387954]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.189021587371826 s
