Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 175, 'sum_payoffs': 45.227386299308804, 'action': [1.0, -1.5707963267948966]}, {'num_count': 154, 'sum_payoffs': 36.80159919426617, 'action': [0.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 45.57721138670678, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 45.577211386706786, 'action': [1.0, 1.5707963267948966]}, {'num_count': 202, 'sum_payoffs': 56.271864058587504, 'action': [2.0, 0.0]}, {'num_count': 154, 'sum_payoffs': 36.84157920425451, 'action': [0.0, -1.5707963267948966]}, {'num_count': 154, 'sum_payoffs': 36.90154921923702, 'action': [0.0, 0.0]}, {'num_count': 205, 'sum_payoffs': 57.56121938071132, 'action': [2.0, 1.5707963267948966]}, {'num_count': 204, 'sum_payoffs': 57.111444268342574, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10930668332292318, 0.09618988132417239, 0.1099312929419113, 0.1099312929419113, 0.12617114303560276, 0.09618988132417239, 0.09618988132417239, 0.12804497189256714, 0.127420362273579]
Actions to choose Agent 1: dict_values([{'num_count': 239, 'sum_payoffs': 72.95352322622182, 'action': [0.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 95.64217889460478, 'action': [1.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 95.7021489095873, 'action': [1.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 75.57221388045811, 'action': [0.0, 0.0]}, {'num_count': 243, 'sum_payoffs': 74.73263367070295, 'action': [0.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 95.6221888896106, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14928169893816365, 0.18176139912554654, 0.18176139912554654, 0.15302935665209244, 0.15178013741411617, 0.18176139912554654]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.2503659725189209 s
