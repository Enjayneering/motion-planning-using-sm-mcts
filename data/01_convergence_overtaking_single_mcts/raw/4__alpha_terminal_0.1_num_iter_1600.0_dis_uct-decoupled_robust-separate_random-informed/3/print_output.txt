Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 154, 'sum_payoffs': 36.84157920425451, 'action': [0.0, 0.0]}, {'num_count': 202, 'sum_payoffs': 56.31184406857578, 'action': [2.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 45.59720139170095, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 45.327336324279656, 'action': [1.0, 0.0]}, {'num_count': 203, 'sum_payoffs': 56.701649165962095, 'action': [2.0, 0.0]}, {'num_count': 154, 'sum_payoffs': 36.82158919926034, 'action': [0.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 45.227386299308804, 'action': [1.0, 1.5707963267948966]}, {'num_count': 155, 'sum_payoffs': 37.19140429165248, 'action': [0.0, 1.5707963267948966]}, {'num_count': 206, 'sum_payoffs': 57.95102447809761, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.09618988132417239, 0.12617114303560276, 0.1099312929419113, 0.10930668332292318, 0.1267957526545909, 0.09618988132417239, 0.10930668332292318, 0.09681449094316052, 0.12866958151155528]
Actions to choose Agent 1: dict_values([{'num_count': 288, 'sum_payoffs': 94.37281357747499, 'action': [1.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 96.52173911434825, 'action': [1.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 73.79310343597695, 'action': [0.0, 0.0]}, {'num_count': 243, 'sum_payoffs': 74.69265366071461, 'action': [0.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 95.29235380720681, 'action': [1.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 75.53223387046977, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17988757026858213, 0.1830106183635228, 0.15053091817613992, 0.15178013741411617, 0.1811367895065584, 0.15302935665209244]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.25200772285461426 s
