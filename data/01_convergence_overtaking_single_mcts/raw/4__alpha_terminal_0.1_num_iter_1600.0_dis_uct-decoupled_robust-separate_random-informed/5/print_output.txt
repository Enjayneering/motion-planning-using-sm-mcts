Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 175, 'sum_payoffs': 45.22738629930881, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 45.26736630929714, 'action': [1.0, 0.0]}, {'num_count': 205, 'sum_payoffs': 57.42128934575217, 'action': [2.0, 0.0]}, {'num_count': 154, 'sum_payoffs': 36.88155921424285, 'action': [0.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 45.597201391700956, 'action': [1.0, 1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 36.39180409188568, 'action': [0.0, 1.5707963267948966]}, {'num_count': 154, 'sum_payoffs': 36.88155921424285, 'action': [0.0, -1.5707963267948966]}, {'num_count': 204, 'sum_payoffs': 57.13143427333674, 'action': [2.0, -1.5707963267948966]}, {'num_count': 204, 'sum_payoffs': 57.01149424337167, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10930668332292318, 0.10930668332292318, 0.12804497189256714, 0.09618988132417239, 0.1099312929419113, 0.09556527170518427, 0.09618988132417239, 0.127420362273579, 0.127420362273579]
Actions to choose Agent 1: dict_values([{'num_count': 244, 'sum_payoffs': 75.12243876808927, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 74.3028485633283, 'action': [0.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 95.1524237722476, 'action': [1.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 96.5617191243366, 'action': [1.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 73.01349324120433, 'action': [0.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 96.05197399698525, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1524047470331043, 0.15115552779512806, 0.1811367895065584, 0.1830106183635228, 0.14928169893816365, 0.18238600874453467]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.24688124656677246 s
