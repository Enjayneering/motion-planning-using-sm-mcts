Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 154, 'sum_payoffs': 36.80159919426617, 'action': [0.0, 0.0]}, {'num_count': 205, 'sum_payoffs': 57.481259360734654, 'action': [2.0, 1.5707963267948966]}, {'num_count': 154, 'sum_payoffs': 36.86156920924868, 'action': [0.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 45.24737630430298, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 45.28735631429132, 'action': [1.0, 0.0]}, {'num_count': 203, 'sum_payoffs': 56.66166915597372, 'action': [2.0, -1.5707963267948966]}, {'num_count': 204, 'sum_payoffs': 57.091454263348375, 'action': [2.0, 0.0]}, {'num_count': 154, 'sum_payoffs': 36.76161918427783, 'action': [0.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 45.59720139170095, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.09618988132417239, 0.12804497189256714, 0.09618988132417239, 0.10930668332292318, 0.10930668332292318, 0.1267957526545909, 0.127420362273579, 0.09618988132417239, 0.1099312929419113]
Actions to choose Agent 1: dict_values([{'num_count': 290, 'sum_payoffs': 95.25237379721847, 'action': [1.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 73.77311343098278, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 94.80259868484964, 'action': [1.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 74.7726136806913, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 74.2228885433516, 'action': [0.0, -1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 97.40129933409172, 'action': [1.0, 0.0]}])
Weights num count: [0.1811367895065584, 0.15053091817613992, 0.18051217988757026, 0.15178013741411617, 0.15115552779512806, 0.18425983760149905]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.2538414001464844 s
