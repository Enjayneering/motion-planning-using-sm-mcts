Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 153, 'sum_payoffs': 36.491754116856534, 'action': [0.0, 1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 36.511744121850704, 'action': [0.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 45.26736630929714, 'action': [1.0, 1.5707963267948966]}, {'num_count': 153, 'sum_payoffs': 36.41179409687985, 'action': [0.0, -1.5707963267948966]}, {'num_count': 206, 'sum_payoffs': 57.93103447310348, 'action': [2.0, -1.5707963267948966]}, {'num_count': 204, 'sum_payoffs': 57.07146425835419, 'action': [2.0, 1.5707963267948966]}, {'num_count': 205, 'sum_payoffs': 57.461269355740505, 'action': [2.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 45.59720139170095, 'action': [1.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 45.20739629431464, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.09556527170518427, 0.09556527170518427, 0.10930668332292318, 0.09556527170518427, 0.12866958151155528, 0.127420362273579, 0.12804497189256714, 0.1099312929419113, 0.10930668332292318]
Actions to choose Agent 1: dict_values([{'num_count': 294, 'sum_payoffs': 96.93153421672874, 'action': [1.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 96.13193401696194, 'action': [1.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 96.91154421173458, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 72.99350323621016, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 74.22288854335163, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 73.03348324619851, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18363522798251092, 0.18238600874453467, 0.18363522798251092, 0.14928169893816365, 0.15115552779512806, 0.14928169893816365]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.24318456649780273 s
