Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 626, 'sum_payoffs': 221.94405016968352, 'action': [2.0, 0.0]}, {'num_count': 547, 'sum_payoffs': 187.61831831379973, 'action': [2.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 156.00515081777226, 'action': [0.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 161.56375966818527, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 162.4340260701512, 'action': [1.0, 1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 176.95378791854372, 'action': [2.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 175.99940728080009, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 143.2911882737008, 'action': [0.0, 1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 165.45737142760373, 'action': [1.0, 0.0]}])
Weights num count: [0.13605737883068897, 0.1188871984351228, 0.102803738317757, 0.10562921104107803, 0.10606389915235818, 0.11345359704412085, 0.11301890893284068, 0.09628341664855466, 0.10758530754183873]
Actions to choose Agent 1: dict_values([{'num_count': 818, 'sum_payoffs': 293.98441684971294, 'action': [1.0, 1.5707963267948966]}, {'num_count': 678, 'sum_payoffs': 234.12367932075287, 'action': [0.0, 1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 290.04119678445966, 'action': [1.0, -1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 217.1486459547355, 'action': [0.0, -1.5707963267948966]}, {'num_count': 735, 'sum_payoffs': 258.387853330119, 'action': [0.0, 0.0]}, {'num_count': 922, 'sum_payoffs': 339.07397895350334, 'action': [1.0, 0.0]}])
Weights num count: [0.177787437513584, 0.14735926972397306, 0.1758313410128233, 0.13866550749836992, 0.15974788089545752, 0.20039121930015213]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.66823673248291 s
