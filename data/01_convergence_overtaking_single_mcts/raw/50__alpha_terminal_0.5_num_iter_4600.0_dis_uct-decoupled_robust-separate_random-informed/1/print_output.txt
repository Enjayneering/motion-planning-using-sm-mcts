Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 497, 'sum_payoffs': 165.94423703460922, 'action': [1.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 169.81195741346755, 'action': [1.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 150.67346828358956, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 148.9884316102391, 'action': [0.0, -1.5707963267948966]}, {'num_count': 550, 'sum_payoffs': 188.66284849734814, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 162.9656331898033, 'action': [0.0, 0.0]}, {'num_count': 600, 'sum_payoffs': 210.3064752572995, 'action': [2.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 170.16685362723834, 'action': [1.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 180.79131517940928, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10801999565311889, 0.10997609215387959, 0.10019560965007607, 0.09932623342751576, 0.11953923060204304, 0.10649858726363834, 0.13040643338404695, 0.11019343620951967, 0.11562703760052162]
Actions to choose Agent 1: dict_values([{'num_count': 680, 'sum_payoffs': 235.77609986144935, 'action': [0.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 229.0776336744971, 'action': [0.0, -1.5707963267948966]}, {'num_count': 807, 'sum_payoffs': 290.4648121170589, 'action': [1.0, -1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 259.90262991478653, 'action': [0.0, 0.0]}, {'num_count': 890, 'sum_payoffs': 326.3533344989679, 'action': [1.0, 0.0]}, {'num_count': 823, 'sum_payoffs': 297.2778992864079, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1477939578352532, 0.14431645294501194, 0.17539665290154313, 0.15996522495109758, 0.19343620951966964, 0.1788741577917844]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.666583776474 s
