Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 489, 'sum_payoffs': 161.91524394733526, 'action': [1.0, 1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 197.40639155169055, 'action': [2.0, 0.0]}, {'num_count': 517, 'sum_payoffs': 173.9070380961287, 'action': [1.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 145.80570874076417, 'action': [0.0, 1.5707963267948966]}, {'num_count': 550, 'sum_payoffs': 187.9365666872737, 'action': [2.0, -1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 174.3357743751237, 'action': [0.0, 0.0]}, {'num_count': 517, 'sum_payoffs': 173.90420150326716, 'action': [2.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 170.9104097422255, 'action': [1.0, -1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 156.48529410929044, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10628124320799825, 0.12432079982612476, 0.11236687676592046, 0.09802216909367528, 0.11953923060204304, 0.11258422082156053, 0.11236687676592046, 0.11084546837643991, 0.10345577048467725]
Actions to choose Agent 1: dict_values([{'num_count': 690, 'sum_payoffs': 239.43321065861534, 'action': [0.0, -1.5707963267948966]}, {'num_count': 789, 'sum_payoffs': 282.00094999976693, 'action': [1.0, 1.5707963267948966]}, {'num_count': 821, 'sum_payoffs': 295.7825424755691, 'action': [1.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 222.5757146303171, 'action': [0.0, 1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 249.85349603985438, 'action': [0.0, 0.0]}, {'num_count': 936, 'sum_payoffs': 345.63273773623337, 'action': [1.0, 0.0]}])
Weights num count: [0.149967398391654, 0.17148445990002173, 0.17843946968050423, 0.14127363616605085, 0.15518365572701587, 0.20343403607911323]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.559089422225952 s
