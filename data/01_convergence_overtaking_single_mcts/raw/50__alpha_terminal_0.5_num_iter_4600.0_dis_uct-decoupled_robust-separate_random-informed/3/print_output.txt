Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 472, 'sum_payoffs': 154.8461516733066, 'action': [0.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 171.34229702650805, 'action': [1.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 154.73088289225447, 'action': [1.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 140.9343440159132, 'action': [0.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 183.3782378263248, 'action': [2.0, 1.5707963267948966]}, {'num_count': 601, 'sum_payoffs': 210.0285622776723, 'action': [2.0, 0.0]}, {'num_count': 551, 'sum_payoffs': 188.52531219915977, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 162.5049388588973, 'action': [0.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 177.36754393160047, 'action': [1.0, 0.0]}])
Weights num count: [0.10258639426211694, 0.11106281243207998, 0.10258639426211694, 0.09541404042599434, 0.11714844599000217, 0.13062377743968703, 0.11975657465768311, 0.10649858726363834, 0.11410562921104107]
Actions to choose Agent 1: dict_values([{'num_count': 820, 'sum_payoffs': 295.90370039164094, 'action': [1.0, 1.5707963267948966]}, {'num_count': 827, 'sum_payoffs': 299.07034146235435, 'action': [1.0, -1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 228.25387771819754, 'action': [0.0, 1.5707963267948966]}, {'num_count': 922, 'sum_payoffs': 340.34230612278986, 'action': [1.0, 0.0]}, {'num_count': 721, 'sum_payoffs': 253.3730687152212, 'action': [0.0, 0.0]}, {'num_count': 648, 'sum_payoffs': 222.2587350118514, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17822212562486417, 0.1797435340143447, 0.1438817648337318, 0.20039121930015213, 0.1567050641164964, 0.1408389480547707]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.333019495010376 s
