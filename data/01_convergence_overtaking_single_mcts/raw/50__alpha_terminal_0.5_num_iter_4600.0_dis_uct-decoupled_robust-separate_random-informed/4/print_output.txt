Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 459, 'sum_payoffs': 149.49632279278012, 'action': [0.0, -1.5707963267948966]}, {'num_count': 613, 'sum_payoffs': 215.40786647464114, 'action': [2.0, 0.0]}, {'num_count': 531, 'sum_payoffs': 180.12019529613707, 'action': [2.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 153.25596860050624, 'action': [0.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 171.12183609877906, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 159.66421546887634, 'action': [0.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 163.10242448384557, 'action': [1.0, 1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 178.00377602233402, 'action': [1.0, -1.5707963267948966]}, {'num_count': 519, 'sum_payoffs': 174.89859049554235, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.09976092153879591, 0.13323190610736796, 0.11540969354488155, 0.10171701803955661, 0.11084546837643991, 0.10497717887415779, 0.10671593131927842, 0.11432297326668116, 0.11280156487720061]
Actions to choose Agent 1: dict_values([{'num_count': 896, 'sum_payoffs': 329.08865087614936, 'action': [1.0, 0.0]}, {'num_count': 837, 'sum_payoffs': 303.43005915299466, 'action': [1.0, 1.5707963267948966]}, {'num_count': 736, 'sum_payoffs': 259.9324491222026, 'action': [0.0, 0.0]}, {'num_count': 649, 'sum_payoffs': 222.8383142082849, 'action': [0.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 232.96653249892518, 'action': [0.0, 1.5707963267948966]}, {'num_count': 809, 'sum_payoffs': 291.37226140270735, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.19474027385351012, 0.1819169745707455, 0.15996522495109758, 0.1410562921104108, 0.14627254944577267, 0.1758313410128233]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.65663981437683 s
