Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 528, 'sum_payoffs': 178.71060889546445, 'action': [1.0, -1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 183.07922603558848, 'action': [2.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 149.841966699904, 'action': [0.0, 1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 202.91360509461745, 'action': [2.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 165.64716844295364, 'action': [1.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 147.4414934930338, 'action': [0.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 163.52402621795125, 'action': [0.0, 0.0]}, {'num_count': 552, 'sum_payoffs': 189.06410745443546, 'action': [2.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 164.73410755518856, 'action': [1.0, 0.0]}])
Weights num count: [0.11475766137796131, 0.1169311019343621, 0.099978265594436, 0.1269289284938057, 0.10801999565311889, 0.09867420126059552, 0.1069332753749185, 0.11997391871332319, 0.10758530754183873]
Actions to choose Agent 1: dict_values([{'num_count': 698, 'sum_payoffs': 243.03573739909703, 'action': [0.0, 0.0]}, {'num_count': 828, 'sum_payoffs': 298.6290619803296, 'action': [1.0, -1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 227.65338980326567, 'action': [0.0, 1.5707963267948966]}, {'num_count': 791, 'sum_payoffs': 282.8279706665357, 'action': [1.0, 1.5707963267948966]}, {'num_count': 971, 'sum_payoffs': 361.03525881903636, 'action': [1.0, 0.0]}, {'num_count': 650, 'sum_payoffs': 222.70677204504693, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1517061508367746, 0.1799608780699848, 0.1438817648337318, 0.1719191480113019, 0.21104107802651598, 0.14127363616605085]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.662094831466675 s
