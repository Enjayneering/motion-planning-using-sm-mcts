Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 462, 'sum_payoffs': 150.55051304803374, 'action': [0.0, -1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 171.0350819049149, 'action': [1.0, -1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 166.34800167163624, 'action': [2.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 168.8522702114078, 'action': [1.0, 1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 157.7797783545006, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 149.0181109355848, 'action': [0.0, 1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 173.16350053979912, 'action': [1.0, 0.0]}, {'num_count': 567, 'sum_payoffs': 195.39945969364982, 'action': [2.0, -1.5707963267948966]}, {'num_count': 605, 'sum_payoffs': 211.85031901592288, 'action': [2.0, 0.0]}])
Weights num count: [0.10041295370571615, 0.11084546837643991, 0.10845468376439904, 0.10975874809823952, 0.10410780265159748, 0.09954357748315583, 0.1119321886546403, 0.12323407954792437, 0.13149315366224734]
Actions to choose Agent 1: dict_values([{'num_count': 680, 'sum_payoffs': 235.9964446115014, 'action': [0.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 226.22675932342082, 'action': [0.0, 1.5707963267948966]}, {'num_count': 849, 'sum_payoffs': 308.84702006580073, 'action': [1.0, 1.5707963267948966]}, {'num_count': 814, 'sum_payoffs': 293.69540902115534, 'action': [1.0, -1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 242.10286739155603, 'action': [0.0, 0.0]}, {'num_count': 906, 'sum_payoffs': 333.6597104752807, 'action': [1.0, 0.0]}])
Weights num count: [0.1477939578352532, 0.1427950445555314, 0.18452510323842644, 0.1769180612910237, 0.1508367746142143, 0.1969137144099109]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.63716697692871 s
