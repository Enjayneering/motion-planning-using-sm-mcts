Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 501, 'sum_payoffs': 168.28957240846808, 'action': [1.0, 0.0]}, {'num_count': 614, 'sum_payoffs': 217.20397895673946, 'action': [2.0, 0.0]}, {'num_count': 589, 'sum_payoffs': 206.32626707383636, 'action': [2.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 159.3384902792636, 'action': [1.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 170.05096772399912, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 146.5475254439688, 'action': [0.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 152.12183704091595, 'action': [0.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 156.44493200949918, 'action': [0.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 178.63059826141534, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1088893718756792, 0.13344925016300804, 0.12801564877200608, 0.10432514670723755, 0.10975874809823952, 0.09780482503803521, 0.10063029776135622, 0.102803738317757, 0.11410562921104107]
Actions to choose Agent 1: dict_values([{'num_count': 791, 'sum_payoffs': 282.5004807207112, 'action': [1.0, 1.5707963267948966]}, {'num_count': 908, 'sum_payoffs': 333.23103002489233, 'action': [1.0, 0.0]}, {'num_count': 670, 'sum_payoffs': 230.7981313913606, 'action': [0.0, -1.5707963267948966]}, {'num_count': 740, 'sum_payoffs': 260.6380561390956, 'action': [0.0, 0.0]}, {'num_count': 678, 'sum_payoffs': 234.28457754423482, 'action': [0.0, 1.5707963267948966]}, {'num_count': 813, 'sum_payoffs': 291.99780784746366, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1719191480113019, 0.19734840252119104, 0.14562051727885242, 0.1608346011736579, 0.14735926972397306, 0.1767007172353836]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.533949851989746 s
