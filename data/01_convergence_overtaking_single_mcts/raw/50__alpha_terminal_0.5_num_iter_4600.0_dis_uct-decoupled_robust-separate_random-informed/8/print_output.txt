Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 485, 'sum_payoffs': 160.83976016525082, 'action': [1.0, 1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 174.81341847781894, 'action': [2.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 173.95324286257875, 'action': [1.0, -1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 199.45029318918537, 'action': [2.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 158.19575482555828, 'action': [0.0, -1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 183.97335296567044, 'action': [1.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 160.78529119536088, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 150.25618032380413, 'action': [0.0, 1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 185.5603010010116, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10541186698543795, 0.11258422082156053, 0.11214953271028037, 0.12497283199304499, 0.10410780265159748, 0.11714844599000217, 0.10541186698543795, 0.099978265594436, 0.11801782221256249]
Actions to choose Agent 1: dict_values([{'num_count': 709, 'sum_payoffs': 247.21567039796113, 'action': [0.0, 0.0]}, {'num_count': 895, 'sum_payoffs': 327.22976344307835, 'action': [1.0, 0.0]}, {'num_count': 818, 'sum_payoffs': 293.9505733903806, 'action': [1.0, 1.5707963267948966]}, {'num_count': 832, 'sum_payoffs': 299.8457627090911, 'action': [1.0, -1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 226.42941495833767, 'action': [0.0, -1.5707963267948966]}, {'num_count': 686, 'sum_payoffs': 237.3868457673078, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15409693544881548, 0.19452292979787003, 0.177787437513584, 0.1808302542925451, 0.14344707672245163, 0.14909802216909368]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.974799633026123 s
