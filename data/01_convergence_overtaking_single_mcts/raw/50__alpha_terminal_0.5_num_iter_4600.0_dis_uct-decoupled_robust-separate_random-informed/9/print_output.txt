Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 560, 'sum_payoffs': 192.34840458554154, 'action': [2.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 167.09361179897186, 'action': [1.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 152.7238451184445, 'action': [0.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 155.6300409603568, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 149.79798575455845, 'action': [0.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 176.97288819807738, 'action': [2.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 167.08112069738047, 'action': [1.0, -1.5707963267948966]}, {'num_count': 615, 'sum_payoffs': 216.1501927201492, 'action': [2.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 165.77567726546377, 'action': [1.0, 0.0]}])
Weights num count: [0.12171267115844382, 0.1088893718756792, 0.10149967398391654, 0.10302108237339709, 0.099978265594436, 0.113888285155401, 0.1088893718756792, 0.13366659421864813, 0.10823733970875897]
Actions to choose Agent 1: dict_values([{'num_count': 812, 'sum_payoffs': 290.81068539525563, 'action': [1.0, 1.5707963267948966]}, {'num_count': 912, 'sum_payoffs': 334.13640551140884, 'action': [1.0, 0.0]}, {'num_count': 834, 'sum_payoffs': 300.2973609844439, 'action': [1.0, -1.5707963267948966]}, {'num_count': 651, 'sum_payoffs': 222.25325012644905, 'action': [0.0, 1.5707963267948966]}, {'num_count': 735, 'sum_payoffs': 257.87640073320836, 'action': [0.0, 0.0]}, {'num_count': 656, 'sum_payoffs': 224.23468578938434, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17648337317974352, 0.19821777874375135, 0.18126494240382526, 0.14149098022169093, 0.15974788089545752, 0.14257770049989132]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.481438636779785 s
