Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 66, 'sum_payoffs': 22.34326912986207, 'action': [1.0, 1.5707963267948966]}, {'num_count': 68, 'sum_payoffs': 23.55398447262976, 'action': [2.0, 1.5707963267948966]}, {'num_count': 71, 'sum_payoffs': 25.147036129776225, 'action': [2.0, -1.5707963267948966]}, {'num_count': 69, 'sum_payoffs': 24.162891069744845, 'action': [1.0, -1.5707963267948966]}, {'num_count': 64, 'sum_payoffs': 21.19395304189247, 'action': [0.0, -1.5707963267948966]}, {'num_count': 66, 'sum_payoffs': 22.372486034815818, 'action': [1.0, 0.0]}, {'num_count': 71, 'sum_payoffs': 24.94563025543263, 'action': [2.0, 0.0]}, {'num_count': 63, 'sum_payoffs': 20.587172923659633, 'action': [0.0, 1.5707963267948966]}, {'num_count': 62, 'sum_payoffs': 20.189080339168594, 'action': [0.0, 0.0]}])
Weights num count: [0.10981697171381032, 0.11314475873544093, 0.11813643926788686, 0.11480865224625623, 0.1064891846921797, 0.10981697171381032, 0.11813643926788686, 0.1048252911813644, 0.10316139767054909]
Actions to choose Agent 1: dict_values([{'num_count': 97, 'sum_payoffs': 34.6173667554736, 'action': [0.0, 1.5707963267948966]}, {'num_count': 104, 'sum_payoffs': 38.41803938668982, 'action': [1.0, 1.5707963267948966]}, {'num_count': 97, 'sum_payoffs': 34.69967708968546, 'action': [0.0, -1.5707963267948966]}, {'num_count': 93, 'sum_payoffs': 32.467218505058156, 'action': [0.0, 0.0]}, {'num_count': 100, 'sum_payoffs': 36.09952530819776, 'action': [1.0, -1.5707963267948966]}, {'num_count': 109, 'sum_payoffs': 41.293303519434716, 'action': [1.0, 0.0]}])
Weights num count: [0.16139767054908485, 0.17304492512479203, 0.16139767054908485, 0.15474209650582363, 0.16638935108153077, 0.18136439267886856]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 3.667234182357788 s
