Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 123, 'sum_payoffs': 42.57615389722279, 'action': [1.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 45.04368933941284, 'action': [2.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 43.597262506364636, 'action': [1.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 45.05505590629532, 'action': [1.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 40.38578175042694, 'action': [0.0, -1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 43.08988951766035, 'action': [2.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 41.07059529493829, 'action': [2.0, 1.5707963267948966]}, {'num_count': 115, 'sum_payoffs': 38.50376021516157, 'action': [0.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 40.027829537099116, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11171662125340599, 0.11625794732061762, 0.11353315168029064, 0.11625794732061762, 0.1080835603996367, 0.11262488646684832, 0.10899182561307902, 0.1044504995458674, 0.10717529518619437]
Actions to choose Agent 1: dict_values([{'num_count': 219, 'sum_payoffs': 84.79288238469525, 'action': [1.0, 0.0]}, {'num_count': 193, 'sum_payoffs': 71.58339146819476, 'action': [1.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 59.97963725392661, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 70.06496354741205, 'action': [1.0, 1.5707963267948966]}, {'num_count': 157, 'sum_payoffs': 53.379982947246596, 'action': [0.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 60.54852515804568, 'action': [0.0, 0.0]}])
Weights num count: [0.1989100817438692, 0.17529518619436876, 0.15440508628519528, 0.17257039055404177, 0.14259763851044505, 0.1553133514986376]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 7.056339740753174 s
