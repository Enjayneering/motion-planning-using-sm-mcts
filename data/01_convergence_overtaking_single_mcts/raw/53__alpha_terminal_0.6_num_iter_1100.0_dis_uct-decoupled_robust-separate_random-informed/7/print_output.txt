Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 122, 'sum_payoffs': 41.582792774155415, 'action': [0.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 43.12815010250392, 'action': [2.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 44.65505402173317, 'action': [1.0, -1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 42.10419765325515, 'action': [1.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 40.09870563473188, 'action': [2.0, 0.0]}, {'num_count': 131, 'sum_payoffs': 46.20643638276837, 'action': [2.0, -1.5707963267948966]}, {'num_count': 115, 'sum_payoffs': 38.02524864189513, 'action': [1.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 38.40906709867609, 'action': [0.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 40.92304287167018, 'action': [0.0, 0.0]}])
Weights num count: [0.11080835603996367, 0.11353315168029064, 0.11625794732061762, 0.11171662125340599, 0.1080835603996367, 0.11898274296094459, 0.1044504995458674, 0.10535876475930972, 0.10990009082652134]
Actions to choose Agent 1: dict_values([{'num_count': 169, 'sum_payoffs': 59.50103070657902, 'action': [0.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 63.91914163288352, 'action': [0.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 70.9393884475579, 'action': [1.0, 1.5707963267948966]}, {'num_count': 205, 'sum_payoffs': 77.49702515000526, 'action': [1.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 69.03085995828613, 'action': [1.0, -1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 58.87787603337937, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15349682107175294, 0.16167120799273388, 0.17438692098092642, 0.18619436875567666, 0.17075386012715713, 0.15258855585831063]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 7.546011209487915 s
