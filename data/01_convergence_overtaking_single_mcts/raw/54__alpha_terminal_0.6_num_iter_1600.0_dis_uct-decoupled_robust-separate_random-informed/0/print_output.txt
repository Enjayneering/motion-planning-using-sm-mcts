Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 183, 'sum_payoffs': 63.64036114566684, 'action': [1.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 56.81805262322903, 'action': [0.0, 1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 55.85738895684752, 'action': [0.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 61.2040357336338, 'action': [1.0, 0.0]}, {'num_count': 166, 'sum_payoffs': 55.251330931577336, 'action': [0.0, -1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 69.09888855191052, 'action': [2.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 60.52388577628717, 'action': [1.0, -1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 66.54735367684316, 'action': [2.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 60.73469755356662, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11430356027482823, 0.10555902560899438, 0.10430980637101811, 0.11118051217988757, 0.10368519675202999, 0.12117426608369769, 0.11055590256089944, 0.11805121798875702, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 285, 'sum_payoffs': 106.1202066567614, 'action': [1.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 111.3926776934537, 'action': [1.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 86.19969926281203, 'action': [0.0, -1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 110.91378946239739, 'action': [1.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 82.43687102721096, 'action': [0.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 86.30048988016573, 'action': [0.0, 0.0]}])
Weights num count: [0.17801374141161774, 0.1848844472204872, 0.1524047470331043, 0.18425983760149905, 0.14740787008119924, 0.1524047470331043]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.221445322036743 s
