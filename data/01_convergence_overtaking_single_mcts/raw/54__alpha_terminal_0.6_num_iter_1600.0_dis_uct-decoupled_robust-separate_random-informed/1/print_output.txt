Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 169, 'sum_payoffs': 57.37480548180324, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 60.67040402048587, 'action': [1.0, 1.5707963267948966]}, {'num_count': 160, 'sum_payoffs': 53.00358117824983, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 63.77021642880705, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 60.294712828682805, 'action': [1.0, 0.0]}, {'num_count': 187, 'sum_payoffs': 66.20732212120275, 'action': [2.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 67.63655765904402, 'action': [2.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 59.71898391732278, 'action': [0.0, 0.0]}, {'num_count': 187, 'sum_payoffs': 66.13078895346165, 'action': [2.0, 0.0]}])
Weights num count: [0.10555902560899438, 0.1099312929419113, 0.09993753903810118, 0.1136789506558401, 0.10930668332292318, 0.11680199875078076, 0.11867582760774516, 0.10868207370393504, 0.11680199875078076]
Actions to choose Agent 1: dict_values([{'num_count': 299, 'sum_payoffs': 111.53460882851847, 'action': [1.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 83.18123249132091, 'action': [0.0, 1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 85.99617545091638, 'action': [0.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 83.77563726003663, 'action': [0.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 106.18056476620538, 'action': [1.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 105.24779348730463, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1867582760774516, 0.14990630855715179, 0.15365396627108058, 0.15053091817613992, 0.17988757026858213, 0.17863835103060588]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 10.826894521713257 s
