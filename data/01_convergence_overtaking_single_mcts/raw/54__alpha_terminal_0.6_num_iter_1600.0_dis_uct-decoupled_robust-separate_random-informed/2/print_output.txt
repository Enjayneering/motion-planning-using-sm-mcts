Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 163, 'sum_payoffs': 53.909141555444336, 'action': [0.0, -1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 70.18988775177478, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 60.31659905268061, 'action': [1.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 66.1664739830286, 'action': [2.0, 0.0]}, {'num_count': 167, 'sum_payoffs': 55.82760694483799, 'action': [0.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 56.360339094986045, 'action': [0.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 60.34726678717634, 'action': [2.0, 1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 65.08887020566812, 'action': [1.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 62.22117376125492, 'action': [1.0, 0.0]}])
Weights num count: [0.10181136789506559, 0.12242348532167395, 0.1099312929419113, 0.1174266083697689, 0.10430980637101811, 0.10493441599000625, 0.1099312929419113, 0.11617738913179262, 0.11242973141786383]
Actions to choose Agent 1: dict_values([{'num_count': 273, 'sum_payoffs': 99.5532765745594, 'action': [1.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 108.27333965662494, 'action': [1.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 92.7466649220753, 'action': [0.0, 0.0]}, {'num_count': 232, 'sum_payoffs': 79.9979606192137, 'action': [0.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 85.23463487966784, 'action': [0.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 113.6423431542573, 'action': [1.0, 0.0]}])
Weights num count: [0.17051842598376016, 0.18176139912554654, 0.1617738913179263, 0.14490943160524672, 0.15178013741411617, 0.18863210493441598]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.91016960144043 s
