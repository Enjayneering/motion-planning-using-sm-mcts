Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 184, 'sum_payoffs': 64.36752220144527, 'action': [1.0, -1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 70.23514955464658, 'action': [2.0, -1.5707963267948966]}, {'num_count': 163, 'sum_payoffs': 54.07746280658118, 'action': [0.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 57.0392381900454, 'action': [0.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 58.369772641728, 'action': [1.0, 0.0]}, {'num_count': 193, 'sum_payoffs': 68.81721923418365, 'action': [2.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 63.1671173282659, 'action': [1.0, 1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 54.94068403093273, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 60.43155918292249, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11492816989381636, 0.12242348532167395, 0.10181136789506559, 0.10555902560899438, 0.10743285446595878, 0.12054965646470955, 0.1136789506558401, 0.10306058713304185, 0.1099312929419113]
Actions to choose Agent 1: dict_values([{'num_count': 301, 'sum_payoffs': 112.88618282835593, 'action': [1.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 86.62111177470558, 'action': [0.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 98.50082296209291, 'action': [1.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 106.18406204394248, 'action': [1.0, 1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 86.13946756662034, 'action': [0.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 88.47475006875037, 'action': [0.0, 0.0]}])
Weights num count: [0.18800749531542785, 0.15365396627108058, 0.16926920674578388, 0.17926296064959402, 0.15302935665209244, 0.1561524047470331]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 11.95514464378357 s
