Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 195, 'sum_payoffs': 69.26620462132186, 'action': [2.0, 0.0]}, {'num_count': 196, 'sum_payoffs': 69.95184504205824, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 60.11233831759214, 'action': [2.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 59.08429756469797, 'action': [0.0, -1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 54.37720841720743, 'action': [0.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 56.17313552893507, 'action': [0.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 62.625499556604765, 'action': [1.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 56.67791724747337, 'action': [1.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 60.67270457712137, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.12179887570268583, 0.12242348532167395, 0.1099312929419113, 0.10868207370393504, 0.10243597751405371, 0.10493441599000625, 0.11305434103685197, 0.10555902560899438, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 248, 'sum_payoffs': 87.92705838286523, 'action': [0.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 104.28702425416517, 'action': [1.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 84.11587164854612, 'action': [0.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 81.09333812813556, 'action': [0.0, -1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 110.5988576628293, 'action': [1.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 113.42422530780492, 'action': [1.0, 0.0]}])
Weights num count: [0.15490318550905685, 0.17613991255465333, 0.14990630855715179, 0.146158650843223, 0.18425983760149905, 0.18800749531542785]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 10.909809589385986 s
