Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 163, 'sum_payoffs': 54.170998124384276, 'action': [0.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 62.40442489395281, 'action': [1.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 57.07688514488989, 'action': [0.0, -1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 68.26622583012572, 'action': [2.0, 1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 54.52498071129394, 'action': [0.0, 0.0]}, {'num_count': 194, 'sum_payoffs': 69.26274507359751, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 60.29219105714465, 'action': [1.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 67.26889008814739, 'action': [2.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 58.506548066020514, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10181136789506559, 0.11242973141786383, 0.10555902560899438, 0.11992504684572143, 0.10243597751405371, 0.12117426608369769, 0.1099312929419113, 0.11867582760774516, 0.10743285446595878]
Actions to choose Agent 1: dict_values([{'num_count': 291, 'sum_payoffs': 109.02093760557806, 'action': [1.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 107.40476173910498, 'action': [1.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 110.23434831302285, 'action': [1.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 87.30899204168001, 'action': [0.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 84.93997255951273, 'action': [0.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 84.28081313043067, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18176139912554654, 0.17988757026858213, 0.18363522798251092, 0.15365396627108058, 0.15053091817613992, 0.14990630855715179]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 10.977572679519653 s
