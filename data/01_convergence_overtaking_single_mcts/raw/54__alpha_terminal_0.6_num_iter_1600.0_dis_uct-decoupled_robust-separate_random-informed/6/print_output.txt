Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 165, 'sum_payoffs': 55.2464205419492, 'action': [0.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 62.965448449755, 'action': [2.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 63.033326098771106, 'action': [1.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 61.0497402608564, 'action': [0.0, -1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 55.24172054707528, 'action': [0.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 57.13919230467242, 'action': [1.0, 0.0]}, {'num_count': 193, 'sum_payoffs': 68.94623851104342, 'action': [2.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 67.38091696784522, 'action': [2.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 62.01452796968933, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10306058713304185, 0.11305434103685197, 0.11305434103685197, 0.11055590256089944, 0.10306058713304185, 0.10555902560899438, 0.12054965646470955, 0.11867582760774516, 0.11180512179887571]
Actions to choose Agent 1: dict_values([{'num_count': 286, 'sum_payoffs': 105.6903978416484, 'action': [1.0, -1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 90.7948297407127, 'action': [0.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 104.28342252349425, 'action': [1.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 85.51138628280266, 'action': [0.0, 1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 114.40632267592254, 'action': [1.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 78.00251195252649, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17863835103060588, 0.15927545284197375, 0.17676452217364147, 0.1524047470331043, 0.18988132417239226, 0.1424109931292942]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 10.195122480392456 s
