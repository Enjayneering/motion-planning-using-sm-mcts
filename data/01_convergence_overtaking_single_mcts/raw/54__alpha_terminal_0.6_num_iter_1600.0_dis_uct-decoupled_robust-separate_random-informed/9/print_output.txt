Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 186, 'sum_payoffs': 65.43324210610895, 'action': [2.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 60.59154866165722, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 60.52228490713559, 'action': [1.0, 1.5707963267948966]}, {'num_count': 163, 'sum_payoffs': 54.19064596258169, 'action': [0.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 58.20298233439428, 'action': [0.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 66.9765936513958, 'action': [2.0, 0.0]}, {'num_count': 185, 'sum_payoffs': 65.06655733341712, 'action': [2.0, 1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 55.23635324440944, 'action': [0.0, -1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 66.982314749597, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11617738913179262, 0.1099312929419113, 0.1099312929419113, 0.10181136789506559, 0.10680824484697064, 0.11805121798875702, 0.1155527795128045, 0.10306058713304185, 0.11805121798875702]
Actions to choose Agent 1: dict_values([{'num_count': 241, 'sum_payoffs': 84.21798300375477, 'action': [0.0, 0.0]}, {'num_count': 299, 'sum_payoffs': 111.99757550131929, 'action': [1.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 82.6863876221803, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 104.21065106610774, 'action': [1.0, -1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 90.30025356851208, 'action': [0.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 105.24080548794687, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15053091817613992, 0.1867582760774516, 0.1486570893191755, 0.17676452217364147, 0.15865084322298564, 0.17801374141161774]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 10.378355979919434 s
