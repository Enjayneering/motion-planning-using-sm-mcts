Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 225, 'sum_payoffs': 76.14389977991493, 'action': [0.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 87.48039840479993, 'action': [2.0, -1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 86.52341722497734, 'action': [2.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 78.399661410714, 'action': [1.0, 1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 72.86248433957246, 'action': [0.0, 0.0]}, {'num_count': 217, 'sum_payoffs': 72.42232584591989, 'action': [1.0, 0.0]}, {'num_count': 214, 'sum_payoffs': 71.02589775868584, 'action': [0.0, 1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 87.52490232689478, 'action': [2.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 88.39296366927755, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10709186101856259, 0.1185149928605426, 0.11756306520704426, 0.10947168015230842, 0.10376011423131842, 0.10328415040456926, 0.10185625892432175, 0.1185149928605426, 0.11946692051404094]
Actions to choose Agent 1: dict_values([{'num_count': 374, 'sum_payoffs': 138.39147961294822, 'action': [1.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 151.31841486078685, 'action': [1.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 114.63667719255136, 'action': [0.0, 0.0]}, {'num_count': 313, 'sum_payoffs': 110.01248976110891, 'action': [0.0, 1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 108.6288795439574, 'action': [0.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 140.76152189826814, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17801047120418848, 0.190861494526416, 0.15373631603998097, 0.1489766777724893, 0.1475487862922418, 0.18039029033793433]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 13.884444236755371 s
