Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 233, 'sum_payoffs': 80.41964258562948, 'action': [1.0, 1.5707963267948966]}, {'num_count': 219, 'sum_payoffs': 73.73609559244771, 'action': [0.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 91.41379353126625, 'action': [2.0, -1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 90.90781019100832, 'action': [2.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 79.45025730026923, 'action': [1.0, -1.5707963267948966]}, {'num_count': 217, 'sum_payoffs': 72.92523293756967, 'action': [1.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 86.60002800236634, 'action': [2.0, 1.5707963267948966]}, {'num_count': 219, 'sum_payoffs': 73.72619605848493, 'action': [0.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 76.18057413241733, 'action': [0.0, 0.0]}])
Weights num count: [0.11089957163255593, 0.10423607805806759, 0.12184673964778676, 0.1213707758210376, 0.1099476439790576, 0.10328415040456926, 0.1170871013802951, 0.10423607805806759, 0.10661589719181343]
Actions to choose Agent 1: dict_values([{'num_count': 360, 'sum_payoffs': 131.47354512865118, 'action': [1.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 153.11953131515398, 'action': [1.0, 0.0]}, {'num_count': 317, 'sum_payoffs': 111.44827189840369, 'action': [0.0, 1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 112.34917700975238, 'action': [0.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 138.4814714395323, 'action': [1.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 114.22629920438013, 'action': [0.0, 0.0]}])
Weights num count: [0.17134697762970014, 0.19324131366016184, 0.15088053307948596, 0.1518324607329843, 0.17848643503093764, 0.15373631603998097]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.248252868652344 s
