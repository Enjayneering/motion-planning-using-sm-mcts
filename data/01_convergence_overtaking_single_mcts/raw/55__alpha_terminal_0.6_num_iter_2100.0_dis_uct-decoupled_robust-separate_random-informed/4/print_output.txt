Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 237, 'sum_payoffs': 81.5932989186528, 'action': [2.0, 1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 79.25450305634907, 'action': [0.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 87.32815550417295, 'action': [2.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 77.82317616063696, 'action': [1.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 75.0876892277272, 'action': [0.0, -1.5707963267948966]}, {'num_count': 212, 'sum_payoffs': 69.8254955271159, 'action': [0.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 86.3917052839355, 'action': [2.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 80.18487808229824, 'action': [1.0, -1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 81.57819267019367, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1128034269395526, 0.11042360780580676, 0.1185149928605426, 0.10899571632555925, 0.10613993336506425, 0.10090433127082342, 0.11756306520704426, 0.11137553545930509, 0.1128034269395526]
Actions to choose Agent 1: dict_values([{'num_count': 381, 'sum_payoffs': 141.4024860741231, 'action': [1.0, 1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 115.25764177611002, 'action': [0.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 113.43607384878729, 'action': [0.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 130.1419440638158, 'action': [1.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 114.27907763667938, 'action': [0.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 147.07459814255873, 'action': [1.0, 0.0]}])
Weights num count: [0.18134221799143266, 0.1546882436934793, 0.15278438838648262, 0.16991908614945264, 0.15373631603998097, 0.18705378391242267]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.4015212059021 s
