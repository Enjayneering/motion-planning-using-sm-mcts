Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 243, 'sum_payoffs': 84.55727968077954, 'action': [1.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 76.05147835102267, 'action': [1.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 89.78619050499076, 'action': [2.0, 0.0]}, {'num_count': 225, 'sum_payoffs': 76.03601249734339, 'action': [0.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 79.31483858175086, 'action': [0.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 73.57283175401899, 'action': [1.0, 0.0]}, {'num_count': 210, 'sum_payoffs': 69.07138303784657, 'action': [0.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 86.38833568929854, 'action': [2.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 85.01706255898792, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11565920990004759, 0.10709186101856259, 0.12089481199428843, 0.10709186101856259, 0.11042360780580676, 0.10471204188481675, 0.09995240361732509, 0.11756306520704426, 0.11613517372679677]
Actions to choose Agent 1: dict_values([{'num_count': 329, 'sum_payoffs': 117.81802830163599, 'action': [0.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 144.649540226886, 'action': [1.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 148.43319080502627, 'action': [1.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 140.79796476761666, 'action': [1.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 105.29279363713722, 'action': [0.0, -1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 109.31503455941832, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15659209900047596, 0.18372203712517848, 0.18752974773917183, 0.17991432651118516, 0.14374107567824845, 0.14802475011899097]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.109155654907227 s
