Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 222, 'sum_payoffs': 75.49914559179814, 'action': [0.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 85.93753798325163, 'action': [2.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 83.95869827310909, 'action': [1.0, -1.5707963267948966]}, {'num_count': 210, 'sum_payoffs': 69.86603048064536, 'action': [0.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 88.73228799295454, 'action': [2.0, -1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 72.18954018135582, 'action': [0.0, 0.0]}, {'num_count': 259, 'sum_payoffs': 92.97928516390711, 'action': [2.0, 0.0]}, {'num_count': 212, 'sum_payoffs': 70.78802961133054, 'action': [1.0, 0.0]}, {'num_count': 248, 'sum_payoffs': 87.84422236879925, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10566396953831508, 0.11613517372679677, 0.1142313184198001, 0.09995240361732509, 0.11899095668729176, 0.10233222275107091, 0.12327463112803427, 0.10090433127082342, 0.11803902903379343]
Actions to choose Agent 1: dict_values([{'num_count': 326, 'sum_payoffs': 115.14931629010073, 'action': [0.0, 1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 137.04906709370437, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 140.21588995838206, 'action': [1.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 110.03023799657106, 'action': [0.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 143.05609028626986, 'action': [1.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 112.34371941105, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15516420752022847, 0.1775345073774393, 0.1808662541646835, 0.14992860542598763, 0.18372203712517848, 0.15230842455973345]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 14.193464279174805 s
