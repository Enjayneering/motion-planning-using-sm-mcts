Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 242, 'sum_payoffs': 84.38663630509853, 'action': [2.0, -1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 73.15722187070489, 'action': [0.0, 1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 75.36389756828392, 'action': [0.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 83.41844214103727, 'action': [1.0, 1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 80.07463105768737, 'action': [1.0, 0.0]}, {'num_count': 252, 'sum_payoffs': 89.12156170739749, 'action': [2.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 84.43015817883872, 'action': [2.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 74.04038398078251, 'action': [0.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 78.70852804404258, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11518324607329843, 0.10376011423131842, 0.10613993336506425, 0.1142313184198001, 0.11089957163255593, 0.1199428843407901, 0.11518324607329843, 0.10471204188481675, 0.10947168015230842]
Actions to choose Agent 1: dict_values([{'num_count': 320, 'sum_payoffs': 113.10707538812515, 'action': [0.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 141.6658917209544, 'action': [1.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 135.97368639580412, 'action': [1.0, -1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 116.81888270585283, 'action': [0.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 148.77578129059793, 'action': [1.0, 0.0]}, {'num_count': 306, 'sum_payoffs': 106.54692836690357, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15230842455973345, 0.18134221799143266, 0.17563065207044265, 0.1561161351737268, 0.18848167539267016, 0.1456449309852451]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.075137615203857 s
