Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 272, 'sum_payoffs': 91.46022524788005, 'action': [1.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 94.19569940895919, 'action': [0.0, 0.0]}, {'num_count': 302, 'sum_payoffs': 105.31605512723897, 'action': [2.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 91.3797428964194, 'action': [0.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 104.28806332937414, 'action': [2.0, 1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 102.4430354429821, 'action': [1.0, 1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 89.46030610178651, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 97.36692744564427, 'action': [1.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 116.78765655225472, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10457516339869281, 0.10688196847366398, 0.11610918877354863, 0.10457516339869281, 0.11534025374855825, 0.11380238369857747, 0.10303729334871203, 0.10957324106113034, 0.12572087658592848]
Actions to choose Agent 1: dict_values([{'num_count': 467, 'sum_payoffs': 172.9451995866062, 'action': [1.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 178.0148225551096, 'action': [1.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 134.36625671392454, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 182.66962407449716, 'action': [1.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 139.0522250836594, 'action': [0.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 138.07684808324262, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17954632833525566, 0.1837754709727028, 0.14725105728565935, 0.18762014609765476, 0.1510957324106113, 0.1503267973856209]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 16.48415970802307 s
