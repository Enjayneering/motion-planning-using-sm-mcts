Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 303, 'sum_payoffs': 105.85274597472231, 'action': [1.0, -1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 105.86798539905607, 'action': [2.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 91.15222723693796, 'action': [1.0, 0.0]}, {'num_count': 313, 'sum_payoffs': 110.55738919330574, 'action': [2.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 104.07761580152368, 'action': [2.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 96.07847825462605, 'action': [0.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 97.06345329754727, 'action': [1.0, 1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 89.77141521528291, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 93.95897475016366, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11649365628604383, 0.11649365628604383, 0.10419069588619762, 0.12033833141099577, 0.11495578623606305, 0.10841983852364476, 0.10918877354863514, 0.10303729334871203, 0.10649750096116878]
Actions to choose Agent 1: dict_values([{'num_count': 381, 'sum_payoffs': 132.64938902792323, 'action': [0.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 182.33015821759315, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 181.82505702061468, 'action': [1.0, 1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 138.89124643142173, 'action': [0.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 160.76701190359344, 'action': [1.0, -1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 142.0942620539148, 'action': [0.0, 0.0]}])
Weights num count: [0.14648212226066898, 0.18838908112264513, 0.18800461361014995, 0.1518646674356017, 0.170319108035371, 0.15455594002306805]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.997782468795776 s
