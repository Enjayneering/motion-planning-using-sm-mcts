Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 268, 'sum_payoffs': 89.38612359543309, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 94.2484721444698, 'action': [0.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 98.46982399077888, 'action': [1.0, 1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 111.78176200584461, 'action': [2.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 93.91703283517708, 'action': [0.0, 0.0]}, {'num_count': 262, 'sum_payoffs': 86.62369554315505, 'action': [1.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 108.58418736010718, 'action': [2.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 106.3437264661763, 'action': [1.0, -1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 100.78074807746965, 'action': [2.0, 0.0]}])
Weights num count: [0.10303729334871203, 0.10726643598615918, 0.11072664359861592, 0.12187620146097655, 0.10688196847366398, 0.10073048827374087, 0.11918492887351019, 0.11726259131103421, 0.11264898116109189]
Actions to choose Agent 1: dict_values([{'num_count': 382, 'sum_payoffs': 134.05022224930633, 'action': [0.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 147.33088214738027, 'action': [0.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 171.19538053897875, 'action': [1.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 181.86978243280205, 'action': [1.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 130.09842792444474, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 181.32840076702064, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14686658977316416, 0.1580161476355248, 0.1780084582852749, 0.18685121107266436, 0.14340638216070742, 0.18646674356016918]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.926370859146118 s
