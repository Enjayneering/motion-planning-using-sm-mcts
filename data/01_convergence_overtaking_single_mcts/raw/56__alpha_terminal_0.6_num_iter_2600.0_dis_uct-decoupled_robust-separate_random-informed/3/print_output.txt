Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 266, 'sum_payoffs': 88.92253283689978, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 99.48650373777814, 'action': [2.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 113.89249325318504, 'action': [2.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 94.76714220988886, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 96.75833376526121, 'action': [1.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 93.91830736409912, 'action': [1.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 108.73462219673024, 'action': [2.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 94.75185934209578, 'action': [0.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 103.61724557264681, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10226835832372165, 0.1111111111111111, 0.12302960399846213, 0.10726643598615918, 0.10880430603613994, 0.10649750096116878, 0.11880046136101499, 0.10726643598615918, 0.11457131872356786]
Actions to choose Agent 1: dict_values([{'num_count': 476, 'sum_payoffs': 175.81513792528585, 'action': [1.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 126.78035868859637, 'action': [0.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 146.071827204717, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 181.42498513668008, 'action': [1.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 179.57954227728115, 'action': [1.0, 1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 129.0189389036739, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1830065359477124, 0.14148404459823144, 0.1580161476355248, 0.18762014609765476, 0.18608227604767397, 0.14340638216070742]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 16.00909972190857 s
