Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 101.44796179086923, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 94.60618991381273, 'action': [1.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 106.00402932148182, 'action': [2.0, -1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 111.51338455857575, 'action': [1.0, -1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 87.28641243026239, 'action': [0.0, 1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 105.52197719595029, 'action': [2.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 93.99068536804883, 'action': [0.0, 0.0]}, {'num_count': 264, 'sum_payoffs': 87.72067396534545, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 103.7522287446081, 'action': [2.0, 0.0]}])
Weights num count: [0.11303344867358708, 0.10726643598615918, 0.11687812379853903, 0.12149173394848135, 0.10111495578623607, 0.11649365628604383, 0.10688196847366398, 0.10149942329873125, 0.11495578623606305]
Actions to choose Agent 1: dict_values([{'num_count': 475, 'sum_payoffs': 176.3250434926255, 'action': [1.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 124.36781932007679, 'action': [0.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 188.74424059595074, 'action': [1.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 144.68149363831267, 'action': [0.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 139.27472555849414, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 170.2615600467534, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18262206843521722, 0.1387927720107651, 0.19300269127258746, 0.15609381007304882, 0.1514801999231065, 0.1776239907727797]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 15.928875923156738 s
