Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 314, 'sum_payoffs': 110.88131901133362, 'action': [2.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 95.80044226667485, 'action': [1.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 96.65910498332984, 'action': [0.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 90.71999187041979, 'action': [1.0, 0.0]}, {'num_count': 305, 'sum_payoffs': 106.83790504977674, 'action': [2.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 106.86464233342663, 'action': [2.0, 0.0]}, {'num_count': 268, 'sum_payoffs': 89.83897319542821, 'action': [0.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 107.76878145359089, 'action': [1.0, -1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 89.24180650771102, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12072279892349097, 0.10803537101114956, 0.10880430603613994, 0.10380622837370242, 0.11726259131103421, 0.11726259131103421, 0.10303729334871203, 0.11803152633602461, 0.10265282583621683]
Actions to choose Agent 1: dict_values([{'num_count': 398, 'sum_payoffs': 140.70654122349103, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 180.55548343596465, 'action': [1.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 135.7859555236318, 'action': [0.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 143.40240457899296, 'action': [0.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 172.182364618191, 'action': [1.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 168.53233584443782, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15301806997308728, 0.18646674356016918, 0.14878892733564014, 0.15532487504805845, 0.17954632833525566, 0.17647058823529413]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.65684723854065 s
