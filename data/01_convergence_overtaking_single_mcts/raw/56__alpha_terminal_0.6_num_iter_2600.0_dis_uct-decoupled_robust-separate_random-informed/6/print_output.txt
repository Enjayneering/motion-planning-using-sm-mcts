Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 303, 'sum_payoffs': 105.67296530356225, 'action': [1.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 111.16031084608362, 'action': [2.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 92.26693327162585, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 98.2910975766386, 'action': [2.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 111.12449551794252, 'action': [2.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 95.53034330259614, 'action': [1.0, 0.0]}, {'num_count': 264, 'sum_payoffs': 87.73995956206662, 'action': [0.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 100.5688332713694, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 89.99760169083491, 'action': [0.0, 0.0]}])
Weights num count: [0.11649365628604383, 0.12110726643598616, 0.1053440984236832, 0.11034217608612072, 0.12110726643598616, 0.10803537101114956, 0.10149942329873125, 0.1122645136485967, 0.10342176086120723]
Actions to choose Agent 1: dict_values([{'num_count': 384, 'sum_payoffs': 134.21334323688353, 'action': [0.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 168.84575670324003, 'action': [1.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 180.81133771187504, 'action': [1.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 137.79828693273512, 'action': [0.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 182.67607266076112, 'action': [1.0, 0.0]}, {'num_count': 388, 'sum_payoffs': 135.94511943625605, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14763552479815456, 0.1768550557477893, 0.18685121107266436, 0.15071126489811612, 0.18838908112264513, 0.14917339484813533]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.825172901153564 s
