Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 289, 'sum_payoffs': 99.63026977469838, 'action': [1.0, -1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 93.23160651991975, 'action': [0.0, -1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 104.6495324304031, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 95.41790248616309, 'action': [1.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 92.14878567875967, 'action': [0.0, 1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 112.38793829188448, 'action': [2.0, 0.0]}, {'num_count': 267, 'sum_payoffs': 89.51872662983048, 'action': [0.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 101.44355742429077, 'action': [1.0, 1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 107.45567413491156, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1111111111111111, 0.1057285659361784, 0.11534025374855825, 0.10765090349865436, 0.104959630911188, 0.12187620146097655, 0.10265282583621683, 0.11264898116109189, 0.11764705882352941]
Actions to choose Agent 1: dict_values([{'num_count': 401, 'sum_payoffs': 141.9637187541456, 'action': [0.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 183.5608988289998, 'action': [1.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 178.1176070719877, 'action': [1.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 166.50853841488177, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 132.4536444963726, 'action': [0.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 137.89966774266384, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15417147251057287, 0.18915801614763553, 0.1845444059976932, 0.17493271818531334, 0.14609765474817377, 0.15071126489811612]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 16.54235577583313 s
