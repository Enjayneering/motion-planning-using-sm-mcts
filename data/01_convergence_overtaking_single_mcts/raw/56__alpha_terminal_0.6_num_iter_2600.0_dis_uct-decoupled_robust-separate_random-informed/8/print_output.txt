Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 295, 'sum_payoffs': 101.67625523694396, 'action': [1.0, 1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 90.07956319873921, 'action': [0.0, 0.0]}, {'num_count': 267, 'sum_payoffs': 88.90607806310548, 'action': [0.0, -1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 108.01473569907319, 'action': [2.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 105.81284512319931, 'action': [2.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 107.07994451555791, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 89.70479872665695, 'action': [1.0, 0.0]}, {'num_count': 305, 'sum_payoffs': 106.24552607128875, 'action': [2.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 92.02059697154347, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11341791618608228, 0.10380622837370242, 0.10265282583621683, 0.11880046136101499, 0.11687812379853903, 0.11803152633602461, 0.10342176086120723, 0.11726259131103421, 0.1053440984236832]
Actions to choose Agent 1: dict_values([{'num_count': 397, 'sum_payoffs': 140.45190885693296, 'action': [0.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 132.29837128888838, 'action': [0.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 133.64655362301335, 'action': [0.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 193.37191661259905, 'action': [1.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 173.87989967184888, 'action': [1.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 169.19578073344866, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15263360246059207, 0.14571318723567858, 0.14686658977316416, 0.19684736639753941, 0.18069973087274124, 0.1768550557477893]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.558767080307007 s
