Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 259, 'sum_payoffs': 85.30294946656446, 'action': [0.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 105.10905026953986, 'action': [2.0, -1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 106.94755427900127, 'action': [1.0, -1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 105.56077334720594, 'action': [2.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 96.84175863732837, 'action': [1.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 103.2571673909436, 'action': [1.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 109.67004701954646, 'action': [2.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 91.74179145796244, 'action': [0.0, -1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 87.11699901075997, 'action': [0.0, 0.0]}])
Weights num count: [0.09957708573625529, 0.11610918877354863, 0.11764705882352941, 0.11649365628604383, 0.10918877354863514, 0.11457131872356786, 0.11995386389850057, 0.104959630911188, 0.10111495578623607]
Actions to choose Agent 1: dict_values([{'num_count': 392, 'sum_payoffs': 138.55891469482327, 'action': [0.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 146.3941065341278, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 173.95673005026285, 'action': [1.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 140.53652035938407, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 181.42273755418796, 'action': [1.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 164.7823943946104, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15071126489811612, 0.1572472126105344, 0.18031526336024606, 0.1522491349480969, 0.18646674356016918, 0.17262591311034217]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.518385648727417 s
