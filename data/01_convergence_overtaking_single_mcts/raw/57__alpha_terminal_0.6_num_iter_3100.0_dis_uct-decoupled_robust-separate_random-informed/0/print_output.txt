Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 305, 'sum_payoffs': 100.13222170022046, 'action': [0.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 128.11974440766267, 'action': [2.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 105.99458738820832, 'action': [1.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 117.6492181108231, 'action': [1.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 134.45217261626618, 'action': [2.0, 0.0]}, {'num_count': 352, 'sum_payoffs': 121.22433228415139, 'action': [1.0, -1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 125.32697448072621, 'action': [2.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 109.5654711289276, 'action': [0.0, -1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 118.5020655818351, 'action': [0.0, 0.0]}])
Weights num count: [0.0983553692357304, 0.11834891970332151, 0.10254756530151564, 0.1109319574330861, 0.12286359238955176, 0.11351177039664624, 0.11641405998065141, 0.10512737826507579, 0.11157691067397614]
Actions to choose Agent 1: dict_values([{'num_count': 485, 'sum_payoffs': 172.24550397832, 'action': [0.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 210.01287047938192, 'action': [1.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 157.05425447220375, 'action': [0.0, 1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 215.45988587767127, 'action': [1.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 156.23490911655614, 'action': [0.0, -1.5707963267948966]}, {'num_count': 565, 'sum_payoffs': 208.11871739730623, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15640116091583361, 0.1834891970332151, 0.145436955820703, 0.1873589164785553, 0.14479200257981295, 0.18219929055143502]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.94348406791687 s
