Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 332, 'sum_payoffs': 112.12446034771295, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 116.66767184633244, 'action': [1.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 124.43263471377523, 'action': [2.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 109.59277779571273, 'action': [0.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 117.6863309886825, 'action': [1.0, 1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 124.00041586535208, 'action': [2.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 127.15881224514061, 'action': [2.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 123.53078364005576, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 105.54252517716631, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10706223798774589, 0.11028700419219607, 0.11576910673976137, 0.10512737826507579, 0.1109319574330861, 0.11544663011931634, 0.11770396646243148, 0.11512415349887133, 0.10222508868107062]
Actions to choose Agent 1: dict_values([{'num_count': 537, 'sum_payoffs': 195.57800215268927, 'action': [1.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 164.81717353058917, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 161.16155560863422, 'action': [0.0, 1.5707963267948966]}, {'num_count': 614, 'sum_payoffs': 230.61077549070592, 'action': [1.0, 0.0]}, {'num_count': 472, 'sum_payoffs': 166.53673245933174, 'action': [0.0, -1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 200.92786394350532, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17316994517897452, 0.1509190583682683, 0.14833924540470816, 0.19800064495324088, 0.15220896485004837, 0.17703966462431472]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.526607275009155 s
