Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 351, 'sum_payoffs': 120.79834217616087, 'action': [1.0, 1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 106.84545396661439, 'action': [1.0, 0.0]}, {'num_count': 326, 'sum_payoffs': 109.54113520541294, 'action': [0.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 128.05607478119427, 'action': [2.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 131.2082903219557, 'action': [2.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 105.96448227221575, 'action': [0.0, 1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 129.72659158174852, 'action': [2.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 121.71649409690619, 'action': [1.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 106.79548415627337, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11318929377620122, 0.10319251854240567, 0.10512737826507579, 0.11834891970332151, 0.12060625604643663, 0.10254756530151564, 0.11963882618510158, 0.11383424701709126, 0.10319251854240567]
Actions to choose Agent 1: dict_values([{'num_count': 555, 'sum_payoffs': 203.6015384236313, 'action': [1.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 177.88027131766776, 'action': [0.0, -1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 165.4727888339273, 'action': [0.0, 0.0]}, {'num_count': 551, 'sum_payoffs': 201.74499253096013, 'action': [1.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 210.40799447321916, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 159.14974397102284, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17897452434698485, 0.16059335698161883, 0.15156401160915833, 0.17768461786520479, 0.1838116736536601, 0.1470493389229281]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.314889669418335 s
