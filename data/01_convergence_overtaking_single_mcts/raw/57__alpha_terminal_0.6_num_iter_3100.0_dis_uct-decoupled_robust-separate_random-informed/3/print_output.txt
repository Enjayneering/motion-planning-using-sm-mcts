Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 385, 'sum_payoffs': 137.08585829605383, 'action': [2.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 124.66597844257684, 'action': [2.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 104.44154638812152, 'action': [0.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 115.63541785749054, 'action': [1.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 121.61206577991814, 'action': [1.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 129.2766278323937, 'action': [2.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 112.40375318542944, 'action': [0.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 108.35452231500334, 'action': [0.0, 0.0]}, {'num_count': 334, 'sum_payoffs': 113.75063797673101, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12415349887133183, 0.11544663011931634, 0.10093518219929055, 0.10899709771041599, 0.11318929377620122, 0.11867139632376653, 0.10673976136730087, 0.1038374717832957, 0.10770719122863592]
Actions to choose Agent 1: dict_values([{'num_count': 580, 'sum_payoffs': 213.91784550726553, 'action': [1.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 159.346738570013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 162.88873755610572, 'action': [0.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 161.06283160678163, 'action': [0.0, 0.0]}, {'num_count': 567, 'sum_payoffs': 207.96478125725773, 'action': [1.0, 1.5707963267948966]}, {'num_count': 567, 'sum_payoffs': 208.04570812999387, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18703643985811028, 0.14769429216381813, 0.15027410512737827, 0.1489841986455982, 0.18284424379232506, 0.18284424379232506]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.00241208076477 s
