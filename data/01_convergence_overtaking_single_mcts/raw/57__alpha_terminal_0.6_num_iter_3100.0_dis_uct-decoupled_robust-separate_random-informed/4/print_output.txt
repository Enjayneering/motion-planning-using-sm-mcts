Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 367, 'sum_payoffs': 127.90684737750252, 'action': [2.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 124.40015067234847, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 105.5821848091855, 'action': [0.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 138.48864380291425, 'action': [2.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 106.89395672383544, 'action': [0.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 120.80837961158254, 'action': [2.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 114.51677333268451, 'action': [1.0, 0.0]}, {'num_count': 317, 'sum_payoffs': 105.56111649477343, 'action': [0.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 116.620154427034, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11834891970332151, 0.11576910673976137, 0.10222508868107062, 0.12576588197355693, 0.10319251854240567, 0.11318929377620122, 0.10867462108997097, 0.10222508868107062, 0.11028700419219607]
Actions to choose Agent 1: dict_values([{'num_count': 574, 'sum_payoffs': 212.1181439032411, 'action': [1.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 191.7369379964259, 'action': [1.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 156.8312799855317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 173.4198888445374, 'action': [0.0, -1.5707963267948966]}, {'num_count': 560, 'sum_payoffs': 205.65640888847298, 'action': [1.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 177.79410706111236, 'action': [0.0, 0.0]}])
Weights num count: [0.18510158013544017, 0.1705901322154144, 0.145436955820703, 0.15736859077716867, 0.18058690744920994, 0.16059335698161883]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.6908917427063 s
