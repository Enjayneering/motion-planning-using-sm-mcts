Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 361, 'sum_payoffs': 125.112843991808, 'action': [2.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 132.83714176099883, 'action': [2.0, 0.0]}, {'num_count': 317, 'sum_payoffs': 105.39020448184378, 'action': [0.0, -1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 106.28506143283907, 'action': [0.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 111.93094255017775, 'action': [1.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 118.22613877732056, 'action': [1.0, -1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 129.08083907272876, 'action': [2.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 109.37383256420193, 'action': [0.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 120.4843947603766, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11641405998065141, 0.12189616252821671, 0.10222508868107062, 0.10287004192196066, 0.10706223798774589, 0.11157691067397614, 0.11931634956465656, 0.10512737826507579, 0.11318929377620122]
Actions to choose Agent 1: dict_values([{'num_count': 552, 'sum_payoffs': 203.10305815300728, 'action': [1.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 164.03028917717958, 'action': [0.0, 1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 209.4398624790033, 'action': [1.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 164.47973500666848, 'action': [0.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 171.58486935311345, 'action': [0.0, -1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 210.85762092306564, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1780070944856498, 0.14995162850693325, 0.18252176717188004, 0.15027410512737827, 0.15543373105449854, 0.1834891970332151]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.83200716972351 s
