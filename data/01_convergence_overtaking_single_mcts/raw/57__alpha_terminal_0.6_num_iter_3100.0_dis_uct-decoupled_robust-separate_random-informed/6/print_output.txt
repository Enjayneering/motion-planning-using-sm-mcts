Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 364, 'sum_payoffs': 126.55701400398316, 'action': [1.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 112.6600698369715, 'action': [0.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 111.71606959889445, 'action': [1.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 105.05174652313687, 'action': [0.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 115.77813409707059, 'action': [1.0, 1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 102.75889762058115, 'action': [0.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 130.2593582270116, 'action': [2.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 127.07129023404751, 'action': [2.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 128.3943913503561, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11738148984198646, 0.1073847146081909, 0.10673976136730087, 0.1019026120606256, 0.10964205095130602, 0.10029022895840052, 0.1199613028055466, 0.11770396646243148, 0.11867139632376653]
Actions to choose Agent 1: dict_values([{'num_count': 555, 'sum_payoffs': 203.43662349966198, 'action': [1.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 158.71434135997424, 'action': [0.0, -1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 215.1770774640384, 'action': [1.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 152.91490740848963, 'action': [0.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 168.81369699753566, 'action': [0.0, 0.0]}, {'num_count': 589, 'sum_payoffs': 218.69414327070677, 'action': [1.0, 0.0]}])
Weights num count: [0.17897452434698485, 0.14672686230248308, 0.1873589164785553, 0.14253466623669783, 0.15414382457271847, 0.18993872944211546]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.526387453079224 s
