Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 359, 'sum_payoffs': 124.40052964167745, 'action': [2.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 110.93323391725848, 'action': [0.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 102.3559316306503, 'action': [0.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 107.79477012691487, 'action': [0.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 122.12494849073758, 'action': [1.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 114.79798138836908, 'action': [1.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 114.03732558286549, 'action': [1.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 129.16368446988795, 'action': [2.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 134.8846515464464, 'action': [2.0, 0.0]}])
Weights num count: [0.11576910673976137, 0.10609480812641084, 0.0999677523379555, 0.1038374717832957, 0.11415672363753628, 0.10899709771041599, 0.10835214446952596, 0.11931634956465656, 0.12318606900999678]
Actions to choose Agent 1: dict_values([{'num_count': 468, 'sum_payoffs': 164.01413401128255, 'action': [0.0, -1.5707963267948966]}, {'num_count': 590, 'sum_payoffs': 218.79955851062007, 'action': [1.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 200.27491549566025, 'action': [1.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 165.64970154621034, 'action': [0.0, 0.0]}, {'num_count': 578, 'sum_payoffs': 213.40147258794264, 'action': [1.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 153.06034481314532, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1509190583682683, 0.19026120606256047, 0.17703966462431472, 0.15220896485004837, 0.18639148661722024, 0.14285714285714285]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.59311842918396 s
