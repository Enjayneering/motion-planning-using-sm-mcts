Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 118.25750454869285, 'action': [2.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 132.81391261151117, 'action': [2.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 114.73229457272461, 'action': [1.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 114.6375105062671, 'action': [1.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 103.12752365887678, 'action': [0.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 123.29228681525868, 'action': [1.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 107.60203955018875, 'action': [0.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 111.13468201098269, 'action': [0.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 133.24551946169728, 'action': [2.0, 0.0]}])
Weights num count: [0.11157691067397614, 0.12189616252821671, 0.10899709771041599, 0.10899709771041599, 0.10061270557884554, 0.11512415349887133, 0.1038374717832957, 0.10641728474685586, 0.12221863914866173]
Actions to choose Agent 1: dict_values([{'num_count': 480, 'sum_payoffs': 170.74908821016368, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 157.81565773135995, 'action': [0.0, 1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 211.4104371000867, 'action': [1.0, -1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 199.98160174283385, 'action': [1.0, 1.5707963267948966]}, {'num_count': 602, 'sum_payoffs': 225.85754281955732, 'action': [1.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 158.29690476424128, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1547887778136085, 0.145436955820703, 0.1838116736536601, 0.17574975814253466, 0.19413092550790068, 0.14575943244114803]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.77660298347473 s
