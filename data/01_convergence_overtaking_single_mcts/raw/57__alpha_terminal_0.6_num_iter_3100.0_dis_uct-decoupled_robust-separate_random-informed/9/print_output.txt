Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 339, 'sum_payoffs': 115.4836889057736, 'action': [1.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 128.55401700732526, 'action': [2.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 130.36196330711363, 'action': [2.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 115.31696559873134, 'action': [0.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 106.97514209847965, 'action': [0.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 97.13506520309429, 'action': [0.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 126.70031407602735, 'action': [1.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 113.66861909315114, 'action': [1.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 127.216996556127, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10931957433086101, 0.11867139632376653, 0.1199613028055466, 0.10931957433086101, 0.10319251854240567, 0.09609803289261529, 0.11738148984198646, 0.10802966784908094, 0.11770396646243148]
Actions to choose Agent 1: dict_values([{'num_count': 443, 'sum_payoffs': 153.21002388996345, 'action': [0.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 160.7914396792434, 'action': [0.0, -1.5707963267948966]}, {'num_count': 611, 'sum_payoffs': 228.5768013293674, 'action': [1.0, 0.0]}, {'num_count': 550, 'sum_payoffs': 200.9071731735187, 'action': [1.0, -1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 169.10642237429053, 'action': [0.0, 0.0]}, {'num_count': 557, 'sum_payoffs': 204.12930394855803, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14285714285714285, 0.14833924540470816, 0.19703321509190583, 0.17736214124475974, 0.15446630119316349, 0.1796194775878749]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.939680814743042 s
