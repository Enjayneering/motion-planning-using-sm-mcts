Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 371, 'sum_payoffs': 123.38424764751413, 'action': [0.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 120.70439041603336, 'action': [0.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 115.90739506676837, 'action': [0.0, 1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 143.23275116046426, 'action': [2.0, 1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 159.27908001189462, 'action': [2.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 144.03164936597395, 'action': [1.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 135.26886014136846, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 146.66409786159565, 'action': [2.0, -1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 136.93044780107894, 'action': [1.0, 0.0]}])
Weights num count: [0.10302693696195502, 0.1013607331296862, 0.09830602610386004, 0.11552346570397112, 0.125520688697584, 0.11607886698139405, 0.11052485420716468, 0.11774507081366287, 0.11163565676201055]
Actions to choose Agent 1: dict_values([{'num_count': 647, 'sum_payoffs': 237.43751067955037, 'action': [1.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 177.269405650021, 'action': [0.0, 1.5707963267948966]}, {'num_count': 687, 'sum_payoffs': 255.35134700649334, 'action': [1.0, 0.0]}, {'num_count': 638, 'sum_payoffs': 233.4069341945897, 'action': [1.0, 1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 205.521194275729, 'action': [0.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 190.88834017108684, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17967231324632046, 0.1419050263815607, 0.19078033879477924, 0.17717300749791726, 0.1596778672590947, 0.15051374618161623]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.282381057739258 s
