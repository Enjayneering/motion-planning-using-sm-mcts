Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 402, 'sum_payoffs': 136.8427980962988, 'action': [1.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 153.37364580164046, 'action': [2.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 144.46819432952583, 'action': [1.0, -1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 123.6707916650552, 'action': [0.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 125.95759990446902, 'action': [0.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 147.0328088206251, 'action': [2.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 140.46281522742646, 'action': [2.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 123.68495072755435, 'action': [0.0, 0.0]}, {'num_count': 384, 'sum_payoffs': 129.0564102514921, 'action': [1.0, 0.0]}])
Weights num count: [0.11163565676201055, 0.1219105803943349, 0.11635656762010553, 0.10330463760066648, 0.10469314079422383, 0.11802277145237434, 0.11385726187170231, 0.10330463760066648, 0.10663704526520411]
Actions to choose Agent 1: dict_values([{'num_count': 554, 'sum_payoffs': 195.056927077111, 'action': [0.0, 0.0]}, {'num_count': 651, 'sum_payoffs': 237.7582378805507, 'action': [1.0, -1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 225.0542286632924, 'action': [1.0, 1.5707963267948966]}, {'num_count': 697, 'sum_payoffs': 258.32338212582783, 'action': [1.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 187.63379368597583, 'action': [0.0, -1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 188.42729852530155, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15384615384615385, 0.18078311580116635, 0.17272979727853374, 0.1935573451818939, 0.14912524298805888, 0.1496806442654818]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.907201766967773 s
