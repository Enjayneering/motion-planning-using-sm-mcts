Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 381, 'sum_payoffs': 127.82549300856314, 'action': [1.0, 1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 151.2497959080008, 'action': [2.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 130.01785678413992, 'action': [0.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 132.65091836584045, 'action': [1.0, 0.0]}, {'num_count': 423, 'sum_payoffs': 146.34980063767935, 'action': [2.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 127.27773534804727, 'action': [0.0, 0.0]}, {'num_count': 355, 'sum_payoffs': 116.42773754968863, 'action': [0.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 148.1880093874699, 'action': [2.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 145.92883450826528, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1058039433490697, 0.12052207720077757, 0.10719244654262705, 0.10885865037489587, 0.1174673701749514, 0.10552624271035824, 0.09858372674257151, 0.11857817272979727, 0.11718966953623994]
Actions to choose Agent 1: dict_values([{'num_count': 544, 'sum_payoffs': 190.49583019115158, 'action': [0.0, 1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 224.40076951877276, 'action': [1.0, 1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 187.92513385103064, 'action': [0.0, -1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 235.94343448670034, 'action': [1.0, -1.5707963267948966]}, {'num_count': 702, 'sum_payoffs': 260.482559023019, 'action': [1.0, 0.0]}, {'num_count': 548, 'sum_payoffs': 192.21515695854464, 'action': [0.0, 0.0]}])
Weights num count: [0.15106914745903915, 0.17245209663982228, 0.14940294362677034, 0.17967231324632046, 0.19494584837545126, 0.15217995001388504]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.310053825378418 s
