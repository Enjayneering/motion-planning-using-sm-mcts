Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 436, 'sum_payoffs': 152.31243282593036, 'action': [2.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 131.31932207102196, 'action': [1.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 144.75482306029483, 'action': [2.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 139.33174714206513, 'action': [1.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 118.28122058711665, 'action': [0.0, 1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 124.40740386466855, 'action': [0.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 155.28965563046253, 'action': [2.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 127.93435671671189, 'action': [0.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 133.18455669434186, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1210774784782005, 0.10802554845876146, 0.11635656762010553, 0.1130241599555679, 0.09969452929741739, 0.10358233823937794, 0.12302138294918079, 0.1058039433490697, 0.10913635101360733]
Actions to choose Agent 1: dict_values([{'num_count': 616, 'sum_payoffs': 222.32847901736915, 'action': [1.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 188.3907050164331, 'action': [0.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 247.7376018049588, 'action': [1.0, -1.5707963267948966]}, {'num_count': 684, 'sum_payoffs': 252.55634817755586, 'action': [1.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 198.43023785115236, 'action': [0.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 182.8807422423762, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17106359344626493, 0.1496806442654818, 0.18689252985281865, 0.18994723687864482, 0.1560677589558456, 0.1460705359622327]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.094103813171387 s
