Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 359, 'sum_payoffs': 118.44478956692882, 'action': [0.0, -1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 149.53685553603694, 'action': [2.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 129.45354059417673, 'action': [1.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 130.74530031809067, 'action': [0.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 143.277398438747, 'action': [2.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 155.7568651017001, 'action': [2.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 146.3340098890985, 'action': [1.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 134.31589291274275, 'action': [1.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 121.64758877780966, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.09969452929741739, 0.11913357400722022, 0.10663704526520411, 0.10747014718133852, 0.11524576506525964, 0.12302138294918079, 0.11718966953623994, 0.10969175229103027, 0.10163843376839767]
Actions to choose Agent 1: dict_values([{'num_count': 517, 'sum_payoffs': 177.68566787219584, 'action': [0.0, 1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 264.35658251640126, 'action': [1.0, 0.0]}, {'num_count': 536, 'sum_payoffs': 185.94534886432254, 'action': [0.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 197.112932862252, 'action': [0.0, -1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 225.3244033249068, 'action': [1.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 233.700814969861, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1435712302138295, 0.1982782560399889, 0.1488475423493474, 0.1560677589558456, 0.17384059983337963, 0.17911691196889754]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.6187961101532 s
