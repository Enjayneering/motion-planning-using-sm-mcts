Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 421, 'sum_payoffs': 144.84203677804393, 'action': [2.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 131.13623400008308, 'action': [1.0, 0.0]}, {'num_count': 434, 'sum_payoffs': 150.54159669233044, 'action': [2.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 118.84960194684163, 'action': [0.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 132.40851941087288, 'action': [1.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 121.91212475431864, 'action': [0.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 125.38559558676609, 'action': [0.0, -1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 137.7701818013199, 'action': [1.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 157.20825177819262, 'action': [2.0, 0.0]}])
Weights num count: [0.11691196889752846, 0.10830324909747292, 0.12052207720077757, 0.1005276312135518, 0.10913635101360733, 0.10247153568453207, 0.10469314079422383, 0.11246875867814496, 0.1246875867814496]
Actions to choose Agent 1: dict_values([{'num_count': 552, 'sum_payoffs': 194.8319797212969, 'action': [0.0, 0.0]}, {'num_count': 698, 'sum_payoffs': 259.7732458889677, 'action': [1.0, 0.0]}, {'num_count': 667, 'sum_payoffs': 245.67529614062104, 'action': [1.0, 1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 224.0357156962219, 'action': [1.0, -1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 187.40522532675723, 'action': [0.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 185.29006434872562, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1532907525687309, 0.1938350458206054, 0.18522632602054984, 0.17161899472368786, 0.14856984171063595, 0.1471813385170786]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.06526803970337 s
