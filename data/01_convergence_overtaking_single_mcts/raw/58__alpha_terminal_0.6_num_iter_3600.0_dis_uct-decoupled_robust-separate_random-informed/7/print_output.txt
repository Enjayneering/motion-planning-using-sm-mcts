Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 456, 'sum_payoffs': 161.9769611603345, 'action': [2.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 119.77280787300263, 'action': [0.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 128.93454225401268, 'action': [1.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 122.35657721431032, 'action': [0.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 153.33029903830797, 'action': [2.0, -1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 146.76456035841963, 'action': [2.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 140.10983485339705, 'action': [1.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 129.4400436738583, 'action': [0.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 130.2555083610937, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12663149125242987, 0.10024993057484032, 0.10608164398778117, 0.10191613440710913, 0.12135517911691197, 0.11718966953623994, 0.1130241599555679, 0.10635934462649264, 0.10691474590391557]
Actions to choose Agent 1: dict_values([{'num_count': 512, 'sum_payoffs': 175.9304314784919, 'action': [0.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 194.75917614991013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 650, 'sum_payoffs': 236.46524304553446, 'action': [1.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 242.70243595487204, 'action': [1.0, -1.5707963267948966]}, {'num_count': 686, 'sum_payoffs': 252.55578293091386, 'action': [1.0, 0.0]}, {'num_count': 533, 'sum_payoffs': 185.14814010520848, 'action': [0.0, 0.0]}])
Weights num count: [0.14218272702027215, 0.15412385448486532, 0.18050541516245489, 0.18439322410441544, 0.19050263815606777, 0.148014440433213]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.91693902015686 s
