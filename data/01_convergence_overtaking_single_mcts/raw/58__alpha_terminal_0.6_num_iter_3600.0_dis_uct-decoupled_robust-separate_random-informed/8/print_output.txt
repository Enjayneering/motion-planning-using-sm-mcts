Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 450, 'sum_payoffs': 158.02855058517275, 'action': [2.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 130.09406701453452, 'action': [1.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 151.71442042634985, 'action': [2.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 132.7275948489752, 'action': [2.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 118.8327800936478, 'action': [0.0, 1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 123.05120200288638, 'action': [0.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 134.02740776977785, 'action': [0.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 135.81700578243064, 'action': [1.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 138.42179585008333, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.12496528742016107, 0.10747014718133852, 0.1210774784782005, 0.10913635101360733, 0.10024993057484032, 0.10302693696195502, 0.10996945292974174, 0.11108025548458761, 0.11274645931685642]
Actions to choose Agent 1: dict_values([{'num_count': 694, 'sum_payoffs': 256.0426326917276, 'action': [1.0, 0.0]}, {'num_count': 614, 'sum_payoffs': 220.63360115973737, 'action': [1.0, -1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 240.11426247574113, 'action': [1.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 205.5932990431807, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 174.18199274319213, 'action': [0.0, 1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 190.80346933210797, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.19272424326575952, 0.170508192168842, 0.18272702027214663, 0.16106637045265204, 0.14107192446542627, 0.15162454873646208]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.796501874923706 s
