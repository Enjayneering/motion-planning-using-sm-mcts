Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 357, 'sum_payoffs': 117.24575901915729, 'action': [0.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 138.71987780568153, 'action': [2.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 137.35810154459924, 'action': [1.0, 0.0]}, {'num_count': 404, 'sum_payoffs': 137.95064386580546, 'action': [1.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 156.13408464374518, 'action': [2.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 124.71949159555497, 'action': [0.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 125.50362233793388, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 153.94635548910085, 'action': [2.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 133.95277254036978, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.09913912801999444, 0.11274645931685642, 0.11191335740072202, 0.1121910580394335, 0.12357678422660372, 0.10386003887808942, 0.10441544015551235, 0.12218828103304638, 0.10969175229103027]
Actions to choose Agent 1: dict_values([{'num_count': 517, 'sum_payoffs': 178.62709090204118, 'action': [0.0, 1.5707963267948966]}, {'num_count': 712, 'sum_payoffs': 264.87030525033816, 'action': [1.0, 0.0]}, {'num_count': 524, 'sum_payoffs': 181.73465592528217, 'action': [0.0, -1.5707963267948966]}, {'num_count': 646, 'sum_payoffs': 235.45639554780126, 'action': [1.0, 1.5707963267948966]}, {'num_count': 638, 'sum_payoffs': 231.82384722516113, 'action': [1.0, -1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 198.59994055383598, 'action': [0.0, 0.0]}])
Weights num count: [0.1435712302138295, 0.19772285476256596, 0.14551513468480978, 0.179394612607609, 0.17717300749791726, 0.15634545959455706]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.592530727386475 s
