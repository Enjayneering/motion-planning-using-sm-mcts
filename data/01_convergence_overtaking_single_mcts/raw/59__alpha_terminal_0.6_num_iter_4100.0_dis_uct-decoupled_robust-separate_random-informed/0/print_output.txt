Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 431, 'sum_payoffs': 143.14753626181795, 'action': [0.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 167.62808837614713, 'action': [2.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 151.0080492774764, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 150.24321057507254, 'action': [0.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 135.55489557836134, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 149.7055541013921, 'action': [1.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 155.78759034674007, 'action': [2.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 158.80597976721126, 'action': [1.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 173.2818018197865, 'action': [2.0, 0.0]}])
Weights num count: [0.10509631797122652, 0.11875152401853206, 0.10948549134357474, 0.10899780541331383, 0.10070714459887832, 0.10875396244818337, 0.11216776396000976, 0.11387466471592295, 0.12192148256522799]
Actions to choose Agent 1: dict_values([{'num_count': 641, 'sum_payoffs': 224.90602963932116, 'action': [0.0, 0.0]}, {'num_count': 724, 'sum_payoffs': 260.9801819518293, 'action': [1.0, -1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 254.38829815088957, 'action': [1.0, 1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 203.4661527935701, 'action': [0.0, -1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 216.6258108657808, 'action': [0.0, 1.5707963267948966]}, {'num_count': 813, 'sum_payoffs': 299.95486295181433, 'action': [1.0, 0.0]}])
Weights num count: [0.15630334064862228, 0.17654230675445012, 0.1728846622774933, 0.1441111923920995, 0.15167032431114363, 0.19824433065106073]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.60853600502014 s
