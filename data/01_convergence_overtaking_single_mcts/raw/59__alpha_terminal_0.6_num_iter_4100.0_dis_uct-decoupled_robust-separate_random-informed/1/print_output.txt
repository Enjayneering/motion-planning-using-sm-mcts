Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 415, 'sum_payoffs': 136.77231669055362, 'action': [0.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 157.1078871720518, 'action': [1.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 130.29313259457854, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 149.30759611830229, 'action': [1.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 175.80472299902593, 'action': [2.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 142.77440878531323, 'action': [0.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 180.74710578695786, 'action': [2.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 167.06538283886815, 'action': [2.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 149.25468240254628, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10119483052913923, 0.11265544989027067, 0.0975371860521824, 0.10826627651792246, 0.12314069739088028, 0.10460863204096561, 0.1258229700073153, 0.11826383808827115, 0.10826627651792246]
Actions to choose Agent 1: dict_values([{'num_count': 733, 'sum_payoffs': 265.9810386820399, 'action': [1.0, 1.5707963267948966]}, {'num_count': 601, 'sum_payoffs': 208.58599175251894, 'action': [0.0, -1.5707963267948966]}, {'num_count': 631, 'sum_payoffs': 221.60278272800932, 'action': [0.0, 0.0]}, {'num_count': 724, 'sum_payoffs': 262.0538173810493, 'action': [1.0, -1.5707963267948966]}, {'num_count': 823, 'sum_payoffs': 305.401918464895, 'action': [1.0, 0.0]}, {'num_count': 588, 'sum_payoffs': 202.99462204337388, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17873689344062424, 0.14654962204340405, 0.15386491099731772, 0.17654230675445012, 0.2006827603023653, 0.1433796634967081]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.264463901519775 s
