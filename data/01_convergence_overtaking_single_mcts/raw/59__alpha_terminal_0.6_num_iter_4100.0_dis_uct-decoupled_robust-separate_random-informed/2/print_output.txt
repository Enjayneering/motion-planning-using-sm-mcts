Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 505, 'sum_payoffs': 174.78270712785613, 'action': [2.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 161.77199079961147, 'action': [2.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 133.3715595865199, 'action': [0.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 141.44876362512707, 'action': [0.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 155.72455662045152, 'action': [1.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 145.8339264431043, 'action': [0.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 158.82325291180447, 'action': [2.0, 1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 157.9495146707164, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 150.9999671958079, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12314069739088028, 0.11582540843696659, 0.0997317727383565, 0.10436478907583516, 0.11241160692514021, 0.10680321872713973, 0.1141185076810534, 0.11363082175079249, 0.1097293343087052]
Actions to choose Agent 1: dict_values([{'num_count': 601, 'sum_payoffs': 209.03394593700867, 'action': [0.0, -1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 269.1222751702697, 'action': [1.0, 1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 233.71243731452657, 'action': [0.0, 0.0]}, {'num_count': 788, 'sum_payoffs': 290.77568841140334, 'action': [1.0, 0.0]}, {'num_count': 603, 'sum_payoffs': 209.93542552930563, 'action': [0.0, 1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 256.90653011893727, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14654962204340405, 0.18019995123140697, 0.16044867105584004, 0.19214825652279932, 0.14703730797366496, 0.1733723482077542]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.498318195343018 s
