Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 414, 'sum_payoffs': 135.62591751729167, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 152.42517470244042, 'action': [1.0, 1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 177.55090304702995, 'action': [2.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 149.84524442459443, 'action': [2.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 155.02588328312902, 'action': [1.0, -1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 142.9209620610282, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 130.3668778171967, 'action': [0.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 148.02041630661728, 'action': [1.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 190.37666679836218, 'action': [2.0, 0.0]}])
Weights num count: [0.10095098756400878, 0.11046086320409657, 0.12460375518166301, 0.10899780541331383, 0.1119239209948793, 0.10509631797122652, 0.0980248719824433, 0.10802243355279201, 0.13167520117044623]
Actions to choose Agent 1: dict_values([{'num_count': 717, 'sum_payoffs': 258.6638076517808, 'action': [1.0, 1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 213.13392292716725, 'action': [0.0, -1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 228.1981945748346, 'action': [0.0, 0.0]}, {'num_count': 618, 'sum_payoffs': 215.70162240028696, 'action': [0.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 255.16331017593922, 'action': [1.0, -1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 293.7715206377601, 'action': [1.0, 0.0]}])
Weights num count: [0.17483540599853695, 0.14923189465983908, 0.15776639843940501, 0.1506949524506218, 0.1728846622774933, 0.1943428432089734]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.430325746536255 s
