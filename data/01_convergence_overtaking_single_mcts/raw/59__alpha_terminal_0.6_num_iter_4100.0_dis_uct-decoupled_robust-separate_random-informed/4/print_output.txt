Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 384, 'sum_payoffs': 122.2664182430545, 'action': [0.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 141.89813751662277, 'action': [0.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 160.36333203945873, 'action': [1.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 153.86218490508682, 'action': [2.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 153.46006338045964, 'action': [1.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 138.07384359946042, 'action': [0.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 180.86947064737564, 'action': [2.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 153.01283481317856, 'action': [1.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 172.5712049741474, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.0936356986100951, 0.10485247500609607, 0.11533772250670568, 0.11168007802974884, 0.11143623506461839, 0.10265788831992197, 0.12679834186783712, 0.11119239209948793, 0.12216532553035844]
Actions to choose Agent 1: dict_values([{'num_count': 812, 'sum_payoffs': 301.2183134519529, 'action': [1.0, 0.0]}, {'num_count': 570, 'sum_payoffs': 195.67107355003984, 'action': [0.0, -1.5707963267948966]}, {'num_count': 734, 'sum_payoffs': 266.9428516290295, 'action': [1.0, -1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 233.69743507076197, 'action': [0.0, 0.0]}, {'num_count': 594, 'sum_payoffs': 205.93492310318558, 'action': [0.0, 1.5707963267948966]}, {'num_count': 732, 'sum_payoffs': 265.96135986856603, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.19800048768593026, 0.1389904901243599, 0.17898073640575468, 0.16044867105584004, 0.14484272128749084, 0.17849305047549377]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.138158082962036 s
