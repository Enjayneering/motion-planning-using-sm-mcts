Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 514, 'sum_payoffs': 179.4181299351046, 'action': [2.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 141.1971130748745, 'action': [0.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 140.18595691335597, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 173.38978056020682, 'action': [2.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 150.30032710085788, 'action': [1.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 165.37642369127224, 'action': [2.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 133.0668232125192, 'action': [0.0, 1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 148.07539628098624, 'action': [1.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 154.92175186667484, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.12533528407705438, 0.10387710314557425, 0.10338941721531333, 0.12192148256522799, 0.10899780541331383, 0.11753230919287978, 0.09924408680809559, 0.10777859058766155, 0.11168007802974884]
Actions to choose Agent 1: dict_values([{'num_count': 744, 'sum_payoffs': 270.98260634769935, 'action': [1.0, 1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 294.2677520765063, 'action': [1.0, 0.0]}, {'num_count': 599, 'sum_payoffs': 207.89309998892142, 'action': [0.0, -1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 220.75853131649828, 'action': [0.0, 0.0]}, {'num_count': 593, 'sum_payoffs': 205.21004798384197, 'action': [0.0, 1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 268.26122094834903, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18141916605705927, 0.1943428432089734, 0.14606193611314314, 0.1533772250670568, 0.1445988783223604, 0.1799561082662765]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.2412531375885 s
