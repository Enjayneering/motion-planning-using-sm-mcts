Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 456, 'sum_payoffs': 153.57160105954134, 'action': [2.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 150.50777723467627, 'action': [1.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 164.6358268523259, 'action': [2.0, 1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 184.4425706807795, 'action': [2.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 155.57583020811958, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 131.54499672081505, 'action': [0.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 149.673795443998, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 130.2878603847276, 'action': [0.0, 1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 160.00640826024784, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11119239209948793, 0.10948549134357474, 0.11753230919287978, 0.1285052426237503, 0.11241160692514021, 0.09875640087783467, 0.10899780541331383, 0.0980248719824433, 0.11485003657644477]
Actions to choose Agent 1: dict_values([{'num_count': 600, 'sum_payoffs': 207.72855269871107, 'action': [0.0, -1.5707963267948966]}, {'num_count': 637, 'sum_payoffs': 223.71928443134405, 'action': [0.0, 0.0]}, {'num_count': 723, 'sum_payoffs': 261.0898835383136, 'action': [1.0, -1.5707963267948966]}, {'num_count': 598, 'sum_payoffs': 206.93121605693992, 'action': [0.0, 1.5707963267948966]}, {'num_count': 747, 'sum_payoffs': 271.60458699912635, 'action': [1.0, 1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 292.7495139893353, 'action': [1.0, 0.0]}])
Weights num count: [0.14630577907827358, 0.15532796878810046, 0.17629846378931968, 0.14581809314801267, 0.18215069495245062, 0.1938551572787125]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.859694242477417 s
