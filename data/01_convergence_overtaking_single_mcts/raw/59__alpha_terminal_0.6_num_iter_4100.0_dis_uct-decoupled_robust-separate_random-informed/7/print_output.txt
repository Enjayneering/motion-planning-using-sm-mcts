Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 537, 'sum_payoffs': 189.75313011401275, 'action': [2.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 152.01673691938421, 'action': [1.0, 0.0]}, {'num_count': 425, 'sum_payoffs': 140.75043150058224, 'action': [0.0, 0.0]}, {'num_count': 431, 'sum_payoffs': 143.3741864530263, 'action': [1.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 173.05490730090875, 'action': [2.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 140.90785593170656, 'action': [0.0, -1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 160.79863658465294, 'action': [1.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 152.4139143032118, 'action': [2.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 133.9386688655993, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.13094367227505485, 0.10997317727383565, 0.10363326018044379, 0.10509631797122652, 0.12167763960009753, 0.10363326018044379, 0.11485003657644477, 0.11021702023896611, 0.0997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 816, 'sum_payoffs': 301.50995787443884, 'action': [1.0, 0.0]}, {'num_count': 710, 'sum_payoffs': 254.99463415470595, 'action': [1.0, 1.5707963267948966]}, {'num_count': 593, 'sum_payoffs': 204.37742922312756, 'action': [0.0, 1.5707963267948966]}, {'num_count': 712, 'sum_payoffs': 255.79030291979208, 'action': [1.0, -1.5707963267948966]}, {'num_count': 608, 'sum_payoffs': 210.83637974448456, 'action': [0.0, -1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 233.65676198456143, 'action': [0.0, 0.0]}])
Weights num count: [0.19897585954645208, 0.17312850524262374, 0.1445988783223604, 0.17361619117288465, 0.14825652279931725, 0.1611801999512314]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.020861864089966 s
