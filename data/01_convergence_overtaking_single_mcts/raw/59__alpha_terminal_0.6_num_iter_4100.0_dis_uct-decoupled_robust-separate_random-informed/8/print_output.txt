Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 411, 'sum_payoffs': 134.22516361267327, 'action': [0.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 157.45495154527057, 'action': [2.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 158.29336827934887, 'action': [1.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 146.20324649969922, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 146.63744583602895, 'action': [1.0, 1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 138.01854610123775, 'action': [0.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 178.38774892434287, 'action': [2.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 153.46203263670066, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 167.7813381891027, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10021945866861741, 0.11338697878566203, 0.11387466471592295, 0.10704706169227018, 0.10729090465740064, 0.10241404535479151, 0.1250914411119239, 0.11119239209948793, 0.11923920994879297]
Actions to choose Agent 1: dict_values([{'num_count': 601, 'sum_payoffs': 208.54457557467782, 'action': [0.0, 1.5707963267948966]}, {'num_count': 820, 'sum_payoffs': 304.16945324051693, 'action': [1.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 265.83616840334565, 'action': [1.0, -1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 263.5835702715044, 'action': [1.0, 1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 225.38754226155575, 'action': [0.0, 0.0]}, {'num_count': 578, 'sum_payoffs': 198.62769772881538, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14654962204340405, 0.1999512314069739, 0.17873689344062424, 0.17751767861497195, 0.15605949768349184, 0.14094123384540355]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.271459341049194 s
