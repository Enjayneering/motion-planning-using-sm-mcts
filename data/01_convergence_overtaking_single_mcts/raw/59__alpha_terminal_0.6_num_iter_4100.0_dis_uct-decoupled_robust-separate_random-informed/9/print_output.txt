Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 411, 'sum_payoffs': 134.34268568599526, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 158.5508067205874, 'action': [1.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 175.6062257428461, 'action': [2.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 162.0609619265099, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 139.9503063492836, 'action': [0.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 162.8210626550696, 'action': [2.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 150.2840714903768, 'action': [1.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 171.66953642524425, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 127.62171537360682, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10021945866861741, 0.11387466471592295, 0.12338454035601074, 0.11582540843696659, 0.10338941721531333, 0.1163130943672275, 0.10924164837844429, 0.12118995366983662, 0.09631797122653012]
Actions to choose Agent 1: dict_values([{'num_count': 609, 'sum_payoffs': 211.7616442786504, 'action': [0.0, 1.5707963267948966]}, {'num_count': 600, 'sum_payoffs': 207.95326699438192, 'action': [0.0, -1.5707963267948966]}, {'num_count': 749, 'sum_payoffs': 272.6082150451484, 'action': [1.0, 1.5707963267948966]}, {'num_count': 769, 'sum_payoffs': 281.4557142067093, 'action': [1.0, 0.0]}, {'num_count': 627, 'sum_payoffs': 219.5680010622537, 'action': [0.0, 0.0]}, {'num_count': 746, 'sum_payoffs': 271.3827780707892, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1485003657644477, 0.14630577907827358, 0.18263838088271153, 0.18751524018532065, 0.1528895391367959, 0.18190685198732018]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.282503366470337 s
