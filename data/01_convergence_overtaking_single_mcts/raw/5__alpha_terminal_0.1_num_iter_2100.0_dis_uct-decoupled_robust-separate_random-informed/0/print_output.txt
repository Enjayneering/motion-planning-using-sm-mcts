Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 196, 'sum_payoffs': 46.4967516164385, 'action': [0.0, 0.0]}, {'num_count': 198, 'sum_payoffs': 47.236381801222784, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 76.06196900281543, 'action': [2.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 59.94002997501779, 'action': [1.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.510244867643145, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 76.47176410519594, 'action': [2.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 76.00199898783296, 'action': [2.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.55022487763147, 'action': [1.0, 0.0]}, {'num_count': 198, 'sum_payoffs': 47.256371806216954, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.09328891004283675, 0.09424083769633508, 0.12946216087577345, 0.1099476439790576, 0.10947168015230842, 0.1299381247025226, 0.12946216087577345, 0.10947168015230842, 0.09424083769633508]
Actions to choose Agent 1: dict_values([{'num_count': 310, 'sum_payoffs': 94.6926536573816, 'action': [0.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 127.6761618977621, 'action': [1.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 129.72513740966443, 'action': [1.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 96.29185405691518, 'action': [0.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 125.09745125351418, 'action': [1.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 95.94202896951721, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1475487862922418, 0.1846739647786768, 0.18705378391242267, 0.14945264159923846, 0.18181818181818182, 0.1489766777724893]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.286724328994751 s
