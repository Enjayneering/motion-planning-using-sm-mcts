Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 268, 'sum_payoffs': 74.42278859329365, 'action': [2.0, -1.5707963267948966]}, {'num_count': 200, 'sum_payoffs': 48.015991995995414, 'action': [0.0, -1.5707963267948966]}, {'num_count': 201, 'sum_payoffs': 48.405797093381736, 'action': [0.0, 0.0]}, {'num_count': 199, 'sum_payoffs': 47.70614691858579, 'action': [0.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 75.67216390542917, 'action': [2.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.470264857654804, 'action': [1.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 59.55022487763148, 'action': [1.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 76.10194901280384, 'action': [2.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 59.12043977025682, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1275583055687768, 0.09519276534983341, 0.09566872917658258, 0.09471680152308425, 0.12898619704902428, 0.10947168015230842, 0.10947168015230842, 0.12946216087577345, 0.10899571632555925]
Actions to choose Agent 1: dict_values([{'num_count': 318, 'sum_payoffs': 98.07096450139632, 'action': [0.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 126.81659168301279, 'action': [1.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 99.30034980853777, 'action': [0.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 96.76161917427815, 'action': [0.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 124.19790102877651, 'action': [1.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 124.21789103377068, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15135649690623512, 0.18372203712517848, 0.15278438838648262, 0.14992860542598763, 0.1808662541646835, 0.1808662541646835]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.2847459316253662 s
