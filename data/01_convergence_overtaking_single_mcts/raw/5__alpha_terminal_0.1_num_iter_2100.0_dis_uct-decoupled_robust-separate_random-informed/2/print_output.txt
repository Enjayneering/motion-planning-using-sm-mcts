Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 200, 'sum_payoffs': 48.07596201097793, 'action': [0.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 59.200399790233504, 'action': [1.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 76.4117940902134, 'action': [2.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 76.12193901779801, 'action': [2.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 58.710644667876345, 'action': [1.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 59.160419780245164, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 76.41179409021343, 'action': [2.0, 1.5707963267948966]}, {'num_count': 199, 'sum_payoffs': 47.60619689361493, 'action': [0.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 46.86656670883064, 'action': [0.0, 0.0]}])
Weights num count: [0.09519276534983341, 0.10899571632555925, 0.1299381247025226, 0.12946216087577345, 0.10851975249881009, 0.10899571632555925, 0.1299381247025226, 0.09471680152308425, 0.09376487386958592]
Actions to choose Agent 1: dict_values([{'num_count': 387, 'sum_payoffs': 127.16641677041076, 'action': [1.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 96.80159918426652, 'action': [0.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 96.80159918426652, 'action': [0.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 126.75662166803026, 'action': [1.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 94.6726636523874, 'action': [0.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 127.22638678539326, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18419800095192765, 0.14992860542598763, 0.14992860542598763, 0.18372203712517848, 0.1475487862922418, 0.18419800095192765]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.2887842655181885 s
