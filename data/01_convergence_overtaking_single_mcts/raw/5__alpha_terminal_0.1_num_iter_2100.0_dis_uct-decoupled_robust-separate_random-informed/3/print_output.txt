Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 231, 'sum_payoffs': 59.90004996502945, 'action': [1.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 59.10044976526266, 'action': [1.0, 0.0]}, {'num_count': 197, 'sum_payoffs': 46.826586698842306, 'action': [0.0, 0.0]}, {'num_count': 197, 'sum_payoffs': 46.8265866988423, 'action': [0.0, -1.5707963267948966]}, {'num_count': 198, 'sum_payoffs': 47.23638180122279, 'action': [0.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 76.49175411019014, 'action': [2.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 59.88005996003528, 'action': [1.0, 1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 75.30234881303704, 'action': [2.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 76.7816091826056, 'action': [2.0, 0.0]}])
Weights num count: [0.1099476439790576, 0.10899571632555925, 0.09376487386958592, 0.09376487386958592, 0.09424083769633508, 0.1299381247025226, 0.1099476439790576, 0.12851023322227512, 0.13041408852927178]
Actions to choose Agent 1: dict_values([{'num_count': 311, 'sum_payoffs': 95.02248873978539, 'action': [0.0, -1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 96.37181407689188, 'action': [0.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 128.10594700513673, 'action': [1.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 126.28685655066728, 'action': [1.0, -1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 95.12243876475624, 'action': [0.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 128.53573211251137, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14802475011899097, 0.14945264159923846, 0.18514992860542598, 0.18324607329842932, 0.14802475011899097, 0.18562589243217514]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.29935789108276367 s
