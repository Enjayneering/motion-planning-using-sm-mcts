Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 230, 'sum_payoffs': 59.59020488761983, 'action': [1.0, 0.0]}, {'num_count': 199, 'sum_payoffs': 47.6261868986091, 'action': [0.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 59.12043977025683, 'action': [1.0, 1.5707963267948966]}, {'num_count': 199, 'sum_payoffs': 47.60619689361493, 'action': [0.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 76.04197899782139, 'action': [2.0, -1.5707963267948966]}, {'num_count': 198, 'sum_payoffs': 47.35632183118781, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 76.04197899782123, 'action': [2.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.49025486264897, 'action': [1.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 75.65217390043499, 'action': [2.0, 0.0]}])
Weights num count: [0.10947168015230842, 0.09471680152308425, 0.10899571632555925, 0.09471680152308425, 0.12946216087577345, 0.09424083769633508, 0.12946216087577345, 0.10947168015230842, 0.12898619704902428]
Actions to choose Agent 1: dict_values([{'num_count': 317, 'sum_payoffs': 97.54122936905081, 'action': [0.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 126.36681657064396, 'action': [1.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 127.24637679038742, 'action': [1.0, 1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 97.56121937404497, 'action': [0.0, -1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 95.16241877474458, 'action': [0.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 125.50724635589464, 'action': [1.0, 0.0]}])
Weights num count: [0.15088053307948596, 0.18324607329842932, 0.18419800095192765, 0.15088053307948596, 0.14802475011899097, 0.182294145644931]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.28452062606811523 s
