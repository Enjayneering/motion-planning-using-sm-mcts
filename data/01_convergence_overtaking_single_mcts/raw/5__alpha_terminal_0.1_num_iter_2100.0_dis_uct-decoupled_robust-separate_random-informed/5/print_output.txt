Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 273, 'sum_payoffs': 76.39180408521928, 'action': [2.0, 1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 46.8265866988423, 'action': [0.0, -1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 46.92653672381315, 'action': [0.0, 1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 46.92653672381316, 'action': [0.0, 0.0]}, {'num_count': 230, 'sum_payoffs': 59.51024486764314, 'action': [1.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 76.43178409520758, 'action': [2.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 75.98200898283886, 'action': [2.0, -1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.570214882625656, 'action': [1.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 59.96001998001197, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1299381247025226, 0.09376487386958592, 0.09376487386958592, 0.09376487386958592, 0.10947168015230842, 0.1299381247025226, 0.12946216087577345, 0.10947168015230842, 0.1099476439790576]
Actions to choose Agent 1: dict_values([{'num_count': 311, 'sum_payoffs': 95.10244875976208, 'action': [0.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 125.48725635090047, 'action': [1.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 128.10594700513673, 'action': [1.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 129.72513740966446, 'action': [1.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 95.12243876475624, 'action': [0.0, -1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 95.8820589545347, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14802475011899097, 0.182294145644931, 0.18514992860542598, 0.18705378391242267, 0.14802475011899097, 0.1489766777724893]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.2900967597961426 s
