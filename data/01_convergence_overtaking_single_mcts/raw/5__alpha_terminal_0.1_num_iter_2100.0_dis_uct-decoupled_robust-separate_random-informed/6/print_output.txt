Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 197, 'sum_payoffs': 46.8265866988423, 'action': [0.0, 1.5707963267948966]}, {'num_count': 198, 'sum_payoffs': 47.27636181121113, 'action': [0.0, -1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 77.15142427499771, 'action': [2.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 75.652173900435, 'action': [2.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 59.90004996502945, 'action': [1.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 59.08045976026848, 'action': [1.0, -1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.53023487263731, 'action': [1.0, 1.5707963267948966]}, {'num_count': 197, 'sum_payoffs': 46.88655671382482, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 76.04197899782132, 'action': [2.0, 0.0]}])
Weights num count: [0.09376487386958592, 0.09424083769633508, 0.13089005235602094, 0.12898619704902428, 0.1099476439790576, 0.10899571632555925, 0.10947168015230842, 0.09376487386958592, 0.12946216087577345]
Actions to choose Agent 1: dict_values([{'num_count': 308, 'sum_payoffs': 93.85307344762643, 'action': [0.0, 1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 97.13143426667033, 'action': [0.0, 0.0]}, {'num_count': 313, 'sum_payoffs': 95.90204895952888, 'action': [0.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 127.5962018777854, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 127.26636679538161, 'action': [1.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 127.65617189276793, 'action': [1.0, 0.0]}])
Weights num count: [0.14659685863874344, 0.1504045692527368, 0.1489766777724893, 0.1846739647786768, 0.18419800095192765, 0.1846739647786768]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.2926521301269531 s
