Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 272, 'sum_payoffs': 76.00199898783298, 'action': [2.0, 1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 74.91254371565077, 'action': [2.0, 0.0]}, {'num_count': 198, 'sum_payoffs': 47.27636181121113, 'action': [0.0, 0.0]}, {'num_count': 200, 'sum_payoffs': 47.9560219810129, 'action': [0.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.57021488262565, 'action': [1.0, 0.0]}, {'num_count': 199, 'sum_payoffs': 47.60619689361493, 'action': [0.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 58.77061468285885, 'action': [1.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 59.530234872637315, 'action': [1.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 76.9215392175647, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.12946216087577345, 0.12803426939552595, 0.09424083769633508, 0.09519276534983341, 0.10947168015230842, 0.09471680152308425, 0.10851975249881009, 0.10947168015230842, 0.13041408852927178]
Actions to choose Agent 1: dict_values([{'num_count': 384, 'sum_payoffs': 125.87706144828681, 'action': [1.0, 1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 126.4067965806323, 'action': [1.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 98.05097449640213, 'action': [0.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 95.53223386713672, 'action': [0.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 126.77661167302443, 'action': [1.0, 0.0]}, {'num_count': 315, 'sum_payoffs': 96.76161917427818, 'action': [0.0, 0.0]}])
Weights num count: [0.18277010947168015, 0.18324607329842932, 0.15135649690623512, 0.14850071394574013, 0.18372203712517848, 0.14992860542598763]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3005082607269287 s
