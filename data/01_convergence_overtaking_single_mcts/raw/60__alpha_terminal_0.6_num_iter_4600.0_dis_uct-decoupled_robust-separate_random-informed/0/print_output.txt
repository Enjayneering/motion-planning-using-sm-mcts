Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 447, 'sum_payoffs': 144.58185027556073, 'action': [0.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 175.9552536443485, 'action': [1.0, -1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 193.54117014624077, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 162.78150967118307, 'action': [0.0, 0.0]}, {'num_count': 507, 'sum_payoffs': 170.04811363700642, 'action': [1.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 176.48046583926612, 'action': [2.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 169.6451487808524, 'action': [1.0, 1.5707963267948966]}, {'num_count': 578, 'sum_payoffs': 200.5868770872522, 'action': [2.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 153.04161984171836, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.09715279287111497, 0.11323625298848077, 0.12214735926972398, 0.10649858726363834, 0.11019343620951967, 0.11345359704412085, 0.10997609215387959, 0.12562486415996524, 0.10149967398391654]
Actions to choose Agent 1: dict_values([{'num_count': 915, 'sum_payoffs': 336.5982398726623, 'action': [1.0, 0.0]}, {'num_count': 655, 'sum_payoffs': 224.74408408544045, 'action': [0.0, -1.5707963267948966]}, {'num_count': 821, 'sum_payoffs': 295.75203679550935, 'action': [1.0, 1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 249.88987229229215, 'action': [0.0, 0.0]}, {'num_count': 689, 'sum_payoffs': 239.2469429396195, 'action': [0.0, 1.5707963267948966]}, {'num_count': 806, 'sum_payoffs': 289.27263869401094, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1988698109106716, 0.14236035644425124, 0.17843946968050423, 0.15518365572701587, 0.1497500543360139, 0.17517930884590308]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.87951636314392 s
