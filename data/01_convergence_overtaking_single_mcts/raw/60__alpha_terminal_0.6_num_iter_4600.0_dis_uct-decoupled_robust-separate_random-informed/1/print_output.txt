Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 496, 'sum_payoffs': 164.92796915966952, 'action': [1.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 175.03942046836224, 'action': [2.0, 1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 176.39222601891242, 'action': [1.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 146.2566575063673, 'action': [0.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 183.70662948045873, 'action': [2.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 178.1809989986047, 'action': [1.0, -1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 203.51231526666385, 'action': [2.0, 0.0]}, {'num_count': 487, 'sum_payoffs': 160.97243435485427, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 153.42056536983668, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10780265159747882, 0.11301890893284068, 0.11367094109976092, 0.09823951314931537, 0.11736579004564225, 0.11454031732232123, 0.12736361660508586, 0.1058465550967181, 0.1019343620951967]
Actions to choose Agent 1: dict_values([{'num_count': 888, 'sum_payoffs': 325.3554452160134, 'action': [1.0, 0.0]}, {'num_count': 676, 'sum_payoffs': 234.0402641113844, 'action': [0.0, 1.5707963267948966]}, {'num_count': 671, 'sum_payoffs': 231.9201897315255, 'action': [0.0, -1.5707963267948966]}, {'num_count': 820, 'sum_payoffs': 295.7175503574367, 'action': [1.0, -1.5707963267948966]}, {'num_count': 819, 'sum_payoffs': 295.44340229829436, 'action': [1.0, 1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 255.37678795366946, 'action': [0.0, 0.0]}])
Weights num count: [0.19300152140838947, 0.1469245816126929, 0.1458378613344925, 0.17822212562486417, 0.17800478156922409, 0.1577917843946968]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.68395709991455 s
