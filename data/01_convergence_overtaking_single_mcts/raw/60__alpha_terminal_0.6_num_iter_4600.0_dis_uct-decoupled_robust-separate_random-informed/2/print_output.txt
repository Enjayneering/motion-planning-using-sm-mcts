Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 483, 'sum_payoffs': 160.136804383398, 'action': [0.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 173.78744801726256, 'action': [1.0, -1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 198.6970285813493, 'action': [2.0, -1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 175.08497832267796, 'action': [2.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 158.02514703405666, 'action': [1.0, 1.5707963267948966]}, {'num_count': 601, 'sum_payoffs': 210.881145934765, 'action': [2.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 175.43188924496272, 'action': [1.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 151.52099777354806, 'action': [0.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 146.16895262217858, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10497717887415779, 0.1119321886546403, 0.12453814388176483, 0.11258422082156053, 0.1038904585959574, 0.13062377743968703, 0.11280156487720061, 0.10063029776135622, 0.09780482503803521]
Actions to choose Agent 1: dict_values([{'num_count': 859, 'sum_payoffs': 312.5008625273174, 'action': [1.0, 0.0]}, {'num_count': 672, 'sum_payoffs': 232.22034758989815, 'action': [0.0, 1.5707963267948966]}, {'num_count': 825, 'sum_payoffs': 297.75065588553633, 'action': [1.0, 1.5707963267948966]}, {'num_count': 688, 'sum_payoffs': 238.8942270832563, 'action': [0.0, -1.5707963267948966]}, {'num_count': 840, 'sum_payoffs': 304.3193391783632, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 250.96333330657325, 'action': [0.0, 0.0]}])
Weights num count: [0.1866985437948272, 0.1460552053901326, 0.17930884590306456, 0.14953271028037382, 0.1825690067376657, 0.15561834383829604]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.82137656211853 s
