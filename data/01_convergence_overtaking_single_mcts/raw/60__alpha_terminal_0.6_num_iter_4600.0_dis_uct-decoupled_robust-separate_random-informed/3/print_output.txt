Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 540, 'sum_payoffs': 183.67815679598075, 'action': [2.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 159.85392173413672, 'action': [0.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 172.17814968350268, 'action': [1.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 144.20719676083868, 'action': [0.0, 1.5707963267948966]}, {'num_count': 560, 'sum_payoffs': 192.30476778850965, 'action': [2.0, 0.0]}, {'num_count': 557, 'sum_payoffs': 191.05644138317882, 'action': [2.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 152.72082470347283, 'action': [0.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 165.4150251393035, 'action': [1.0, 0.0]}, {'num_count': 535, 'sum_payoffs': 181.51424552351767, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11736579004564225, 0.10519452292979788, 0.11149750054336013, 0.09715279287111497, 0.12171267115844382, 0.12106063899152358, 0.10149967398391654, 0.10801999565311889, 0.11627906976744186]
Actions to choose Agent 1: dict_values([{'num_count': 729, 'sum_payoffs': 256.7764512793143, 'action': [0.0, 0.0]}, {'num_count': 944, 'sum_payoffs': 349.97297177803637, 'action': [1.0, 0.0]}, {'num_count': 641, 'sum_payoffs': 219.33120251847632, 'action': [0.0, 1.5707963267948966]}, {'num_count': 828, 'sum_payoffs': 299.35902068709424, 'action': [1.0, 1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 286.1100925491357, 'action': [1.0, -1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 227.83838741453582, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15844381656161705, 0.20517278852423387, 0.13931753966529015, 0.1799608780699848, 0.17322321234514235, 0.14366442077809172]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.864840030670166 s
