Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 531, 'sum_payoffs': 179.49874940212976, 'action': [2.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 146.50251237279616, 'action': [0.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 167.61731034015887, 'action': [1.0, 0.0]}, {'num_count': 588, 'sum_payoffs': 203.94401922642808, 'action': [2.0, 0.0]}, {'num_count': 567, 'sum_payoffs': 194.98534758908457, 'action': [2.0, -1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 156.1565316744472, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 152.28487726372848, 'action': [0.0, 0.0]}, {'num_count': 514, 'sum_payoffs': 172.20591308829674, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 166.69958730640087, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11540969354488155, 0.09845685720495545, 0.10932405998695936, 0.127798304716366, 0.12323407954792437, 0.10345577048467725, 0.10149967398391654, 0.11171484459900022, 0.1088893718756792]
Actions to choose Agent 1: dict_values([{'num_count': 657, 'sum_payoffs': 227.01233520562212, 'action': [0.0, -1.5707963267948966]}, {'num_count': 802, 'sum_payoffs': 289.3016581699257, 'action': [1.0, 1.5707963267948966]}, {'num_count': 840, 'sum_payoffs': 305.7440644689432, 'action': [1.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 226.9084921357367, 'action': [0.0, 1.5707963267948966]}, {'num_count': 717, 'sum_payoffs': 252.44398838505177, 'action': [0.0, 0.0]}, {'num_count': 927, 'sum_payoffs': 343.7895297274229, 'action': [1.0, 0.0]}])
Weights num count: [0.1427950445555314, 0.17430993262334274, 0.1825690067376657, 0.1427950445555314, 0.1558356878939361, 0.20147793957835253]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.857358694076538 s
