Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 506, 'sum_payoffs': 170.89812693489822, 'action': [1.0, 1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 180.75230108171442, 'action': [1.0, 0.0]}, {'num_count': 475, 'sum_payoffs': 157.58665415196694, 'action': [0.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 184.23046915114202, 'action': [2.0, 1.5707963267948966]}, {'num_count': 588, 'sum_payoffs': 206.2432467580171, 'action': [2.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 150.79655183997485, 'action': [0.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 183.7856106870066, 'action': [2.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 177.65784052341687, 'action': [1.0, -1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 146.08509193112803, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10997609215387959, 0.1149750054336014, 0.10323842642903716, 0.11671375787872201, 0.127798304716366, 0.09976092153879591, 0.11649641382308194, 0.11345359704412085, 0.09737013692675506]
Actions to choose Agent 1: dict_values([{'num_count': 830, 'sum_payoffs': 298.98368974392633, 'action': [1.0, -1.5707963267948966]}, {'num_count': 720, 'sum_payoffs': 251.81678150607073, 'action': [0.0, 0.0]}, {'num_count': 888, 'sum_payoffs': 324.2117682671764, 'action': [1.0, 0.0]}, {'num_count': 652, 'sum_payoffs': 222.92449888760896, 'action': [0.0, -1.5707963267948966]}, {'num_count': 682, 'sum_payoffs': 235.74520501570308, 'action': [0.0, 1.5707963267948966]}, {'num_count': 828, 'sum_payoffs': 298.2389909577078, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18039556618126495, 0.15648772006085634, 0.19300152140838947, 0.14170832427733102, 0.14822864594653337, 0.1799608780699848]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.729655981063843 s
