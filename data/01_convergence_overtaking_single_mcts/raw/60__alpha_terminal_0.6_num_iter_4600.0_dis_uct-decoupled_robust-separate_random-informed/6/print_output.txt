Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 530, 'sum_payoffs': 179.978585702307, 'action': [1.0, 0.0]}, {'num_count': 523, 'sum_payoffs': 177.08482172280884, 'action': [2.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 155.21156783655553, 'action': [0.0, -1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 171.51294998902543, 'action': [1.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 165.97113162970362, 'action': [1.0, 1.5707963267948966]}, {'num_count': 606, 'sum_payoffs': 212.96019903420876, 'action': [2.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 151.85512318844798, 'action': [0.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 158.756759842127, 'action': [0.0, 0.0]}, {'num_count': 518, 'sum_payoffs': 174.91833454760067, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11519234948924147, 0.11367094109976092, 0.10258639426211694, 0.11084546837643991, 0.10801999565311889, 0.13171049771788743, 0.1008476418169963, 0.10432514670723755, 0.11258422082156053]
Actions to choose Agent 1: dict_values([{'num_count': 785, 'sum_payoffs': 280.1986428505855, 'action': [1.0, 1.5707963267948966]}, {'num_count': 810, 'sum_payoffs': 291.0414867796511, 'action': [1.0, -1.5707963267948966]}, {'num_count': 693, 'sum_payoffs': 240.91752889821552, 'action': [0.0, -1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 224.6111447421275, 'action': [0.0, 1.5707963267948966]}, {'num_count': 758, 'sum_payoffs': 268.5741354396371, 'action': [0.0, 0.0]}, {'num_count': 899, 'sum_payoffs': 329.6320181252891, 'action': [1.0, 0.0]}])
Weights num count: [0.17061508367746142, 0.17604868506846338, 0.1506194305585742, 0.14236035644425124, 0.16474679417517932, 0.19539230602043034]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.020869731903076 s
