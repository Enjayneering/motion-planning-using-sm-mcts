Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 468, 'sum_payoffs': 153.4661220066902, 'action': [0.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 175.95901151649704, 'action': [1.0, -1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 181.0050202657394, 'action': [1.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 165.76832716410425, 'action': [0.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 163.16911891444937, 'action': [1.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 172.6189742903676, 'action': [2.0, 1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 197.92136488334145, 'action': [2.0, -1.5707963267948966]}, {'num_count': 556, 'sum_payoffs': 191.09669651392133, 'action': [2.0, 0.0]}, {'num_count': 449, 'sum_payoffs': 145.43167306560676, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10171701803955661, 0.11323625298848077, 0.1158443816561617, 0.10801999565311889, 0.10671593131927842, 0.11149750054336013, 0.12432079982612476, 0.1208432949358835, 0.09758748098239513]
Actions to choose Agent 1: dict_values([{'num_count': 807, 'sum_payoffs': 290.1149604418151, 'action': [1.0, -1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 252.3724242379101, 'action': [0.0, 0.0]}, {'num_count': 656, 'sum_payoffs': 225.56622600005204, 'action': [0.0, 1.5707963267948966]}, {'num_count': 887, 'sum_payoffs': 324.93264788656006, 'action': [1.0, 0.0]}, {'num_count': 825, 'sum_payoffs': 297.9792839185748, 'action': [1.0, 1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 246.75886117751745, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17539665290154313, 0.15627037600521626, 0.14257770049989132, 0.19278417735274941, 0.17930884590306456, 0.15344490328189525]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.322651386260986 s
