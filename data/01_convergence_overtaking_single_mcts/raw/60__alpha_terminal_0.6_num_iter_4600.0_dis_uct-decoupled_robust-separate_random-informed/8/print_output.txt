Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 444, 'sum_payoffs': 143.57381917868986, 'action': [0.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 151.20162021302946, 'action': [0.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 185.40340796875645, 'action': [1.0, 0.0]}, {'num_count': 521, 'sum_payoffs': 176.4460141890325, 'action': [2.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 165.62115378296522, 'action': [1.0, 1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 188.37312647061017, 'action': [2.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 158.94259454131483, 'action': [0.0, 0.0]}, {'num_count': 596, 'sum_payoffs': 208.70490746664444, 'action': [2.0, 0.0]}, {'num_count': 510, 'sum_payoffs': 171.73627750772386, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.09650076070419474, 0.10041295370571615, 0.1178004781569224, 0.11323625298848077, 0.10780265159747882, 0.11932188654640295, 0.10432514670723755, 0.12953705716148664, 0.11084546837643991]
Actions to choose Agent 1: dict_values([{'num_count': 809, 'sum_payoffs': 290.6578074585493, 'action': [1.0, 1.5707963267948966]}, {'num_count': 795, 'sum_payoffs': 284.46261115224297, 'action': [1.0, -1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 226.86411446944527, 'action': [0.0, 1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 239.54703513662278, 'action': [0.0, -1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 248.45379984466078, 'action': [0.0, 0.0]}, {'num_count': 935, 'sum_payoffs': 345.22765396751527, 'action': [1.0, 0.0]}])
Weights num count: [0.1758313410128233, 0.1727885242338622, 0.14344707672245163, 0.149967398391654, 0.15453162356009564, 0.20321669202347314]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.6556875705719 s
