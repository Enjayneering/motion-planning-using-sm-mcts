Searching game tree in timestep 0...
Max timehorizon: 4
Actions to choose Agent 0: dict_values([{'num_count': 465, 'sum_payoffs': 152.45648835890708, 'action': [0.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 144.91056603735518, 'action': [0.0, 1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 198.12837894617937, 'action': [2.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 172.88457776177825, 'action': [1.0, 1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 183.23436287224987, 'action': [2.0, 1.5707963267948966]}, {'num_count': 510, 'sum_payoffs': 171.7045944826634, 'action': [1.0, -1.5707963267948966]}, {'num_count': 511, 'sum_payoffs': 172.10747275705697, 'action': [1.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 157.0587020703295, 'action': [0.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 197.02036582069596, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10106498587263639, 0.09715279287111497, 0.12432079982612476, 0.11149750054336013, 0.11671375787872201, 0.11084546837643991, 0.11106281243207998, 0.10345577048467725, 0.12366876765920452]
Actions to choose Agent 1: dict_values([{'num_count': 686, 'sum_payoffs': 237.7793104523829, 'action': [0.0, 0.0]}, {'num_count': 852, 'sum_payoffs': 308.95380409950303, 'action': [1.0, -1.5707963267948966]}, {'num_count': 824, 'sum_payoffs': 296.93523661516673, 'action': [1.0, 1.5707963267948966]}, {'num_count': 916, 'sum_payoffs': 336.77161939864493, 'action': [1.0, 0.0]}, {'num_count': 664, 'sum_payoffs': 228.3745548509832, 'action': [0.0, 1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 225.83554588424965, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14909802216909368, 0.18517713540534667, 0.17909150184742448, 0.19908715496631166, 0.14431645294501194, 0.1430123886111715]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.798734426498413 s
