Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 66, 'sum_payoffs': 21.382908568964332, 'action': [0.0, 0.0]}, {'num_count': 66, 'sum_payoffs': 21.538929207617528, 'action': [0.0, -1.5707963267948966]}, {'num_count': 66, 'sum_payoffs': 21.600752133153648, 'action': [1.0, 0.0]}, {'num_count': 66, 'sum_payoffs': 21.54049126432954, 'action': [1.0, 1.5707963267948966]}, {'num_count': 65, 'sum_payoffs': 21.01683994087089, 'action': [2.0, 1.5707963267948966]}, {'num_count': 64, 'sum_payoffs': 20.472911405645338, 'action': [0.0, 1.5707963267948966]}, {'num_count': 69, 'sum_payoffs': 23.12408942252381, 'action': [1.0, -1.5707963267948966]}, {'num_count': 69, 'sum_payoffs': 23.10002787553273, 'action': [2.0, -1.5707963267948966]}, {'num_count': 69, 'sum_payoffs': 23.200861261009795, 'action': [2.0, 0.0]}])
Weights num count: [0.10981697171381032, 0.10981697171381032, 0.10981697171381032, 0.10981697171381032, 0.10815307820299501, 0.1064891846921797, 0.11480865224625623, 0.11480865224625623, 0.11480865224625623]
Actions to choose Agent 1: dict_values([{'num_count': 103, 'sum_payoffs': 34.787347676924945, 'action': [1.0, 1.5707963267948966]}, {'num_count': 98, 'sum_payoffs': 32.18391272040965, 'action': [0.0, 0.0]}, {'num_count': 100, 'sum_payoffs': 33.33234793706392, 'action': [0.0, -1.5707963267948966]}, {'num_count': 104, 'sum_payoffs': 35.37389279307183, 'action': [1.0, 0.0]}, {'num_count': 91, 'sum_payoffs': 28.655293955077237, 'action': [0.0, 1.5707963267948966]}, {'num_count': 104, 'sum_payoffs': 35.29227967129606, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1713810316139767, 0.16306156405990016, 0.16638935108153077, 0.17304492512479203, 0.15141430948419302, 0.17304492512479203]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 5.366487979888916 s
