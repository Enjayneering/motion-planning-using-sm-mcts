Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 63, 'sum_payoffs': 19.854084888562124, 'action': [0.0, 0.0]}, {'num_count': 68, 'sum_payoffs': 22.374043499442493, 'action': [2.0, 0.0]}, {'num_count': 69, 'sum_payoffs': 23.017565088757472, 'action': [2.0, 1.5707963267948966]}, {'num_count': 67, 'sum_payoffs': 21.890772390765772, 'action': [1.0, -1.5707963267948966]}, {'num_count': 66, 'sum_payoffs': 21.439707522798933, 'action': [0.0, 1.5707963267948966]}, {'num_count': 70, 'sum_payoffs': 23.595453820562806, 'action': [2.0, -1.5707963267948966]}, {'num_count': 67, 'sum_payoffs': 22.044860964718882, 'action': [1.0, 0.0]}, {'num_count': 64, 'sum_payoffs': 20.423132420963025, 'action': [0.0, -1.5707963267948966]}, {'num_count': 66, 'sum_payoffs': 21.489302985308402, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1048252911813644, 0.11314475873544093, 0.11480865224625623, 0.11148086522462562, 0.10981697171381032, 0.11647254575707154, 0.11148086522462562, 0.1064891846921797, 0.10981697171381032]
Actions to choose Agent 1: dict_values([{'num_count': 99, 'sum_payoffs': 32.837360808164334, 'action': [1.0, 1.5707963267948966]}, {'num_count': 97, 'sum_payoffs': 31.896893175954833, 'action': [0.0, -1.5707963267948966]}, {'num_count': 94, 'sum_payoffs': 30.302072102292584, 'action': [0.0, 0.0]}, {'num_count': 98, 'sum_payoffs': 32.36129512054404, 'action': [0.0, 1.5707963267948966]}, {'num_count': 105, 'sum_payoffs': 35.82452796720363, 'action': [1.0, 0.0]}, {'num_count': 107, 'sum_payoffs': 37.017125577316236, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16472545757071547, 0.16139767054908485, 0.15640599001663893, 0.16306156405990016, 0.17470881863560733, 0.17803660565723795]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 5.7850635051727295 s
