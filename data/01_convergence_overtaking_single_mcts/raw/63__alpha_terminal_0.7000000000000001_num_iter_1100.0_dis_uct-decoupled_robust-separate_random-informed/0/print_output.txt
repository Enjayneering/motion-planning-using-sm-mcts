Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 127, 'sum_payoffs': 42.0317662604716, 'action': [2.0, 1.5707963267948966]}, {'num_count': 133, 'sum_payoffs': 45.11966846893601, 'action': [2.0, 0.0]}, {'num_count': 134, 'sum_payoffs': 45.67759284095943, 'action': [2.0, -1.5707963267948966]}, {'num_count': 114, 'sum_payoffs': 35.54813189393528, 'action': [0.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 38.56770587630568, 'action': [1.0, 1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 36.6281491611609, 'action': [1.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 39.13493018559119, 'action': [1.0, -1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 36.7076399180583, 'action': [0.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 38.149447177271355, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11534968210717529, 0.12079927338782924, 0.12170753860127158, 0.10354223433242507, 0.10899182561307902, 0.10535876475930972, 0.10990009082652134, 0.10535876475930972, 0.1080835603996367]
Actions to choose Agent 1: dict_values([{'num_count': 198, 'sum_payoffs': 68.50432531846837, 'action': [1.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 64.65200721227541, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 57.89541364461541, 'action': [0.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 55.18993590840605, 'action': [0.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 56.10759293747153, 'action': [0.0, 0.0]}, {'num_count': 194, 'sum_payoffs': 66.56823017979391, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17983651226158037, 0.17257039055404177, 0.15985467756584923, 0.15440508628519528, 0.15622161671207993, 0.17620345140781107]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.389131307601929 s
