Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 118, 'sum_payoffs': 38.01325502794479, 'action': [1.0, 1.5707963267948966]}, {'num_count': 115, 'sum_payoffs': 36.53143875584931, 'action': [0.0, 0.0]}, {'num_count': 125, 'sum_payoffs': 41.44265951610657, 'action': [2.0, 1.5707963267948966]}, {'num_count': 134, 'sum_payoffs': 45.94131054370003, 'action': [2.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 40.92354679284361, 'action': [1.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 39.30792382572912, 'action': [1.0, -1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 36.964471071984285, 'action': [0.0, -1.5707963267948966]}, {'num_count': 111, 'sum_payoffs': 34.45708548580042, 'action': [0.0, 1.5707963267948966]}, {'num_count': 136, 'sum_payoffs': 46.804159534426915, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10717529518619437, 0.1044504995458674, 0.11353315168029064, 0.12170753860127158, 0.11262488646684832, 0.10990009082652134, 0.10535876475930972, 0.1008174386920981, 0.12352406902815623]
Actions to choose Agent 1: dict_values([{'num_count': 168, 'sum_payoffs': 54.094768484592365, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 56.93674814367465, 'action': [0.0, -1.5707963267948966]}, {'num_count': 196, 'sum_payoffs': 67.23507030548004, 'action': [1.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 65.42484004156454, 'action': [1.0, 0.0]}, {'num_count': 199, 'sum_payoffs': 68.77230267878457, 'action': [1.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 55.50338399654836, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15258855585831063, 0.15803814713896458, 0.17801998183469572, 0.17438692098092642, 0.1807447774750227, 0.1553133514986376]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 9.226754426956177 s
