Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 178, 'sum_payoffs': 58.06485927979251, 'action': [1.0, 1.5707963267948966]}, {'num_count': 163, 'sum_payoffs': 51.01131289233317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 63.67196811937785, 'action': [2.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 56.542214870213364, 'action': [0.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 53.707823951537435, 'action': [0.0, -1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 60.288874681373166, 'action': [2.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 58.90810552432921, 'action': [1.0, -1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 63.24590881791799, 'action': [2.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 55.708562117102595, 'action': [1.0, 0.0]}])
Weights num count: [0.11118051217988757, 0.10181136789506559, 0.11867582760774516, 0.10930668332292318, 0.10555902560899438, 0.11430356027482823, 0.11242973141786383, 0.11805121798875702, 0.1080574640849469]
Actions to choose Agent 1: dict_values([{'num_count': 276, 'sum_payoffs': 93.61741424012746, 'action': [1.0, -1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 83.1411045701052, 'action': [0.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 98.63562286664445, 'action': [1.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 82.2572304841107, 'action': [0.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 99.0263133877063, 'action': [1.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 79.62081563752942, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17239225484072454, 0.1580262336039975, 0.17926296064959402, 0.15677701436602123, 0.17988757026858213, 0.15302935665209244]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.384267568588257 s
