Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 54.21986479712715, 'action': [0.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 59.871039578948256, 'action': [2.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 54.23918077474883, 'action': [0.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 63.59944774893655, 'action': [2.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 59.765637028061484, 'action': [1.0, 1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 60.28978944632623, 'action': [2.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 53.72895325443293, 'action': [0.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 57.51784672404049, 'action': [1.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 57.40082802581322, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1061836352279825, 0.1136789506558401, 0.1061836352279825, 0.11867582760774516, 0.1136789506558401, 0.11430356027482823, 0.10555902560899438, 0.11055590256089944, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 247, 'sum_payoffs': 79.92085473405791, 'action': [0.0, 1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 84.05471911151324, 'action': [0.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 98.92429297688537, 'action': [1.0, -1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 77.81168469504524, 'action': [0.0, -1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 93.00419366719481, 'action': [1.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 99.36462512578673, 'action': [1.0, 0.0]}])
Weights num count: [0.15427857589006871, 0.1599000624609619, 0.18051217988757026, 0.15115552779512806, 0.17239225484072454, 0.1811367895065584]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.227678775787354 s
