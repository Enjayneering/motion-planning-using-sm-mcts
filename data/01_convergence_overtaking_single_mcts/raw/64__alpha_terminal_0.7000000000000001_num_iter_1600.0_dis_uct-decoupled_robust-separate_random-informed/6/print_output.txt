Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 189, 'sum_payoffs': 63.15111552635841, 'action': [2.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 55.0789410802601, 'action': [0.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 55.56491366253642, 'action': [1.0, 1.5707963267948966]}, {'num_count': 201, 'sum_payoffs': 68.75297779380273, 'action': [2.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 56.567560329350215, 'action': [0.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 54.612811068996834, 'action': [1.0, 0.0]}, {'num_count': 164, 'sum_payoffs': 51.4326550368796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 56.09603955227911, 'action': [2.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 59.248368967192384, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11805121798875702, 0.10743285446595878, 0.1080574640849469, 0.12554653341661462, 0.10930668332292318, 0.10680824484697064, 0.10243597751405371, 0.10868207370393504, 0.11305434103685197]
Actions to choose Agent 1: dict_values([{'num_count': 250, 'sum_payoffs': 81.28858894296543, 'action': [0.0, 1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 85.36808237534751, 'action': [0.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 97.21299141181188, 'action': [1.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 75.88292055962891, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 96.19406659315895, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 97.18982042496427, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1561524047470331, 0.1617738913179263, 0.17801374141161774, 0.1486570893191755, 0.17676452217364147, 0.17801374141161774]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.554889678955078 s
