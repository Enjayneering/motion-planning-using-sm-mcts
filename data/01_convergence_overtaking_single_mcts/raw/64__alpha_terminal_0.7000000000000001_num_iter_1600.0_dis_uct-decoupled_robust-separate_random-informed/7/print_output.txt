Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 165, 'sum_payoffs': 51.95355378851979, 'action': [0.0, 1.5707963267948966]}, {'num_count': 162, 'sum_payoffs': 50.52139530612273, 'action': [0.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 58.46114120186063, 'action': [1.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 62.71294934976254, 'action': [2.0, -1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 64.09942111952948, 'action': [2.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 55.52161959865421, 'action': [0.0, -1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 55.64452057236534, 'action': [1.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 64.51620113777707, 'action': [2.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 57.475752100208965, 'action': [1.0, 0.0]}])
Weights num count: [0.10306058713304185, 0.10118675827607745, 0.11180512179887571, 0.1174266083697689, 0.1193004372267333, 0.1080574640849469, 0.1080574640849469, 0.11992504684572143, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 246, 'sum_payoffs': 79.57187962028696, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 97.26871342850997, 'action': [1.0, 1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 80.5092092539964, 'action': [0.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 95.38786897455732, 'action': [1.0, -1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 83.27201663303909, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 97.71674378402034, 'action': [1.0, 0.0]}])
Weights num count: [0.15365396627108058, 0.17801374141161774, 0.15490318550905685, 0.1755153029356652, 0.15865084322298564, 0.17863835103060588]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.866814613342285 s
