Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 54.11901429785676, 'action': [1.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 58.21739936770077, 'action': [1.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 58.78664537038009, 'action': [2.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 57.40183997916759, 'action': [0.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 62.59931264939718, 'action': [2.0, -1.5707963267948966]}, {'num_count': 183, 'sum_payoffs': 60.231374263541305, 'action': [2.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 58.30240692539199, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 56.887409028940624, 'action': [0.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 53.224821991794144, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1061836352279825, 0.11180512179887571, 0.11242973141786383, 0.11055590256089944, 0.1174266083697689, 0.11430356027482823, 0.11180512179887571, 0.1099312929419113, 0.10493441599000625]
Actions to choose Agent 1: dict_values([{'num_count': 287, 'sum_payoffs': 98.38756919582069, 'action': [1.0, 0.0]}, {'num_count': 247, 'sum_payoffs': 80.33583517986814, 'action': [0.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 101.15725552228749, 'action': [1.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 81.68663006317426, 'action': [0.0, -1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 80.80025291044807, 'action': [0.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 92.98390673325143, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17926296064959402, 0.15427857589006871, 0.1830106183635228, 0.1561524047470331, 0.15490318550905685, 0.1717676452217364]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 13.853972434997559 s
