Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 221, 'sum_payoffs': 70.51043788899065, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 70.13324504546813, 'action': [0.0, -1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 68.3277469290418, 'action': [0.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 79.27702673636706, 'action': [1.0, 1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 86.07357727964106, 'action': [2.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 77.38253379919591, 'action': [1.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 73.72387981615091, 'action': [0.0, 1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 82.90847729214747, 'action': [2.0, -1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 77.43622540936295, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10518800571156592, 0.10471204188481675, 0.10280818657782008, 0.1142313184198001, 0.1213707758210376, 0.11232746311280342, 0.10851975249881009, 0.11803902903379343, 0.11232746311280342]
Actions to choose Agent 1: dict_values([{'num_count': 313, 'sum_payoffs': 101.01303192785836, 'action': [0.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 107.9541976492567, 'action': [0.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 132.5025087320369, 'action': [1.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 126.86698539832798, 'action': [1.0, 1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 100.04606470427187, 'action': [0.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 134.75371557525415, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1489766777724893, 0.15659209900047596, 0.18324607329842932, 0.17705854355069015, 0.14802475011899097, 0.18562589243217514]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.97045660018921 s
