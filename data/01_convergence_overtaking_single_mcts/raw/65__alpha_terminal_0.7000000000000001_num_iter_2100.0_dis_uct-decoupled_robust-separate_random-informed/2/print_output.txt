Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 230, 'sum_payoffs': 74.68658647643522, 'action': [0.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 79.25440935843699, 'action': [1.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 82.41981454685259, 'action': [2.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 75.07384166544023, 'action': [1.0, -1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 70.13201519672698, 'action': [1.0, 0.0]}, {'num_count': 247, 'sum_payoffs': 82.35865828964039, 'action': [2.0, -1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 69.15128961579494, 'action': [0.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 78.2468995026447, 'action': [2.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 74.0880780068368, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10947168015230842, 0.1142313184198001, 0.11756306520704426, 0.1099476439790576, 0.10471204188481675, 0.11756306520704426, 0.10376011423131842, 0.11327939076630177, 0.10899571632555925]
Actions to choose Agent 1: dict_values([{'num_count': 312, 'sum_payoffs': 100.03400552673476, 'action': [0.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 134.64717546558276, 'action': [1.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 136.01695032785517, 'action': [1.0, 0.0]}, {'num_count': 317, 'sum_payoffs': 102.18207706939886, 'action': [0.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 103.10644727979081, 'action': [0.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 124.06032665611187, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14850071394574013, 0.1861018562589243, 0.18752974773917183, 0.15088053307948596, 0.1518324607329843, 0.17467872441694432]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.199753999710083 s
