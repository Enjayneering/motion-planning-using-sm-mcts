Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 248, 'sum_payoffs': 82.76681434041325, 'action': [2.0, 0.0]}, {'num_count': 232, 'sum_payoffs': 75.49366924485203, 'action': [1.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 85.48847294771328, 'action': [2.0, 1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 69.0833403308683, 'action': [0.0, 1.5707963267948966]}, {'num_count': 213, 'sum_payoffs': 66.85520082435832, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 76.872810687866, 'action': [1.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 78.65557290270526, 'action': [2.0, -1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 67.77040154190459, 'action': [0.0, -1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 81.85949672086916, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11803902903379343, 0.11042360780580676, 0.12089481199428843, 0.10376011423131842, 0.10138029509757258, 0.11185149928605426, 0.11375535459305093, 0.10233222275107091, 0.1170871013802951]
Actions to choose Agent 1: dict_values([{'num_count': 384, 'sum_payoffs': 131.42437108219386, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 102.13527992419932, 'action': [0.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 101.27579366104541, 'action': [0.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 118.71164394337238, 'action': [1.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 134.09741788541942, 'action': [1.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 111.76238996677777, 'action': [0.0, 0.0]}])
Weights num count: [0.18277010947168015, 0.15088053307948596, 0.14992860542598763, 0.1689671584959543, 0.18562589243217514, 0.16135173726796764]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.87054705619812 s
