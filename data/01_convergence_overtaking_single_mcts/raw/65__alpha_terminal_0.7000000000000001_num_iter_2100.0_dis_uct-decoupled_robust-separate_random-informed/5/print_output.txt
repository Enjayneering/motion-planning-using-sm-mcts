Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 249, 'sum_payoffs': 83.64935997909674, 'action': [2.0, 0.0]}, {'num_count': 257, 'sum_payoffs': 87.37566284343113, 'action': [2.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 73.92397278925907, 'action': [1.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 82.27492078470151, 'action': [2.0, 1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 68.11897018660441, 'action': [0.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 71.83734403850487, 'action': [0.0, 0.0]}, {'num_count': 222, 'sum_payoffs': 71.23330750766452, 'action': [1.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 79.9506644413282, 'action': [1.0, -1.5707963267948966]}, {'num_count': 219, 'sum_payoffs': 69.94375744071925, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1185149928605426, 0.12232270347453593, 0.10851975249881009, 0.1170871013802951, 0.10233222275107091, 0.10613993336506425, 0.10566396953831508, 0.11470728224654926, 0.10423607805806759]
Actions to choose Agent 1: dict_values([{'num_count': 326, 'sum_payoffs': 106.36502005437981, 'action': [0.0, 0.0]}, {'num_count': 306, 'sum_payoffs': 97.66317610856498, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 127.856929185843, 'action': [1.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 137.64656410884373, 'action': [1.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 128.68965252717786, 'action': [1.0, 1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 103.28900927044937, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15516420752022847, 0.1456449309852451, 0.17848643503093764, 0.18895763921941933, 0.17943836268443597, 0.1518324607329843]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.497066497802734 s
