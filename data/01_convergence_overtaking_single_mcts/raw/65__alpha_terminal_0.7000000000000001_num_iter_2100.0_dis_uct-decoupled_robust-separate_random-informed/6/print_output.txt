Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 222, 'sum_payoffs': 70.84399639519131, 'action': [0.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 72.10215239168396, 'action': [1.0, 1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 67.66478197412964, 'action': [0.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 78.90153356059177, 'action': [1.0, -1.5707963267948966]}, {'num_count': 260, 'sum_payoffs': 88.16450503116457, 'action': [2.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 70.40773070900393, 'action': [1.0, 0.0]}, {'num_count': 244, 'sum_payoffs': 80.80829017549176, 'action': [2.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 83.06422302849049, 'action': [2.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 71.7248821867414, 'action': [0.0, 0.0]}])
Weights num count: [0.10566396953831508, 0.10709186101856259, 0.10233222275107091, 0.1142313184198001, 0.12375059495478344, 0.10518800571156592, 0.11613517372679677, 0.1185149928605426, 0.10661589719181343]
Actions to choose Agent 1: dict_values([{'num_count': 312, 'sum_payoffs': 100.89114181412081, 'action': [0.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 108.80865269188443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 102.62439432462925, 'action': [0.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 133.09077168940715, 'action': [1.0, 0.0]}, {'num_count': 391, 'sum_payoffs': 135.71536955488034, 'action': [1.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 124.65758958405027, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14850071394574013, 0.15706806282722513, 0.1504045692527368, 0.18324607329842932, 0.1861018562589243, 0.17420276059019515]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.659202098846436 s
