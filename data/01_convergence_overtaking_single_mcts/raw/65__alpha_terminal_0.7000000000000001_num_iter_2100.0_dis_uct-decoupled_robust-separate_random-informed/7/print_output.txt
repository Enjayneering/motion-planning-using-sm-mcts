Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 211, 'sum_payoffs': 65.98929084544096, 'action': [0.0, 1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 70.40495053556278, 'action': [0.0, 0.0]}, {'num_count': 247, 'sum_payoffs': 82.10425765805316, 'action': [2.0, 0.0]}, {'num_count': 252, 'sum_payoffs': 84.4621230840303, 'action': [2.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 74.04163058590058, 'action': [1.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 73.53246374606616, 'action': [0.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 78.62182367923482, 'action': [2.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 78.6324268225331, 'action': [1.0, -1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 76.23763970602302, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10042836744407425, 0.10518800571156592, 0.11756306520704426, 0.1199428843407901, 0.10899571632555925, 0.10851975249881009, 0.11375535459305093, 0.11375535459305093, 0.11137553545930509]
Actions to choose Agent 1: dict_values([{'num_count': 395, 'sum_payoffs': 136.9215653550599, 'action': [1.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 102.15244309953472, 'action': [0.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 103.87798193219221, 'action': [0.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 127.55905276683337, 'action': [1.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 127.9557250217913, 'action': [1.0, 1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 103.78925456622775, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.188005711565921, 0.1504045692527368, 0.15230842455973345, 0.17801047120418848, 0.17848643503093764, 0.15230842455973345]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.671660661697388 s
