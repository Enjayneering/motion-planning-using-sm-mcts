Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 251, 'sum_payoffs': 84.35119604118961, 'action': [2.0, 0.0]}, {'num_count': 217, 'sum_payoffs': 68.73859617512403, 'action': [0.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 79.67837682144847, 'action': [1.0, 1.5707963267948966]}, {'num_count': 212, 'sum_payoffs': 66.63996859886444, 'action': [0.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 78.38590030430835, 'action': [1.0, -1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 83.74052876823201, 'action': [2.0, 1.5707963267948966]}, {'num_count': 214, 'sum_payoffs': 67.55767376273133, 'action': [1.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 83.4570836039915, 'action': [2.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 73.85103698352177, 'action': [0.0, 0.0]}])
Weights num count: [0.11946692051404094, 0.10328415040456926, 0.11470728224654926, 0.10090433127082342, 0.11327939076630177, 0.11899095668729176, 0.10185625892432175, 0.1185149928605426, 0.10851975249881009]
Actions to choose Agent 1: dict_values([{'num_count': 314, 'sum_payoffs': 101.28458037846829, 'action': [0.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 127.6054124087015, 'action': [1.0, -1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 106.96925512746428, 'action': [0.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 136.38627528618397, 'action': [1.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 99.11460989474024, 'action': [0.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 131.10974835500195, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14945264159923846, 0.17801047120418848, 0.15564017134697763, 0.18752974773917183, 0.14707282246549264, 0.18181818181818182]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.662368059158325 s
