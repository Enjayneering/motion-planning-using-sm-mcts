Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 296, 'sum_payoffs': 96.71343259959455, 'action': [2.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 95.34344722539107, 'action': [1.0, 1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 101.1310013401199, 'action': [2.0, 0.0]}, {'num_count': 266, 'sum_payoffs': 83.50770220150841, 'action': [0.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 87.98469491081201, 'action': [0.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 106.99302706781016, 'action': [2.0, -1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 84.41912360422671, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 92.76810497625445, 'action': [1.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 93.71105428770956, 'action': [1.0, 0.0]}])
Weights num count: [0.11380238369857747, 0.11264898116109189, 0.11764705882352941, 0.10226835832372165, 0.1061130334486736, 0.12264513648596694, 0.10303729334871203, 0.11034217608612072, 0.1111111111111111]
Actions to choose Agent 1: dict_values([{'num_count': 413, 'sum_payoffs': 135.62205004470488, 'action': [0.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 161.2767600464931, 'action': [1.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 125.81240940228628, 'action': [0.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 169.57690491431697, 'action': [1.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 154.88142882718265, 'action': [1.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 119.03689523580685, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1587850826605152, 0.18185313341022682, 0.14994232987312572, 0.18915801614763553, 0.17608612072279892, 0.1437908496732026]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.267168283462524 s
