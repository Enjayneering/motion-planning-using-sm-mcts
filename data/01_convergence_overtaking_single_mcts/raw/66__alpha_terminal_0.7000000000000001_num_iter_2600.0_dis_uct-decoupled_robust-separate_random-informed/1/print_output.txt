Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 321, 'sum_payoffs': 108.66560997920564, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 90.28037191145417, 'action': [0.0, -1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 84.56187077266314, 'action': [0.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 88.10900319991109, 'action': [1.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 93.05225419362216, 'action': [1.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 100.12874410657854, 'action': [2.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 93.50072914111396, 'action': [1.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 101.47858881096904, 'action': [2.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 88.97377493259741, 'action': [0.0, 0.0]}])
Weights num count: [0.12341407151095732, 0.10765090349865436, 0.10265282583621683, 0.1057285659361784, 0.10995770857362552, 0.11610918877354863, 0.11034217608612072, 0.11726259131103421, 0.10649750096116878]
Actions to choose Agent 1: dict_values([{'num_count': 479, 'sum_payoffs': 163.4569292078646, 'action': [1.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 130.8520259413908, 'action': [0.0, 0.0]}, {'num_count': 393, 'sum_payoffs': 126.6852716905116, 'action': [0.0, 1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 163.4080331450778, 'action': [1.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 124.1089281906829, 'action': [0.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 154.87865787224607, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.184159938485198, 0.15494040753556323, 0.1510957324106113, 0.184159938485198, 0.14878892733564014, 0.17647058823529413]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.24596118927002 s
