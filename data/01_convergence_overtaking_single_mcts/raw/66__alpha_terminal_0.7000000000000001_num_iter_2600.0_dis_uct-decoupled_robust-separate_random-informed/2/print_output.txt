Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 96.78170996417663, 'action': [2.0, 1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 85.28621999833095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 89.28388027661738, 'action': [0.0, -1.5707963267948966]}, {'num_count': 311, 'sum_payoffs': 104.38529651181592, 'action': [2.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 92.6789226078403, 'action': [1.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 93.30166148782031, 'action': [1.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 91.40443851291012, 'action': [0.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 106.56982675244099, 'action': [2.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 91.08617613089251, 'action': [1.0, 0.0]}])
Weights num count: [0.11303344867358708, 0.10303729334871203, 0.10649750096116878, 0.11956939638600539, 0.10957324106113034, 0.10995770857362552, 0.10841983852364476, 0.12149173394848135, 0.10803537101114956]
Actions to choose Agent 1: dict_values([{'num_count': 464, 'sum_payoffs': 157.5700626798529, 'action': [1.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 128.58703964971104, 'action': [0.0, 1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 144.75916348772446, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 157.64604604903352, 'action': [1.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 155.0852952513133, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 123.44156230045172, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17839292579777008, 0.1522491349480969, 0.16685890042291426, 0.17839292579777008, 0.17608612072279892, 0.14763552479815456]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.23278522491455 s
