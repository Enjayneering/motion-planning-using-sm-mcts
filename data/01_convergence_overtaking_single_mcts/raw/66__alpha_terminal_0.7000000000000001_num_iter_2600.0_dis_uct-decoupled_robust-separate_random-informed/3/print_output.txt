Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 297, 'sum_payoffs': 97.54956680665241, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 91.87209581819839, 'action': [1.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 87.86280581842236, 'action': [0.0, -1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 102.88991456248297, 'action': [2.0, -1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 107.3810144392064, 'action': [2.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 95.86637412808173, 'action': [2.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 90.40710542161438, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 86.18184449401285, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 86.07747285321608, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11418685121107267, 0.10918877354863514, 0.1057285659361784, 0.11880046136101499, 0.12264513648596694, 0.11264898116109189, 0.10803537101114956, 0.10419069588619762, 0.10419069588619762]
Actions to choose Agent 1: dict_values([{'num_count': 476, 'sum_payoffs': 162.3557454048263, 'action': [1.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 126.05223645661745, 'action': [0.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 143.8837176029782, 'action': [1.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 126.38962517930885, 'action': [0.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 176.58482938036548, 'action': [1.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 129.45566297565796, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1830065359477124, 0.1503267973856209, 0.16647443291041908, 0.15071126489811612, 0.19569396386005383, 0.15340253748558247]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.641401529312134 s
