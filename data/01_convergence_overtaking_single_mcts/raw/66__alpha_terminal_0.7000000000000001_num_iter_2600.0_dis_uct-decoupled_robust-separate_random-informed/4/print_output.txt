Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 302, 'sum_payoffs': 99.50706011913093, 'action': [1.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 102.15035518514084, 'action': [2.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 94.98364291561016, 'action': [1.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 104.44545297023298, 'action': [2.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 87.53132212419634, 'action': [0.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 85.37782725564118, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 91.9530174872021, 'action': [1.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 91.63033551978877, 'action': [2.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 85.89596167036889, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11610918877354863, 0.1184159938485198, 0.1122645136485967, 0.12033833141099577, 0.1057285659361784, 0.10380622837370242, 0.10957324106113034, 0.10918877354863514, 0.10419069588619762]
Actions to choose Agent 1: dict_values([{'num_count': 389, 'sum_payoffs': 124.96157818678299, 'action': [0.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 152.3328160420995, 'action': [1.0, 1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 132.5802651601439, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 156.09105600886673, 'action': [1.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 167.85107516066773, 'action': [1.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 129.66751348407152, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14955786236063054, 0.17416378316032297, 0.15647827758554403, 0.1776239907727797, 0.18800461361014995, 0.15378700499807765]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.56049633026123 s
