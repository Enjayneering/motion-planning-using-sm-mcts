Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 262, 'sum_payoffs': 82.55745563831296, 'action': [0.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 93.26580825008463, 'action': [1.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 92.27348005734916, 'action': [0.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 88.80229939276558, 'action': [0.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 100.34582953130807, 'action': [2.0, 1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 99.99101424973351, 'action': [2.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 99.87902959844686, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 87.53848639808918, 'action': [1.0, 0.0]}, {'num_count': 315, 'sum_payoffs': 106.21229017387941, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10073048827374087, 0.10995770857362552, 0.10918877354863514, 0.1061130334486736, 0.11610918877354863, 0.11572472126105345, 0.11572472126105345, 0.104959630911188, 0.12110726643598616]
Actions to choose Agent 1: dict_values([{'num_count': 401, 'sum_payoffs': 129.96543800245294, 'action': [0.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 121.52146133687377, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 126.5055973856046, 'action': [0.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 162.38727470447546, 'action': [1.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 163.55650190055707, 'action': [1.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 158.5461728482984, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15417147251057287, 0.14648212226066898, 0.1510957324106113, 0.18339100346020762, 0.1845444059976932, 0.17993079584775087]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.276992082595825 s
