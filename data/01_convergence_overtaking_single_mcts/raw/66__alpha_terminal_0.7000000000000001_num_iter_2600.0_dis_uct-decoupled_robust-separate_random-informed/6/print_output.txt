Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 273, 'sum_payoffs': 87.15183895290927, 'action': [0.0, 1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 89.36968179706068, 'action': [0.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 85.84306084856662, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 92.4725538978433, 'action': [1.0, 1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 101.82619020672517, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 90.18721940625629, 'action': [1.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 107.66114708309104, 'action': [2.0, 0.0]}, {'num_count': 293, 'sum_payoffs': 96.04251395217844, 'action': [1.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 97.36504684738799, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.104959630911188, 0.10688196847366398, 0.10380622837370242, 0.10957324106113034, 0.11764705882352941, 0.10765090349865436, 0.12264513648596694, 0.11264898116109189, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 463, 'sum_payoffs': 157.02520995969937, 'action': [1.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 129.57029072417097, 'action': [0.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 132.0827355522001, 'action': [0.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 146.73492227938175, 'action': [1.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 171.75052495710557, 'action': [1.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 128.72908829329603, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1780084582852749, 0.15340253748558247, 0.15570934256055363, 0.16878123798539024, 0.1910803537101115, 0.15263360246059207]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.59001922607422 s
