Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 322, 'sum_payoffs': 108.97044290680033, 'action': [2.0, 0.0]}, {'num_count': 298, 'sum_payoffs': 98.28485316241002, 'action': [1.0, -1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 84.18240707206562, 'action': [0.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 88.52100498674528, 'action': [0.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 92.07444452842019, 'action': [1.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 105.06839739260151, 'action': [2.0, -1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 84.11426452518045, 'action': [0.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 92.1946189575602, 'action': [1.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 95.14415762350531, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12379853902345252, 0.11457131872356786, 0.10226835832372165, 0.1061130334486736, 0.10918877354863514, 0.12033833141099577, 0.10226835832372165, 0.10918877354863514, 0.1118800461361015]
Actions to choose Agent 1: dict_values([{'num_count': 390, 'sum_payoffs': 125.54795474499696, 'action': [0.0, -1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 163.62602699253708, 'action': [1.0, 0.0]}, {'num_count': 388, 'sum_payoffs': 124.74080615480959, 'action': [0.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 160.628850174495, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 157.52643675608633, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 132.28699162256478, 'action': [0.0, 0.0]}])
Weights num count: [0.14994232987312572, 0.184159938485198, 0.14917339484813533, 0.18146866589773164, 0.1787773933102653, 0.15609381007304882]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.241276741027832 s
