Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 280, 'sum_payoffs': 90.0106001132222, 'action': [1.0, 0.0]}, {'num_count': 318, 'sum_payoffs': 106.89073559171537, 'action': [2.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 92.66522005489955, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 85.1513410720604, 'action': [0.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 93.48153921172826, 'action': [1.0, 1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 86.98953128173682, 'action': [0.0, 0.0]}, {'num_count': 308, 'sum_payoffs': 102.42963391120044, 'action': [2.0, 0.0]}, {'num_count': 266, 'sum_payoffs': 83.85638175245055, 'action': [0.0, 1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 104.25180891265184, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10765090349865436, 0.12226066897347174, 0.10995770857362552, 0.10342176086120723, 0.11072664359861592, 0.104959630911188, 0.1184159938485198, 0.10226835832372165, 0.11995386389850057]
Actions to choose Agent 1: dict_values([{'num_count': 376, 'sum_payoffs': 120.44729623666741, 'action': [0.0, -1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 122.9561005303533, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 159.44350020097835, 'action': [1.0, -1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 172.3869298266932, 'action': [1.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 164.54923680175295, 'action': [1.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 130.02132700359772, 'action': [0.0, 0.0]}])
Weights num count: [0.144559784698193, 0.14686658977316416, 0.17954632833525566, 0.1910803537101115, 0.184159938485198, 0.15340253748558247]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.326514720916748 s
