Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 261, 'sum_payoffs': 81.97962660803132, 'action': [0.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 96.94063797193058, 'action': [1.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 88.86412515293716, 'action': [0.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 92.01743528868174, 'action': [1.0, 0.0]}, {'num_count': 302, 'sum_payoffs': 99.99684191393862, 'action': [2.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 92.11639376464049, 'action': [1.0, -1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 104.47633701137926, 'action': [2.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 105.90724045767817, 'action': [2.0, 1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 85.91543706737347, 'action': [0.0, 0.0]}])
Weights num count: [0.10034602076124567, 0.11341791618608228, 0.10649750096116878, 0.10918877354863514, 0.11610918877354863, 0.10918877354863514, 0.11995386389850057, 0.12110726643598616, 0.10380622837370242]
Actions to choose Agent 1: dict_values([{'num_count': 474, 'sum_payoffs': 161.20846286187722, 'action': [1.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 157.7774322788449, 'action': [1.0, 1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 125.0035650958034, 'action': [0.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 121.57337115787682, 'action': [0.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 132.21737726759008, 'action': [0.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 165.50970866865916, 'action': [1.0, 0.0]}])
Weights num count: [0.18223760092272204, 0.17916186082276048, 0.14955786236063054, 0.14648212226066898, 0.15609381007304882, 0.18608227604767397]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.459156274795532 s
