Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 357, 'sum_payoffs': 117.2862114325585, 'action': [2.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 129.04032257921097, 'action': [2.0, 0.0]}, {'num_count': 326, 'sum_payoffs': 103.8751830876287, 'action': [0.0, -1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 99.52275495682773, 'action': [0.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 118.15515289334496, 'action': [2.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 111.61682595511202, 'action': [1.0, 0.0]}, {'num_count': 348, 'sum_payoffs': 113.33626338742653, 'action': [1.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 111.63256132859783, 'action': [1.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 102.17566531480905, 'action': [0.0, 0.0]}])
Weights num count: [0.11512415349887133, 0.12383102225088681, 0.10512737826507579, 0.1019026120606256, 0.11576910673976137, 0.1109319574330861, 0.11222186391486617, 0.1109319574330861, 0.1038374717832957]
Actions to choose Agent 1: dict_values([{'num_count': 543, 'sum_payoffs': 182.7883071032119, 'action': [1.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 151.8468245641705, 'action': [0.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 147.70953918638241, 'action': [0.0, 0.0]}, {'num_count': 613, 'sum_payoffs': 212.73108690814522, 'action': [1.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 180.3745208723805, 'action': [1.0, -1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 155.98970454364044, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17510480490164462, 0.15124153498871332, 0.14801676878426315, 0.19767816833279586, 0.17316994517897452, 0.15446630119316349]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.99881649017334 s
