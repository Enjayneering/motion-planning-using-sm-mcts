Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 353, 'sum_payoffs': 115.1975097640867, 'action': [1.0, -1.5707963267948966]}, {'num_count': 356, 'sum_payoffs': 116.41268860328903, 'action': [2.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 108.62295779163075, 'action': [1.0, 1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 99.16144066576153, 'action': [0.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 120.83422889905403, 'action': [2.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 122.92793282088535, 'action': [2.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 111.71371651943717, 'action': [1.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 106.09004597968315, 'action': [0.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 102.14380631634552, 'action': [0.0, 0.0]}])
Weights num count: [0.11383424701709126, 0.11480167687842631, 0.10899709771041599, 0.1019026120606256, 0.11802644308287649, 0.11963882618510158, 0.11125443405353112, 0.10706223798774589, 0.10415994840374072]
Actions to choose Agent 1: dict_values([{'num_count': 560, 'sum_payoffs': 190.56866322315963, 'action': [1.0, -1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 184.63969109381455, 'action': [1.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 158.54131154326254, 'action': [0.0, -1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 190.10780691404483, 'action': [1.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 146.4165477023991, 'action': [0.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 163.49418370681136, 'action': [0.0, 0.0]}])
Weights num count: [0.18058690744920994, 0.17607223476297967, 0.1560786842953886, 0.18026443082876492, 0.14672686230248308, 0.1599484037407288]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 25.18887162208557 s
