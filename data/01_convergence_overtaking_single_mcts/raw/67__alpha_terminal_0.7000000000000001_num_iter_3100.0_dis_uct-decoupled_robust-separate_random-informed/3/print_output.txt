Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 327, 'sum_payoffs': 103.6712676365426, 'action': [0.0, -1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 97.37530425785822, 'action': [0.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 111.95339722555649, 'action': [1.0, -1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 115.45413609485458, 'action': [1.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 121.11398192884033, 'action': [2.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 124.47740597735941, 'action': [2.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 117.10532261846409, 'action': [2.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 102.1070429903953, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 108.5573617041051, 'action': [1.0, 0.0]}])
Weights num count: [0.1054498548855208, 0.10061270557884554, 0.11157691067397614, 0.11415672363753628, 0.11834891970332151, 0.12092873266688164, 0.11544663011931634, 0.10415994840374072, 0.10899709771041599]
Actions to choose Agent 1: dict_values([{'num_count': 458, 'sum_payoffs': 146.7575309943942, 'action': [0.0, -1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 202.62176423863323, 'action': [1.0, 0.0]}, {'num_count': 559, 'sum_payoffs': 189.034324463229, 'action': [1.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 152.56110254965688, 'action': [0.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 155.035887912453, 'action': [0.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 181.89084312787094, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14769429216381813, 0.1905836826830055, 0.18026443082876492, 0.15220896485004837, 0.15414382457271847, 0.1747823282811996]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 25.075111150741577 s
