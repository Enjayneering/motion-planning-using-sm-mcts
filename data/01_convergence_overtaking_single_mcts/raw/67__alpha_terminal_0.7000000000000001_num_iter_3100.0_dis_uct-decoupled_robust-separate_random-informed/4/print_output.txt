Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 311, 'sum_payoffs': 97.24345373232772, 'action': [0.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 101.02692864630718, 'action': [0.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 110.09894921972496, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 110.55351395880525, 'action': [1.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 125.3008324110574, 'action': [2.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 117.44393685644236, 'action': [1.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 114.90056740980243, 'action': [1.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 101.49443334404181, 'action': [0.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 126.66115870759313, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10029022895840052, 0.10319251854240567, 0.10996452757175104, 0.11028700419219607, 0.12125120928732668, 0.11544663011931634, 0.11351177039664624, 0.10351499516285069, 0.12221863914866173]
Actions to choose Agent 1: dict_values([{'num_count': 483, 'sum_payoffs': 157.3577772736006, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 148.5586019600077, 'action': [0.0, -1.5707963267948966]}, {'num_count': 593, 'sum_payoffs': 203.70712192585952, 'action': [1.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 143.61774428907628, 'action': [0.0, 1.5707963267948966]}, {'num_count': 560, 'sum_payoffs': 189.7587461033999, 'action': [1.0, -1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 186.38324195473726, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15575620767494355, 0.1489841986455982, 0.19122863592389552, 0.14511447920025797, 0.18058690744920994, 0.1780070944856498]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.915472984313965 s
