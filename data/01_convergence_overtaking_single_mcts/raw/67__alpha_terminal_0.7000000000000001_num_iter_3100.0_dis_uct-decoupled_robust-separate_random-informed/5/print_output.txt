Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 341, 'sum_payoffs': 109.8347106290329, 'action': [1.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 113.61296528355797, 'action': [1.0, -1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 102.91303967674338, 'action': [0.0, 0.0]}, {'num_count': 364, 'sum_payoffs': 119.68553391725376, 'action': [2.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 106.26118128417932, 'action': [0.0, -1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 97.39243344653819, 'action': [0.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 120.56038451773134, 'action': [2.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 109.75911275858333, 'action': [1.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 121.51513167471958, 'action': [2.0, 0.0]}])
Weights num count: [0.10996452757175104, 0.11286681715575621, 0.10480490164463076, 0.11738148984198646, 0.1073847146081909, 0.10061270557884554, 0.11802644308287649, 0.10996452757175104, 0.11867139632376653]
Actions to choose Agent 1: dict_values([{'num_count': 455, 'sum_payoffs': 146.3086138337414, 'action': [0.0, 1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 198.56431851792038, 'action': [1.0, 0.0]}, {'num_count': 476, 'sum_payoffs': 155.0540790695035, 'action': [0.0, -1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 184.47738093158728, 'action': [1.0, 1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 153.7750859738537, 'action': [0.0, 0.0]}, {'num_count': 571, 'sum_payoffs': 195.11509114609595, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14672686230248308, 0.18671396323766526, 0.15349887133182843, 0.17607223476297967, 0.15253144147049338, 0.18413415027410512]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.692389965057373 s
