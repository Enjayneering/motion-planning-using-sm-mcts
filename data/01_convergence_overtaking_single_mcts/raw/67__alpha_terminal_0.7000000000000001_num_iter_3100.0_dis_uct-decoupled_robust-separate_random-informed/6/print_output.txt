Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 365, 'sum_payoffs': 121.26070897093159, 'action': [2.0, 1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 101.74374113265867, 'action': [0.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 112.9014511242461, 'action': [1.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 103.38295005167055, 'action': [0.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 106.51191793257374, 'action': [0.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 119.92171455690831, 'action': [2.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 110.66786393319447, 'action': [1.0, 1.5707963267948966]}, {'num_count': 347, 'sum_payoffs': 113.3527600574679, 'action': [1.0, 0.0]}, {'num_count': 364, 'sum_payoffs': 120.82243050125129, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11770396646243148, 0.10319251854240567, 0.11157691067397614, 0.10448242502418574, 0.10673976136730087, 0.11673653660109642, 0.10996452757175104, 0.11189938729442116, 0.11738148984198646]
Actions to choose Agent 1: dict_values([{'num_count': 490, 'sum_payoffs': 160.12335187615784, 'action': [0.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 179.43395692060278, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 156.78982272140422, 'action': [0.0, 0.0]}, {'num_count': 595, 'sum_payoffs': 204.40460237071957, 'action': [1.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 150.99872534003603, 'action': [0.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 176.41192214964767, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1580135440180587, 0.1728474685585295, 0.15543373105449854, 0.19187358916478556, 0.1509190583682683, 0.1705901322154144]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.864931344985962 s
