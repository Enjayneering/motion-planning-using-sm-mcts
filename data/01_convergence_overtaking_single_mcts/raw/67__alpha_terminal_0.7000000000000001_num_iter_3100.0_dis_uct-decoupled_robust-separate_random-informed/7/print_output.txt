Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 310, 'sum_payoffs': 97.2309536379132, 'action': [0.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 127.89388028030213, 'action': [2.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 115.74246029679382, 'action': [1.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 103.08434624472824, 'action': [0.0, -1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 118.7480327874263, 'action': [2.0, 1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 103.62855390925225, 'action': [0.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 110.94175516627266, 'action': [1.0, 1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 107.49991139614914, 'action': [1.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 123.63409351170245, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.0999677523379555, 0.12286359238955176, 0.11383424701709126, 0.10448242502418574, 0.11609158336020639, 0.10480490164463076, 0.11028700419219607, 0.10770719122863592, 0.11963882618510158]
Actions to choose Agent 1: dict_values([{'num_count': 544, 'sum_payoffs': 182.91074337617408, 'action': [1.0, -1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 202.68915278400956, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 157.2741732396161, 'action': [0.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 148.89451210813152, 'action': [0.0, -1.5707963267948966]}, {'num_count': 547, 'sum_payoffs': 184.16176160283055, 'action': [1.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 152.69879599157792, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17542728152208964, 0.1905836826830055, 0.15575620767494355, 0.14930667526604322, 0.1763947113834247, 0.15220896485004837]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 25.361475467681885 s
