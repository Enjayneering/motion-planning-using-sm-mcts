Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 375, 'sum_payoffs': 125.08211490184254, 'action': [2.0, 0.0]}, {'num_count': 325, 'sum_payoffs': 103.4841597743387, 'action': [0.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 108.1880640499054, 'action': [1.0, 1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 115.98589302199949, 'action': [2.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 111.6157146235392, 'action': [1.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 101.65630663324691, 'action': [0.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 116.35757682728334, 'action': [1.0, -1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 103.89990408347464, 'action': [0.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 120.38143269097333, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.12092873266688164, 0.10480490164463076, 0.10835214446952596, 0.11415672363753628, 0.1109319574330861, 0.10351499516285069, 0.11447920025798129, 0.10512737826507579, 0.11738148984198646]
Actions to choose Agent 1: dict_values([{'num_count': 582, 'sum_payoffs': 199.6246891843276, 'action': [1.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 151.89168433252448, 'action': [0.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 181.39245425588126, 'action': [1.0, 1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 194.0006574666316, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 157.3612945314787, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 147.80312798195897, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1876813930990003, 0.15124153498871332, 0.17381489841986456, 0.1834891970332151, 0.15543373105449854, 0.14801676878426315]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 25.266796112060547 s
