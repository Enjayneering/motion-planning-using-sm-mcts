Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 385, 'sum_payoffs': 129.28500554321087, 'action': [2.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 94.33501526083451, 'action': [0.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 121.9188867996836, 'action': [2.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 104.97229048314625, 'action': [0.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 124.88145306151618, 'action': [2.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 107.05088977836799, 'action': [1.0, 1.5707963267948966]}, {'num_count': 321, 'sum_payoffs': 101.62370395241048, 'action': [1.0, 0.0]}, {'num_count': 360, 'sum_payoffs': 118.38547853194234, 'action': [1.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 102.85480214703479, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12415349887133183, 0.09803289261528539, 0.11867139632376653, 0.10609480812641084, 0.12092873266688164, 0.10770719122863592, 0.10351499516285069, 0.11609158336020639, 0.10448242502418574]
Actions to choose Agent 1: dict_values([{'num_count': 543, 'sum_payoffs': 181.9585854424722, 'action': [1.0, -1.5707963267948966]}, {'num_count': 586, 'sum_payoffs': 200.0440590243882, 'action': [1.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 156.89865622557693, 'action': [0.0, 0.0]}, {'num_count': 558, 'sum_payoffs': 188.2925171206074, 'action': [1.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 144.77394933576065, 'action': [0.0, -1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 153.9890916770588, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17510480490164462, 0.1889712995807804, 0.15575620767494355, 0.1799419542083199, 0.14640438568203806, 0.15349887133182843]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.701085329055786 s
