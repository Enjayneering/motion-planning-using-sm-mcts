Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 433, 'sum_payoffs': 143.30032552940042, 'action': [2.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 151.63345518810291, 'action': [2.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 123.71572259853004, 'action': [1.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 127.93524509111184, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 133.49541111863175, 'action': [2.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 112.60590613576481, 'action': [0.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 117.38304604215367, 'action': [0.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 132.64310846439489, 'action': [1.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 120.25383735231458, 'action': [0.0, 0.0]}])
Weights num count: [0.12024437656206609, 0.12579838933629547, 0.10747014718133852, 0.1102471535684532, 0.11385726187170231, 0.10024993057484032, 0.10330463760066648, 0.11330186059427937, 0.10524854207164676]
Actions to choose Agent 1: dict_values([{'num_count': 573, 'sum_payoffs': 186.80716071992214, 'action': [0.0, 0.0]}, {'num_count': 625, 'sum_payoffs': 208.4006411283783, 'action': [1.0, 1.5707963267948966]}, {'num_count': 687, 'sum_payoffs': 234.18414494932895, 'action': [1.0, 0.0]}, {'num_count': 631, 'sum_payoffs': 210.86771822645412, 'action': [1.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 174.18968231617936, 'action': [0.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 174.20113086931977, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15912246598167176, 0.17356289919466814, 0.19078033879477924, 0.17522910302693695, 0.15051374618161623, 0.15051374618161623]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.52949547767639 s
