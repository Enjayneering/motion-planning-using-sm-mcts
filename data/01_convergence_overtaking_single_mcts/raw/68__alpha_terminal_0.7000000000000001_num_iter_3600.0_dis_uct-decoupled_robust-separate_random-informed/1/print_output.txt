Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 376, 'sum_payoffs': 118.49588027887404, 'action': [0.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 144.65693107230172, 'action': [2.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 132.671417218148, 'action': [2.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 122.5880177859631, 'action': [0.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 122.24734483003708, 'action': [1.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 131.50986807918125, 'action': [1.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 114.6495731788529, 'action': [0.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 135.65954719678297, 'action': [2.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 134.47768049100216, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10441544015551235, 0.12163287975562344, 0.11385726187170231, 0.10719244654262705, 0.10691474590391557, 0.1130241599555679, 0.10191613440710913, 0.11580116634268259, 0.11496806442654818]
Actions to choose Agent 1: dict_values([{'num_count': 552, 'sum_payoffs': 178.28887673736912, 'action': [0.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 185.67149817022644, 'action': [0.0, 0.0]}, {'num_count': 524, 'sum_payoffs': 166.82590048284496, 'action': [0.0, 1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 209.17652800656555, 'action': [1.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 216.61038786842408, 'action': [1.0, 1.5707963267948966]}, {'num_count': 682, 'sum_payoffs': 231.9807195984846, 'action': [1.0, 0.0]}])
Weights num count: [0.1532907525687309, 0.15828936406553734, 0.14551513468480978, 0.1741183004720911, 0.17911691196889754, 0.1893918356012219]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.770976781845093 s
