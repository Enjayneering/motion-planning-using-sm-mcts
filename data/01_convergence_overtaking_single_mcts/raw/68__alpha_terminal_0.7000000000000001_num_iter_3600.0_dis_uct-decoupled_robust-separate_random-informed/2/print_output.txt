Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 407, 'sum_payoffs': 131.62253267098387, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 119.79311103841307, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 127.81643614336988, 'action': [1.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 146.83350833447346, 'action': [2.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 137.1566952288207, 'action': [2.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 140.1265252351942, 'action': [2.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 111.41478095624912, 'action': [0.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 126.89019985551865, 'action': [1.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 116.41609449888502, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1130241599555679, 0.10524854207164676, 0.11052485420716468, 0.12302138294918079, 0.116634268258817, 0.11857817272979727, 0.09969452929741739, 0.10996945292974174, 0.10302693696195502]
Actions to choose Agent 1: dict_values([{'num_count': 645, 'sum_payoffs': 217.35869041527346, 'action': [1.0, -1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 171.51035103995406, 'action': [0.0, -1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 172.68857270709012, 'action': [0.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 212.3335097852465, 'action': [1.0, 1.5707963267948966]}, {'num_count': 561, 'sum_payoffs': 182.4661383766709, 'action': [0.0, 0.0]}, {'num_count': 690, 'sum_payoffs': 236.16302911124282, 'action': [1.0, 0.0]}])
Weights num count: [0.17911691196889754, 0.14829214107192445, 0.14912524298805888, 0.1757845043043599, 0.15579005831713413, 0.19161344071091363]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.717559099197388 s
