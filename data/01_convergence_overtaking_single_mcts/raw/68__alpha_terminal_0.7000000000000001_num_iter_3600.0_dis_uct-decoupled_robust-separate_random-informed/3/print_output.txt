Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 389, 'sum_payoffs': 123.72142965557927, 'action': [1.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 136.4934426592979, 'action': [2.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 114.56541862937947, 'action': [0.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 122.55137995449984, 'action': [1.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 142.46230306331498, 'action': [2.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 147.16435394418588, 'action': [2.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 128.3885125726013, 'action': [1.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 119.12234353183725, 'action': [0.0, 0.0]}, {'num_count': 384, 'sum_payoffs': 121.66927350003765, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10802554845876146, 0.11635656762010553, 0.10191613440710913, 0.10719244654262705, 0.12024437656206609, 0.12329908358789225, 0.11108025548458761, 0.1049708414329353, 0.10663704526520411]
Actions to choose Agent 1: dict_values([{'num_count': 686, 'sum_payoffs': 235.28579850210141, 'action': [1.0, 0.0]}, {'num_count': 574, 'sum_payoffs': 188.62994103946787, 'action': [0.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 168.80453519348663, 'action': [0.0, 1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 214.2909392206701, 'action': [1.0, 1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 175.7616065267124, 'action': [0.0, -1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 213.93996217995428, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.19050263815606777, 0.15940016662038323, 0.1460705359622327, 0.1766176062204943, 0.1507914468203277, 0.17633990558178284]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.78226947784424 s
