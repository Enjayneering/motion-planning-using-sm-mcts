Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 416, 'sum_payoffs': 135.75444923245797, 'action': [2.0, 1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 122.56016614632178, 'action': [0.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 142.68580211179363, 'action': [2.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 133.164228992198, 'action': [1.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 126.0943182480325, 'action': [1.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 114.13709802041485, 'action': [0.0, 1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 140.9544555604541, 'action': [2.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 121.78251905033412, 'action': [1.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 123.92583795514618, 'action': [0.0, 0.0]}])
Weights num count: [0.11552346570397112, 0.10691474590391557, 0.11996667592335462, 0.11385726187170231, 0.10913635101360733, 0.1013607331296862, 0.11885587336850875, 0.10635934462649264, 0.10774784782004998]
Actions to choose Agent 1: dict_values([{'num_count': 544, 'sum_payoffs': 174.38461500619283, 'action': [0.0, 1.5707963267948966]}, {'num_count': 634, 'sum_payoffs': 211.42965478072992, 'action': [1.0, -1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 176.05881394714478, 'action': [0.0, 0.0]}, {'num_count': 631, 'sum_payoffs': 210.08163925943975, 'action': [1.0, 1.5707963267948966]}, {'num_count': 681, 'sum_payoffs': 230.84138904908724, 'action': [1.0, 0.0]}, {'num_count': 562, 'sum_payoffs': 181.7742947910432, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15106914745903915, 0.17606220494307137, 0.15217995001388504, 0.17522910302693695, 0.18911413496251042, 0.1560677589558456]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.049970388412476 s
