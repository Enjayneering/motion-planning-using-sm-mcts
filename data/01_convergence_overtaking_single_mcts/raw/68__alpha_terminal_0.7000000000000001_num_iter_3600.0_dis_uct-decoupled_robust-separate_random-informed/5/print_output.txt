Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 379, 'sum_payoffs': 119.63251525495997, 'action': [0.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 118.80914622812276, 'action': [0.0, -1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 146.76278565553582, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 131.9676408994282, 'action': [2.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 131.94303490208233, 'action': [1.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 130.13338582181458, 'action': [1.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 125.20036810258736, 'action': [1.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 114.31231900738362, 'action': [0.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 138.1565100081246, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10524854207164676, 0.10469314079422383, 0.12302138294918079, 0.11330186059427937, 0.11330186059427937, 0.1121910580394335, 0.10885865037489587, 0.10163843376839767, 0.1174673701749514]
Actions to choose Agent 1: dict_values([{'num_count': 648, 'sum_payoffs': 218.55244167317204, 'action': [1.0, 1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 213.6152437543107, 'action': [1.0, -1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 167.02537417745125, 'action': [0.0, 1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 175.08490913143464, 'action': [0.0, -1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 183.28944016083892, 'action': [0.0, 0.0]}, {'num_count': 687, 'sum_payoffs': 234.90408238644298, 'action': [1.0, 0.0]}])
Weights num count: [0.17995001388503193, 0.1766176062204943, 0.14523743404609832, 0.1507914468203277, 0.15634545959455706, 0.19078033879477924]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.077740907669067 s
