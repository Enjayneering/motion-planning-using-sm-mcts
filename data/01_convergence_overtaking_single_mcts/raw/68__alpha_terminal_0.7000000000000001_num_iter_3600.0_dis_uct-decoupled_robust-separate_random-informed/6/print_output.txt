Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 387, 'sum_payoffs': 122.88299498831338, 'action': [0.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 132.50979258493646, 'action': [1.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 113.6928383456244, 'action': [0.0, -1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 115.79930897241758, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 136.34584001164515, 'action': [2.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 144.1073651909354, 'action': [2.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 135.6094839097331, 'action': [1.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 122.9480972235003, 'action': [1.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 131.78420889140372, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10747014718133852, 0.11385726187170231, 0.1013607331296862, 0.10274923632324354, 0.11635656762010553, 0.12135517911691197, 0.11580116634268259, 0.10747014718133852, 0.11330186059427937]
Actions to choose Agent 1: dict_values([{'num_count': 663, 'sum_payoffs': 224.98497715135602, 'action': [1.0, 0.0]}, {'num_count': 651, 'sum_payoffs': 220.1068490746917, 'action': [1.0, -1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 187.27555890559955, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 216.29455401855935, 'action': [1.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 172.010849551043, 'action': [0.0, -1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 172.8674873290468, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18411552346570398, 0.18078311580116635, 0.1588447653429603, 0.17828381005276311, 0.14856984171063595, 0.14912524298805888]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 29.422126054763794 s
