Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 373, 'sum_payoffs': 117.21676591207171, 'action': [0.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 140.8789361251496, 'action': [2.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 150.68717175428694, 'action': [2.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 134.17564807020915, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 121.96755194576068, 'action': [1.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 121.09714772748552, 'action': [0.0, -1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 108.89044107652262, 'action': [0.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 127.01220505828658, 'action': [1.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 136.2754011212972, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10358233823937794, 0.11913357400722022, 0.125520688697584, 0.11469036378783672, 0.10663704526520411, 0.10608164398778117, 0.09802832546514857, 0.10996945292974174, 0.11607886698139405]
Actions to choose Agent 1: dict_values([{'num_count': 567, 'sum_payoffs': 185.32161054254075, 'action': [0.0, 0.0]}, {'num_count': 543, 'sum_payoffs': 175.41004820981138, 'action': [0.0, 1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 215.5730661208273, 'action': [1.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 169.67189338301338, 'action': [0.0, -1.5707963267948966]}, {'num_count': 687, 'sum_payoffs': 235.23017594442558, 'action': [1.0, 0.0]}, {'num_count': 634, 'sum_payoffs': 212.97855134689763, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15745626214940295, 0.1507914468203277, 0.17772840877534019, 0.14690363787836713, 0.19078033879477924, 0.17606220494307137]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.29781985282898 s
