Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 410, 'sum_payoffs': 132.701698444232, 'action': [2.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 136.07160879666128, 'action': [1.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 144.66175469179356, 'action': [2.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 122.15032407207009, 'action': [0.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 118.73603724322429, 'action': [0.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 124.70381134924729, 'action': [1.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 143.27315071475843, 'action': [2.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 122.14948014785878, 'action': [1.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 112.13855476106735, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11385726187170231, 0.11607886698139405, 0.12163287975562344, 0.10691474590391557, 0.10469314079422383, 0.10858094973618439, 0.12079977783948903, 0.10691474590391557, 0.10024993057484032]
Actions to choose Agent 1: dict_values([{'num_count': 524, 'sum_payoffs': 166.71026336703872, 'action': [0.0, -1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 208.75573826126598, 'action': [1.0, 1.5707963267948966]}, {'num_count': 597, 'sum_payoffs': 196.75016550140091, 'action': [0.0, 0.0]}, {'num_count': 638, 'sum_payoffs': 213.70599982992104, 'action': [1.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 169.28466891233666, 'action': [0.0, 1.5707963267948966]}, {'num_count': 685, 'sum_payoffs': 233.28579952035284, 'action': [1.0, 0.0]}])
Weights num count: [0.14551513468480978, 0.17384059983337963, 0.16578728131074702, 0.17717300749791726, 0.1471813385170786, 0.19022493751735628]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.129265546798706 s
