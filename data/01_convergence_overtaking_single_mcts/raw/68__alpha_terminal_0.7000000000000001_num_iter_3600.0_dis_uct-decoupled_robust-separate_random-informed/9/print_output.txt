Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 428, 'sum_payoffs': 140.32905156503895, 'action': [2.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 125.05960521296569, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 134.30423111822003, 'action': [2.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 119.99849642040242, 'action': [0.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 125.38743240224119, 'action': [1.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 146.31103224693385, 'action': [2.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 119.18849794501251, 'action': [0.0, 0.0]}, {'num_count': 399, 'sum_payoffs': 127.97660340008925, 'action': [1.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 117.45766015271906, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11885587336850875, 0.10885865037489587, 0.11496806442654818, 0.10552624271035824, 0.10913635101360733, 0.12274368231046931, 0.1049708414329353, 0.11080255484587614, 0.10386003887808942]
Actions to choose Agent 1: dict_values([{'num_count': 613, 'sum_payoffs': 203.8071991723665, 'action': [1.0, -1.5707963267948966]}, {'num_count': 672, 'sum_payoffs': 228.42479300111285, 'action': [1.0, 0.0]}, {'num_count': 557, 'sum_payoffs': 180.76652671684607, 'action': [0.0, -1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 178.32601385810761, 'action': [0.0, 1.5707963267948966]}, {'num_count': 565, 'sum_payoffs': 184.01399920325372, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 215.83742849787322, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1702304915301305, 0.1866148292141072, 0.15467925576228825, 0.15301305193001943, 0.15690086087198002, 0.17828381005276311]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.38092279434204 s
