Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 459, 'sum_payoffs': 147.27360942311572, 'action': [1.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 142.58230865759242, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 149.7573190286008, 'action': [2.0, 1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 135.24220180534584, 'action': [0.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 157.1981887135994, 'action': [2.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 121.21627044323331, 'action': [0.0, 1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 139.77157592260508, 'action': [1.0, 0.0]}, {'num_count': 539, 'sum_payoffs': 180.8016864609549, 'action': [2.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 138.91738399351456, 'action': [0.0, 0.0]}])
Weights num count: [0.1119239209948793, 0.10924164837844429, 0.11338697878566203, 0.10485247500609607, 0.11777615215801024, 0.09656181419166057, 0.1075347476225311, 0.1314313582053158, 0.10704706169227018]
Actions to choose Agent 1: dict_values([{'num_count': 725, 'sum_payoffs': 241.4047644513529, 'action': [1.0, 1.5707963267948966]}, {'num_count': 605, 'sum_payoffs': 192.80297979863866, 'action': [0.0, -1.5707963267948966]}, {'num_count': 802, 'sum_payoffs': 273.04918524900694, 'action': [1.0, 0.0]}, {'num_count': 592, 'sum_payoffs': 187.58483917031896, 'action': [0.0, 1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 252.06585590439917, 'action': [1.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 200.830568005828, 'action': [0.0, 0.0]}])
Weights num count: [0.1767861497195806, 0.14752499390392587, 0.1955620580346257, 0.14435503535722993, 0.18312606681297244, 0.152401853206535]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.12477135658264 s
