Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 518, 'sum_payoffs': 172.21090874410677, 'action': [2.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 162.72627231442576, 'action': [2.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 138.41591805973215, 'action': [1.0, 1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 153.76533177639462, 'action': [2.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 145.0238664528532, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 129.2906144879447, 'action': [0.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 133.48273395920916, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 147.64037096954257, 'action': [1.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 133.00034101143774, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1263106559375762, 0.12070226773957571, 0.10655937576200927, 0.11558156547183614, 0.11046086320409657, 0.10119483052913923, 0.10363326018044379, 0.1119239209948793, 0.10338941721531333]
Actions to choose Agent 1: dict_values([{'num_count': 709, 'sum_payoffs': 234.04620582109123, 'action': [1.0, 1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 200.8449378330272, 'action': [0.0, 1.5707963267948966]}, {'num_count': 789, 'sum_payoffs': 266.69513935193595, 'action': [1.0, 0.0]}, {'num_count': 621, 'sum_payoffs': 198.51938445358886, 'action': [0.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 200.9366241429951, 'action': [0.0, 0.0]}, {'num_count': 727, 'sum_payoffs': 241.41368836239283, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1728846622774933, 0.1528895391367959, 0.19239209948792976, 0.15142648134601316, 0.1528895391367959, 0.1772738356498415]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.44436812400818 s
