Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 435, 'sum_payoffs': 136.729193984969, 'action': [0.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 130.13764608425257, 'action': [0.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 156.64407329878918, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 135.5113039124188, 'action': [0.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 139.19787096585927, 'action': [1.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 148.6570260697631, 'action': [1.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 164.612653852238, 'action': [2.0, 0.0]}, {'num_count': 438, 'sum_payoffs': 137.8887958675681, 'action': [1.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 157.79952405814308, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10607168983174835, 0.10217020238966106, 0.11777615215801024, 0.10534016093635698, 0.1075347476225311, 0.11314313582053158, 0.1224091684954889, 0.10680321872713973, 0.1185076810534016]
Actions to choose Agent 1: dict_values([{'num_count': 775, 'sum_payoffs': 261.88463452835344, 'action': [1.0, 0.0]}, {'num_count': 663, 'sum_payoffs': 216.1432164734364, 'action': [0.0, 0.0]}, {'num_count': 618, 'sum_payoffs': 197.9399482088427, 'action': [0.0, -1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 240.06878100111172, 'action': [1.0, 1.5707963267948966]}, {'num_count': 710, 'sum_payoffs': 235.23601037399237, 'action': [1.0, -1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 195.5572555361192, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18897829797610338, 0.1616678858814923, 0.1506949524506218, 0.1760546208241892, 0.17312850524262374, 0.14923189465983908]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.95488691329956 s
