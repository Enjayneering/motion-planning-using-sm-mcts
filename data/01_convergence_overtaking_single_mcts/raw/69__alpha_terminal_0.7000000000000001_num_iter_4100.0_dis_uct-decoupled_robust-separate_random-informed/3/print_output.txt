Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 462, 'sum_payoffs': 148.11809928146405, 'action': [1.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 126.61764421594009, 'action': [0.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 146.87651216020723, 'action': [1.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 158.07702295209668, 'action': [2.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 132.69146707488073, 'action': [0.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 164.3757257140425, 'action': [2.0, 0.0]}, {'num_count': 427, 'sum_payoffs': 133.56102998266502, 'action': [0.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 150.93166032757642, 'action': [2.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 147.6573662951716, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11265544989027067, 0.09997561570348695, 0.1119239209948793, 0.1185076810534016, 0.10363326018044379, 0.12216532553035844, 0.1041209461107047, 0.11436235064618386, 0.11241160692514021]
Actions to choose Agent 1: dict_values([{'num_count': 606, 'sum_payoffs': 193.75348974389303, 'action': [0.0, 1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 238.1207337724508, 'action': [1.0, 1.5707963267948966]}, {'num_count': 759, 'sum_payoffs': 256.14914141840137, 'action': [1.0, -1.5707963267948966]}, {'num_count': 777, 'sum_payoffs': 263.5777162501608, 'action': [1.0, 0.0]}, {'num_count': 646, 'sum_payoffs': 209.91872140020396, 'action': [0.0, 0.0]}, {'num_count': 597, 'sum_payoffs': 190.09039102522945, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14776883686905634, 0.17434772006827604, 0.1850768105340161, 0.1894659839063643, 0.15752255547427457, 0.14557425018288223]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 33.38149166107178 s
