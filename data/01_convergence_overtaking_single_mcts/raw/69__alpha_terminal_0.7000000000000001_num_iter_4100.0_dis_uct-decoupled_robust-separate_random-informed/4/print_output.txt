Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 501, 'sum_payoffs': 164.25786708546926, 'action': [2.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 133.61588088151075, 'action': [1.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 146.84755169463958, 'action': [1.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 148.48175121570213, 'action': [2.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 131.82195018078997, 'action': [0.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 166.50416011537408, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 141.4035825218396, 'action': [0.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 147.70658982819998, 'action': [1.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 128.2135911733101, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12216532553035844, 0.1041209461107047, 0.1119239209948793, 0.11289929285540112, 0.10314557425018288, 0.12338454035601074, 0.10875396244818337, 0.11241160692514021, 0.10095098756400878]
Actions to choose Agent 1: dict_values([{'num_count': 771, 'sum_payoffs': 260.94827757955034, 'action': [1.0, 0.0]}, {'num_count': 730, 'sum_payoffs': 244.1384614879604, 'action': [1.0, 1.5707963267948966]}, {'num_count': 619, 'sum_payoffs': 198.9603119400111, 'action': [0.0, 1.5707963267948966]}, {'num_count': 639, 'sum_payoffs': 207.05828325949943, 'action': [0.0, 0.0]}, {'num_count': 722, 'sum_payoffs': 240.77086297362192, 'action': [1.0, -1.5707963267948966]}, {'num_count': 619, 'sum_payoffs': 198.96215972444992, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18800292611558156, 0.17800536454523286, 0.15093879541575225, 0.15581565471836137, 0.1760546208241892, 0.15093879541575225]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.12308168411255 s
