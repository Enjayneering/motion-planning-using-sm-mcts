Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 418, 'sum_payoffs': 130.11510295120186, 'action': [0.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 153.28968390078032, 'action': [2.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 162.45905994411441, 'action': [2.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 130.36376325387153, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 137.47784671490064, 'action': [0.0, 0.0]}, {'num_count': 526, 'sum_payoffs': 175.06039887884586, 'action': [2.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 144.95909691990903, 'action': [1.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 142.0249715489178, 'action': [1.0, -1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 135.0460713013438, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1019263594245306, 0.11558156547183614, 0.12094611070470616, 0.10217020238966106, 0.1063155327968788, 0.12826139965861985, 0.11070470616922702, 0.10899780541331383, 0.10485247500609607]
Actions to choose Agent 1: dict_values([{'num_count': 606, 'sum_payoffs': 193.13386981121468, 'action': [0.0, -1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 241.71807577301416, 'action': [1.0, 1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 243.8394683507045, 'action': [1.0, -1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 208.0365033252651, 'action': [0.0, 0.0]}, {'num_count': 770, 'sum_payoffs': 259.81673185624453, 'action': [1.0, 0.0]}, {'num_count': 624, 'sum_payoffs': 200.3356802722258, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14776883686905634, 0.17702999268471104, 0.17824920751036333, 0.1567910265788832, 0.18775908315045112, 0.15215801024140455]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.148821592330933 s
