Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 506, 'sum_payoffs': 166.30872208925013, 'action': [2.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 138.45128396145492, 'action': [1.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 128.1294035194076, 'action': [0.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 134.22766352860023, 'action': [0.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 166.65273655411954, 'action': [2.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 150.84366824565905, 'action': [1.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 132.599699053101, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 144.60032074498562, 'action': [2.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 145.94860546737291, 'action': [1.0, 0.0]}])
Weights num count: [0.12338454035601074, 0.10704706169227018, 0.10095098756400878, 0.10460863204096561, 0.12362838332114119, 0.11436235064618386, 0.10363326018044379, 0.11070470616922702, 0.11143623506461839]
Actions to choose Agent 1: dict_values([{'num_count': 739, 'sum_payoffs': 247.00892160115757, 'action': [1.0, 1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 199.86851775747243, 'action': [0.0, 1.5707963267948966]}, {'num_count': 712, 'sum_payoffs': 236.0106400841235, 'action': [1.0, -1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 210.20121825815778, 'action': [0.0, 0.0]}, {'num_count': 605, 'sum_payoffs': 192.60042247298156, 'action': [0.0, -1.5707963267948966]}, {'num_count': 772, 'sum_payoffs': 260.56823368001255, 'action': [1.0, 0.0]}])
Weights num count: [0.18019995123140697, 0.15191416727627408, 0.17361619117288465, 0.15825408436966593, 0.14752499390392587, 0.18824676908071203]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 30.883315563201904 s
