Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 493, 'sum_payoffs': 160.72493582285966, 'action': [2.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 143.19137909498497, 'action': [1.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 145.7989982693899, 'action': [1.0, 0.0]}, {'num_count': 441, 'sum_payoffs': 139.07875881571312, 'action': [1.0, 1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 154.88324757432738, 'action': [2.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 163.1795561826076, 'action': [2.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 128.49043298202076, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 129.93982999990865, 'action': [0.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 141.09802358246864, 'action': [0.0, 0.0]}])
Weights num count: [0.1202145818093148, 0.10997317727383565, 0.11143623506461839, 0.1075347476225311, 0.11680078029748842, 0.12167763960009753, 0.10119483052913923, 0.10217020238966106, 0.10875396244818337]
Actions to choose Agent 1: dict_values([{'num_count': 722, 'sum_payoffs': 241.36494788030316, 'action': [1.0, -1.5707963267948966]}, {'num_count': 624, 'sum_payoffs': 201.3835921975331, 'action': [0.0, -1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 203.0305568384868, 'action': [0.0, 0.0]}, {'num_count': 615, 'sum_payoffs': 197.81795901434637, 'action': [0.0, 1.5707963267948966]}, {'num_count': 794, 'sum_payoffs': 270.92304273403596, 'action': [1.0, 0.0]}, {'num_count': 717, 'sum_payoffs': 239.28347187511338, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1760546208241892, 0.15215801024140455, 0.15313338210192637, 0.14996342355523043, 0.19361131431358206, 0.17483540599853695]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.040767431259155 s
