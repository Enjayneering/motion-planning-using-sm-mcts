Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 507, 'sum_payoffs': 167.61434668380608, 'action': [2.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 133.81176389493865, 'action': [0.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 145.77470694799288, 'action': [1.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 141.32170433617912, 'action': [1.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 156.34699513656162, 'action': [2.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 148.38945749868924, 'action': [1.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 138.30612512495455, 'action': [0.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 131.79241167908634, 'action': [0.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 151.6942118187442, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12362838332114119, 0.10387710314557425, 0.11094854913435748, 0.10826627651792246, 0.11704462326261887, 0.11241160692514021, 0.10655937576200927, 0.10265788831992197, 0.11436235064618386]
Actions to choose Agent 1: dict_values([{'num_count': 638, 'sum_payoffs': 205.86700252894445, 'action': [0.0, 0.0]}, {'num_count': 745, 'sum_payoffs': 249.384169864894, 'action': [1.0, 0.0]}, {'num_count': 633, 'sum_payoffs': 203.87700951323737, 'action': [0.0, -1.5707963267948966]}, {'num_count': 725, 'sum_payoffs': 241.20526194655616, 'action': [1.0, -1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 206.65711500621023, 'action': [0.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 238.6949307469037, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15557181175323093, 0.1816630090221897, 0.15435259692757863, 0.1767861497195806, 0.15605949768349184, 0.17532309192879786]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.41198682785034 s
