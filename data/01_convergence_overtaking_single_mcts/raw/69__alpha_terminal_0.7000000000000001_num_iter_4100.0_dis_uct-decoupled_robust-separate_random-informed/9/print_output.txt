Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 450, 'sum_payoffs': 143.26011734061183, 'action': [1.0, 1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 130.8382283610349, 'action': [0.0, -1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 138.13201660726557, 'action': [0.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 146.07485761907165, 'action': [1.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 156.12412980174125, 'action': [2.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 126.31363419805625, 'action': [0.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 151.06575408320745, 'action': [2.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 150.30562507301076, 'action': [1.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 167.89755373792065, 'action': [2.0, 0.0]}])
Weights num count: [0.1097293343087052, 0.10241404535479151, 0.10680321872713973, 0.11143623506461839, 0.11728846622774933, 0.0997317727383565, 0.11436235064618386, 0.11387466471592295, 0.1241160692514021]
Actions to choose Agent 1: dict_values([{'num_count': 766, 'sum_payoffs': 258.45956020661794, 'action': [1.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 234.0890928530456, 'action': [1.0, -1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 202.6071080162948, 'action': [0.0, 1.5707963267948966]}, {'num_count': 652, 'sum_payoffs': 211.882551228572, 'action': [0.0, 0.0]}, {'num_count': 627, 'sum_payoffs': 201.78804578116376, 'action': [0.0, -1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 239.20387377194157, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1867837112899293, 0.1723969763472324, 0.1533772250670568, 0.1589856132650573, 0.1528895391367959, 0.17532309192879786]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.460236072540283 s
