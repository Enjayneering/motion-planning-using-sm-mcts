Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 282, 'sum_payoffs': 72.76361817877772, 'action': [1.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 73.42328834358533, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.85307345095998, 'action': [1.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 57.48125936073479, 'action': [0.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 94.85257369733519, 'action': [2.0, -1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 57.83108444813276, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 95.63218389210773, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 95.67216390209613, 'action': [2.0, 1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 57.851074453126934, 'action': [0.0, 0.0]}])
Weights num count: [0.10841983852364476, 0.10918877354863514, 0.10957324106113034, 0.09265667051134178, 0.13071895424836602, 0.09304113802383698, 0.1314878892733564, 0.1314878892733564, 0.09304113802383698]
Actions to choose Agent 1: dict_values([{'num_count': 479, 'sum_payoffs': 156.99150422171132, 'action': [1.0, -1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 157.85107443646075, 'action': [1.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 116.82158918592766, 'action': [0.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 117.6611693956828, 'action': [0.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 159.02048972861965, 'action': [1.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 120.13993001495992, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.184159938485198, 0.18492887351018839, 0.14686658977316416, 0.14763552479815456, 0.18608227604767397, 0.14994232987312572]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.33426594734191895 s
