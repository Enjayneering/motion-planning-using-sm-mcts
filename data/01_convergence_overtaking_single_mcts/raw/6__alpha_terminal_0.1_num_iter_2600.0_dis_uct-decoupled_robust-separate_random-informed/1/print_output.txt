Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 73.42328834358533, 'action': [1.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 73.05347325119318, 'action': [1.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.52123937072313, 'action': [0.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 95.63218389210776, 'action': [2.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 94.85257369733519, 'action': [2.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 57.52123937072313, 'action': [0.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 57.46126935574062, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 95.63218389210778, 'action': [2.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 74.20289853835796, 'action': [1.0, 0.0]}])
Weights num count: [0.10918877354863514, 0.10880430603613994, 0.09265667051134178, 0.1314878892733564, 0.13071895424836602, 0.09265667051134178, 0.09265667051134178, 0.1314878892733564, 0.10995770857362552]
Actions to choose Agent 1: dict_values([{'num_count': 387, 'sum_payoffs': 118.87056469783009, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 159.5102448509768, 'action': [1.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 157.38130931909774, 'action': [1.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 116.84157919092183, 'action': [0.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 157.38130931909762, 'action': [1.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 118.48075960044378, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14878892733564014, 0.18646674356016918, 0.1845444059976932, 0.14686658977316416, 0.1845444059976932, 0.14840445982314496]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3588261604309082 s
