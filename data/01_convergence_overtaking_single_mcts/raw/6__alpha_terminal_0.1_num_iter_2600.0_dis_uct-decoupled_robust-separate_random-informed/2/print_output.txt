Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 285, 'sum_payoffs': 73.9130434659425, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.83308344596581, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 73.4432783485795, 'action': [1.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 94.4427785949547, 'action': [2.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 57.151424278330985, 'action': [0.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.461269355740626, 'action': [0.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 96.02198898949408, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 95.63218389210775, 'action': [2.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.421289345752285, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10957324106113034, 0.10957324106113034, 0.10918877354863514, 0.1303344867358708, 0.0922722029988466, 0.09265667051134178, 0.1318723567858516, 0.1314878892733564, 0.09265667051134178]
Actions to choose Agent 1: dict_values([{'num_count': 487, 'sum_payoffs': 160.28985504574942, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 118.89055470282426, 'action': [0.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 116.84157919092182, 'action': [0.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 116.9215392108985, 'action': [0.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 159.47026484098845, 'action': [1.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 156.11194400196794, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18723567858515955, 0.14878892733564014, 0.14686658977316416, 0.14686658977316416, 0.18646674356016918, 0.18339100346020762]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.33516407012939453 s
