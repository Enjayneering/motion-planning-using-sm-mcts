Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 341, 'sum_payoffs': 95.24237879472147, 'action': [2.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.48125936073479, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 73.4432783485795, 'action': [1.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.56121938071147, 'action': [0.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 73.4432783485795, 'action': [1.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.56121938071147, 'action': [0.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 73.07346325618735, 'action': [1.0, 0.0]}, {'num_count': 341, 'sum_payoffs': 95.24237879472147, 'action': [2.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 96.39180408188615, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1311034217608612, 0.09265667051134178, 0.10918877354863514, 0.09265667051134178, 0.10918877354863514, 0.09265667051134178, 0.10880430603613994, 0.1311034217608612, 0.1322568242983468]
Actions to choose Agent 1: dict_values([{'num_count': 479, 'sum_payoffs': 156.97151421671714, 'action': [1.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 117.2513742933023, 'action': [0.0, 1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 159.8800599433689, 'action': [1.0, 1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 157.83108443146662, 'action': [1.0, 0.0]}, {'num_count': 384, 'sum_payoffs': 117.64117939068863, 'action': [0.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 118.87056469783009, 'action': [0.0, 0.0]}])
Weights num count: [0.184159938485198, 0.14725105728565935, 0.18685121107266436, 0.18492887351018839, 0.14763552479815456, 0.14878892733564014]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3353235721588135 s
