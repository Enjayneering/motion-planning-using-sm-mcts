Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 241, 'sum_payoffs': 57.54122937571731, 'action': [0.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.83308344596581, 'action': [1.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 94.832583692341, 'action': [2.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 94.83258369234102, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 95.6121938871136, 'action': [2.0, -1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 57.871064458121104, 'action': [0.0, 0.0]}, {'num_count': 242, 'sum_payoffs': 57.851074453126934, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.89305346094832, 'action': [1.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 73.05347325119318, 'action': [1.0, 0.0]}])
Weights num count: [0.09265667051134178, 0.10957324106113034, 0.13071895424836602, 0.13071895424836602, 0.1314878892733564, 0.09304113802383698, 0.09304113802383698, 0.10957324106113034, 0.10880430603613994]
Actions to choose Agent 1: dict_values([{'num_count': 389, 'sum_payoffs': 119.6501748926027, 'action': [0.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 157.36131931410364, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 117.72113941066532, 'action': [0.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 156.56171911433663, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 118.9305347128126, 'action': [0.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 158.22088952885295, 'action': [1.0, 0.0]}])
Weights num count: [0.14955786236063054, 0.1845444059976932, 0.14763552479815456, 0.1837754709727028, 0.14878892733564014, 0.18531334102268357]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.33275580406188965 s
