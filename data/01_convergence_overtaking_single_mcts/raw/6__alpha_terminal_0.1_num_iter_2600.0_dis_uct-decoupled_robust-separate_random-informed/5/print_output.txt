Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 342, 'sum_payoffs': 95.55222387213111, 'action': [2.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 94.50274860993721, 'action': [2.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.52123937072313, 'action': [0.0, 1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 57.8110944431386, 'action': [0.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 72.72363816878938, 'action': [1.0, 1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 58.59070463791122, 'action': [0.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 73.79310343597747, 'action': [1.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 95.65217389710196, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 73.1134432661757, 'action': [1.0, 0.0]}])
Weights num count: [0.1314878892733564, 0.1303344867358708, 0.09265667051134178, 0.09304113802383698, 0.10841983852364476, 0.09381007304882738, 0.10957324106113034, 0.1314878892733564, 0.10880430603613994]
Actions to choose Agent 1: dict_values([{'num_count': 388, 'sum_payoffs': 119.36031982018727, 'action': [0.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 157.4012993240919, 'action': [1.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 116.08195900114339, 'action': [0.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 120.93953021472667, 'action': [0.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 158.16091951387037, 'action': [1.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 156.52173910434848, 'action': [1.0, 0.0]}])
Weights num count: [0.14917339484813533, 0.1845444059976932, 0.14609765474817377, 0.15071126489811612, 0.18531334102268357, 0.1837754709727028]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3825361728668213 s
