Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 339, 'sum_payoffs': 94.50274860993726, 'action': [2.0, 1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.50124936572896, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 73.40329833859116, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 73.48325835856784, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 73.7731134309833, 'action': [1.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 57.44127935074645, 'action': [0.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 96.04197899448819, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 95.65217389710193, 'action': [2.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 57.46126935574062, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1303344867358708, 0.09265667051134178, 0.10918877354863514, 0.10918877354863514, 0.10957324106113034, 0.09265667051134178, 0.1318723567858516, 0.1314878892733564, 0.09265667051134178]
Actions to choose Agent 1: dict_values([{'num_count': 384, 'sum_payoffs': 117.70114940567115, 'action': [0.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 119.3403298151931, 'action': [0.0, -1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 157.83108443146662, 'action': [1.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 159.0204897286196, 'action': [1.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 159.00049972362535, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 115.65217389376872, 'action': [0.0, 0.0]}])
Weights num count: [0.14763552479815456, 0.14917339484813533, 0.18492887351018839, 0.18608227604767397, 0.18608227604767397, 0.14571318723567858]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3336677551269531 s
