Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 340, 'sum_payoffs': 94.87256370232933, 'action': [2.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 74.24287854834628, 'action': [1.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 94.48275860494304, 'action': [2.0, 0.0]}, {'num_count': 344, 'sum_payoffs': 96.45177409686866, 'action': [2.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 72.76361817877772, 'action': [1.0, 0.0]}, {'num_count': 243, 'sum_payoffs': 58.1409295255424, 'action': [0.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 58.220889545519086, 'action': [0.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 57.151424278330985, 'action': [0.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 73.09345326118152, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.13071895424836602, 0.10995770857362552, 0.1303344867358708, 0.1322568242983468, 0.10841983852364476, 0.09342560553633218, 0.09342560553633218, 0.0922722029988466, 0.10880430603613994]
Actions to choose Agent 1: dict_values([{'num_count': 479, 'sum_payoffs': 156.97151421671725, 'action': [1.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 118.13093451304582, 'action': [0.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 159.4502748359943, 'action': [1.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 118.89055470282425, 'action': [0.0, -1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 156.951524211723, 'action': [1.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 118.11094450805162, 'action': [0.0, 0.0]}])
Weights num count: [0.184159938485198, 0.14801999231064975, 0.18646674356016918, 0.14878892733564014, 0.184159938485198, 0.14801999231064975]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.34130263328552246 s
