Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 73.48325835856784, 'action': [1.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 72.76361817877772, 'action': [1.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 57.051474253360134, 'action': [0.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 95.96201897451155, 'action': [2.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 73.44327834857948, 'action': [1.0, 1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 57.871064458121104, 'action': [0.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 57.091454263348474, 'action': [0.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 95.59220388211939, 'action': [2.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 95.96201897451158, 'action': [2.0, 0.0]}])
Weights num count: [0.10918877354863514, 0.10841983852364476, 0.0922722029988466, 0.1318723567858516, 0.10918877354863514, 0.09304113802383698, 0.0922722029988466, 0.1314878892733564, 0.1318723567858516]
Actions to choose Agent 1: dict_values([{'num_count': 482, 'sum_payoffs': 158.2208895288528, 'action': [1.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 117.31134430828483, 'action': [0.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 118.42078958546126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 161.08945524551618, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 115.67216389876289, 'action': [0.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 157.79110442147814, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18531334102268357, 0.14725105728565935, 0.14840445982314496, 0.18800461361014995, 0.14571318723567858, 0.18492887351018839]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3417470455169678 s
