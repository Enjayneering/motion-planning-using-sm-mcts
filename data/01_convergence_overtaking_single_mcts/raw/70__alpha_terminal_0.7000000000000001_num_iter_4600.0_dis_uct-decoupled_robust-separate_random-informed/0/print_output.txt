Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 539, 'sum_payoffs': 173.57489251475639, 'action': [2.0, 1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 195.02326508070712, 'action': [2.0, 0.0]}, {'num_count': 540, 'sum_payoffs': 174.03023185913253, 'action': [2.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 139.02730773516572, 'action': [0.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 150.67079906516025, 'action': [0.0, -1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 158.827815504647, 'action': [1.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 150.72506788053425, 'action': [1.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 156.0250086314428, 'action': [0.0, 0.0]}, {'num_count': 511, 'sum_payoffs': 162.096865337189, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11714844599000217, 0.12845033688328625, 0.11736579004564225, 0.09867420126059552, 0.10497717887415779, 0.10932405998695936, 0.10497717887415779, 0.10780265159747882, 0.11106281243207998]
Actions to choose Agent 1: dict_values([{'num_count': 822, 'sum_payoffs': 273.7806348413074, 'action': [1.0, -1.5707963267948966]}, {'num_count': 704, 'sum_payoffs': 226.30452181119887, 'action': [0.0, 0.0]}, {'num_count': 684, 'sum_payoffs': 218.4033970139844, 'action': [0.0, 1.5707963267948966]}, {'num_count': 683, 'sum_payoffs': 218.03084054024222, 'action': [0.0, -1.5707963267948966]}, {'num_count': 897, 'sum_payoffs': 304.2111658082618, 'action': [1.0, 0.0]}, {'num_count': 810, 'sum_payoffs': 268.82560827930854, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1786568137361443, 0.15301021517061508, 0.1486633340578135, 0.14844599000217343, 0.19495761790915017, 0.17604868506846338]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.50376272201538 s
