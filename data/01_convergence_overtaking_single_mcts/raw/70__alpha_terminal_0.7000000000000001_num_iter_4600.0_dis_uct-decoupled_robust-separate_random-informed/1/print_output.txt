Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 515, 'sum_payoffs': 164.4057672948697, 'action': [1.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 157.41582146085287, 'action': [1.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 186.6566311773144, 'action': [2.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 150.85620418326653, 'action': [0.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 156.92118601232352, 'action': [1.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 148.05260668231708, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 140.27262448535544, 'action': [0.0, 1.5707963267948966]}, {'num_count': 549, 'sum_payoffs': 178.37529449072915, 'action': [2.0, 1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 182.4774205788207, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1119321886546403, 0.10823733970875897, 0.12366876765920452, 0.10475983481851771, 0.10801999565311889, 0.10323842642903716, 0.09910888937187567, 0.11932188654640295, 0.12149532710280374]
Actions to choose Agent 1: dict_values([{'num_count': 818, 'sum_payoffs': 271.6008255516819, 'action': [1.0, -1.5707963267948966]}, {'num_count': 682, 'sum_payoffs': 217.07440837149338, 'action': [0.0, 1.5707963267948966]}, {'num_count': 723, 'sum_payoffs': 233.3612457235051, 'action': [0.0, 0.0]}, {'num_count': 678, 'sum_payoffs': 215.52273511631876, 'action': [0.0, -1.5707963267948966]}, {'num_count': 869, 'sum_payoffs': 292.22327564218307, 'action': [1.0, 0.0]}, {'num_count': 830, 'sum_payoffs': 276.2845748562373, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.177787437513584, 0.14822864594653337, 0.15713975222777657, 0.14735926972397306, 0.18887198435122798, 0.18039556618126495]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.18909931182861 s
