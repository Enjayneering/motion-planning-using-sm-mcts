Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 464, 'sum_payoffs': 143.46092632272612, 'action': [0.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 155.6898798773533, 'action': [1.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 150.80163492871853, 'action': [0.0, 0.0]}, {'num_count': 532, 'sum_payoffs': 171.25665921002437, 'action': [2.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 165.10284809724192, 'action': [1.0, 0.0]}, {'num_count': 545, 'sum_payoffs': 176.53950759338963, 'action': [2.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 144.657653780373, 'action': [0.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 169.14987163100201, 'action': [1.0, -1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 187.78398587498828, 'action': [2.0, 0.0]}])
Weights num count: [0.1008476418169963, 0.10736796348619865, 0.10475983481851771, 0.11562703760052162, 0.11236687676592046, 0.11845251032384264, 0.10149967398391654, 0.11454031732232123, 0.12432079982612476]
Actions to choose Agent 1: dict_values([{'num_count': 699, 'sum_payoffs': 223.56876335806163, 'action': [0.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 230.38692112340894, 'action': [0.0, 0.0]}, {'num_count': 808, 'sum_payoffs': 267.2869847299243, 'action': [1.0, -1.5707963267948966]}, {'num_count': 877, 'sum_payoffs': 295.1957483981965, 'action': [1.0, 0.0]}, {'num_count': 817, 'sum_payoffs': 270.9275053731251, 'action': [1.0, 1.5707963267948966]}, {'num_count': 683, 'sum_payoffs': 217.2617223688486, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1519234948924147, 0.15561834383829604, 0.17561399695718322, 0.19061073679634863, 0.17757009345794392, 0.14844599000217343]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.313703775405884 s
