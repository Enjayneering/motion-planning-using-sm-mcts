Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 467, 'sum_payoffs': 145.0874729293311, 'action': [0.0, 1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 176.614345682957, 'action': [2.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 153.61545956169786, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 143.82204390184606, 'action': [0.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 157.6402597275941, 'action': [1.0, 1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 165.77553713012523, 'action': [1.0, -1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 186.97869507792956, 'action': [2.0, 0.0]}, {'num_count': 522, 'sum_payoffs': 167.5454254030891, 'action': [1.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 170.79607319876305, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10149967398391654, 0.11823516626820256, 0.10606389915235818, 0.1008476418169963, 0.10823733970875897, 0.11258422082156053, 0.12366876765920452, 0.11345359704412085, 0.11519234948924147]
Actions to choose Agent 1: dict_values([{'num_count': 728, 'sum_payoffs': 235.50521769527094, 'action': [0.0, 0.0]}, {'num_count': 876, 'sum_payoffs': 295.22667789914937, 'action': [1.0, 0.0]}, {'num_count': 683, 'sum_payoffs': 217.66877569229013, 'action': [0.0, -1.5707963267948966]}, {'num_count': 829, 'sum_payoffs': 276.18425415328034, 'action': [1.0, 1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 210.4102606404054, 'action': [0.0, 1.5707963267948966]}, {'num_count': 819, 'sum_payoffs': 272.16753595413184, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15822647250597696, 0.19039339274070854, 0.14844599000217343, 0.18017822212562487, 0.14453379700065203, 0.17800478156922409]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.6586697101593 s
