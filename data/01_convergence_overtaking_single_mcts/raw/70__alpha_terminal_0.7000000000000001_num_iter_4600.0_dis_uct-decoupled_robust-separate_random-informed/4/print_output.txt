Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 468, 'sum_payoffs': 145.41283728461332, 'action': [0.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 159.36873514991865, 'action': [1.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 176.12717913250404, 'action': [1.0, 0.0]}, {'num_count': 545, 'sum_payoffs': 176.99514498213003, 'action': [2.0, 1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 187.84821158350405, 'action': [2.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 153.67553748863492, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 137.7910782236416, 'action': [0.0, 1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 176.62744949638937, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 154.49062067901724, 'action': [0.0, 0.0]}])
Weights num count: [0.10171701803955661, 0.10910671593131928, 0.11801782221256249, 0.11845251032384264, 0.12410345577048468, 0.10606389915235818, 0.09758748098239513, 0.11823516626820256, 0.10649858726363834]
Actions to choose Agent 1: dict_values([{'num_count': 746, 'sum_payoffs': 240.79743966025057, 'action': [0.0, 0.0]}, {'num_count': 856, 'sum_payoffs': 284.907513322164, 'action': [1.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 205.99622715159592, 'action': [0.0, -1.5707963267948966]}, {'num_count': 685, 'sum_payoffs': 216.56310512753993, 'action': [0.0, 1.5707963267948966]}, {'num_count': 823, 'sum_payoffs': 271.609111789972, 'action': [1.0, 1.5707963267948966]}, {'num_count': 832, 'sum_payoffs': 275.12046996791867, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16213866550749836, 0.18604651162790697, 0.1430123886111715, 0.1488806781134536, 0.1788741577917844, 0.1808302542925451]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.11394023895264 s
