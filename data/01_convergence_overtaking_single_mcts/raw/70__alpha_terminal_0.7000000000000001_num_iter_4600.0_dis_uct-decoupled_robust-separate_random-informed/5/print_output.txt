Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 520, 'sum_payoffs': 166.18514198375894, 'action': [1.0, -1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 195.49013224926426, 'action': [2.0, 0.0]}, {'num_count': 559, 'sum_payoffs': 182.22153766933653, 'action': [2.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 164.93305983943537, 'action': [1.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 145.41390944271785, 'action': [0.0, -1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 154.7105117943265, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 139.61448808468734, 'action': [0.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 160.45146155277362, 'action': [2.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 154.27243730486794, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11301890893284068, 0.12845033688328625, 0.12149532710280374, 0.11236687676592046, 0.1019343620951967, 0.1069332753749185, 0.0988915453162356, 0.10997609215387959, 0.10671593131927842]
Actions to choose Agent 1: dict_values([{'num_count': 834, 'sum_payoffs': 279.0991244295799, 'action': [1.0, -1.5707963267948966]}, {'num_count': 683, 'sum_payoffs': 218.2536747728175, 'action': [0.0, -1.5707963267948966]}, {'num_count': 796, 'sum_payoffs': 263.6455236558274, 'action': [1.0, 1.5707963267948966]}, {'num_count': 682, 'sum_payoffs': 217.89752275634766, 'action': [0.0, 1.5707963267948966]}, {'num_count': 884, 'sum_payoffs': 299.44535612318623, 'action': [1.0, 0.0]}, {'num_count': 721, 'sum_payoffs': 233.3991216060622, 'action': [0.0, 0.0]}])
Weights num count: [0.18126494240382526, 0.14844599000217343, 0.1730058682895023, 0.14822864594653337, 0.19213214518582916, 0.1567050641164964]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.926681995391846 s
