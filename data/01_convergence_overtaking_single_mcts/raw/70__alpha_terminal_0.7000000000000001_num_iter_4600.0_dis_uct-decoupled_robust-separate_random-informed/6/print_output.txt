Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 524, 'sum_payoffs': 168.0140061836295, 'action': [2.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 161.86881665930676, 'action': [1.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 147.60763887199334, 'action': [0.0, -1.5707963267948966]}, {'num_count': 559, 'sum_payoffs': 182.35075806806742, 'action': [2.0, -1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 159.0070263969135, 'action': [1.0, 1.5707963267948966]}, {'num_count': 470, 'sum_payoffs': 145.91072343268002, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 143.5412563511062, 'action': [0.0, 1.5707963267948966]}, {'num_count': 596, 'sum_payoffs': 197.73490581328934, 'action': [2.0, 0.0]}, {'num_count': 502, 'sum_payoffs': 158.99041917264628, 'action': [1.0, 0.0]}])
Weights num count: [0.113888285155401, 0.11062812432079983, 0.10302108237339709, 0.12149532710280374, 0.10910671593131928, 0.10215170615083677, 0.1008476418169963, 0.12953705716148664, 0.10910671593131928]
Actions to choose Agent 1: dict_values([{'num_count': 715, 'sum_payoffs': 230.25368108532626, 'action': [0.0, 0.0]}, {'num_count': 695, 'sum_payoffs': 222.3356653864661, 'action': [0.0, 1.5707963267948966]}, {'num_count': 884, 'sum_payoffs': 298.42067462204153, 'action': [1.0, 0.0]}, {'num_count': 681, 'sum_payoffs': 216.7441788020976, 'action': [0.0, -1.5707963267948966]}, {'num_count': 815, 'sum_payoffs': 270.4903054893093, 'action': [1.0, 1.5707963267948966]}, {'num_count': 810, 'sum_payoffs': 268.46449382915597, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15540099978265595, 0.15105411866985438, 0.19213214518582916, 0.1480113018908933, 0.17713540534666378, 0.17604868506846338]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 37.46591353416443 s
