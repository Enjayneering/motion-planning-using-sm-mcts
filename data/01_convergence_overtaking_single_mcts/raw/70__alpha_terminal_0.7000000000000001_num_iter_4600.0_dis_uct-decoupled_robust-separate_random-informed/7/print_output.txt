Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 547, 'sum_payoffs': 177.34617148523537, 'action': [2.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 166.98034841933108, 'action': [2.0, 1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 192.4657022835199, 'action': [2.0, 0.0]}, {'num_count': 520, 'sum_payoffs': 166.26767794479932, 'action': [1.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 156.46112356278903, 'action': [1.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 139.72873281721732, 'action': [0.0, 1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 154.3940811496778, 'action': [0.0, -1.5707963267948966]}, {'num_count': 504, 'sum_payoffs': 159.7083143819875, 'action': [1.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 150.25456836337528, 'action': [0.0, 0.0]}])
Weights num count: [0.1188871984351228, 0.11345359704412085, 0.1269289284938057, 0.11301890893284068, 0.10780265159747882, 0.0988915453162356, 0.10671593131927842, 0.10954140404259943, 0.10454249076287764]
Actions to choose Agent 1: dict_values([{'num_count': 854, 'sum_payoffs': 286.27086199780456, 'action': [1.0, -1.5707963267948966]}, {'num_count': 671, 'sum_payoffs': 212.8343747477507, 'action': [0.0, -1.5707963267948966]}, {'num_count': 819, 'sum_payoffs': 272.01156582216316, 'action': [1.0, 1.5707963267948966]}, {'num_count': 682, 'sum_payoffs': 217.2310034808691, 'action': [0.0, 0.0]}, {'num_count': 871, 'sum_payoffs': 293.17105827616984, 'action': [1.0, 0.0]}, {'num_count': 703, 'sum_payoffs': 225.57898901348358, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18561182351662683, 0.1458378613344925, 0.17800478156922409, 0.14822864594653337, 0.18930667246250815, 0.152792871114975]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.93060040473938 s
