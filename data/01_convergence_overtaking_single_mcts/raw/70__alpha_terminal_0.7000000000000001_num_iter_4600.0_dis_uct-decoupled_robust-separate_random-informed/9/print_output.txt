Searching game tree in timestep 0...
Max timehorizon: 5
Actions to choose Agent 0: dict_values([{'num_count': 565, 'sum_payoffs': 184.74602642514483, 'action': [2.0, -1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 160.5487350863872, 'action': [1.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 184.38011336811022, 'action': [2.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 170.21634878152184, 'action': [2.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 147.9337799317729, 'action': [0.0, 0.0]}, {'num_count': 519, 'sum_payoffs': 165.75203136105674, 'action': [1.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 152.01218471681682, 'action': [0.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 158.0509425081842, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 140.1972224286857, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.12279939143664421, 0.10997609215387959, 0.12258204738100413, 0.11519234948924147, 0.10323842642903716, 0.11280156487720061, 0.10541186698543795, 0.10867202782003912, 0.09910888937187567]
Actions to choose Agent 1: dict_values([{'num_count': 880, 'sum_payoffs': 296.55645631056746, 'action': [1.0, 0.0]}, {'num_count': 797, 'sum_payoffs': 262.9823044577113, 'action': [1.0, -1.5707963267948966]}, {'num_count': 808, 'sum_payoffs': 267.3624277902331, 'action': [1.0, 1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 220.0919296739351, 'action': [0.0, -1.5707963267948966]}, {'num_count': 711, 'sum_payoffs': 228.50833817933585, 'action': [0.0, 0.0]}, {'num_count': 714, 'sum_payoffs': 229.69160775418595, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.19126276896326885, 0.17322321234514235, 0.17561399695718322, 0.149967398391654, 0.15453162356009564, 0.15518365572701587]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.78841280937195 s
