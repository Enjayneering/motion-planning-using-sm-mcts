Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 123, 'sum_payoffs': 38.180862483046454, 'action': [1.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 36.13406209621401, 'action': [2.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 37.17669247622287, 'action': [0.0, 0.0]}, {'num_count': 126, 'sum_payoffs': 39.46622959789722, 'action': [2.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 36.661150222138374, 'action': [0.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 38.52703171221475, 'action': [1.0, 0.0]}, {'num_count': 126, 'sum_payoffs': 39.51202711904373, 'action': [2.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 38.4889712812041, 'action': [1.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 35.292393675403225, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11171662125340599, 0.1080835603996367, 0.10990009082652134, 0.11444141689373297, 0.10899182561307902, 0.11262488646684832, 0.11444141689373297, 0.11262488646684832, 0.10626702997275204]
Actions to choose Agent 1: dict_values([{'num_count': 202, 'sum_payoffs': 65.14589256752367, 'action': [1.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 53.83837864541294, 'action': [0.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 51.64229130194422, 'action': [0.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 49.899174350068975, 'action': [0.0, -1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 60.11938791155278, 'action': [1.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 59.69271774260115, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18346957311534967, 0.16076294277929154, 0.15622161671207993, 0.15258855585831063, 0.1734786557674841, 0.17257039055404177]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.531346797943115 s
