Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 131, 'sum_payoffs': 42.427219119480455, 'action': [2.0, -1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 35.17471961856521, 'action': [0.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 39.331122471694016, 'action': [1.0, -1.5707963267948966]}, {'num_count': 134, 'sum_payoffs': 43.97728408567902, 'action': [2.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 36.21847315134376, 'action': [1.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 36.14075281449554, 'action': [0.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 37.4795130838443, 'action': [1.0, 1.5707963267948966]}, {'num_count': 111, 'sum_payoffs': 32.67705621184981, 'action': [0.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 40.034881657983476, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11898274296094459, 0.10535876475930972, 0.11353315168029064, 0.12170753860127158, 0.10717529518619437, 0.10717529518619437, 0.10990009082652134, 0.1008174386920981, 0.11444141689373297]
Actions to choose Agent 1: dict_values([{'num_count': 170, 'sum_payoffs': 50.61472821944651, 'action': [0.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 54.68869297493045, 'action': [0.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 51.005637990874725, 'action': [0.0, -1.5707963267948966]}, {'num_count': 198, 'sum_payoffs': 63.14641032834401, 'action': [1.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 59.168441258951304, 'action': [1.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 60.99479168844827, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15440508628519528, 0.16257947320617622, 0.1553133514986376, 0.17983651226158037, 0.17166212534059946, 0.17529518619436876]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.846592426300049 s
