Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 122, 'sum_payoffs': 37.775783922792996, 'action': [1.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 40.55568482285252, 'action': [2.0, 0.0]}, {'num_count': 122, 'sum_payoffs': 37.79574008630612, 'action': [2.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 36.23898340800427, 'action': [0.0, -1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 40.522825641978315, 'action': [2.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 39.22991897134353, 'action': [1.0, -1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 37.769088456196975, 'action': [0.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 35.857530295240416, 'action': [1.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 34.8499654851859, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11080835603996367, 0.11625794732061762, 0.11080835603996367, 0.1080835603996367, 0.11625794732061762, 0.11353315168029064, 0.11080835603996367, 0.10717529518619437, 0.10535876475930972]
Actions to choose Agent 1: dict_values([{'num_count': 190, 'sum_payoffs': 59.74151641353208, 'action': [1.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 60.51827878986126, 'action': [1.0, 0.0]}, {'num_count': 201, 'sum_payoffs': 64.63038147277972, 'action': [1.0, -1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 49.39318122735724, 'action': [0.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 53.00827362570887, 'action': [0.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 52.873071649022286, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17257039055404177, 0.17438692098092642, 0.18256130790190736, 0.1516802906448683, 0.1589464123524069, 0.1589464123524069]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.014899253845215 s
