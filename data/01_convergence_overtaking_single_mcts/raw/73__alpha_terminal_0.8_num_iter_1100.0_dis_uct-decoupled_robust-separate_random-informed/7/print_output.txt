Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 123, 'sum_payoffs': 38.41159421956997, 'action': [1.0, 1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 35.460676849890035, 'action': [1.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 39.32848125530017, 'action': [2.0, -1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 38.8269635711686, 'action': [2.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 36.45746555155588, 'action': [0.0, 1.5707963267948966]}, {'num_count': 122, 'sum_payoffs': 37.85480750724472, 'action': [1.0, 0.0]}, {'num_count': 131, 'sum_payoffs': 42.19091248662504, 'action': [2.0, 0.0]}, {'num_count': 120, 'sum_payoffs': 36.88133980223683, 'action': [0.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 36.33738238590724, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11171662125340599, 0.10626702997275204, 0.11353315168029064, 0.11262488646684832, 0.1080835603996367, 0.11080835603996367, 0.11898274296094459, 0.10899182561307902, 0.1080835603996367]
Actions to choose Agent 1: dict_values([{'num_count': 199, 'sum_payoffs': 63.53700860384911, 'action': [1.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 50.43818880481993, 'action': [0.0, -1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 60.29608577080221, 'action': [1.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 52.67370554764163, 'action': [0.0, 1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 61.16281013934535, 'action': [1.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 50.51957931536334, 'action': [0.0, 0.0]}])
Weights num count: [0.1807447774750227, 0.15440508628519528, 0.17438692098092642, 0.1589464123524069, 0.17620345140781107, 0.15440508628519528]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.311814785003662 s
