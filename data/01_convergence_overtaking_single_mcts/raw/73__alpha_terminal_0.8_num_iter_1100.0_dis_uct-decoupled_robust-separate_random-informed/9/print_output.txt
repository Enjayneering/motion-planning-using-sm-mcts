Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 127, 'sum_payoffs': 40.13432697748334, 'action': [1.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 36.22747912333961, 'action': [0.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 40.04669895070745, 'action': [2.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 36.77511693338139, 'action': [1.0, 0.0]}, {'num_count': 114, 'sum_payoffs': 33.84192102511814, 'action': [0.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 40.539574102606274, 'action': [2.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 37.241552300912375, 'action': [1.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 40.12754254270799, 'action': [2.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 35.35669171486728, 'action': [0.0, 0.0]}])
Weights num count: [0.11534968210717529, 0.1080835603996367, 0.11534968210717529, 0.10899182561307902, 0.10354223433242507, 0.11625794732061762, 0.10990009082652134, 0.11534968210717529, 0.10626702997275204]
Actions to choose Agent 1: dict_values([{'num_count': 198, 'sum_payoffs': 63.32183044084053, 'action': [1.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 51.12307996193864, 'action': [0.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 51.59355930390277, 'action': [0.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 53.314813985888634, 'action': [0.0, 1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 61.049041595540615, 'action': [1.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 59.64381922600911, 'action': [1.0, 0.0]}])
Weights num count: [0.17983651226158037, 0.1553133514986376, 0.15622161671207993, 0.15985467756584923, 0.17529518619436876, 0.17257039055404177]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.229362964630127 s
