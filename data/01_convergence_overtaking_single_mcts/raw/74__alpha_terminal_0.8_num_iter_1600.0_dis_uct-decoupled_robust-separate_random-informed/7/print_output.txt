Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 181, 'sum_payoffs': 56.2157751740334, 'action': [1.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 54.4418975323241, 'action': [1.0, 0.0]}, {'num_count': 158, 'sum_payoffs': 45.99880635825281, 'action': [0.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 50.9288437285785, 'action': [0.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 57.19248114320121, 'action': [2.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 59.9634309810865, 'action': [2.0, 0.0]}, {'num_count': 190, 'sum_payoffs': 60.44298531268056, 'action': [2.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 55.730274670674255, 'action': [1.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 52.67575265377637, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11305434103685197, 0.11055590256089944, 0.09868831980012492, 0.10555902560899438, 0.11430356027482823, 0.11805121798875702, 0.11867582760774516, 0.11242973141786383, 0.1080574640849469]
Actions to choose Agent 1: dict_values([{'num_count': 256, 'sum_payoffs': 78.32229042596079, 'action': [0.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 74.4775530163199, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 90.58225924853713, 'action': [1.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 88.99675102269511, 'action': [1.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 74.4585509463267, 'action': [0.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 90.35381144366622, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1599000624609619, 0.15427857589006871, 0.17801374141161774, 0.1755153029356652, 0.15427857589006871, 0.1773891317926296]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.410129070281982 s
