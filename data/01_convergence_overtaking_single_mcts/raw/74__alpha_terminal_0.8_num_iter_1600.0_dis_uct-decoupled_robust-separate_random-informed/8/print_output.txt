Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 167, 'sum_payoffs': 50.01350565847264, 'action': [0.0, 0.0]}, {'num_count': 183, 'sum_payoffs': 57.40059342721167, 'action': [1.0, 1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 54.668915516265926, 'action': [1.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 50.58334496691018, 'action': [0.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 53.79561195641126, 'action': [2.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 56.908118445113985, 'action': [1.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 51.8668058480527, 'action': [0.0, 1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 60.966600856975774, 'action': [2.0, 0.0]}, {'num_count': 186, 'sum_payoffs': 58.777634099011905, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10430980637101811, 0.11430356027482823, 0.11055590256089944, 0.10493441599000625, 0.10930668332292318, 0.1136789506558401, 0.10680824484697064, 0.1193004372267333, 0.11617738913179262]
Actions to choose Agent 1: dict_values([{'num_count': 288, 'sum_payoffs': 91.35816606714317, 'action': [1.0, -1.5707963267948966]}, {'num_count': 278, 'sum_payoffs': 87.10373065668611, 'action': [1.0, 1.5707963267948966]}, {'num_count': 253, 'sum_payoffs': 76.3332973695632, 'action': [0.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 76.8327572737415, 'action': [0.0, 0.0]}, {'num_count': 237, 'sum_payoffs': 69.69841477676663, 'action': [0.0, -1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 92.23789492315512, 'action': [1.0, 0.0]}])
Weights num count: [0.17988757026858213, 0.1736414740787008, 0.1580262336039975, 0.15865084322298564, 0.14803247970018737, 0.1811367895065584]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.715393781661987 s
