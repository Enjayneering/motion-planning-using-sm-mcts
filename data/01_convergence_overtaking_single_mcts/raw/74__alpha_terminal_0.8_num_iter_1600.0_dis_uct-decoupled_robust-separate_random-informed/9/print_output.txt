Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 51.746703073444976, 'action': [0.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 50.76785011312338, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 54.427685889060065, 'action': [1.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 54.81097138976988, 'action': [1.0, 0.0]}, {'num_count': 188, 'sum_payoffs': 59.87686625177361, 'action': [2.0, 1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 61.18351191251805, 'action': [2.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 51.74237704849799, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 57.12623766738393, 'action': [2.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 55.215608345812065, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1061836352279825, 0.10493441599000625, 0.1099312929419113, 0.11055590256089944, 0.1174266083697689, 0.1193004372267333, 0.1061836352279825, 0.1136789506558401, 0.11118051217988757]
Actions to choose Agent 1: dict_values([{'num_count': 243, 'sum_payoffs': 72.54089911368287, 'action': [0.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 92.6566481312657, 'action': [1.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 87.07944551461624, 'action': [1.0, -1.5707963267948966]}, {'num_count': 265, 'sum_payoffs': 81.83803520478492, 'action': [0.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 87.93762747430429, 'action': [1.0, 1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 73.88911854176612, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15178013741411617, 0.1811367895065584, 0.17301686445971268, 0.1655215490318551, 0.17426608369768895, 0.15365396627108058]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.77051568031311 s
