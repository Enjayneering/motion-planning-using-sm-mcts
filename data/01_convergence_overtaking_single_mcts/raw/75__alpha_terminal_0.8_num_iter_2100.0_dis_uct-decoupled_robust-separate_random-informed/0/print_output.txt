Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 239, 'sum_payoffs': 74.69192729361767, 'action': [2.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 75.02207862629565, 'action': [2.0, 0.0]}, {'num_count': 224, 'sum_payoffs': 68.07430714833524, 'action': [0.0, 1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 75.90255783373952, 'action': [2.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 66.30055730882682, 'action': [0.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 75.45651768204667, 'action': [1.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 74.12885369817901, 'action': [1.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 76.35882031456434, 'action': [1.0, -1.5707963267948966]}, {'num_count': 213, 'sum_payoffs': 63.30588440887634, 'action': [0.0, 0.0]}])
Weights num count: [0.11375535459305093, 0.1142313184198001, 0.10661589719181343, 0.11518324607329843, 0.10471204188481675, 0.11470728224654926, 0.11327939076630177, 0.11565920990004759, 0.10138029509757258]
Actions to choose Agent 1: dict_values([{'num_count': 320, 'sum_payoffs': 95.77888127376151, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 116.44981360925567, 'action': [1.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 120.16377729525574, 'action': [1.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 119.86091525551834, 'action': [1.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 95.76476896066912, 'action': [0.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 101.10833086212489, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15230842455973345, 0.17610661589719181, 0.18039029033793433, 0.17991432651118516, 0.15230842455973345, 0.15849595430747263]
Selected final action: [1.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 23.71275019645691 s
