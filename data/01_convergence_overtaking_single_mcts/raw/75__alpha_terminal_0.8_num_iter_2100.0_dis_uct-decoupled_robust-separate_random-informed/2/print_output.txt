Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 214, 'sum_payoffs': 64.00504986979126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 73.16524467166788, 'action': [1.0, 0.0]}, {'num_count': 251, 'sum_payoffs': 79.98950432164371, 'action': [2.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 73.13539808721076, 'action': [1.0, 1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 73.95345296397312, 'action': [1.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 74.87049070653875, 'action': [2.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 79.59950467060531, 'action': [2.0, -1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 66.64478230364608, 'action': [0.0, -1.5707963267948966]}, {'num_count': 219, 'sum_payoffs': 66.04871756252997, 'action': [0.0, 0.0]}])
Weights num count: [0.10185625892432175, 0.11185149928605426, 0.11946692051404094, 0.11185149928605426, 0.1128034269395526, 0.11375535459305093, 0.11899095668729176, 0.10471204188481675, 0.10423607805806759]
Actions to choose Agent 1: dict_values([{'num_count': 372, 'sum_payoffs': 117.42831419043388, 'action': [1.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 121.12532804854548, 'action': [1.0, -1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 97.98116985180893, 'action': [0.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 123.2092458579671, 'action': [1.0, 0.0]}, {'num_count': 324, 'sum_payoffs': 97.53332516231767, 'action': [0.0, -1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 92.51866946501657, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17705854355069015, 0.18134221799143266, 0.1546882436934793, 0.18372203712517848, 0.15421227986673014, 0.14850071394574013]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.877290964126587 s
