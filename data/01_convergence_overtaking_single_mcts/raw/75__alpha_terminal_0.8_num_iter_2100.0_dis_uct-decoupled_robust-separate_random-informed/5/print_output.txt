Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 224, 'sum_payoffs': 67.88082925884713, 'action': [0.0, -1.5707963267948966]}, {'num_count': 213, 'sum_payoffs': 63.188907907098006, 'action': [0.0, 1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 71.3159615763651, 'action': [1.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 73.1414364339002, 'action': [2.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 74.43433998481528, 'action': [2.0, -1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 73.50996923763202, 'action': [1.0, 1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 75.79561877645936, 'action': [1.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 66.60142907971269, 'action': [0.0, 0.0]}, {'num_count': 256, 'sum_payoffs': 81.97802695286413, 'action': [2.0, 0.0]}])
Weights num count: [0.10661589719181343, 0.10138029509757258, 0.11042360780580676, 0.11232746311280342, 0.11375535459305093, 0.1128034269395526, 0.11518324607329843, 0.10518800571156592, 0.12184673964778676]
Actions to choose Agent 1: dict_values([{'num_count': 315, 'sum_payoffs': 93.42239231254118, 'action': [0.0, 1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 110.66010216175891, 'action': [1.0, 1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 94.22073759239571, 'action': [0.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 126.3724777276704, 'action': [1.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 116.42131320759881, 'action': [1.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 105.50357252595778, 'action': [0.0, 0.0]}])
Weights num count: [0.14992860542598763, 0.16991908614945264, 0.15088053307948596, 0.188005711565921, 0.17658257972394098, 0.16420752022846263]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.996978282928467 s
