Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 231, 'sum_payoffs': 71.32444551840453, 'action': [2.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 68.15989294024824, 'action': [0.0, -1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 71.2590050505957, 'action': [1.0, 0.0]}, {'num_count': 254, 'sum_payoffs': 81.40913591098592, 'action': [2.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 73.5021198894836, 'action': [1.0, -1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 69.16270265287302, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 73.09757152627539, 'action': [2.0, -1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 66.95180694533339, 'action': [0.0, 1.5707963267948966]}, {'num_count': 242, 'sum_payoffs': 76.04787583691183, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1099476439790576, 0.10661589719181343, 0.1099476439790576, 0.12089481199428843, 0.11232746311280342, 0.10756782484531176, 0.11185149928605426, 0.10518800571156592, 0.11518324607329843]
Actions to choose Agent 1: dict_values([{'num_count': 368, 'sum_payoffs': 115.60909718829669, 'action': [1.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 101.64543289856397, 'action': [0.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 102.47165582046881, 'action': [0.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 97.55415748266245, 'action': [0.0, 1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 119.3388234881944, 'action': [1.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 112.74730583319868, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17515468824369348, 0.1589719181342218, 0.15992384578772012, 0.15421227986673014, 0.17943836268443597, 0.1718229414564493]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.822242498397827 s
