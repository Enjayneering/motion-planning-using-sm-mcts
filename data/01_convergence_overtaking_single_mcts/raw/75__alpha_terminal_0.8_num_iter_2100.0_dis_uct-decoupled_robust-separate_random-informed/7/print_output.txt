Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 222, 'sum_payoffs': 67.34466246663648, 'action': [1.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 79.09107612575691, 'action': [2.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 81.34055681597202, 'action': [2.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 72.62428300090636, 'action': [2.0, -1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 65.6634863779246, 'action': [0.0, 1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 66.75742304929253, 'action': [0.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 67.77486952566441, 'action': [0.0, -1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 77.78824281931252, 'action': [1.0, -1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 72.12400214056665, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10566396953831508, 0.1185149928605426, 0.12089481199428843, 0.11137553545930509, 0.10376011423131842, 0.10518800571156592, 0.10613993336506425, 0.1170871013802951, 0.11089957163255593]
Actions to choose Agent 1: dict_values([{'num_count': 325, 'sum_payoffs': 97.96947195548297, 'action': [0.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 123.59727386418848, 'action': [1.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 118.56768428269739, 'action': [1.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 94.93241713747686, 'action': [0.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 121.48936684190551, 'action': [1.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 92.94823404616575, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1546882436934793, 0.18419800095192765, 0.17848643503093764, 0.15135649690623512, 0.18181818181818182, 0.1489766777724893]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.907765865325928 s
