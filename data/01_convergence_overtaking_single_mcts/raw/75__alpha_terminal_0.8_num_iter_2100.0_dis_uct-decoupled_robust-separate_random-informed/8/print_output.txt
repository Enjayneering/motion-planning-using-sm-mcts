Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 225, 'sum_payoffs': 68.97574507307658, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 75.07745616810243, 'action': [2.0, 1.5707963267948966]}, {'num_count': 255, 'sum_payoffs': 82.18222978226575, 'action': [2.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 73.32049164395913, 'action': [1.0, -1.5707963267948966]}, {'num_count': 232, 'sum_payoffs': 71.89782765022062, 'action': [0.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 78.16701677061538, 'action': [2.0, -1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 65.04375882157107, 'action': [0.0, 1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 67.18423092599473, 'action': [0.0, -1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 71.52005953122979, 'action': [1.0, 0.0]}])
Weights num count: [0.10709186101856259, 0.11375535459305093, 0.1213707758210376, 0.11185149928605426, 0.11042360780580676, 0.1170871013802951, 0.10280818657782008, 0.10518800571156592, 0.1099476439790576]
Actions to choose Agent 1: dict_values([{'num_count': 376, 'sum_payoffs': 119.34602552418272, 'action': [1.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 115.034690833659, 'action': [1.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 101.34430797147114, 'action': [0.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 99.39612127321259, 'action': [0.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 120.00960311966455, 'action': [1.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 95.64888454484304, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1789623988576868, 0.17420276059019515, 0.15849595430747263, 0.1561161351737268, 0.17991432651118516, 0.1518324607329843]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.746936321258545 s
