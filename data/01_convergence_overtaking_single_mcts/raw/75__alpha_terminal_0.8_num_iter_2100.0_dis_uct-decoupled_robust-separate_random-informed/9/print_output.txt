Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 234, 'sum_payoffs': 72.38218507506659, 'action': [1.0, 0.0]}, {'num_count': 236, 'sum_payoffs': 73.3071882866334, 'action': [2.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 69.27358404735529, 'action': [1.0, 1.5707963267948966]}, {'num_count': 238, 'sum_payoffs': 74.15966817271804, 'action': [2.0, 1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 64.09888478721007, 'action': [0.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 82.10559010708428, 'action': [2.0, 0.0]}, {'num_count': 232, 'sum_payoffs': 71.57253533930118, 'action': [0.0, 1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 71.97974393657867, 'action': [1.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 70.23317842820924, 'action': [0.0, 0.0]}])
Weights num count: [0.11137553545930509, 0.11232746311280342, 0.10804378867206092, 0.11327939076630177, 0.10233222275107091, 0.12184673964778676, 0.11042360780580676, 0.11089957163255593, 0.10899571632555925]
Actions to choose Agent 1: dict_values([{'num_count': 327, 'sum_payoffs': 98.99004106815502, 'action': [0.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 118.4395776275837, 'action': [1.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 118.34973974732914, 'action': [1.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 98.56935355252861, 'action': [0.0, -1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 123.75520433775894, 'action': [1.0, -1.5707963267948966]}, {'num_count': 312, 'sum_payoffs': 92.82531828866159, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15564017134697763, 0.17801047120418848, 0.17801047120418848, 0.15516420752022847, 0.18419800095192765, 0.14850071394574013]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.727720975875854 s
