Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 282, 'sum_payoffs': 86.43871515701228, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 85.1977152455218, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 82.14316423007487, 'action': [0.0, 0.0]}, {'num_count': 302, 'sum_payoffs': 94.97681587913246, 'action': [2.0, 0.0]}, {'num_count': 305, 'sum_payoffs': 96.21471301157496, 'action': [2.0, -1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 94.05585884818932, 'action': [2.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 96.30093888759349, 'action': [1.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 85.64493385095946, 'action': [1.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 83.52846040433977, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10841983852364476, 0.10726643598615918, 0.10457516339869281, 0.11610918877354863, 0.11726259131103421, 0.11534025374855825, 0.11726259131103421, 0.10765090349865436, 0.1057285659361784]
Actions to choose Agent 1: dict_values([{'num_count': 478, 'sum_payoffs': 151.46897241355697, 'action': [1.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 149.39686482892432, 'action': [1.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 116.44854693469755, 'action': [0.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 123.63673794453041, 'action': [0.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 121.95692258630984, 'action': [0.0, 1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 137.63939295944976, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1837754709727028, 0.18185313341022682, 0.1503267973856209, 0.1572472126105344, 0.15570934256055363, 0.1707035755478662]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.94688081741333 s
