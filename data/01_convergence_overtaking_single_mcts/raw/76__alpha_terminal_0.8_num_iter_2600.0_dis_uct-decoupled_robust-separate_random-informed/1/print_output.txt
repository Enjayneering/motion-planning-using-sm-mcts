Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 322, 'sum_payoffs': 103.07396389951916, 'action': [2.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 88.46517257464905, 'action': [1.0, 1.5707963267948966]}, {'num_count': 265, 'sum_payoffs': 78.86471818048156, 'action': [0.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 83.5040543781892, 'action': [0.0, 0.0]}, {'num_count': 307, 'sum_payoffs': 96.64118710641264, 'action': [2.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 95.840023702529, 'action': [2.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 82.23833985281267, 'action': [1.0, 0.0]}, {'num_count': 266, 'sum_payoffs': 79.27978124062064, 'action': [0.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 92.87909167524188, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.12379853902345252, 0.11072664359861592, 0.10188389081122645, 0.1061130334486736, 0.11803152633602461, 0.11726259131103421, 0.104959630911188, 0.10226835832372165, 0.11457131872356786]
Actions to choose Agent 1: dict_values([{'num_count': 398, 'sum_payoffs': 119.70701694870262, 'action': [0.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 125.33204565077467, 'action': [0.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 146.65484476637837, 'action': [1.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 154.08995564989635, 'action': [1.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 113.25738412050032, 'action': [0.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 144.72183784048232, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15301806997308728, 0.15840061514801998, 0.1787773933102653, 0.18569780853517878, 0.14686658977316416, 0.1768550557477893]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.01062512397766 s
