Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 297, 'sum_payoffs': 92.48284509680964, 'action': [1.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 91.11804853752004, 'action': [2.0, -1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 93.82548982075713, 'action': [2.0, 1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 80.70762690847492, 'action': [0.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 88.31378932802174, 'action': [1.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 86.13994823534433, 'action': [0.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 97.28237289494177, 'action': [2.0, 0.0]}, {'num_count': 263, 'sum_payoffs': 78.13839807972074, 'action': [0.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 93.78371874467047, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11418685121107267, 0.11303344867358708, 0.11534025374855825, 0.10342176086120723, 0.11034217608612072, 0.10841983852364476, 0.1184159938485198, 0.10111495578623607, 0.11534025374855825]
Actions to choose Agent 1: dict_values([{'num_count': 396, 'sum_payoffs': 118.67488793044728, 'action': [0.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 155.88032148459098, 'action': [1.0, 0.0]}, {'num_count': 405, 'sum_payoffs': 122.20575453431988, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 145.29928457296097, 'action': [1.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 138.7321574612677, 'action': [1.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 121.34179616321026, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1522491349480969, 0.18762014609765476, 0.15570934256055363, 0.1776239907727797, 0.1714725105728566, 0.15494040753556323]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.171481609344482 s
