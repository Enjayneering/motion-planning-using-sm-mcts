Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 273, 'sum_payoffs': 82.68937143984361, 'action': [1.0, 0.0]}, {'num_count': 302, 'sum_payoffs': 95.15994143083307, 'action': [1.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 81.4961636083242, 'action': [0.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 84.45799857859267, 'action': [0.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 88.28916336509974, 'action': [0.0, 0.0]}, {'num_count': 311, 'sum_payoffs': 98.95450219490627, 'action': [2.0, 0.0]}, {'num_count': 295, 'sum_payoffs': 92.14511315746512, 'action': [2.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 97.16772126040894, 'action': [2.0, -1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 85.29088324704438, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.104959630911188, 0.11610918877354863, 0.10380622837370242, 0.10649750096116878, 0.10995770857362552, 0.11956939638600539, 0.11341791618608228, 0.11803152633602461, 0.10726643598615918]
Actions to choose Agent 1: dict_values([{'num_count': 476, 'sum_payoffs': 150.96845333748072, 'action': [1.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 123.06514945894172, 'action': [0.0, 0.0]}, {'num_count': 392, 'sum_payoffs': 117.08912485062842, 'action': [0.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 138.24983640607482, 'action': [1.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 121.3965483612385, 'action': [0.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 151.37833864313765, 'action': [1.0, 0.0]}])
Weights num count: [0.1830065359477124, 0.15647827758554403, 0.15071126489811612, 0.1710880430603614, 0.15494040753556323, 0.18339100346020762]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.911258220672607 s
