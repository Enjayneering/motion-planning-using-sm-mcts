Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 286, 'sum_payoffs': 88.12622557560891, 'action': [1.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 85.53117733830999, 'action': [1.0, 1.5707963267948966]}, {'num_count': 309, 'sum_payoffs': 97.92804564794207, 'action': [2.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 94.55534489058564, 'action': [2.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 85.22799088367663, 'action': [0.0, 0.0]}, {'num_count': 304, 'sum_payoffs': 95.91341700609156, 'action': [2.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 82.21083890527068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 91.95325295758883, 'action': [1.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 83.06081740763776, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10995770857362552, 0.10765090349865436, 0.11880046136101499, 0.11572472126105345, 0.10726643598615918, 0.11687812379853903, 0.10457516339869281, 0.11341791618608228, 0.1053440984236832]
Actions to choose Agent 1: dict_values([{'num_count': 447, 'sum_payoffs': 139.33446905087158, 'action': [1.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 117.24563800144264, 'action': [0.0, -1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 126.32707122592932, 'action': [0.0, 0.0]}, {'num_count': 400, 'sum_payoffs': 120.34998009831095, 'action': [0.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 145.84162019825604, 'action': [1.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 153.92104198776772, 'action': [1.0, 0.0]}])
Weights num count: [0.17185697808535177, 0.15071126489811612, 0.15955401768550556, 0.15378700499807765, 0.1780084582852749, 0.18569780853517878]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.752867698669434 s
