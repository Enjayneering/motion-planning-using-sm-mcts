Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 267, 'sum_payoffs': 80.23715959247787, 'action': [0.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 96.34762098128645, 'action': [2.0, 1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 97.20953385745075, 'action': [2.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 82.83021102442626, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 87.38469216380797, 'action': [1.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 86.06460000756442, 'action': [0.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 89.59273840917848, 'action': [1.0, -1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 100.21843929594453, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 85.70936503295046, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10265282583621683, 0.11726259131103421, 0.11803152633602461, 0.104959630911188, 0.10918877354863514, 0.10803537101114956, 0.1111111111111111, 0.12072279892349097, 0.10765090349865436]
Actions to choose Agent 1: dict_values([{'num_count': 413, 'sum_payoffs': 125.59514667789362, 'action': [0.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 119.570777367256, 'action': [0.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 151.8938699476662, 'action': [1.0, 0.0]}, {'num_count': 442, 'sum_payoffs': 137.29283108510333, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 146.51315306334632, 'action': [1.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 121.96017169810115, 'action': [0.0, 0.0]}])
Weights num count: [0.1587850826605152, 0.15301806997308728, 0.1837754709727028, 0.16993464052287582, 0.1787773933102653, 0.15532487504805845]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.833585262298584 s
