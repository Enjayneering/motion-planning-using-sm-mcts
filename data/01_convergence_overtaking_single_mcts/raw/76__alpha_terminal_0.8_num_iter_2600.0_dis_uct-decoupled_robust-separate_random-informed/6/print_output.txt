Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 272, 'sum_payoffs': 82.1822231670137, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 88.03078586919227, 'action': [1.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 85.81551129335622, 'action': [0.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 94.94616436088263, 'action': [2.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 81.75533808475329, 'action': [0.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 88.04035575667477, 'action': [1.0, -1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 94.03598154124923, 'action': [2.0, 1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 97.44522941628588, 'action': [2.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 91.50789215479594, 'action': [1.0, 0.0]}])
Weights num count: [0.10457516339869281, 0.10995770857362552, 0.10803537101114956, 0.11610918877354863, 0.10419069588619762, 0.10995770857362552, 0.11534025374855825, 0.1184159938485198, 0.11303344867358708]
Actions to choose Agent 1: dict_values([{'num_count': 480, 'sum_payoffs': 152.8743279951595, 'action': [1.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 148.81049725211886, 'action': [1.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 140.67822868206147, 'action': [1.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 124.37826000308738, 'action': [0.0, 0.0]}, {'num_count': 388, 'sum_payoffs': 115.7224901341435, 'action': [0.0, -1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 121.25926883434886, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1845444059976932, 0.18069973087274124, 0.17301038062283736, 0.1576316801230296, 0.14917339484813533, 0.15455594002306805]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.767587900161743 s
