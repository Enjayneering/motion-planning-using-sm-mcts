Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 280, 'sum_payoffs': 85.82295509822899, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 88.41530867549858, 'action': [2.0, 1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 92.16506040435921, 'action': [1.0, -1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 77.78253583742311, 'action': [0.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 86.20568812542079, 'action': [0.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 95.22702509201233, 'action': [2.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 102.05630512989173, 'action': [2.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 90.0131287761233, 'action': [1.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 88.7442236079628, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10765090349865436, 0.10995770857362552, 0.11341791618608228, 0.10034602076124567, 0.10803537101114956, 0.11610918877354863, 0.12226066897347174, 0.1114955786236063, 0.11034217608612072]
Actions to choose Agent 1: dict_values([{'num_count': 492, 'sum_payoffs': 157.2768725233089, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 125.98308834205156, 'action': [0.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 151.1446196454451, 'action': [1.0, -1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 115.23725202358955, 'action': [0.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 138.04509500766451, 'action': [1.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 113.22296035147, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18915801614763553, 0.15955401768550556, 0.18339100346020762, 0.14917339484813533, 0.1710880430603614, 0.14725105728565935]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.33911943435669 s
