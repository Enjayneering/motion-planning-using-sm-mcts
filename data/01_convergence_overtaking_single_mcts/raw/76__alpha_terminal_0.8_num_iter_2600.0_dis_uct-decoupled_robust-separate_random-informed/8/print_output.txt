Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 302, 'sum_payoffs': 94.78865083812929, 'action': [1.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 92.17451460491736, 'action': [1.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 99.45394452482384, 'action': [2.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 82.05108446813634, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 88.39839395185021, 'action': [2.0, 1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 81.52364916863233, 'action': [0.0, 0.0]}, {'num_count': 291, 'sum_payoffs': 90.12425172338948, 'action': [1.0, 0.0]}, {'num_count': 263, 'sum_payoffs': 78.2653874024193, 'action': [0.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 96.1047868919947, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11610918877354863, 0.11380238369857747, 0.12033833141099577, 0.10457516339869281, 0.11034217608612072, 0.10419069588619762, 0.1118800461361015, 0.10111495578623607, 0.11726259131103421]
Actions to choose Agent 1: dict_values([{'num_count': 455, 'sum_payoffs': 141.84820947184284, 'action': [1.0, -1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 130.93624708361446, 'action': [0.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 151.9315542953982, 'action': [1.0, 0.0]}, {'num_count': 385, 'sum_payoffs': 113.69670602608656, 'action': [0.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 142.97127948948085, 'action': [1.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 117.36684560480042, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17493271818531334, 0.1645520953479431, 0.1845444059976932, 0.14801999231064975, 0.17608612072279892, 0.1514801999231065]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.98404574394226 s
