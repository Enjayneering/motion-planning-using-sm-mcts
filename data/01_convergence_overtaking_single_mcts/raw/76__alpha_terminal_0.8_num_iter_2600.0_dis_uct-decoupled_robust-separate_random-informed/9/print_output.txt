Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 274, 'sum_payoffs': 83.10519774378456, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 84.5250992203194, 'action': [0.0, 1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 80.23386519054394, 'action': [0.0, -1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 86.09119213758274, 'action': [1.0, 1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 98.45601702566937, 'action': [2.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 96.00936984571094, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 87.76924557892305, 'action': [1.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 98.6035277308886, 'action': [2.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 90.75285259309676, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1053440984236832, 0.10649750096116878, 0.10265282583621683, 0.10803537101114956, 0.11918492887351019, 0.11687812379853903, 0.10957324106113034, 0.11918492887351019, 0.1122645136485967]
Actions to choose Agent 1: dict_values([{'num_count': 409, 'sum_payoffs': 123.52184867437563, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 142.03857561670551, 'action': [1.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 159.56158471951707, 'action': [1.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 120.33909218836489, 'action': [0.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 134.7810405203013, 'action': [1.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 119.95718740247247, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1572472126105344, 0.17493271818531334, 0.1914648212226067, 0.15417147251057287, 0.16801230296039985, 0.15378700499807765]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.98370933532715 s
