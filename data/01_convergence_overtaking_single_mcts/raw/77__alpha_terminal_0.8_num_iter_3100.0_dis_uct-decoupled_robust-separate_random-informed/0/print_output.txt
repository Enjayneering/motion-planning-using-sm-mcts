Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 106.44979377928898, 'action': [1.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 92.34891858215806, 'action': [0.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 117.15980612384512, 'action': [2.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 111.46570016143984, 'action': [2.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 103.91292207630018, 'action': [1.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 100.0942194924658, 'action': [0.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 110.1719463784196, 'action': [1.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 107.33648266264215, 'action': [2.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 103.13840392286751, 'action': [0.0, 0.0]}])
Weights num count: [0.11157691067397614, 0.10061270557884554, 0.1199613028055466, 0.11544663011931634, 0.10964205095130602, 0.10673976136730087, 0.11447920025798129, 0.11222186391486617, 0.10899709771041599]
Actions to choose Agent 1: dict_values([{'num_count': 508, 'sum_payoffs': 155.2951445809857, 'action': [0.0, 0.0]}, {'num_count': 453, 'sum_payoffs': 133.79395266609242, 'action': [0.0, 1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 168.7969616644411, 'action': [1.0, -1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 182.78546296258665, 'action': [1.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 168.7049990124208, 'action': [1.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 143.515673874137, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.163818123186069, 0.14608190906159305, 0.1747823282811996, 0.18606900999677523, 0.1747823282811996, 0.15414382457271847]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.482593774795532 s
