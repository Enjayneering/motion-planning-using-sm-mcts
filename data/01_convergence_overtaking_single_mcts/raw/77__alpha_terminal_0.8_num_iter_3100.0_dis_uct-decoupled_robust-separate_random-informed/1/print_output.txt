Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 323, 'sum_payoffs': 97.24475774436033, 'action': [0.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 121.48697734804995, 'action': [2.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 101.01311545192995, 'action': [0.0, 0.0]}, {'num_count': 330, 'sum_payoffs': 100.19522989597749, 'action': [0.0, 1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 107.91661858221107, 'action': [1.0, 0.0]}, {'num_count': 343, 'sum_payoffs': 105.4536388252852, 'action': [2.0, 1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 105.10893391459312, 'action': [1.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 104.73945449769124, 'action': [1.0, -1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 112.23794992739498, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10415994840374072, 0.12286359238955176, 0.10706223798774589, 0.10641728474685586, 0.11254434053531119, 0.11060948081264109, 0.11028700419219607, 0.10996452757175104, 0.11576910673976137]
Actions to choose Agent 1: dict_values([{'num_count': 480, 'sum_payoffs': 144.17540684148415, 'action': [0.0, -1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 165.00957154589005, 'action': [1.0, -1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 152.34371187971902, 'action': [0.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 144.55925813011123, 'action': [0.0, 1.5707963267948966]}, {'num_count': 566, 'sum_payoffs': 178.14582676865749, 'action': [1.0, 0.0]}, {'num_count': 539, 'sum_payoffs': 167.38989527559266, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1547887778136085, 0.17188003869719445, 0.16156078684295389, 0.15511125443405352, 0.18252176717188004, 0.17381489841986456]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.793458461761475 s
