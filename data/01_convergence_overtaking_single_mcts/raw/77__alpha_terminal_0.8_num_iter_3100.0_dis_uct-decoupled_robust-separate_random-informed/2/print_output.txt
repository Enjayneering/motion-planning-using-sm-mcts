Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 372, 'sum_payoffs': 117.7546302023452, 'action': [2.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 103.5564748841882, 'action': [0.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 89.45126427910385, 'action': [0.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 109.83475771755376, 'action': [1.0, -1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 103.10765310008833, 'action': [1.0, 0.0]}, {'num_count': 324, 'sum_payoffs': 97.78945620790984, 'action': [0.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 108.14958828462575, 'action': [1.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 114.25020138399528, 'action': [2.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 112.34313199694473, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1199613028055466, 0.10899709771041599, 0.09803289261528539, 0.11383424701709126, 0.10867462108997097, 0.10448242502418574, 0.11254434053531119, 0.11738148984198646, 0.11576910673976137]
Actions to choose Agent 1: dict_values([{'num_count': 556, 'sum_payoffs': 174.13811820613464, 'action': [1.0, 0.0]}, {'num_count': 543, 'sum_payoffs': 169.00479783390753, 'action': [1.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 146.47124928503234, 'action': [0.0, -1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 168.91460438317552, 'action': [1.0, 1.5707963267948966]}, {'num_count': 492, 'sum_payoffs': 148.85002603153637, 'action': [0.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 144.15195598326545, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17929700096742987, 0.17510480490164462, 0.15672363753627863, 0.17510480490164462, 0.15865849725894873, 0.1547887778136085]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.568129539489746 s
