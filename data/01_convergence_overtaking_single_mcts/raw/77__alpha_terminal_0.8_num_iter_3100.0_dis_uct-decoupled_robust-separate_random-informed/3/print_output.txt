Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 333, 'sum_payoffs': 101.35939800377515, 'action': [0.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 109.18723297355102, 'action': [2.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 106.22378803277067, 'action': [1.0, 0.0]}, {'num_count': 330, 'sum_payoffs': 100.03429428107361, 'action': [1.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 97.14097638618001, 'action': [0.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 97.18561517969904, 'action': [0.0, 1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 108.83681349424798, 'action': [1.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 117.15180045308558, 'action': [2.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 117.55618452869813, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1073847146081909, 0.11351177039664624, 0.11125443405353112, 0.10641728474685586, 0.10415994840374072, 0.10415994840374072, 0.11318929377620122, 0.11963882618510158, 0.1199613028055466]
Actions to choose Agent 1: dict_values([{'num_count': 548, 'sum_payoffs': 171.626531434408, 'action': [1.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 163.73690555592847, 'action': [1.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 143.13110378941053, 'action': [0.0, -1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 181.68203657786424, 'action': [1.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 149.4352108536325, 'action': [0.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 145.86690294349282, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1767171880038697, 0.17026765559496937, 0.15349887133182843, 0.18477910351499516, 0.15865849725894873, 0.15575620767494355]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.287811040878296 s
