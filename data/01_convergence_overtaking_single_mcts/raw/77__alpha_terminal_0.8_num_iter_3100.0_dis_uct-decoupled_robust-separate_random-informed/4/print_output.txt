Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 347, 'sum_payoffs': 106.76042891144411, 'action': [2.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 118.03933694235266, 'action': [2.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 108.44510563175675, 'action': [1.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 98.80617948143542, 'action': [0.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 103.00551218999593, 'action': [1.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 112.97246157951373, 'action': [2.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 108.73550508137784, 'action': [1.0, -1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 98.92243860832558, 'action': [0.0, 0.0]}, {'num_count': 320, 'sum_payoffs': 95.57802019999359, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11189938729442116, 0.12060625604643663, 0.11318929377620122, 0.10577233150596582, 0.10899709771041599, 0.11673653660109642, 0.11351177039664624, 0.10577233150596582, 0.10319251854240567]
Actions to choose Agent 1: dict_values([{'num_count': 478, 'sum_payoffs': 143.8576792528745, 'action': [0.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 165.96008087214747, 'action': [1.0, 1.5707963267948966]}, {'num_count': 597, 'sum_payoffs': 191.15722048139654, 'action': [1.0, 0.0]}, {'num_count': 491, 'sum_payoffs': 148.8508576038902, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 136.6446219264981, 'action': [0.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 168.3360500875371, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15414382457271847, 0.17220251531763947, 0.1925185424056756, 0.15833602063850372, 0.14833924540470816, 0.17413737504030957]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.6230685710907 s
