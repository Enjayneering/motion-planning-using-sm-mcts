Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 365, 'sum_payoffs': 114.49012503473371, 'action': [2.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 118.33947743965537, 'action': [2.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 103.24767816726178, 'action': [1.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 96.17309523461043, 'action': [0.0, 1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 101.14407442610938, 'action': [1.0, 1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 108.30355200858828, 'action': [1.0, -1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 93.70633344544967, 'action': [0.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 114.93935563844569, 'action': [2.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 103.20679301007392, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11770396646243148, 0.12060625604643663, 0.10899709771041599, 0.10351499516285069, 0.1073847146081909, 0.11286681715575621, 0.10158013544018059, 0.11802644308287649, 0.10899709771041599]
Actions to choose Agent 1: dict_values([{'num_count': 469, 'sum_payoffs': 139.82748714563664, 'action': [0.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 145.02489671442834, 'action': [0.0, 0.0]}, {'num_count': 541, 'sum_payoffs': 168.24354235061026, 'action': [1.0, -1.5707963267948966]}, {'num_count': 571, 'sum_payoffs': 180.22932553847465, 'action': [1.0, 0.0]}, {'num_count': 550, 'sum_payoffs': 171.91384956965018, 'action': [1.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 146.904385655878, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15124153498871332, 0.15543373105449854, 0.1744598516607546, 0.18413415027410512, 0.17736214124475974, 0.15704611415672365]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.50926899909973 s
