Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 373, 'sum_payoffs': 118.01735081336247, 'action': [2.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 115.85445105952893, 'action': [2.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 102.13945537008671, 'action': [0.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 100.80860902822226, 'action': [0.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 108.81298821113049, 'action': [2.0, 1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 95.4912167955977, 'action': [0.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 109.23715871483837, 'action': [1.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 104.21849215306638, 'action': [1.0, 1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 100.07534812701381, 'action': [1.0, 0.0]}])
Weights num count: [0.12028377942599161, 0.11867139632376653, 0.10802966784908094, 0.10706223798774589, 0.11318929377620122, 0.10287004192196066, 0.11351177039664624, 0.10964205095130602, 0.10641728474685586]
Actions to choose Agent 1: dict_values([{'num_count': 491, 'sum_payoffs': 148.53518221741288, 'action': [0.0, -1.5707963267948966]}, {'num_count': 546, 'sum_payoffs': 170.2995092512038, 'action': [1.0, -1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 173.80474829274476, 'action': [1.0, 0.0]}, {'num_count': 465, 'sum_payoffs': 138.34209611956388, 'action': [0.0, 1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 167.89921874762123, 'action': [1.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 153.1963145626445, 'action': [0.0, 0.0]}])
Weights num count: [0.15833602063850372, 0.17607223476297967, 0.17897452434698485, 0.14995162850693325, 0.17413737504030957, 0.16220574008384392]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.635350942611694 s
