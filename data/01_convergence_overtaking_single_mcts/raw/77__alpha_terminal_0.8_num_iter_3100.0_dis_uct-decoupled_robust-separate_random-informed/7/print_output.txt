Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 309, 'sum_payoffs': 91.42044480091658, 'action': [0.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 103.73835535229067, 'action': [0.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 101.25813446975431, 'action': [1.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 108.3164773541775, 'action': [1.0, -1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 104.50344647034706, 'action': [0.0, -1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 102.15531229150203, 'action': [1.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 115.8271579707317, 'action': [2.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 111.68812711767015, 'action': [2.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 115.4556091935514, 'action': [2.0, 0.0]}])
Weights num count: [0.09964527571751049, 0.10931957433086101, 0.1073847146081909, 0.11286681715575621, 0.10996452757175104, 0.10802966784908094, 0.11867139632376653, 0.11544663011931634, 0.11834891970332151]
Actions to choose Agent 1: dict_values([{'num_count': 497, 'sum_payoffs': 150.94495660315047, 'action': [0.0, 0.0]}, {'num_count': 560, 'sum_payoffs': 175.91830731092065, 'action': [1.0, 0.0]}, {'num_count': 534, 'sum_payoffs': 165.5984318217656, 'action': [1.0, 1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 141.84441819491832, 'action': [0.0, 1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 182.3321223639473, 'action': [1.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 136.00030908406708, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16027088036117382, 0.18058690744920994, 0.17220251531763947, 0.1528539180909384, 0.1857465333763302, 0.14801676878426315]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.65719747543335 s
