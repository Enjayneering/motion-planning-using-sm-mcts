Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 361, 'sum_payoffs': 112.73206075667987, 'action': [2.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 99.79908731522414, 'action': [0.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 111.09892823881155, 'action': [2.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 115.25309872911443, 'action': [2.0, 0.0]}, {'num_count': 333, 'sum_payoffs': 101.08795129387977, 'action': [1.0, 0.0]}, {'num_count': 349, 'sum_payoffs': 107.75317898847285, 'action': [1.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 108.01507501950809, 'action': [1.0, 1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 93.9783770508068, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 102.77841225402419, 'action': [0.0, 0.0]}])
Weights num count: [0.11641405998065141, 0.10641728474685586, 0.11512415349887133, 0.11834891970332151, 0.1073847146081909, 0.11254434053531119, 0.11286681715575621, 0.1019026120606256, 0.10867462108997097]
Actions to choose Agent 1: dict_values([{'num_count': 563, 'sum_payoffs': 176.90231097941233, 'action': [1.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 182.4806257304364, 'action': [1.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 135.77596283947585, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 139.01689000446692, 'action': [0.0, 1.5707963267948966]}, {'num_count': 536, 'sum_payoffs': 166.14526020354475, 'action': [1.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 151.05431277536445, 'action': [0.0, 0.0]}])
Weights num count: [0.181554337310545, 0.18606900999677523, 0.14801676878426315, 0.15059658174782328, 0.1728474685585295, 0.16059335698161883]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.6494140625 s
