Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 311, 'sum_payoffs': 92.36693239506951, 'action': [0.0, 1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 108.29000928877916, 'action': [1.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 116.26181293830686, 'action': [2.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 110.71330853128933, 'action': [1.0, 0.0]}, {'num_count': 329, 'sum_payoffs': 99.93949147337389, 'action': [0.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 111.61035599987044, 'action': [2.0, 1.5707963267948966]}, {'num_count': 327, 'sum_payoffs': 99.13220190749094, 'action': [0.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 113.65765679412553, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 105.31231896190859, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10029022895840052, 0.11254434053531119, 0.11867139632376653, 0.11447920025798129, 0.10609480812641084, 0.11512415349887133, 0.1054498548855208, 0.11673653660109642, 0.11028700419219607]
Actions to choose Agent 1: dict_values([{'num_count': 473, 'sum_payoffs': 141.30879087797928, 'action': [0.0, 1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 151.05779760956725, 'action': [0.0, 0.0]}, {'num_count': 564, 'sum_payoffs': 177.25690071129327, 'action': [1.0, 0.0]}, {'num_count': 546, 'sum_payoffs': 170.04877150543172, 'action': [1.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 140.9109370338004, 'action': [0.0, -1.5707963267948966]}, {'num_count': 547, 'sum_payoffs': 170.460341148783, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15253144147049338, 0.16059335698161883, 0.18187681393099, 0.17607223476297967, 0.15220896485004837, 0.1763947113834247]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.673280000686646 s
