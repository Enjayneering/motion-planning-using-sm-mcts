Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 377, 'sum_payoffs': 112.91779351477453, 'action': [0.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 135.34826491605182, 'action': [2.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 125.5656860491926, 'action': [1.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 119.05747050023423, 'action': [1.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 127.49720323338693, 'action': [2.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 121.77659059057912, 'action': [1.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 108.93912011384317, 'action': [0.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 141.90324350718282, 'action': [2.0, 0.0]}, {'num_count': 364, 'sum_payoffs': 107.73783138626034, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10469314079422383, 0.11996667592335462, 0.11330186059427937, 0.10885865037489587, 0.11469036378783672, 0.11080255484587614, 0.10191613440710913, 0.12440988614273812, 0.10108303249097472]
Actions to choose Agent 1: dict_values([{'num_count': 575, 'sum_payoffs': 173.50235249377775, 'action': [0.0, 0.0]}, {'num_count': 553, 'sum_payoffs': 165.0927230854816, 'action': [0.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 193.70147300926243, 'action': [1.0, -1.5707963267948966]}, {'num_count': 662, 'sum_payoffs': 207.4284567285545, 'action': [1.0, 0.0]}, {'num_count': 551, 'sum_payoffs': 164.2718025927862, 'action': [0.0, 1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 195.64221085630467, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1596778672590947, 0.1535684532074424, 0.1741183004720911, 0.18383782282699251, 0.15301305193001943, 0.17550680366564844]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.87270665168762 s
