Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 396, 'sum_payoffs': 120.33573998536882, 'action': [0.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 126.90247722102269, 'action': [2.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 127.97144768548476, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 119.95191030420163, 'action': [1.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 111.46085316385546, 'action': [0.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 120.3162142478766, 'action': [1.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 111.79426978893777, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 136.6974464719897, 'action': [2.0, 0.0]}, {'num_count': 401, 'sum_payoffs': 122.41211374262892, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10996945292974174, 0.11441266314912524, 0.11524576506525964, 0.10969175229103027, 0.10386003887808942, 0.10996945292974174, 0.10413773951680089, 0.1210774784782005, 0.11135795612329909]
Actions to choose Agent 1: dict_values([{'num_count': 640, 'sum_payoffs': 199.67817399850998, 'action': [1.0, -1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 212.5793396566608, 'action': [1.0, 0.0]}, {'num_count': 625, 'sum_payoffs': 193.7266924538323, 'action': [1.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 158.36850575187458, 'action': [0.0, -1.5707963267948966]}, {'num_count': 565, 'sum_payoffs': 170.41708431915043, 'action': [0.0, 0.0]}, {'num_count': 563, 'sum_payoffs': 169.47901058153832, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17772840877534019, 0.18689252985281865, 0.17356289919466814, 0.14829214107192445, 0.15690086087198002, 0.15634545959455706]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.53197431564331 s
