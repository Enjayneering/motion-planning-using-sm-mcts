Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 428, 'sum_payoffs': 134.06423203473466, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 120.57437434815584, 'action': [1.0, -1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 119.73481322488684, 'action': [0.0, 0.0]}, {'num_count': 369, 'sum_payoffs': 110.03652628809586, 'action': [0.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 145.6176805164339, 'action': [2.0, 0.0]}, {'num_count': 389, 'sum_payoffs': 118.07278027454699, 'action': [1.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 117.63944053849956, 'action': [1.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 111.97205642171767, 'action': [0.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 125.79731476268894, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11885587336850875, 0.10969175229103027, 0.10913635101360733, 0.10247153568453207, 0.12663149125242987, 0.10802554845876146, 0.10774784782004998, 0.10386003887808942, 0.11330186059427937]
Actions to choose Agent 1: dict_values([{'num_count': 623, 'sum_payoffs': 192.19336336911343, 'action': [1.0, -1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 190.90227556660733, 'action': [1.0, 1.5707963267948966]}, {'num_count': 686, 'sum_payoffs': 216.87367734107076, 'action': [1.0, 0.0]}, {'num_count': 569, 'sum_payoffs': 171.13242187557037, 'action': [0.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 163.49389390593768, 'action': [0.0, -1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 165.0632798945271, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1730074979172452, 0.1721743960011108, 0.19050263815606777, 0.15801166342682588, 0.1524576506525965, 0.1535684532074424]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.614954471588135 s
