Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 403, 'sum_payoffs': 123.46073367122075, 'action': [2.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 115.78347097025808, 'action': [1.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 123.40906123718001, 'action': [1.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 116.14197871466206, 'action': [0.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 143.44253481845817, 'action': [2.0, 0.0]}, {'num_count': 418, 'sum_payoffs': 129.60895031898457, 'action': [2.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 112.02967844373839, 'action': [0.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 112.06578965071546, 'action': [0.0, 1.5707963267948966]}, {'num_count': 405, 'sum_payoffs': 124.28968242826991, 'action': [1.0, 0.0]}])
Weights num count: [0.11191335740072202, 0.10663704526520411, 0.11191335740072202, 0.10691474590391557, 0.125520688697584, 0.11607886698139405, 0.10413773951680089, 0.10413773951680089, 0.11246875867814496]
Actions to choose Agent 1: dict_values([{'num_count': 626, 'sum_payoffs': 192.61636885545053, 'action': [1.0, -1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 209.49228662245963, 'action': [1.0, 0.0]}, {'num_count': 567, 'sum_payoffs': 169.83420501945315, 'action': [0.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 160.2412968942719, 'action': [0.0, 1.5707963267948966]}, {'num_count': 561, 'sum_payoffs': 167.56445989697139, 'action': [0.0, -1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 196.20724714334668, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17384059983337963, 0.1857817272979728, 0.15745626214940295, 0.15051374618161623, 0.15579005831713413, 0.17633990558178284]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.99428701400757 s
