Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 362, 'sum_payoffs': 107.04582470421856, 'action': [0.0, 1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 121.70817658691757, 'action': [1.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 120.83987123255197, 'action': [1.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 139.29312744215872, 'action': [2.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 124.89651091999482, 'action': [2.0, 1.5707963267948966]}, {'num_count': 387, 'sum_payoffs': 117.24719316418813, 'action': [0.0, 0.0]}, {'num_count': 434, 'sum_payoffs': 136.40805041121746, 'action': [2.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 113.91214088220623, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 121.33983411685998, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1005276312135518, 0.11052485420716468, 0.10996945292974174, 0.12246598167175785, 0.11274645931685642, 0.10747014718133852, 0.12052207720077757, 0.10524854207164676, 0.1102471535684532]
Actions to choose Agent 1: dict_values([{'num_count': 554, 'sum_payoffs': 165.00681731847737, 'action': [0.0, -1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 195.52574733890532, 'action': [1.0, 1.5707963267948966]}, {'num_count': 568, 'sum_payoffs': 170.44249413512208, 'action': [0.0, 0.0]}, {'num_count': 666, 'sum_payoffs': 208.48657364170123, 'action': [1.0, 0.0]}, {'num_count': 544, 'sum_payoffs': 161.15184091335317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 196.34677624654992, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15384615384615385, 0.1757845043043599, 0.1577339627881144, 0.18494862538183837, 0.15106914745903915, 0.17633990558178284]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.9034264087677 s
