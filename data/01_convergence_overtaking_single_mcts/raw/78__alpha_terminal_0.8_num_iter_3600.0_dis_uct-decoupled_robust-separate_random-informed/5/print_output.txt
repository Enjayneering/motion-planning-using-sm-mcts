Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 391, 'sum_payoffs': 118.55480609281123, 'action': [1.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 117.35557612939573, 'action': [0.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 114.86423472709303, 'action': [0.0, -1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 132.40034369795953, 'action': [2.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 109.64648078621609, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 136.91539756624135, 'action': [2.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 118.06559913739324, 'action': [1.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 133.2058239366655, 'action': [2.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 119.0177200526708, 'action': [1.0, 0.0]}])
Weights num count: [0.10858094973618439, 0.10774784782004998, 0.10608164398778117, 0.11802277145237434, 0.10247153568453207, 0.1210774784782005, 0.10830324909747292, 0.11857817272979727, 0.10885865037489587]
Actions to choose Agent 1: dict_values([{'num_count': 590, 'sum_payoffs': 179.5827312750934, 'action': [0.0, 0.0]}, {'num_count': 649, 'sum_payoffs': 202.581796047228, 'action': [1.0, 0.0]}, {'num_count': 633, 'sum_payoffs': 196.24777456844572, 'action': [1.0, -1.5707963267948966]}, {'num_count': 558, 'sum_payoffs': 167.06783195338184, 'action': [0.0, 1.5707963267948966]}, {'num_count': 543, 'sum_payoffs': 161.40098949336945, 'action': [0.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 193.85196644778054, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16384337683976674, 0.1802277145237434, 0.1757845043043599, 0.1549569564009997, 0.1507914468203277, 0.1741183004720911]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.78638744354248 s
