Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 403, 'sum_payoffs': 123.24177322644421, 'action': [1.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 121.31813410513148, 'action': [1.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 120.85899900954234, 'action': [1.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 113.55040471683968, 'action': [0.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 125.36976738975504, 'action': [2.0, 1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 115.65799860748852, 'action': [0.0, -1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 131.48165407264779, 'action': [2.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 139.29061515012046, 'action': [2.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 108.28552130074308, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11191335740072202, 0.11052485420716468, 0.1102471535684532, 0.10524854207164676, 0.11330186059427937, 0.10663704526520411, 0.1174673701749514, 0.12274368231046931, 0.10163843376839767]
Actions to choose Agent 1: dict_values([{'num_count': 547, 'sum_payoffs': 163.1603367418071, 'action': [0.0, -1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 201.31171890752822, 'action': [1.0, -1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 202.90341821906, 'action': [1.0, 1.5707963267948966]}, {'num_count': 640, 'sum_payoffs': 199.25944309299504, 'action': [1.0, 0.0]}, {'num_count': 570, 'sum_payoffs': 172.02374153320508, 'action': [0.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 163.9703675633105, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15190224937517358, 0.17911691196889754, 0.1802277145237434, 0.17772840877534019, 0.15828936406553734, 0.1524576506525965]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.553709506988525 s
