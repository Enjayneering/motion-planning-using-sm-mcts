Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 409, 'sum_payoffs': 126.62881646053934, 'action': [1.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 142.42091136341134, 'action': [2.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 128.0021851480214, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 121.01196745114648, 'action': [1.0, 1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 111.26638664290861, 'action': [0.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 111.65296625071795, 'action': [0.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 109.99378648583205, 'action': [0.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 134.90975288756138, 'action': [2.0, 1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 121.80842124481485, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11357956123299083, 0.12413218550402666, 0.11441266314912524, 0.10969175229103027, 0.10302693696195502, 0.10330463760066648, 0.10219383504582061, 0.11913357400722022, 0.1102471535684532]
Actions to choose Agent 1: dict_values([{'num_count': 635, 'sum_payoffs': 196.51533102070974, 'action': [1.0, -1.5707963267948966]}, {'num_count': 631, 'sum_payoffs': 194.92527774327866, 'action': [1.0, 1.5707963267948966]}, {'num_count': 558, 'sum_payoffs': 166.68670092164874, 'action': [0.0, 1.5707963267948966]}, {'num_count': 656, 'sum_payoffs': 204.68338492276428, 'action': [1.0, 0.0]}, {'num_count': 556, 'sum_payoffs': 165.9493400932219, 'action': [0.0, -1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 168.9305101901227, 'action': [0.0, 0.0]}])
Weights num count: [0.17633990558178284, 0.17522910302693695, 0.1549569564009997, 0.1821716189947237, 0.15440155512357678, 0.15662316023326853]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.95581412315369 s
