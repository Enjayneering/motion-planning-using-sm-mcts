Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 395, 'sum_payoffs': 120.32327510321379, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 124.787466456293, 'action': [1.0, -1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 114.6494266071365, 'action': [0.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 113.38333410559906, 'action': [0.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 127.70470771424398, 'action': [2.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 138.731598382312, 'action': [2.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 109.01079354766584, 'action': [0.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 127.97793393152703, 'action': [2.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 124.73612168208788, 'action': [1.0, 0.0]}])
Weights num count: [0.10969175229103027, 0.11274645931685642, 0.1058039433490697, 0.1049708414329353, 0.11469036378783672, 0.12218828103304638, 0.10191613440710913, 0.11496806442654818, 0.11274645931685642]
Actions to choose Agent 1: dict_values([{'num_count': 645, 'sum_payoffs': 200.4740819754016, 'action': [1.0, -1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 160.22047889928155, 'action': [0.0, 1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 197.01749712224694, 'action': [1.0, 1.5707963267948966]}, {'num_count': 574, 'sum_payoffs': 172.94881892044458, 'action': [0.0, 0.0]}, {'num_count': 676, 'sum_payoffs': 212.6148223645635, 'action': [1.0, 0.0]}, {'num_count': 528, 'sum_payoffs': 155.25027237646228, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17911691196889754, 0.15023604554290476, 0.1766176062204943, 0.15940016662038323, 0.18772563176895307, 0.14662593723965564]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.626067876815796 s
