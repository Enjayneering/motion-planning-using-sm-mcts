Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 376, 'sum_payoffs': 112.52640382940261, 'action': [0.0, -1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 122.20037601507076, 'action': [1.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 129.93843531824464, 'action': [1.0, -1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 132.81452690191858, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 120.09815621586606, 'action': [2.0, 1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 136.04867790899667, 'action': [2.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 124.72929600291427, 'action': [1.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 107.59401047382558, 'action': [0.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 114.11962121527583, 'action': [0.0, 0.0]}])
Weights num count: [0.10441544015551235, 0.11108025548458761, 0.11635656762010553, 0.11830047209108581, 0.10969175229103027, 0.12052207720077757, 0.11274645931685642, 0.10108303249097472, 0.10552624271035824]
Actions to choose Agent 1: dict_values([{'num_count': 641, 'sum_payoffs': 199.58937341333043, 'action': [1.0, 0.0]}, {'num_count': 629, 'sum_payoffs': 194.89842380268922, 'action': [1.0, 1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 200.33854263351716, 'action': [1.0, -1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 164.54090868509275, 'action': [0.0, 1.5707963267948966]}, {'num_count': 561, 'sum_payoffs': 168.3982602785787, 'action': [0.0, -1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 173.8706182801188, 'action': [0.0, 0.0]}])
Weights num count: [0.17800610941405165, 0.17467370174951402, 0.17856151069147458, 0.15301305193001943, 0.15579005831713413, 0.1596778672590947]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.09768295288086 s
