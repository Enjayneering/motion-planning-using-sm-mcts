Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 435, 'sum_payoffs': 129.96654959382013, 'action': [1.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 141.1887319917305, 'action': [1.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 122.84990485122478, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 130.36386716102976, 'action': [0.0, -1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 159.71282782206853, 'action': [2.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 127.5879715372905, 'action': [0.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 151.6349587300877, 'action': [2.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 141.96958109598935, 'action': [2.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 138.72988829520062, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10607168983174835, 0.11289929285540112, 0.10168251645940014, 0.1063155327968788, 0.1241160692514021, 0.10460863204096561, 0.11923920994879297, 0.11338697878566203, 0.11143623506461839]
Actions to choose Agent 1: dict_values([{'num_count': 636, 'sum_payoffs': 190.11423661702617, 'action': [0.0, 0.0]}, {'num_count': 762, 'sum_payoffs': 238.49278519032205, 'action': [1.0, 0.0]}, {'num_count': 716, 'sum_payoffs': 220.77094306027516, 'action': [1.0, 1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 225.3666706555967, 'action': [1.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 185.8548572426284, 'action': [0.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 188.9435343918121, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15508412582297001, 0.18580833942940747, 0.17459156303340648, 0.17751767861497195, 0.152401853206535, 0.15435259692757863]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.45573377609253 s
