Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 535, 'sum_payoffs': 170.22843318907843, 'action': [2.0, 0.0]}, {'num_count': 437, 'sum_payoffs': 130.90498243106822, 'action': [1.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 125.32575833842796, 'action': [0.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 138.80380372069243, 'action': [1.0, 0.0]}, {'num_count': 474, 'sum_payoffs': 145.67860482510366, 'action': [2.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 138.4455052922637, 'action': [1.0, -1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 128.02953651005006, 'action': [0.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 140.45788160981547, 'action': [2.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 126.93351598930066, 'action': [0.0, 0.0]}])
Weights num count: [0.13045598634479394, 0.10655937576200927, 0.10314557425018288, 0.11143623506461839, 0.11558156547183614, 0.11119239209948793, 0.10485247500609607, 0.11241160692514021, 0.1041209461107047]
Actions to choose Agent 1: dict_values([{'num_count': 628, 'sum_payoffs': 187.09973526628838, 'action': [0.0, -1.5707963267948966]}, {'num_count': 606, 'sum_payoffs': 178.67534045471774, 'action': [0.0, 1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 220.34642424917132, 'action': [1.0, 1.5707963267948966]}, {'num_count': 751, 'sum_payoffs': 234.26398548408537, 'action': [1.0, 0.0]}, {'num_count': 654, 'sum_payoffs': 197.01093226935762, 'action': [0.0, 0.0]}, {'num_count': 746, 'sum_payoffs': 232.23009423019147, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15313338210192637, 0.14776883686905634, 0.17434772006827604, 0.18312606681297244, 0.15947329919531822, 0.18190685198732018]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.187010765075684 s
