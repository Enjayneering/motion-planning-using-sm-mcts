Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 461, 'sum_payoffs': 140.45737473214967, 'action': [1.0, 1.5707963267948966]}, {'num_count': 479, 'sum_payoffs': 147.74885861824293, 'action': [2.0, -1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 144.50128977951792, 'action': [2.0, 1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 140.12850484298576, 'action': [1.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 133.69559852459645, 'action': [0.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 138.49737815268168, 'action': [1.0, -1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 127.76648374953368, 'action': [0.0, -1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 147.29157212849958, 'action': [2.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 124.98763092534139, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11241160692514021, 0.11680078029748842, 0.11485003657644477, 0.11216776396000976, 0.10826627651792246, 0.11119239209948793, 0.10460863204096561, 0.11655693733235796, 0.10290173128505242]
Actions to choose Agent 1: dict_values([{'num_count': 720, 'sum_payoffs': 221.1154409955518, 'action': [1.0, -1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 183.96545371213713, 'action': [0.0, -1.5707963267948966]}, {'num_count': 726, 'sum_payoffs': 223.5516215560295, 'action': [1.0, 1.5707963267948966]}, {'num_count': 645, 'sum_payoffs': 192.6342685411674, 'action': [0.0, 0.0]}, {'num_count': 629, 'sum_payoffs': 186.57703176446222, 'action': [0.0, 1.5707963267948966]}, {'num_count': 758, 'sum_payoffs': 235.83528885518956, 'action': [1.0, 0.0]}])
Weights num count: [0.1755669348939283, 0.15167032431114363, 0.17702999268471104, 0.1572787125091441, 0.1533772250670568, 0.18483296756888565]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 43.24764442443848 s
