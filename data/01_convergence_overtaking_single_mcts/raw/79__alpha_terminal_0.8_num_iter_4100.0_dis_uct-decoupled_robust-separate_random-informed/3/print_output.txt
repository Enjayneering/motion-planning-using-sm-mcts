Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 458, 'sum_payoffs': 139.44628965486143, 'action': [1.0, 0.0]}, {'num_count': 425, 'sum_payoffs': 126.2736528933836, 'action': [0.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 149.72661605732063, 'action': [2.0, -1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 128.30943916941854, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 139.46951165518536, 'action': [1.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 145.77621657956766, 'action': [2.0, 1.5707963267948966]}, {'num_count': 497, 'sum_payoffs': 155.11017737580295, 'action': [2.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 138.58453787237374, 'action': [1.0, 1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 123.51088352086711, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11168007802974884, 0.10363326018044379, 0.1180199951231407, 0.10485247500609607, 0.11168007802974884, 0.11558156547183614, 0.12118995366983662, 0.11119239209948793, 0.1019263594245306]
Actions to choose Agent 1: dict_values([{'num_count': 603, 'sum_payoffs': 177.1945213808416, 'action': [0.0, 1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 228.662299764893, 'action': [1.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 183.94729847684908, 'action': [0.0, -1.5707963267948966]}, {'num_count': 754, 'sum_payoffs': 234.70799841324543, 'action': [1.0, 0.0]}, {'num_count': 735, 'sum_payoffs': 227.5265971591545, 'action': [1.0, 1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 194.62553864397205, 'action': [0.0, 0.0]}])
Weights num count: [0.14703730797366496, 0.1799561082662765, 0.15142648134601316, 0.18385759570836382, 0.17922457937088515, 0.15825408436966593]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.31395435333252 s
