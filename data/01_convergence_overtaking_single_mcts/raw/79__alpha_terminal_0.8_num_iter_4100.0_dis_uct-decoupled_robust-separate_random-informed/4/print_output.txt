Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 469, 'sum_payoffs': 144.57430829016056, 'action': [2.0, 1.5707963267948966]}, {'num_count': 478, 'sum_payoffs': 148.1409673124191, 'action': [2.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 126.52771956581495, 'action': [0.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 123.2964903330377, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 131.20942796845148, 'action': [0.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 140.02498527560545, 'action': [1.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 155.32382714333397, 'action': [2.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 138.54589901484835, 'action': [1.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 144.44474005458832, 'action': [1.0, 0.0]}])
Weights num count: [0.11436235064618386, 0.11655693733235796, 0.10338941721531333, 0.10143867349426969, 0.1063155327968788, 0.11168007802974884, 0.12094611070470616, 0.11070470616922702, 0.11436235064618386]
Actions to choose Agent 1: dict_values([{'num_count': 632, 'sum_payoffs': 187.59658334248175, 'action': [0.0, -1.5707963267948966]}, {'num_count': 773, 'sum_payoffs': 241.44968475425398, 'action': [1.0, 0.0]}, {'num_count': 655, 'sum_payoffs': 196.38542619697003, 'action': [0.0, 0.0]}, {'num_count': 684, 'sum_payoffs': 207.4104610219777, 'action': [1.0, -1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 228.49720158336405, 'action': [1.0, 1.5707963267948966]}, {'num_count': 617, 'sum_payoffs': 182.01816040212958, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1541087539624482, 0.18849061204584247, 0.15971714216044866, 0.1667885881492319, 0.18019995123140697, 0.15045110948549134]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.240142822265625 s
