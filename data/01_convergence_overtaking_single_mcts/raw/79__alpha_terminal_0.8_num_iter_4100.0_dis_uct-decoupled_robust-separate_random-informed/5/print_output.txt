Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 413, 'sum_payoffs': 121.39385173578724, 'action': [0.0, 1.5707963267948966]}, {'num_count': 506, 'sum_payoffs': 158.5227610705783, 'action': [2.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 150.0752446711466, 'action': [2.0, -1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 150.46603966992475, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 128.88166771238394, 'action': [0.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 142.3411874602387, 'action': [1.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 135.5274163275351, 'action': [1.0, 0.0]}, {'num_count': 424, 'sum_payoffs': 125.64028238695937, 'action': [0.0, -1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 131.5844633830528, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10070714459887832, 0.12338454035601074, 0.11826383808827115, 0.1185076810534016, 0.10534016093635698, 0.11363082175079249, 0.10948549134357474, 0.10338941721531333, 0.10704706169227018]
Actions to choose Agent 1: dict_values([{'num_count': 675, 'sum_payoffs': 204.41376694779314, 'action': [0.0, 0.0]}, {'num_count': 613, 'sum_payoffs': 180.93151768717587, 'action': [0.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 221.22623468088202, 'action': [1.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 220.09327835892728, 'action': [1.0, 1.5707963267948966]}, {'num_count': 759, 'sum_payoffs': 236.63565108284644, 'action': [1.0, 0.0]}, {'num_count': 618, 'sum_payoffs': 182.75450067316763, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1645940014630578, 0.14947573762496952, 0.17532309192879786, 0.17459156303340648, 0.1850768105340161, 0.1506949524506218]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.00088691711426 s
