Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 472, 'sum_payoffs': 144.89034010878734, 'action': [2.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 126.44632045807538, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 137.35022489834526, 'action': [1.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 122.9771532172922, 'action': [0.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 131.72632936005562, 'action': [0.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 155.71606131051107, 'action': [2.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 141.33454155940694, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 152.11808031373076, 'action': [2.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 132.42633398046726, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11509387954157523, 0.10387710314557425, 0.11046086320409657, 0.10168251645940014, 0.10704706169227018, 0.12167763960009753, 0.11289929285540112, 0.11948305291392343, 0.1075347476225311]
Actions to choose Agent 1: dict_values([{'num_count': 767, 'sum_payoffs': 239.23015058837424, 'action': [1.0, 0.0]}, {'num_count': 718, 'sum_payoffs': 220.50584602496093, 'action': [1.0, -1.5707963267948966]}, {'num_count': 654, 'sum_payoffs': 196.12197716350943, 'action': [0.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 185.4196276147111, 'action': [0.0, -1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 189.28822645307056, 'action': [0.0, 1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 213.25269885899954, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18702755425505974, 0.1750792489636674, 0.15947329919531822, 0.15264569617166546, 0.15508412582297001, 0.17044623262618874]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.15313792228699 s
