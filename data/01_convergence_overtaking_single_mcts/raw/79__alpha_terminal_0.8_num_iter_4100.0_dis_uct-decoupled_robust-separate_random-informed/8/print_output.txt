Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 516, 'sum_payoffs': 162.43051994378658, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 134.35054785255173, 'action': [1.0, 1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 146.74048999120006, 'action': [2.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 137.46678574395648, 'action': [1.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 120.785381818693, 'action': [0.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 146.21459667802748, 'action': [2.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 137.90294402458397, 'action': [1.0, 0.0]}, {'num_count': 433, 'sum_payoffs': 129.14682712011688, 'action': [0.0, -1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 128.3162366521732, 'action': [0.0, 0.0]}])
Weights num count: [0.1258229700073153, 0.10875396244818337, 0.1163130943672275, 0.11070470616922702, 0.10046330163374786, 0.11606925140209705, 0.11094854913435748, 0.10558400390148744, 0.10509631797122652]
Actions to choose Agent 1: dict_values([{'num_count': 700, 'sum_payoffs': 214.0586921276933, 'action': [1.0, 1.5707963267948966]}, {'num_count': 791, 'sum_payoffs': 249.17482029495847, 'action': [1.0, 0.0]}, {'num_count': 612, 'sum_payoffs': 180.6849744769472, 'action': [0.0, -1.5707963267948966]}, {'num_count': 643, 'sum_payoffs': 192.3947341700611, 'action': [0.0, 0.0]}, {'num_count': 733, 'sum_payoffs': 226.755000983045, 'action': [1.0, -1.5707963267948966]}, {'num_count': 621, 'sum_payoffs': 184.03088886908128, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17069007559131918, 0.19287978541819067, 0.14923189465983908, 0.1567910265788832, 0.17873689344062424, 0.15142648134601316]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 39.981287717819214 s
