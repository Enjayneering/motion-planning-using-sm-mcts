Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 419, 'sum_payoffs': 123.69705107906753, 'action': [0.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 141.6260436863568, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 141.9726914832379, 'action': [2.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 138.76915730740913, 'action': [1.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 125.70428155927088, 'action': [0.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 158.39459355631197, 'action': [2.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 145.19745857619287, 'action': [1.0, 0.0]}, {'num_count': 489, 'sum_payoffs': 151.5966159521248, 'action': [2.0, -1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 117.36449094868497, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10217020238966106, 0.11314313582053158, 0.11338697878566203, 0.11143623506461839, 0.10338941721531333, 0.12338454035601074, 0.11533772250670568, 0.11923920994879297, 0.09826871494757376]
Actions to choose Agent 1: dict_values([{'num_count': 707, 'sum_payoffs': 216.91962821462334, 'action': [1.0, 1.5707963267948966]}, {'num_count': 756, 'sum_payoffs': 235.7497258966357, 'action': [1.0, 0.0]}, {'num_count': 719, 'sum_payoffs': 221.51715154145054, 'action': [1.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 185.56387378888343, 'action': [0.0, -1.5707963267948966]}, {'num_count': 679, 'sum_payoffs': 206.2015022357426, 'action': [0.0, 0.0]}, {'num_count': 614, 'sum_payoffs': 181.47708253772683, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1723969763472324, 0.18434528163862474, 0.17532309192879786, 0.152401853206535, 0.16556937332357963, 0.1497195805901]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 41.95947813987732 s
