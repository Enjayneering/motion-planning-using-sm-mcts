Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 412, 'sum_payoffs': 115.08245875143498, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 68.15592202762146, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 87.0864567571004, 'action': [1.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 115.0024987314584, 'action': [2.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 87.45627184949254, 'action': [1.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 87.84607694687885, 'action': [1.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.80609694022348, 'action': [0.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.41629184283715, 'action': [0.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 114.3428285666508, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1328603676233473, 0.09190583682683005, 0.10867462108997097, 0.1328603676233473, 0.10899709771041599, 0.10931957433086101, 0.09158336020638504, 0.09126088358594002, 0.13221541438245726]
Actions to choose Agent 1: dict_values([{'num_count': 578, 'sum_payoffs': 189.46526733473954, 'action': [1.0, -1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 190.75462265686338, 'action': [1.0, 1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 187.4762618378197, 'action': [1.0, 0.0]}, {'num_count': 458, 'sum_payoffs': 140.72963515895464, 'action': [0.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 139.15042476441525, 'action': [0.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 139.930034959188, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18639148661722024, 0.1873589164785553, 0.18477910351499516, 0.14769429216381813, 0.14640438568203806, 0.1470493389229281]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3864288330078125 s
