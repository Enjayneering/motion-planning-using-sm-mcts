Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 410, 'sum_payoffs': 114.26286854667416, 'action': [2.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 115.45227384382711, 'action': [2.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 87.746126921908, 'action': [1.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.4562718528255, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 87.37631182951586, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 115.1024487564292, 'action': [2.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.47626185781967, 'action': [0.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.4562718528255, 'action': [0.0, -1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 87.80609693689051, 'action': [1.0, 0.0]}])
Weights num count: [0.13221541438245726, 0.13318284424379231, 0.10931957433086101, 0.09126088358594002, 0.10899709771041599, 0.1328603676233473, 0.09126088358594002, 0.09126088358594002, 0.10931957433086101]
Actions to choose Agent 1: dict_values([{'num_count': 454, 'sum_payoffs': 139.15042476441525, 'action': [0.0, 1.5707963267948966]}, {'num_count': 578, 'sum_payoffs': 189.54522735471633, 'action': [1.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 190.32483754948876, 'action': [1.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 137.10144925251288, 'action': [0.0, -1.5707963267948966]}, {'num_count': 460, 'sum_payoffs': 141.46926534373895, 'action': [0.0, 0.0]}, {'num_count': 579, 'sum_payoffs': 189.93503245210226, 'action': [1.0, 0.0]}])
Weights num count: [0.14640438568203806, 0.18639148661722024, 0.18703643985811028, 0.14479200257981295, 0.14833924540470816, 0.18671396323766526]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3988006114959717 s
