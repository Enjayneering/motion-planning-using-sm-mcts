Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 336, 'sum_payoffs': 86.73663166970242, 'action': [1.0, -1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 67.43628184783134, 'action': [0.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 87.80609693689051, 'action': [1.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 87.43628184449837, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 68.13593202262727, 'action': [0.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 113.97301347425869, 'action': [2.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 115.76211892123675, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 68.1159420176331, 'action': [0.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 114.73263366403705, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10835214446952596, 0.09126088358594002, 0.10931957433086101, 0.10899709771041599, 0.09190583682683005, 0.13189293776201225, 0.13350532086423733, 0.09190583682683005, 0.13253789100290228]
Actions to choose Agent 1: dict_values([{'num_count': 456, 'sum_payoffs': 139.93003495918785, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 141.1394302613351, 'action': [0.0, -1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 187.49625184281376, 'action': [1.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 138.74062966203482, 'action': [0.0, 1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 188.64567712997854, 'action': [1.0, 1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 191.57421286162443, 'action': [1.0, 0.0]}])
Weights num count: [0.1470493389229281, 0.14801676878426315, 0.18477910351499516, 0.14608190906159305, 0.1857465333763302, 0.18800386971944533]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3995969295501709 s
