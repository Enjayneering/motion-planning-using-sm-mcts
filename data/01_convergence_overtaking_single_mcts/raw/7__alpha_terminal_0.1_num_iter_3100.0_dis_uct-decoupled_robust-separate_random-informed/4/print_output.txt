Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 411, 'sum_payoffs': 114.69265365404868, 'action': [2.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 114.69265365404873, 'action': [2.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.76611693023513, 'action': [0.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.82608694521764, 'action': [0.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 87.04647674711205, 'action': [1.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 88.11594201430013, 'action': [1.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 67.10644676542752, 'action': [0.0, 1.5707963267948966]}, {'num_count': 341, 'sum_payoffs': 88.52573711668063, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 114.3228385616566, 'action': [2.0, 0.0]}])
Weights num count: [0.13253789100290228, 0.13253789100290228, 0.09158336020638504, 0.09158336020638504, 0.10867462108997097, 0.10964205095130602, 0.090938406965495, 0.10996452757175104, 0.13221541438245726]
Actions to choose Agent 1: dict_values([{'num_count': 453, 'sum_payoffs': 138.70064965204645, 'action': [0.0, 1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 187.0864567404333, 'action': [1.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 192.00399796899902, 'action': [1.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 140.31984005657424, 'action': [0.0, -1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 189.0954522423473, 'action': [1.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 140.31984005657412, 'action': [0.0, 0.0]}])
Weights num count: [0.14608190906159305, 0.18445662689455014, 0.18832634633989037, 0.1473718155433731, 0.18606900999677523, 0.1473718155433731]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.3931558132171631 s
