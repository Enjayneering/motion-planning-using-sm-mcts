Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 414, 'sum_payoffs': 115.72213891124845, 'action': [2.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 114.69265365404873, 'action': [2.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 114.69265365404871, 'action': [2.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 87.80609693689051, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.84607695021181, 'action': [0.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 87.37631182951586, 'action': [1.0, 0.0]}, {'num_count': 284, 'sum_payoffs': 67.74612692524097, 'action': [0.0, 0.0]}, {'num_count': 337, 'sum_payoffs': 87.06646675210622, 'action': [1.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 67.06646675543918, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.13350532086423733, 0.13253789100290228, 0.13253789100290228, 0.10931957433086101, 0.09158336020638504, 0.10899709771041599, 0.09158336020638504, 0.10867462108997097, 0.090938406965495]
Actions to choose Agent 1: dict_values([{'num_count': 454, 'sum_payoffs': 139.15042476441525, 'action': [0.0, -1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 188.62568712498435, 'action': [1.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 137.94102946226798, 'action': [0.0, 1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 189.09545224234722, 'action': [1.0, 1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 191.9840079640049, 'action': [1.0, -1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 140.72963515895466, 'action': [0.0, 0.0]}])
Weights num count: [0.14640438568203806, 0.1857465333763302, 0.145436955820703, 0.18606900999677523, 0.18832634633989037, 0.14769429216381813]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.38202643394470215 s
