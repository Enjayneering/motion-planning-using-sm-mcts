Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 284, 'sum_payoffs': 67.84607695021182, 'action': [0.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 115.37231382385042, 'action': [2.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 113.87306344928786, 'action': [2.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 87.0864567571004, 'action': [1.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 67.06646675543918, 'action': [0.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 88.85557219908443, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 114.3428285666508, 'action': [2.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 87.47626185448671, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 68.17591203261563, 'action': [0.0, 0.0]}])
Weights num count: [0.09158336020638504, 0.13318284424379231, 0.13189293776201225, 0.10867462108997097, 0.090938406965495, 0.11028700419219607, 0.13221541438245726, 0.10899709771041599, 0.09190583682683005]
Actions to choose Agent 1: dict_values([{'num_count': 459, 'sum_payoffs': 141.13943026133515, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 139.54022986180155, 'action': [0.0, 1.5707963267948966]}, {'num_count': 573, 'sum_payoffs': 187.4762618378194, 'action': [1.0, -1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 189.0754622373532, 'action': [1.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 139.950024964182, 'action': [0.0, -1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 190.30484754449475, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.14801676878426315, 0.14672686230248308, 0.18477910351499516, 0.18606900999677523, 0.1470493389229281, 0.18703643985811028]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.400787353515625 s
