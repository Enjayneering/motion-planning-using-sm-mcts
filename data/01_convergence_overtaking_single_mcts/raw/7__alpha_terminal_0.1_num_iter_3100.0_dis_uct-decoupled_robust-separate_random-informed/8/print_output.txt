Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 409, 'sum_payoffs': 113.93303346427031, 'action': [2.0, -1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 88.13593201929432, 'action': [1.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 115.432283838833, 'action': [2.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 67.12643677042169, 'action': [0.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 67.4562718528255, 'action': [0.0, 1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 67.80609694022348, 'action': [0.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 115.06246874644084, 'action': [2.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 88.17591202928266, 'action': [1.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 87.04647674711205, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.13189293776201225, 0.10964205095130602, 0.13318284424379231, 0.090938406965495, 0.09126088358594002, 0.09158336020638504, 0.1328603676233473, 0.10964205095130602, 0.10867462108997097]
Actions to choose Agent 1: dict_values([{'num_count': 458, 'sum_payoffs': 140.70964515396048, 'action': [0.0, -1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 138.70064965204645, 'action': [0.0, 1.5707963267948966]}, {'num_count': 580, 'sum_payoffs': 190.3448275544829, 'action': [1.0, 1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 191.14442775424982, 'action': [1.0, 0.0]}, {'num_count': 453, 'sum_payoffs': 138.74062966203476, 'action': [0.0, 0.0]}, {'num_count': 574, 'sum_payoffs': 187.88605694020015, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14769429216381813, 0.14608190906159305, 0.18703643985811028, 0.1876813930990003, 0.14608190906159305, 0.18510158013544017]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.40035295486450195 s
