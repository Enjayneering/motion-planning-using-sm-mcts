Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 489, 'sum_payoffs': 145.48949011230567, 'action': [0.0, 0.0]}, {'num_count': 541, 'sum_payoffs': 165.78978330207485, 'action': [2.0, -1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 161.79795234384497, 'action': [2.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 153.30747184426156, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 145.79831152387578, 'action': [0.0, -1.5707963267948966]}, {'num_count': 557, 'sum_payoffs': 172.16722951594636, 'action': [2.0, 0.0]}, {'num_count': 513, 'sum_payoffs': 154.86353899129446, 'action': [1.0, 0.0]}, {'num_count': 505, 'sum_payoffs': 151.65442118814465, 'action': [1.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 136.06592785805088, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10628124320799825, 0.11758313410128234, 0.11540969354488155, 0.11062812432079983, 0.10649858726363834, 0.12106063899152358, 0.11149750054336013, 0.10975874809823952, 0.10106498587263639]
Actions to choose Agent 1: dict_values([{'num_count': 703, 'sum_payoffs': 208.27362911732627, 'action': [0.0, -1.5707963267948966]}, {'num_count': 797, 'sum_payoffs': 243.59685525325378, 'action': [1.0, -1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 221.6778960187714, 'action': [0.0, 0.0]}, {'num_count': 798, 'sum_payoffs': 243.9880163182258, 'action': [1.0, 1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 203.3478544204612, 'action': [0.0, 1.5707963267948966]}, {'num_count': 873, 'sum_payoffs': 272.47962314014194, 'action': [1.0, 0.0]}])
Weights num count: [0.152792871114975, 0.17322321234514235, 0.16061725711801783, 0.17344055640078243, 0.149967398391654, 0.18974136057378832]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 48.19767928123474 s
