Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 487, 'sum_payoffs': 144.69654174437395, 'action': [0.0, 0.0]}, {'num_count': 551, 'sum_payoffs': 169.91310019412862, 'action': [2.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 155.68982655579714, 'action': [2.0, 1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 180.97102248561222, 'action': [2.0, 0.0]}, {'num_count': 499, 'sum_payoffs': 149.43476178022746, 'action': [1.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 142.71473922748902, 'action': [0.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 131.84148553713945, 'action': [0.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 156.07274814115502, 'action': [1.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 156.3919888396023, 'action': [1.0, 0.0]}])
Weights num count: [0.1058465550967181, 0.11975657465768311, 0.1119321886546403, 0.1258422082156053, 0.10845468376439904, 0.10475983481851771, 0.09867420126059552, 0.11214953271028037, 0.11236687676592046]
Actions to choose Agent 1: dict_values([{'num_count': 695, 'sum_payoffs': 205.5409016697683, 'action': [0.0, 1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 194.6822514885775, 'action': [0.0, -1.5707963267948966]}, {'num_count': 853, 'sum_payoffs': 265.20344586137986, 'action': [1.0, 0.0]}, {'num_count': 736, 'sum_payoffs': 220.80024348759164, 'action': [0.0, 0.0]}, {'num_count': 794, 'sum_payoffs': 242.75994779007416, 'action': [1.0, 1.5707963267948966]}, {'num_count': 856, 'sum_payoffs': 266.2847214368138, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15105411866985438, 0.1447511410562921, 0.18539447946098675, 0.15996522495109758, 0.17257118017822212, 0.18604651162790697]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.04292702674866 s
