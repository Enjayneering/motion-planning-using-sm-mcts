Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 453, 'sum_payoffs': 131.2267837655518, 'action': [0.0, 1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 162.12397719855863, 'action': [2.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 156.1720004762236, 'action': [1.0, 0.0]}, {'num_count': 547, 'sum_payoffs': 168.00010703138378, 'action': [2.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 146.82111550316867, 'action': [1.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 157.29153355556343, 'action': [1.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 139.42405378900196, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 144.76405775089563, 'action': [0.0, -1.5707963267948966]}, {'num_count': 576, 'sum_payoffs': 179.4859353775522, 'action': [2.0, 0.0]}])
Weights num count: [0.09845685720495545, 0.11562703760052162, 0.11236687676592046, 0.1188871984351228, 0.10715061943055858, 0.11301890893284068, 0.10302108237339709, 0.10606389915235818, 0.12519017604868507]
Actions to choose Agent 1: dict_values([{'num_count': 726, 'sum_payoffs': 216.75003511633363, 'action': [0.0, 0.0]}, {'num_count': 808, 'sum_payoffs': 247.61632511583764, 'action': [1.0, 1.5707963267948966]}, {'num_count': 804, 'sum_payoffs': 246.06563042668387, 'action': [1.0, -1.5707963267948966]}, {'num_count': 690, 'sum_payoffs': 203.25654351248414, 'action': [0.0, 1.5707963267948966]}, {'num_count': 875, 'sum_payoffs': 273.01913640252724, 'action': [1.0, 0.0]}, {'num_count': 697, 'sum_payoffs': 205.7857816638618, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1577917843946968, 0.17561399695718322, 0.1747446207346229, 0.149967398391654, 0.19017604868506846, 0.15148880678113455]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.62835931777954 s
