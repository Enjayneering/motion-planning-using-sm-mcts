Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 474, 'sum_payoffs': 139.19504872103929, 'action': [0.0, -1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 158.33931287979465, 'action': [1.0, -1.5707963267948966]}, {'num_count': 530, 'sum_payoffs': 161.11877409663197, 'action': [2.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 154.53134506783096, 'action': [1.0, 0.0]}, {'num_count': 563, 'sum_payoffs': 174.21633519461213, 'action': [2.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 148.2349576861588, 'action': [0.0, 0.0]}, {'num_count': 546, 'sum_payoffs': 167.43599728386528, 'action': [2.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 145.4508932921602, 'action': [1.0, 1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 135.40932155967138, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10302108237339709, 0.11367094109976092, 0.11519234948924147, 0.11149750054336013, 0.12236470332536405, 0.10801999565311889, 0.11866985437948271, 0.10649858726363834, 0.1008476418169963]
Actions to choose Agent 1: dict_values([{'num_count': 729, 'sum_payoffs': 218.28552510377614, 'action': [0.0, 0.0]}, {'num_count': 807, 'sum_payoffs': 247.7194595982035, 'action': [1.0, -1.5707963267948966]}, {'num_count': 840, 'sum_payoffs': 260.21333867241935, 'action': [1.0, 0.0]}, {'num_count': 700, 'sum_payoffs': 207.34526671193365, 'action': [0.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 213.41051843659844, 'action': [0.0, 1.5707963267948966]}, {'num_count': 808, 'sum_payoffs': 248.0292157693717, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15844381656161705, 0.17539665290154313, 0.1825690067376657, 0.15214083894805477, 0.15561834383829604, 0.17561399695718322]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.76286029815674 s
