Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 481, 'sum_payoffs': 142.14886094395405, 'action': [0.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 156.32592246108953, 'action': [2.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 147.2311471427466, 'action': [1.0, -1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 180.71099046963252, 'action': [2.0, 0.0]}, {'num_count': 524, 'sum_payoffs': 159.1182887878391, 'action': [1.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 136.33837545240365, 'action': [0.0, 1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 142.19821218574717, 'action': [0.0, 0.0]}, {'num_count': 563, 'sum_payoffs': 174.5191655052069, 'action': [2.0, -1.5707963267948966]}, {'num_count': 495, 'sum_payoffs': 147.70860366055976, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10454249076287764, 0.11236687676592046, 0.10736796348619865, 0.1258422082156053, 0.113888285155401, 0.10128232992827646, 0.10454249076287764, 0.12236470332536405, 0.10758530754183873]
Actions to choose Agent 1: dict_values([{'num_count': 823, 'sum_payoffs': 253.52997650805958, 'action': [1.0, -1.5707963267948966]}, {'num_count': 691, 'sum_payoffs': 203.83678234535276, 'action': [0.0, 1.5707963267948966]}, {'num_count': 713, 'sum_payoffs': 212.01505108041823, 'action': [0.0, 0.0]}, {'num_count': 823, 'sum_payoffs': 253.5330701591898, 'action': [1.0, 1.5707963267948966]}, {'num_count': 859, 'sum_payoffs': 267.2220364573113, 'action': [1.0, 0.0]}, {'num_count': 691, 'sum_payoffs': 203.8027769832909, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1788741577917844, 0.15018474244729407, 0.15496631167137578, 0.1788741577917844, 0.1866985437948272, 0.15018474244729407]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.397658348083496 s
