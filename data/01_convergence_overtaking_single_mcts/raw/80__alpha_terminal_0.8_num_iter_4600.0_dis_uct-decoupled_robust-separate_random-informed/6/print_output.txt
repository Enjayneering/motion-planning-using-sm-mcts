Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 464, 'sum_payoffs': 135.08424781040563, 'action': [0.0, 1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 180.25493269480418, 'action': [2.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 137.5030695057333, 'action': [0.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 164.88512801933322, 'action': [2.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 160.48040549249825, 'action': [2.0, 1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 156.22943589321187, 'action': [1.0, 0.0]}, {'num_count': 512, 'sum_payoffs': 153.75175191471223, 'action': [1.0, -1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 149.12909607181442, 'action': [1.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 144.512282176762, 'action': [0.0, 0.0]}])
Weights num count: [0.1008476418169963, 0.1258422082156053, 0.10215170615083677, 0.11736579004564225, 0.1149750054336014, 0.11258422082156053, 0.11128015648772006, 0.10867202782003912, 0.10606389915235818]
Actions to choose Agent 1: dict_values([{'num_count': 829, 'sum_payoffs': 255.80638361643278, 'action': [1.0, 0.0]}, {'num_count': 812, 'sum_payoffs': 249.40794112557145, 'action': [1.0, 1.5707963267948966]}, {'num_count': 841, 'sum_payoffs': 260.4568488671054, 'action': [1.0, -1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 194.57446773171324, 'action': [0.0, -1.5707963267948966]}, {'num_count': 732, 'sum_payoffs': 219.2360975561272, 'action': [0.0, 0.0]}, {'num_count': 720, 'sum_payoffs': 214.6482467697285, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18017822212562487, 0.17648337317974352, 0.1827863507933058, 0.1447511410562921, 0.15909584872853727, 0.15648772006085634]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.26870799064636 s
