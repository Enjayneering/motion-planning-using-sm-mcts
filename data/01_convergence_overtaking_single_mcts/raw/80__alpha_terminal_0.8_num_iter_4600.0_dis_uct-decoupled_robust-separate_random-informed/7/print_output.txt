Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 503, 'sum_payoffs': 150.97533375687829, 'action': [1.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 156.47278097680177, 'action': [2.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 142.77688744962643, 'action': [0.0, -1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 147.0375779111143, 'action': [0.0, 0.0]}, {'num_count': 492, 'sum_payoffs': 146.56076397329986, 'action': [1.0, 1.5707963267948966]}, {'num_count': 503, 'sum_payoffs': 150.9918631440623, 'action': [1.0, 0.0]}, {'num_count': 546, 'sum_payoffs': 167.87250263166118, 'action': [2.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 135.26496012108075, 'action': [0.0, 1.5707963267948966]}, {'num_count': 601, 'sum_payoffs': 189.76648855375103, 'action': [2.0, 0.0]}])
Weights num count: [0.10932405998695936, 0.11236687676592046, 0.10475983481851771, 0.10715061943055858, 0.1069332753749185, 0.10932405998695936, 0.11866985437948271, 0.10063029776135622, 0.13062377743968703]
Actions to choose Agent 1: dict_values([{'num_count': 710, 'sum_payoffs': 210.15327384258492, 'action': [0.0, -1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 212.39019382834033, 'action': [0.0, 0.0]}, {'num_count': 817, 'sum_payoffs': 250.34225308300736, 'action': [1.0, 1.5707963267948966]}, {'num_count': 815, 'sum_payoffs': 249.5140367652001, 'action': [1.0, -1.5707963267948966]}, {'num_count': 677, 'sum_payoffs': 197.85256675888633, 'action': [0.0, 1.5707963267948966]}, {'num_count': 865, 'sum_payoffs': 268.60576323227934, 'action': [1.0, 0.0]}])
Weights num count: [0.15431427950445556, 0.15561834383829604, 0.17757009345794392, 0.17713540534666378, 0.14714192566833298, 0.18800260812866768]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 48.03100919723511 s
