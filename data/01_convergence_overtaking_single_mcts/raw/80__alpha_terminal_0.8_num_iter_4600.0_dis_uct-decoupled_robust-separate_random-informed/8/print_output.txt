Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 486, 'sum_payoffs': 144.66858407315098, 'action': [0.0, 0.0]}, {'num_count': 506, 'sum_payoffs': 152.59375277842693, 'action': [1.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 171.94637249909283, 'action': [2.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 166.7541067453729, 'action': [2.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 154.84223832652265, 'action': [1.0, 0.0]}, {'num_count': 539, 'sum_payoffs': 165.60543488379687, 'action': [2.0, 1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 139.30050382652942, 'action': [0.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 135.69267005333265, 'action': [0.0, 1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 159.99359629142361, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10562921104107803, 0.10997609215387959, 0.12062595088024343, 0.1178004781569224, 0.11128015648772006, 0.11714844599000217, 0.10258639426211694, 0.10063029776135622, 0.11410562921104107]
Actions to choose Agent 1: dict_values([{'num_count': 691, 'sum_payoffs': 202.96653388555774, 'action': [0.0, 1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 214.30557478755864, 'action': [0.0, 0.0]}, {'num_count': 817, 'sum_payoffs': 250.35062436871434, 'action': [1.0, 1.5707963267948966]}, {'num_count': 861, 'sum_payoffs': 267.04136641855706, 'action': [1.0, 0.0]}, {'num_count': 796, 'sum_payoffs': 242.44776021413998, 'action': [1.0, -1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 211.68649267681405, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15018474244729407, 0.1567050641164964, 0.17757009345794392, 0.18713323190610737, 0.1730058682895023, 0.15518365572701587]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 48.32280254364014 s
