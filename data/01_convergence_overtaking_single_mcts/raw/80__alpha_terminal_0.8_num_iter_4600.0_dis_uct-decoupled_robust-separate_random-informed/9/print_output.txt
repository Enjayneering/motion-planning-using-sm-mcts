Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 574, 'sum_payoffs': 179.62018834151505, 'action': [2.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 133.5148475642992, 'action': [0.0, 1.5707963267948966]}, {'num_count': 520, 'sum_payoffs': 158.21415147111554, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 143.24398287053643, 'action': [0.0, -1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 156.98854375866858, 'action': [1.0, 0.0]}, {'num_count': 558, 'sum_payoffs': 173.22169449755418, 'action': [2.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 141.30712487307292, 'action': [1.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 144.3701872558469, 'action': [0.0, 0.0]}, {'num_count': 530, 'sum_payoffs': 162.057491458871, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12475548793740492, 0.09932623342751576, 0.11301890893284068, 0.10475983481851771, 0.11236687676592046, 0.12127798304716365, 0.10367311454031732, 0.10541186698543795, 0.11519234948924147]
Actions to choose Agent 1: dict_values([{'num_count': 752, 'sum_payoffs': 226.8478364112469, 'action': [0.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 206.9640756961412, 'action': [0.0, -1.5707963267948966]}, {'num_count': 834, 'sum_payoffs': 257.8156630022706, 'action': [1.0, -1.5707963267948966]}, {'num_count': 793, 'sum_payoffs': 242.30287957970413, 'action': [1.0, 1.5707963267948966]}, {'num_count': 688, 'sum_payoffs': 202.87799727663318, 'action': [0.0, 1.5707963267948966]}, {'num_count': 834, 'sum_payoffs': 257.90828150132165, 'action': [1.0, 0.0]}])
Weights num count: [0.16344272984133884, 0.1519234948924147, 0.18126494240382526, 0.17235383612258204, 0.14953271028037382, 0.18126494240382526]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.62844443321228 s
