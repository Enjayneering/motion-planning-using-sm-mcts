Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 121, 'sum_payoffs': 37.22467472933144, 'action': [1.0, 1.5707963267948966]}, {'num_count': 115, 'sum_payoffs': 34.39987654146051, 'action': [0.0, 1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 40.1964446779681, 'action': [1.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 36.65382643887671, 'action': [0.0, 0.0]}, {'num_count': 127, 'sum_payoffs': 40.17218858428623, 'action': [2.0, -1.5707963267948966]}, {'num_count': 127, 'sum_payoffs': 40.20000617241298, 'action': [2.0, 1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 34.84812644968636, 'action': [0.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 36.32061149876381, 'action': [1.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 40.45723615709082, 'action': [2.0, 0.0]}])
Weights num count: [0.10990009082652134, 0.1044504995458674, 0.11534968210717529, 0.10899182561307902, 0.11534968210717529, 0.11534968210717529, 0.10535876475930972, 0.1080835603996367, 0.11625794732061762]
Actions to choose Agent 1: dict_values([{'num_count': 172, 'sum_payoffs': 51.446405191450765, 'action': [0.0, -1.5707963267948966]}, {'num_count': 198, 'sum_payoffs': 63.09944551344803, 'action': [1.0, 0.0]}, {'num_count': 175, 'sum_payoffs': 52.662176935340554, 'action': [0.0, 0.0]}, {'num_count': 192, 'sum_payoffs': 60.38154637704991, 'action': [1.0, -1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 58.12000563917411, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 53.23552395000739, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15622161671207993, 0.17983651226158037, 0.1589464123524069, 0.17438692098092642, 0.16984559491371481, 0.15985467756584923]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.214653015136719 s
