Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 126, 'sum_payoffs': 39.68006084752665, 'action': [2.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 37.25172923704891, 'action': [0.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 39.598030030457416, 'action': [1.0, -1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 34.893685891779185, 'action': [0.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 37.327960470420436, 'action': [2.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 36.198763619939754, 'action': [1.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 38.23173571967142, 'action': [1.0, 1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 34.92172212586656, 'action': [0.0, 0.0]}, {'num_count': 132, 'sum_payoffs': 42.476873910341446, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11444141689373297, 0.10990009082652134, 0.11444141689373297, 0.10535876475930972, 0.10990009082652134, 0.1080835603996367, 0.11171662125340599, 0.10535876475930972, 0.11989100817438691]
Actions to choose Agent 1: dict_values([{'num_count': 173, 'sum_payoffs': 51.46447641384406, 'action': [0.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 49.40303526005589, 'action': [0.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 58.677397624772325, 'action': [1.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 51.091390122442114, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 59.07216894456501, 'action': [1.0, 0.0]}, {'num_count': 208, 'sum_payoffs': 67.23602539681356, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15712988192552224, 0.15258855585831063, 0.17166212534059946, 0.15622161671207993, 0.17257039055404177, 0.18891916439600362]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 11.988433361053467 s
