Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 116, 'sum_payoffs': 34.833505706502685, 'action': [0.0, -1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 40.60363702682574, 'action': [2.0, 0.0]}, {'num_count': 117, 'sum_payoffs': 35.35575288085395, 'action': [1.0, 0.0]}, {'num_count': 127, 'sum_payoffs': 40.18258412495034, 'action': [2.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 39.652220763656324, 'action': [2.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 35.86258906061298, 'action': [0.0, 1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 39.12149917084797, 'action': [1.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 36.72178849957602, 'action': [0.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 38.271529520025936, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10535876475930972, 0.11625794732061762, 0.10626702997275204, 0.11534968210717529, 0.11444141689373297, 0.10717529518619437, 0.11353315168029064, 0.10899182561307902, 0.11171662125340599]
Actions to choose Agent 1: dict_values([{'num_count': 188, 'sum_payoffs': 59.13144342536107, 'action': [1.0, -1.5707963267948966]}, {'num_count': 200, 'sum_payoffs': 64.63145992707679, 'action': [1.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 51.62440217336394, 'action': [0.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 51.13369145453488, 'action': [0.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 53.77146947267725, 'action': [0.0, 0.0]}, {'num_count': 195, 'sum_payoffs': 62.38407361674756, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17075386012715713, 0.18165304268846502, 0.1553133514986376, 0.15440508628519528, 0.15985467756584923, 0.1771117166212534]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.192265033721924 s
