Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 122, 'sum_payoffs': 37.93003476052199, 'action': [1.0, 1.5707963267948966]}, {'num_count': 129, 'sum_payoffs': 41.28321632425969, 'action': [2.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 34.910142944614066, 'action': [1.0, 0.0]}, {'num_count': 130, 'sum_payoffs': 41.688463675993304, 'action': [2.0, 1.5707963267948966]}, {'num_count': 115, 'sum_payoffs': 34.578606985804, 'action': [0.0, 1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 35.948823333662176, 'action': [0.0, -1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 38.86034603541466, 'action': [1.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 36.94849727455862, 'action': [0.0, 0.0]}, {'num_count': 126, 'sum_payoffs': 39.74506923624463, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11080835603996367, 0.11716621253405994, 0.10535876475930972, 0.11807447774750227, 0.1044504995458674, 0.10717529518619437, 0.11262488646684832, 0.10899182561307902, 0.11444141689373297]
Actions to choose Agent 1: dict_values([{'num_count': 188, 'sum_payoffs': 58.69151309409718, 'action': [1.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 54.638312316314256, 'action': [0.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 55.06682543060275, 'action': [0.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 49.82017663784401, 'action': [0.0, 0.0]}, {'num_count': 194, 'sum_payoffs': 61.43534118149323, 'action': [1.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 60.02963186516552, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17075386012715713, 0.16257947320617622, 0.16348773841961853, 0.15258855585831063, 0.17620345140781107, 0.1734786557674841]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 12.1610746383667 s
