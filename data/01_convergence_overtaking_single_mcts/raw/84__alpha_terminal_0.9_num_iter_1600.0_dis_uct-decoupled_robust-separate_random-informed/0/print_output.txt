Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 170, 'sum_payoffs': 51.350417147332635, 'action': [0.0, 1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 48.72436943165688, 'action': [0.0, -1.5707963267948966]}, {'num_count': 191, 'sum_payoffs': 60.93806155199637, 'action': [2.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 56.37558929662515, 'action': [1.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 51.78672533894402, 'action': [1.0, 1.5707963267948966]}, {'num_count': 187, 'sum_payoffs': 59.12518120633055, 'action': [2.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 55.48855239354224, 'action': [1.0, 0.0]}, {'num_count': 168, 'sum_payoffs': 50.45351228823927, 'action': [0.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 60.051562492466324, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1061836352279825, 0.10243597751405371, 0.1193004372267333, 0.11305434103685197, 0.10680824484697064, 0.11680199875078076, 0.11180512179887571, 0.10493441599000625, 0.11805121798875702]
Actions to choose Agent 1: dict_values([{'num_count': 253, 'sum_payoffs': 76.58756251958437, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 92.01624103733347, 'action': [1.0, 0.0]}, {'num_count': 247, 'sum_payoffs': 74.05137042749693, 'action': [0.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 84.33958189826106, 'action': [1.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 77.80924179317985, 'action': [0.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 89.74619165926556, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1580262336039975, 0.18051217988757026, 0.15427857589006871, 0.16926920674578388, 0.1599000624609619, 0.1773891317926296]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.97109341621399 s
