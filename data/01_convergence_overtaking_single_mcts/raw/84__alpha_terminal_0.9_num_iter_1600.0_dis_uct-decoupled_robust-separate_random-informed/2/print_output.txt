Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 195, 'sum_payoffs': 62.932964864843264, 'action': [2.0, 0.0]}, {'num_count': 167, 'sum_payoffs': 50.23329880661039, 'action': [0.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 53.71165367861447, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 56.99514043567049, 'action': [1.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 57.01490867195196, 'action': [2.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 56.12799030639741, 'action': [2.0, 1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 49.65940681681952, 'action': [0.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 57.04494311917577, 'action': [1.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 52.02231104761939, 'action': [1.0, 0.0]}])
Weights num count: [0.12179887570268583, 0.10430980637101811, 0.10930668332292318, 0.1136789506558401, 0.1136789506558401, 0.11242973141786383, 0.10368519675202999, 0.1136789506558401, 0.10680824484697064]
Actions to choose Agent 1: dict_values([{'num_count': 243, 'sum_payoffs': 72.329047795185, 'action': [0.0, -1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 78.3489203354276, 'action': [0.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 89.43608874635343, 'action': [1.0, 1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 91.68136052799099, 'action': [1.0, -1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 75.44114310261843, 'action': [0.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 87.79812287292663, 'action': [1.0, 0.0]}])
Weights num count: [0.15178013741411617, 0.16052467207995003, 0.17676452217364147, 0.17988757026858213, 0.1561524047470331, 0.17426608369768895]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 16.988221883773804 s
