Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 193, 'sum_payoffs': 61.76192331357435, 'action': [2.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 52.287360046171294, 'action': [0.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 57.584893918905436, 'action': [1.0, -1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 49.92096912922626, 'action': [0.0, 1.5707963267948966]}, {'num_count': 166, 'sum_payoffs': 49.47346004036753, 'action': [0.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 59.886836546540316, 'action': [2.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 51.66015827153978, 'action': [1.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 57.55237777164743, 'action': [2.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 53.1312587621964, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12054965646470955, 0.10743285446595878, 0.11492816989381636, 0.10430980637101811, 0.10368519675202999, 0.11805121798875702, 0.10680824484697064, 0.11492816989381636, 0.10868207370393504]
Actions to choose Agent 1: dict_values([{'num_count': 245, 'sum_payoffs': 73.44298422822689, 'action': [0.0, 1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 78.22005395728303, 'action': [0.0, 0.0]}, {'num_count': 296, 'sum_payoffs': 95.41240288009715, 'action': [1.0, 0.0]}, {'num_count': 279, 'sum_payoffs': 88.00844275434204, 'action': [1.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 85.74034870075884, 'action': [1.0, 1.5707963267948966]}, {'num_count': 250, 'sum_payoffs': 75.65025931758052, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15302935665209244, 0.1599000624609619, 0.1848844472204872, 0.17426608369768895, 0.1711430356027483, 0.1561524047470331]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 16.973726987838745 s
