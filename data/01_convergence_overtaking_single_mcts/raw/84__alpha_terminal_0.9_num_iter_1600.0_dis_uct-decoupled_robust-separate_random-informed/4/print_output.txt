Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 189, 'sum_payoffs': 60.660585597038285, 'action': [2.0, 1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 50.48743356675646, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 53.71506279809929, 'action': [0.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 55.038701548838084, 'action': [1.0, 1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 58.381740655219815, 'action': [2.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 51.80371874723973, 'action': [0.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 54.2744490140706, 'action': [1.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 56.00131954716492, 'action': [1.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 58.79314760273648, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11805121798875702, 0.10430980637101811, 0.10868207370393504, 0.11055590256089944, 0.11492816989381636, 0.1061836352279825, 0.10930668332292318, 0.11180512179887571, 0.1155527795128045]
Actions to choose Agent 1: dict_values([{'num_count': 266, 'sum_payoffs': 82.28609504901547, 'action': [1.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 92.96913186421455, 'action': [1.0, 0.0]}, {'num_count': 258, 'sum_payoffs': 78.81703153339265, 'action': [0.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 75.00060732926529, 'action': [0.0, 1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 78.08246033570558, 'action': [0.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 88.27935295822876, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16614615865084323, 0.18176139912554654, 0.16114928169893816, 0.15552779512804496, 0.1599000624609619, 0.1748906933166771]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.098151206970215 s
