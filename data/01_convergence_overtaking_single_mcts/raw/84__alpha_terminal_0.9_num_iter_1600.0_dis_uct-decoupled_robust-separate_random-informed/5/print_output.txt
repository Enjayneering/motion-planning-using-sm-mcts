Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 185, 'sum_payoffs': 58.56963951383694, 'action': [2.0, 1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 55.21333627687889, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 54.03463587164415, 'action': [1.0, 1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 58.44835723301681, 'action': [2.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 56.7799408154361, 'action': [1.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 51.18168870942567, 'action': [0.0, 0.0]}, {'num_count': 166, 'sum_payoffs': 49.90847781813929, 'action': [0.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 53.026174254733775, 'action': [0.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 59.83041759901053, 'action': [2.0, 0.0]}])
Weights num count: [0.1155527795128045, 0.11118051217988757, 0.10930668332292318, 0.1155527795128045, 0.11305434103685197, 0.10555902560899438, 0.10368519675202999, 0.1080574640849469, 0.1174266083697689]
Actions to choose Agent 1: dict_values([{'num_count': 253, 'sum_payoffs': 76.3682248602002, 'action': [0.0, 1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 74.70849486032978, 'action': [0.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 95.1287514565948, 'action': [1.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 83.63947623199431, 'action': [1.0, -1.5707963267948966]}, {'num_count': 284, 'sum_payoffs': 89.65222785213611, 'action': [1.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 73.89903033679575, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1580262336039975, 0.15552779512804496, 0.18550905683947533, 0.16864459712679575, 0.1773891317926296, 0.15427857589006871]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.281491994857788 s
