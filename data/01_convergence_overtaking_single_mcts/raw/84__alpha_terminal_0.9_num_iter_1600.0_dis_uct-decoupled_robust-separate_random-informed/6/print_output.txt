Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 169, 'sum_payoffs': 51.50872926912127, 'action': [0.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 55.642226466979736, 'action': [1.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 57.35323651420459, 'action': [2.0, 1.5707963267948966]}, {'num_count': 168, 'sum_payoffs': 50.92996256479826, 'action': [0.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 54.228113484569626, 'action': [1.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 52.872983647149596, 'action': [0.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 60.5339469621095, 'action': [2.0, -1.5707963267948966]}, {'num_count': 178, 'sum_payoffs': 55.65467317921118, 'action': [1.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 60.60372568743246, 'action': [2.0, 0.0]}])
Weights num count: [0.10555902560899438, 0.11118051217988757, 0.1136789506558401, 0.10493441599000625, 0.10930668332292318, 0.10743285446595878, 0.11805121798875702, 0.11118051217988757, 0.11805121798875702]
Actions to choose Agent 1: dict_values([{'num_count': 267, 'sum_payoffs': 82.62965007199868, 'action': [1.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 74.93106748572558, 'action': [0.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 74.9413940601838, 'action': [0.0, 1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 83.74336660478262, 'action': [0.0, 0.0]}, {'num_count': 282, 'sum_payoffs': 89.07645070543178, 'action': [1.0, 1.5707963267948966]}, {'num_count': 283, 'sum_payoffs': 89.4679424588447, 'action': [1.0, 0.0]}])
Weights num count: [0.16677076826983137, 0.15552779512804496, 0.15552779512804496, 0.16864459712679575, 0.17613991255465333, 0.17676452217364147]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 16.89345693588257 s
