Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 190, 'sum_payoffs': 60.62954413654438, 'action': [2.0, 0.0]}, {'num_count': 178, 'sum_payoffs': 55.12705293761649, 'action': [1.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 52.761656813549855, 'action': [0.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 58.810402221399386, 'action': [2.0, -1.5707963267948966]}, {'num_count': 164, 'sum_payoffs': 48.810868724695624, 'action': [0.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 53.80088879907577, 'action': [0.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 52.462051956407045, 'action': [1.0, 1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 58.32991770094109, 'action': [1.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 54.727292016958025, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11867582760774516, 0.11118051217988757, 0.1080574640849469, 0.11617738913179262, 0.10243597751405371, 0.10930668332292318, 0.10743285446595878, 0.1155527795128045, 0.11055590256089944]
Actions to choose Agent 1: dict_values([{'num_count': 258, 'sum_payoffs': 78.8383532944865, 'action': [0.0, 0.0]}, {'num_count': 253, 'sum_payoffs': 76.72879792542953, 'action': [0.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 67.42373812124897, 'action': [0.0, -1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 92.9844552622332, 'action': [1.0, 0.0]}, {'num_count': 285, 'sum_payoffs': 90.31100399487471, 'action': [1.0, 1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 89.11588576076639, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16114928169893816, 0.1580262336039975, 0.14428482198625858, 0.18176139912554654, 0.17801374141161774, 0.17613991255465333]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 17.002514839172363 s
