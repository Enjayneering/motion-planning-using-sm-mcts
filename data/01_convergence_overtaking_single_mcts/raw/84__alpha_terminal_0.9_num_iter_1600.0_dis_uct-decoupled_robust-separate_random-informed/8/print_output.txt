Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 173, 'sum_payoffs': 52.91665911714387, 'action': [1.0, -1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 55.624618986773804, 'action': [1.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 52.12635721290304, 'action': [0.0, 0.0]}, {'num_count': 201, 'sum_payoffs': 65.89114448962738, 'action': [2.0, 0.0]}, {'num_count': 167, 'sum_payoffs': 50.315394481671845, 'action': [0.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 52.146749224694275, 'action': [0.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 58.81437248548123, 'action': [2.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 56.67195563029342, 'action': [2.0, 1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 52.10653521456675, 'action': [1.0, 0.0]}])
Weights num count: [0.1080574640849469, 0.11180512179887571, 0.10680824484697064, 0.12554653341661462, 0.10430980637101811, 0.10680824484697064, 0.11617738913179262, 0.11305434103685197, 0.10680824484697064]
Actions to choose Agent 1: dict_values([{'num_count': 252, 'sum_payoffs': 76.3660738823216, 'action': [0.0, 0.0]}, {'num_count': 246, 'sum_payoffs': 73.7066952074887, 'action': [0.0, 1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 95.67596465162958, 'action': [1.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 87.01900068329655, 'action': [1.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 87.04025970099892, 'action': [1.0, -1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 75.93145556760749, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15740162398500937, 0.15365396627108058, 0.18550905683947533, 0.17301686445971268, 0.17301686445971268, 0.15677701436602123]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 18.383622884750366 s
