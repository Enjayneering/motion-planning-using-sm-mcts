Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 186, 'sum_payoffs': 59.00207947007701, 'action': [1.0, -1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 54.39226845415078, 'action': [0.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 58.06311476634552, 'action': [2.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 56.166525790757944, 'action': [2.0, 1.5707963267948966]}, {'num_count': 192, 'sum_payoffs': 61.62018834156602, 'action': [2.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 52.50093616779385, 'action': [1.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 51.130826575181096, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 54.34570337977591, 'action': [0.0, -1.5707963267948966]}, {'num_count': 165, 'sum_payoffs': 49.44765570070143, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11617738913179262, 0.1099312929419113, 0.11492816989381636, 0.11242973141786383, 0.11992504684572143, 0.10743285446595878, 0.10555902560899438, 0.1099312929419113, 0.10306058713304185]
Actions to choose Agent 1: dict_values([{'num_count': 299, 'sum_payoffs': 96.17164638424434, 'action': [1.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 73.15990227323317, 'action': [0.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 72.69744976626956, 'action': [0.0, 1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 90.63477332577973, 'action': [1.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 86.6858149028769, 'action': [1.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 74.82823142043306, 'action': [0.0, 0.0]}])
Weights num count: [0.1867582760774516, 0.15302935665209244, 0.1524047470331043, 0.17863835103060588, 0.17301686445971268, 0.15552779512804496]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 19.074779987335205 s
