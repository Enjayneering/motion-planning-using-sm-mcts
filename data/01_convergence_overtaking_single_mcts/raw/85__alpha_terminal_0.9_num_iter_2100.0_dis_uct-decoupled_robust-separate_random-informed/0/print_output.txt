Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 238, 'sum_payoffs': 74.10094366749365, 'action': [1.0, -1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 70.09761428734842, 'action': [0.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 76.26780880069325, 'action': [2.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 70.09297286622578, 'action': [1.0, 1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 78.47651568660427, 'action': [2.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 74.49740750597107, 'action': [2.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 71.00063249110282, 'action': [0.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 67.53357911956167, 'action': [1.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 66.27376764733506, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11327939076630177, 0.10899571632555925, 0.11565920990004759, 0.10899571632555925, 0.11803902903379343, 0.11375535459305093, 0.1099476439790576, 0.10613993336506425, 0.10471204188481675]
Actions to choose Agent 1: dict_values([{'num_count': 318, 'sum_payoffs': 94.84384983633106, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 102.55127696440901, 'action': [0.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 120.05583567264928, 'action': [1.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 110.84264271612989, 'action': [1.0, 1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 117.5534822133388, 'action': [1.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 102.20553902099545, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15135649690623512, 0.1603998096144693, 0.18039029033793433, 0.16991908614945264, 0.1775345073774393, 0.15992384578772012]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.49659776687622 s
