Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 239, 'sum_payoffs': 74.52721604645211, 'action': [2.0, 1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 78.57036839065997, 'action': [2.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 68.4877857235677, 'action': [1.0, 0.0]}, {'num_count': 228, 'sum_payoffs': 69.80439435492714, 'action': [1.0, 1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 79.04157788439284, 'action': [2.0, 0.0]}, {'num_count': 218, 'sum_payoffs': 65.47288777714583, 'action': [0.0, 1.5707963267948966]}, {'num_count': 220, 'sum_payoffs': 66.22203823582582, 'action': [0.0, 0.0]}, {'num_count': 232, 'sum_payoffs': 71.51269176896527, 'action': [0.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 75.43068148212208, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11375535459305093, 0.11803902903379343, 0.10709186101856259, 0.10851975249881009, 0.1185149928605426, 0.10376011423131842, 0.10471204188481675, 0.11042360780580676, 0.11470728224654926]
Actions to choose Agent 1: dict_values([{'num_count': 366, 'sum_payoffs': 115.1736479658644, 'action': [1.0, -1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 92.00466183558837, 'action': [0.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 118.9815074039694, 'action': [1.0, 1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 100.30941353807468, 'action': [0.0, 0.0]}, {'num_count': 327, 'sum_payoffs': 99.10795313778328, 'action': [0.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 126.1837251757544, 'action': [1.0, 0.0]}])
Weights num count: [0.17420276059019515, 0.1475487862922418, 0.17848643503093764, 0.15706806282722513, 0.15564017134697763, 0.1865778200856735]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.633357524871826 s
