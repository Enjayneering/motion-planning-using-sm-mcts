Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 247, 'sum_payoffs': 78.34878935816438, 'action': [2.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 68.66062416220753, 'action': [0.0, -1.5707963267948966]}, {'num_count': 218, 'sum_payoffs': 65.65773596374409, 'action': [0.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 72.13291125507556, 'action': [1.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 70.38587326242099, 'action': [1.0, 0.0]}, {'num_count': 250, 'sum_payoffs': 79.6289796963899, 'action': [2.0, 1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 77.3969088672475, 'action': [2.0, 0.0]}, {'num_count': 214, 'sum_payoffs': 63.89430287663096, 'action': [0.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 74.84228586581379, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11756306520704426, 0.10709186101856259, 0.10376011423131842, 0.11089957163255593, 0.10899571632555925, 0.11899095668729176, 0.11661113755354593, 0.10185625892432175, 0.11375535459305093]
Actions to choose Agent 1: dict_values([{'num_count': 361, 'sum_payoffs': 113.16226871649823, 'action': [1.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 108.95910620393262, 'action': [1.0, 1.5707963267948966]}, {'num_count': 343, 'sum_payoffs': 105.68408752223847, 'action': [0.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 121.13152603906484, 'action': [1.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 100.75084548946711, 'action': [0.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 102.04154959177166, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1718229414564493, 0.16706330318895765, 0.1632555925749643, 0.1808662541646835, 0.1575440266539743, 0.1589719181342218]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.664623022079468 s
