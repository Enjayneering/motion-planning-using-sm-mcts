Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 233, 'sum_payoffs': 72.02289361996549, 'action': [2.0, 1.5707963267948966]}, {'num_count': 234, 'sum_payoffs': 72.55871489307438, 'action': [1.0, 1.5707963267948966]}, {'num_count': 251, 'sum_payoffs': 80.03303052846883, 'action': [2.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 66.47320773214707, 'action': [0.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 79.04472467570697, 'action': [2.0, -1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 64.69138968024583, 'action': [0.0, -1.5707963267948966]}, {'num_count': 237, 'sum_payoffs': 73.7666968152616, 'action': [1.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 69.50689486388579, 'action': [0.0, 1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 72.07434925059401, 'action': [1.0, 0.0]}])
Weights num count: [0.11089957163255593, 0.11137553545930509, 0.11946692051404094, 0.10471204188481675, 0.1185149928605426, 0.10280818657782008, 0.1128034269395526, 0.10804378867206092, 0.11089957163255593]
Actions to choose Agent 1: dict_values([{'num_count': 381, 'sum_payoffs': 121.49371518710875, 'action': [1.0, 0.0]}, {'num_count': 376, 'sum_payoffs': 119.33188383373032, 'action': [1.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 103.60546191166746, 'action': [0.0, 0.0]}, {'num_count': 321, 'sum_payoffs': 96.5584251133607, 'action': [0.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 113.46459803127536, 'action': [1.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 97.07407731309063, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18134221799143266, 0.1789623988576868, 0.16087577344121848, 0.15278438838648262, 0.17229890528319847, 0.15326035221323178]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.51322078704834 s
