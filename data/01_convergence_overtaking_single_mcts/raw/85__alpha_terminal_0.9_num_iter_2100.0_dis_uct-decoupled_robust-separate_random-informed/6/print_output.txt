Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 257, 'sum_payoffs': 82.53679777735397, 'action': [2.0, 0.0]}, {'num_count': 227, 'sum_payoffs': 69.43260716276234, 'action': [2.0, 1.5707963267948966]}, {'num_count': 236, 'sum_payoffs': 73.43240719267796, 'action': [1.0, -1.5707963267948966]}, {'num_count': 244, 'sum_payoffs': 76.89885503884585, 'action': [2.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 67.66552626611538, 'action': [0.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 70.76510324755972, 'action': [0.0, 0.0]}, {'num_count': 220, 'sum_payoffs': 66.46135237739122, 'action': [0.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 68.63958333239357, 'action': [1.0, 0.0]}, {'num_count': 238, 'sum_payoffs': 74.17431473650514, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12232270347453593, 0.10804378867206092, 0.11232746311280342, 0.11613517372679677, 0.10613993336506425, 0.10947168015230842, 0.10471204188481675, 0.10709186101856259, 0.11327939076630177]
Actions to choose Agent 1: dict_values([{'num_count': 334, 'sum_payoffs': 101.35452294494311, 'action': [0.0, 1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 95.246431440298, 'action': [0.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 122.44979342858731, 'action': [1.0, 0.0]}, {'num_count': 324, 'sum_payoffs': 97.208698827763, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 116.14832072678026, 'action': [1.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 115.37039706696187, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1589719181342218, 0.1518324607329843, 0.18324607329842932, 0.15421227986673014, 0.17610661589719181, 0.17515468824369348]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 24.073116064071655 s
