Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 225, 'sum_payoffs': 68.33475895234791, 'action': [0.0, 0.0]}, {'num_count': 243, 'sum_payoffs': 76.20234594583799, 'action': [2.0, 1.5707963267948966]}, {'num_count': 230, 'sum_payoffs': 70.52814700988701, 'action': [0.0, -1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 78.01268200860687, 'action': [2.0, -1.5707963267948966]}, {'num_count': 216, 'sum_payoffs': 64.47944592836187, 'action': [0.0, 1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 69.55103490168075, 'action': [1.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 75.42724180209841, 'action': [2.0, 0.0]}, {'num_count': 231, 'sum_payoffs': 71.03204372742653, 'action': [1.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 74.57313653963135, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10709186101856259, 0.11565920990004759, 0.10947168015230842, 0.11756306520704426, 0.10280818657782008, 0.10851975249881009, 0.11470728224654926, 0.1099476439790576, 0.11375535459305093]
Actions to choose Agent 1: dict_values([{'num_count': 374, 'sum_payoffs': 118.5429083262068, 'action': [1.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 97.40051243684572, 'action': [0.0, 1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 110.17388495603299, 'action': [1.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 103.16092177802801, 'action': [0.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 102.65329013496577, 'action': [0.0, -1.5707963267948966]}, {'num_count': 376, 'sum_payoffs': 119.19388046748617, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.17801047120418848, 0.15373631603998097, 0.16849119466920515, 0.1603998096144693, 0.15992384578772012, 0.1789623988576868]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.433099031448364 s
