Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 241, 'sum_payoffs': 75.75437605850337, 'action': [2.0, 1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 73.11797484409313, 'action': [1.0, 0.0]}, {'num_count': 212, 'sum_payoffs': 63.10635362354317, 'action': [0.0, 1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 67.90847075838242, 'action': [0.0, -1.5707963267948966]}, {'num_count': 219, 'sum_payoffs': 66.09167296908488, 'action': [0.0, 0.0]}, {'num_count': 233, 'sum_payoffs': 72.1695357795473, 'action': [1.0, 1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 79.2271630831858, 'action': [2.0, 0.0]}, {'num_count': 253, 'sum_payoffs': 81.12129851041884, 'action': [2.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 73.09022600202178, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11470728224654926, 0.11185149928605426, 0.10090433127082342, 0.10613993336506425, 0.10423607805806759, 0.11089957163255593, 0.1185149928605426, 0.12041884816753927, 0.11185149928605426]
Actions to choose Agent 1: dict_values([{'num_count': 383, 'sum_payoffs': 121.75354825613223, 'action': [1.0, 0.0]}, {'num_count': 326, 'sum_payoffs': 98.14473813723862, 'action': [0.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 99.3639508575661, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 116.28516135733159, 'action': [1.0, -1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 118.01062021886776, 'action': [1.0, 1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 94.77309986196502, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.182294145644931, 0.15516420752022847, 0.15659209900047596, 0.17610661589719181, 0.17801047120418848, 0.15135649690623512]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.24915599822998 s
