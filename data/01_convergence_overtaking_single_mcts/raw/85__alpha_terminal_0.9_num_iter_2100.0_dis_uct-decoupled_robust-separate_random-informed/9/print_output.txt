Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 253, 'sum_payoffs': 80.5128138375249, 'action': [2.0, 0.0]}, {'num_count': 222, 'sum_payoffs': 66.92233183946611, 'action': [0.0, -1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 74.69547779954753, 'action': [2.0, -1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 74.31920010352407, 'action': [1.0, -1.5707963267948966]}, {'num_count': 241, 'sum_payoffs': 75.24311876735175, 'action': [2.0, 1.5707963267948966]}, {'num_count': 224, 'sum_payoffs': 67.67923727220845, 'action': [0.0, 0.0]}, {'num_count': 240, 'sum_payoffs': 74.71944804071285, 'action': [1.0, 0.0]}, {'num_count': 219, 'sum_payoffs': 65.62961289865126, 'action': [0.0, 1.5707963267948966]}, {'num_count': 222, 'sum_payoffs': 66.93311230852743, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12041884816753927, 0.10566396953831508, 0.1142313184198001, 0.11375535459305093, 0.11470728224654926, 0.10661589719181343, 0.1142313184198001, 0.10423607805806759, 0.10566396953831508]
Actions to choose Agent 1: dict_values([{'num_count': 335, 'sum_payoffs': 101.92603812416701, 'action': [0.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 98.99946981217558, 'action': [0.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 123.92154543462647, 'action': [1.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 115.16306671451468, 'action': [1.0, -1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 95.37228265651136, 'action': [0.0, -1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 113.49886257872161, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15944788196097096, 0.1561161351737268, 0.1846739647786768, 0.17467872441694432, 0.1518324607329843, 0.17277486910994763]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 23.57491636276245 s
