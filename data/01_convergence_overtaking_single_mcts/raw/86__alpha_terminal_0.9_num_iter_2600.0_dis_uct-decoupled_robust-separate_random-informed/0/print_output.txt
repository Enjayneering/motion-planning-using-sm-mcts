Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 272, 'sum_payoffs': 81.99969115533604, 'action': [0.0, 0.0]}, {'num_count': 319, 'sum_payoffs': 102.05370239327996, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 85.43023876734692, 'action': [1.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 82.80798843516803, 'action': [1.0, 0.0]}, {'num_count': 308, 'sum_payoffs': 97.34329110784537, 'action': [2.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 94.75384068210464, 'action': [1.0, -1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 80.65564406495646, 'action': [0.0, -1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 78.21054982595678, 'action': [0.0, 1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 99.51160798232607, 'action': [2.0, 0.0]}])
Weights num count: [0.10457516339869281, 0.12264513648596694, 0.10765090349865436, 0.1053440984236832, 0.1184159938485198, 0.11610918877354863, 0.10342176086120723, 0.10111495578623607, 0.12033833141099577]
Actions to choose Agent 1: dict_values([{'num_count': 464, 'sum_payoffs': 146.2791254980005, 'action': [1.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 122.07577579810659, 'action': [0.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 121.56641769331966, 'action': [0.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 140.58475984370318, 'action': [1.0, 1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 118.88476410801927, 'action': [0.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 153.9554370206624, 'action': [1.0, 0.0]}])
Weights num count: [0.17839292579777008, 0.15532487504805845, 0.15494040753556323, 0.17301038062283736, 0.1522491349480969, 0.18569780853517878]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.564294576644897 s
