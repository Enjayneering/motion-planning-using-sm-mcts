Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 286, 'sum_payoffs': 88.21953762511109, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 81.84262666450725, 'action': [0.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 90.76688928685152, 'action': [1.0, 1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 95.53469730669494, 'action': [2.0, -1.5707963267948966]}, {'num_count': 271, 'sum_payoffs': 81.92757393236913, 'action': [0.0, 1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 96.42747982474012, 'action': [2.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 90.72936978187761, 'action': [1.0, -1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 82.28069411952877, 'action': [0.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 97.63624310666079, 'action': [2.0, 0.0]}])
Weights num count: [0.10995770857362552, 0.10419069588619762, 0.1122645136485967, 0.11649365628604383, 0.10419069588619762, 0.11726259131103421, 0.1122645136485967, 0.10457516339869281, 0.1184159938485198]
Actions to choose Agent 1: dict_values([{'num_count': 454, 'sum_payoffs': 141.78229448128778, 'action': [1.0, -1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 148.74499759014213, 'action': [1.0, 0.0]}, {'num_count': 461, 'sum_payoffs': 144.62243625799383, 'action': [1.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 117.2513044406686, 'action': [0.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 124.84904438814404, 'action': [0.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 123.68102488303565, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17454825067281815, 0.18108419838523646, 0.1772395232602845, 0.1510957324106113, 0.15840061514801998, 0.1572472126105344]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 29.712830066680908 s
