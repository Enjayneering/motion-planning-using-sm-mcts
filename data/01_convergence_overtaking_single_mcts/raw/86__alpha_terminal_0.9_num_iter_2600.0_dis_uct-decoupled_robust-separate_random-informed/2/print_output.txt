Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 302, 'sum_payoffs': 94.85552952598006, 'action': [2.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 82.96476362429503, 'action': [0.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 87.92915194954028, 'action': [1.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 92.59306933576102, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 87.61506489794436, 'action': [1.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 79.59013674878513, 'action': [0.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 84.65740732802503, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 89.32627903727519, 'action': [1.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 103.7944733182282, 'action': [2.0, 0.0]}])
Weights num count: [0.11610918877354863, 0.1053440984236832, 0.10995770857362552, 0.11418685121107267, 0.10957324106113034, 0.10226835832372165, 0.10688196847366398, 0.1111111111111111, 0.12418300653594772]
Actions to choose Agent 1: dict_values([{'num_count': 398, 'sum_payoffs': 119.70785317296384, 'action': [0.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 147.55187499116877, 'action': [1.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 124.02501257717343, 'action': [0.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 116.9439450651311, 'action': [0.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 154.81063801746785, 'action': [1.0, 0.0]}, {'num_count': 450, 'sum_payoffs': 140.71594058655216, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15301806997308728, 0.17954632833525566, 0.1572472126105344, 0.1503267973856209, 0.18646674356016918, 0.17301038062283736]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.45557165145874 s
