Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 288, 'sum_payoffs': 88.99935963742584, 'action': [1.0, 1.5707963267948966]}, {'num_count': 319, 'sum_payoffs': 102.4116965638445, 'action': [2.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 84.66941602215182, 'action': [1.0, 0.0]}, {'num_count': 274, 'sum_payoffs': 83.11527671935596, 'action': [0.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 93.33803006583747, 'action': [1.0, -1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 93.2089504641686, 'action': [2.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 84.29607563337963, 'action': [0.0, 1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 80.58605805517092, 'action': [0.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 94.21831054958058, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11072664359861592, 0.12264513648596694, 0.10688196847366398, 0.1053440984236832, 0.11457131872356786, 0.11457131872356786, 0.10649750096116878, 0.10303729334871203, 0.11534025374855825]
Actions to choose Agent 1: dict_values([{'num_count': 475, 'sum_payoffs': 150.61406427216815, 'action': [1.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 121.39154746682223, 'action': [0.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 123.838504944312, 'action': [0.0, 0.0]}, {'num_count': 457, 'sum_payoffs': 143.32076519613528, 'action': [1.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 139.66927938339057, 'action': [1.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 123.36682844504291, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18262206843521722, 0.15494040753556323, 0.1572472126105344, 0.17570165321030373, 0.172241445597847, 0.1568627450980392]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 30.61701798439026 s
