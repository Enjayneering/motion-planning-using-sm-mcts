Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 91.22191410417484, 'action': [1.0, -1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 100.73428139239475, 'action': [2.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 93.47272914308948, 'action': [1.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 79.35928308159198, 'action': [0.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 87.4260558769635, 'action': [1.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 93.85425016802922, 'action': [2.0, 0.0]}, {'num_count': 300, 'sum_payoffs': 93.89580167212469, 'action': [2.0, 1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 80.32006166406893, 'action': [0.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 81.89357320438445, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11303344867358708, 0.12149173394848135, 0.11495578623606305, 0.10226835832372165, 0.10957324106113034, 0.11534025374855825, 0.11534025374855825, 0.10303729334871203, 0.10457516339869281]
Actions to choose Agent 1: dict_values([{'num_count': 402, 'sum_payoffs': 121.24757919828082, 'action': [0.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 142.91095550660543, 'action': [1.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 140.0915912265184, 'action': [1.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 145.84640345841908, 'action': [1.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 124.36260052167383, 'action': [0.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 128.42844760019187, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15455594002306805, 0.17531718569780855, 0.17262591311034217, 0.1780084582852749, 0.1576316801230296, 0.16147635524798154]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.535294771194458 s
