Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 275, 'sum_payoffs': 83.45716260102063, 'action': [1.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 83.47047556838635, 'action': [0.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 82.52243520663501, 'action': [0.0, -1.5707963267948966]}, {'num_count': 301, 'sum_payoffs': 94.44920757499419, 'action': [2.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 100.98374615316666, 'action': [2.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 92.72657061446074, 'action': [1.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 88.59447436629392, 'action': [1.0, 1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 77.56962303852168, 'action': [0.0, 1.5707963267948966]}, {'num_count': 315, 'sum_payoffs': 100.53402165251573, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1057285659361784, 0.1057285659361784, 0.104959630911188, 0.11572472126105345, 0.12149173394848135, 0.11418685121107267, 0.11034217608612072, 0.10034602076124567, 0.12110726643598616]
Actions to choose Agent 1: dict_values([{'num_count': 472, 'sum_payoffs': 149.84201957715098, 'action': [1.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 141.76694771065087, 'action': [1.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 116.71888937719453, 'action': [0.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 121.15612798035878, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 147.73844514769115, 'action': [1.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 127.99582816395373, 'action': [0.0, 0.0]}])
Weights num count: [0.18146866589773164, 0.17377931564782775, 0.14994232987312572, 0.15417147251057287, 0.17954632833525566, 0.16070742022299117]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.06147313117981 s
