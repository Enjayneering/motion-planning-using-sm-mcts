Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 292, 'sum_payoffs': 90.33709557771796, 'action': [1.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 94.66484880966863, 'action': [2.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 83.62664900254914, 'action': [0.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 91.19124856636085, 'action': [1.0, -1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 98.09506081189848, 'action': [2.0, -1.5707963267948966]}, {'num_count': 264, 'sum_payoffs': 78.46791335325359, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 88.97707591133852, 'action': [1.0, 0.0]}, {'num_count': 272, 'sum_payoffs': 81.89388319323831, 'action': [0.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 94.25477944241479, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1122645136485967, 0.11610918877354863, 0.1061130334486736, 0.11303344867358708, 0.11918492887351019, 0.10149942329873125, 0.1111111111111111, 0.10457516339869281, 0.11572472126105345]
Actions to choose Agent 1: dict_values([{'num_count': 455, 'sum_payoffs': 142.43295441636346, 'action': [1.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 143.63087267729205, 'action': [1.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 153.92054905844515, 'action': [1.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 124.28418847301738, 'action': [0.0, 0.0]}, {'num_count': 394, 'sum_payoffs': 117.9071682711987, 'action': [0.0, 1.5707963267948966]}, {'num_count': 400, 'sum_payoffs': 120.15821149929826, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17493271818531334, 0.17608612072279892, 0.18569780853517878, 0.1576316801230296, 0.1514801999231065, 0.15378700499807765]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.46709632873535 s
