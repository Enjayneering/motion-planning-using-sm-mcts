Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 280, 'sum_payoffs': 85.55203122470455, 'action': [0.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 82.59569280007746, 'action': [0.0, -1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 94.03482058020094, 'action': [2.0, -1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 79.66087706557411, 'action': [0.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 94.98280225254297, 'action': [2.0, 0.0]}, {'num_count': 299, 'sum_payoffs': 93.62967427955739, 'action': [1.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 94.96507430866798, 'action': [2.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 91.1055626624443, 'action': [1.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 87.69795734547465, 'action': [1.0, 0.0]}])
Weights num count: [0.10765090349865436, 0.104959630911188, 0.11534025374855825, 0.10226835832372165, 0.11610918877354863, 0.11495578623606305, 0.11610918877354863, 0.11264898116109189, 0.10957324106113034]
Actions to choose Agent 1: dict_values([{'num_count': 414, 'sum_payoffs': 125.98805246874622, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 145.55366581346163, 'action': [1.0, 1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 142.6396025781379, 'action': [1.0, -1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 147.95806627203444, 'action': [1.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 121.20182136135732, 'action': [0.0, 1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 119.98503201243373, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15916955017301038, 0.1776239907727797, 0.17493271818531334, 0.17993079584775087, 0.15455594002306805, 0.15340253748558247]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.425435781478882 s
