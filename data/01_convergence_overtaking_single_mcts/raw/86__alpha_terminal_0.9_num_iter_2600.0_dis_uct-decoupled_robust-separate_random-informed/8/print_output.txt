Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 287, 'sum_payoffs': 88.32247837132644, 'action': [1.0, 0.0]}, {'num_count': 268, 'sum_payoffs': 80.2836218711039, 'action': [0.0, 1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 88.97885517474866, 'action': [1.0, -1.5707963267948966]}, {'num_count': 273, 'sum_payoffs': 82.39786958529058, 'action': [0.0, 0.0]}, {'num_count': 292, 'sum_payoffs': 90.42755652882484, 'action': [2.0, 0.0]}, {'num_count': 264, 'sum_payoffs': 78.491641028624, 'action': [0.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 101.09348193364438, 'action': [2.0, 1.5707963267948966]}, {'num_count': 314, 'sum_payoffs': 99.80936702295078, 'action': [2.0, -1.5707963267948966]}, {'num_count': 296, 'sum_payoffs': 92.12120958795543, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.11034217608612072, 0.10303729334871203, 0.1111111111111111, 0.104959630911188, 0.1122645136485967, 0.10149942329873125, 0.12187620146097655, 0.12072279892349097, 0.11380238369857747]
Actions to choose Agent 1: dict_values([{'num_count': 413, 'sum_payoffs': 125.47655826871085, 'action': [0.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 116.34300158296244, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 134.8108761301439, 'action': [1.0, -1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 114.69069258610008, 'action': [0.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 156.01368836606264, 'action': [1.0, 0.0]}, {'num_count': 487, 'sum_payoffs': 155.59552409314932, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1587850826605152, 0.14994232987312572, 0.16762783544790466, 0.14840445982314496, 0.18762014609765476, 0.18723567858515955]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.386502265930176 s
