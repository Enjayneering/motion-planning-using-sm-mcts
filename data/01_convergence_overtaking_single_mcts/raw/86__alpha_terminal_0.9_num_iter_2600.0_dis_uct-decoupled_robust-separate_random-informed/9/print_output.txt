Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 266, 'sum_payoffs': 79.57744835547888, 'action': [0.0, 1.5707963267948966]}, {'num_count': 293, 'sum_payoffs': 90.88685471089019, 'action': [1.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 87.52991754910418, 'action': [0.0, -1.5707963267948966]}, {'num_count': 295, 'sum_payoffs': 91.77477691680211, 'action': [2.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 88.52146544921453, 'action': [1.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 84.23684716126064, 'action': [1.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 99.12533143569154, 'action': [2.0, -1.5707963267948966]}, {'num_count': 305, 'sum_payoffs': 96.17757026332818, 'action': [2.0, 0.0]}, {'num_count': 280, 'sum_payoffs': 85.53144779279809, 'action': [0.0, 0.0]}])
Weights num count: [0.10226835832372165, 0.11264898116109189, 0.10957324106113034, 0.11341791618608228, 0.11034217608612072, 0.10649750096116878, 0.11995386389850057, 0.11726259131103421, 0.10765090349865436]
Actions to choose Agent 1: dict_values([{'num_count': 396, 'sum_payoffs': 118.59638022144352, 'action': [0.0, 1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 148.88762644481116, 'action': [1.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 118.63924940486953, 'action': [0.0, -1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 145.67693432961062, 'action': [1.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 143.2295786947879, 'action': [1.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 126.99397518963806, 'action': [0.0, 0.0]}])
Weights num count: [0.1522491349480969, 0.18108419838523646, 0.1522491349480969, 0.1780084582852749, 0.17570165321030373, 0.16032295271049596]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 30.20500349998474 s
