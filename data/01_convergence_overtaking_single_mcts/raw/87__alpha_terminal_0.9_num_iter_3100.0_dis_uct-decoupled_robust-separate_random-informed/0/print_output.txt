Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 333, 'sum_payoffs': 101.4715077665171, 'action': [0.0, 0.0]}, {'num_count': 365, 'sum_payoffs': 114.80915935816714, 'action': [2.0, 0.0]}, {'num_count': 314, 'sum_payoffs': 93.54552555084942, 'action': [0.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 115.61004492371947, 'action': [2.0, -1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 100.9867093282799, 'action': [1.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 98.53941990501687, 'action': [0.0, 1.5707963267948966]}, {'num_count': 354, 'sum_payoffs': 110.05923738335505, 'action': [1.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 114.78041860968496, 'action': [2.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 106.00258228428669, 'action': [1.0, 0.0]}])
Weights num count: [0.1073847146081909, 0.11770396646243148, 0.10125765881973557, 0.11834891970332151, 0.10706223798774589, 0.10512737826507579, 0.11415672363753628, 0.11770396646243148, 0.1109319574330861]
Actions to choose Agent 1: dict_values([{'num_count': 478, 'sum_payoffs': 143.40707120241694, 'action': [0.0, -1.5707963267948966]}, {'num_count': 583, 'sum_payoffs': 184.92271874301633, 'action': [1.0, 0.0]}, {'num_count': 527, 'sum_payoffs': 162.62841124126305, 'action': [1.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 139.47139600262426, 'action': [0.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 148.07024346772076, 'action': [0.0, 0.0]}, {'num_count': 554, 'sum_payoffs': 173.37495869942003, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15414382457271847, 0.18800386971944533, 0.16994517897452435, 0.1509190583682683, 0.1580135440180587, 0.17865204772653984]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.78557300567627 s
