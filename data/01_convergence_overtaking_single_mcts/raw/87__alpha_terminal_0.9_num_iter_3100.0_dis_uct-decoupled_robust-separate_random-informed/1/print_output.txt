Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 324, 'sum_payoffs': 97.50628048308693, 'action': [0.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 111.1932046750087, 'action': [2.0, 1.5707963267948966]}, {'num_count': 335, 'sum_payoffs': 102.14523567729093, 'action': [1.0, -1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 97.96951831950457, 'action': [0.0, 1.5707963267948966]}, {'num_count': 316, 'sum_payoffs': 94.23342045971647, 'action': [0.0, -1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 106.25219176397168, 'action': [1.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 108.23102141539891, 'action': [1.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 125.10519696853491, 'action': [2.0, 0.0]}, {'num_count': 358, 'sum_payoffs': 111.67597234217779, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10448242502418574, 0.11512415349887133, 0.10802966784908094, 0.10480490164463076, 0.1019026120606256, 0.11125443405353112, 0.11286681715575621, 0.12576588197355693, 0.11544663011931634]
Actions to choose Agent 1: dict_values([{'num_count': 470, 'sum_payoffs': 140.69242697861512, 'action': [0.0, 1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 155.1995389723706, 'action': [0.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 167.15329535015144, 'action': [1.0, -1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 167.09770662618607, 'action': [1.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 147.27816572275643, 'action': [0.0, -1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 177.1253805567558, 'action': [1.0, 0.0]}])
Weights num count: [0.15156401160915833, 0.163495646565624, 0.17316994517897452, 0.17316994517897452, 0.15704611415672365, 0.18123186069009997]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 36.03956389427185 s
