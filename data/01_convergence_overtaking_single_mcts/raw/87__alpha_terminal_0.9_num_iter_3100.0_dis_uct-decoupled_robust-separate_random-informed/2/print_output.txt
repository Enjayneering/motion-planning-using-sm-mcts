Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 352, 'sum_payoffs': 108.65743013202744, 'action': [1.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 108.31336206850717, 'action': [2.0, 1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 99.98234470728644, 'action': [0.0, -1.5707963267948966]}, {'num_count': 349, 'sum_payoffs': 107.41485729001282, 'action': [1.0, 1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 95.41193011889705, 'action': [0.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 97.02172819177162, 'action': [0.0, 0.0]}, {'num_count': 336, 'sum_payoffs': 102.08235962659595, 'action': [1.0, 0.0]}, {'num_count': 375, 'sum_payoffs': 118.33248787355001, 'action': [2.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 112.85697793112757, 'action': [2.0, 0.0]}])
Weights num count: [0.11351177039664624, 0.11318929377620122, 0.10673976136730087, 0.11254434053531119, 0.10319251854240567, 0.10448242502418574, 0.10835214446952596, 0.12092873266688164, 0.11673653660109642]
Actions to choose Agent 1: dict_values([{'num_count': 558, 'sum_payoffs': 174.87511821520258, 'action': [1.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 138.94776916215423, 'action': [0.0, 1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 177.21641808269797, 'action': [1.0, -1.5707963267948966]}, {'num_count': 557, 'sum_payoffs': 174.4798600542387, 'action': [1.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 145.50344707392716, 'action': [0.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 140.11538649686733, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1799419542083199, 0.15059658174782328, 0.18187681393099, 0.1796194775878749, 0.1560786842953886, 0.15156401160915833]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 33.07886552810669 s
