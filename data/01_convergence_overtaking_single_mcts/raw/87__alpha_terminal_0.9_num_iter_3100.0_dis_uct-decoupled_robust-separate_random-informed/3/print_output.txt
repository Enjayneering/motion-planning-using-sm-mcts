Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 338, 'sum_payoffs': 103.07693904933596, 'action': [1.0, 0.0]}, {'num_count': 332, 'sum_payoffs': 100.5509349995228, 'action': [0.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 103.76461835747506, 'action': [1.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 105.558912593607, 'action': [1.0, -1.5707963267948966]}, {'num_count': 317, 'sum_payoffs': 94.38764216947537, 'action': [0.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 124.81118188265104, 'action': [2.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 109.67862720925847, 'action': [2.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 110.119032134025, 'action': [2.0, 1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 99.6656256961168, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10899709771041599, 0.10706223798774589, 0.10964205095130602, 0.1109319574330861, 0.10222508868107062, 0.12576588197355693, 0.11415672363753628, 0.11447920025798129, 0.10641728474685586]
Actions to choose Agent 1: dict_values([{'num_count': 544, 'sum_payoffs': 169.3194330457484, 'action': [1.0, -1.5707963267948966]}, {'num_count': 474, 'sum_payoffs': 141.69330152758707, 'action': [0.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 141.32365779287116, 'action': [0.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 172.82344687111484, 'action': [1.0, 0.0]}, {'num_count': 556, 'sum_payoffs': 174.09336490475863, 'action': [1.0, 1.5707963267948966]}, {'num_count': 500, 'sum_payoffs': 151.90081043532498, 'action': [0.0, 0.0]}])
Weights num count: [0.17542728152208964, 0.1528539180909384, 0.15253144147049338, 0.17832957110609482, 0.17929700096742987, 0.16123831022250887]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.437766313552856 s
