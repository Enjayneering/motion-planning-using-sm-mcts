Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 334, 'sum_payoffs': 101.46707206327389, 'action': [1.0, 0.0]}, {'num_count': 326, 'sum_payoffs': 98.12620503379347, 'action': [0.0, -1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 99.32801758975809, 'action': [0.0, 0.0]}, {'num_count': 346, 'sum_payoffs': 106.38048122749585, 'action': [2.0, 1.5707963267948966]}, {'num_count': 345, 'sum_payoffs': 106.02716878405467, 'action': [1.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 110.19992870150652, 'action': [1.0, -1.5707963267948966]}, {'num_count': 389, 'sum_payoffs': 124.34000552439676, 'action': [2.0, 0.0]}, {'num_count': 316, 'sum_payoffs': 93.96822127125472, 'action': [0.0, 1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 112.25972199944104, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10770719122863592, 0.10512737826507579, 0.10609480812641084, 0.11157691067397614, 0.11125443405353112, 0.11447920025798129, 0.1254434053531119, 0.1019026120606256, 0.11609158336020639]
Actions to choose Agent 1: dict_values([{'num_count': 494, 'sum_payoffs': 149.66015182040576, 'action': [0.0, 0.0]}, {'num_count': 556, 'sum_payoffs': 174.27151609149928, 'action': [1.0, 0.0]}, {'num_count': 478, 'sum_payoffs': 143.4016558999342, 'action': [0.0, 1.5707963267948966]}, {'num_count': 533, 'sum_payoffs': 165.0523533977005, 'action': [1.0, -1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 169.8950195176827, 'action': [1.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 149.6886967145294, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15930345049983877, 0.17929700096742987, 0.15414382457271847, 0.17188003869719445, 0.17574975814253466, 0.15930345049983877]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.68746042251587 s
