Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 326, 'sum_payoffs': 98.26144166414281, 'action': [0.0, -1.5707963267948966]}, {'num_count': 331, 'sum_payoffs': 100.33498547454678, 'action': [1.0, 1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 96.66560807211464, 'action': [0.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 119.95103111893407, 'action': [2.0, 0.0]}, {'num_count': 342, 'sum_payoffs': 104.89607477597048, 'action': [1.0, -1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 116.52598975124792, 'action': [2.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 110.33174450644458, 'action': [1.0, 0.0]}, {'num_count': 351, 'sum_payoffs': 108.5320876928071, 'action': [2.0, 1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 97.90962140875844, 'action': [0.0, 0.0]}])
Weights num count: [0.10512737826507579, 0.10673976136730087, 0.1038374717832957, 0.12189616252821671, 0.11028700419219607, 0.11931634956465656, 0.11447920025798129, 0.11318929377620122, 0.10480490164463076]
Actions to choose Agent 1: dict_values([{'num_count': 479, 'sum_payoffs': 144.01046546461652, 'action': [0.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 147.9330662009299, 'action': [0.0, 0.0]}, {'num_count': 568, 'sum_payoffs': 179.32527150846735, 'action': [1.0, 0.0]}, {'num_count': 536, 'sum_payoffs': 166.58878728358565, 'action': [1.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 146.01554844610936, 'action': [0.0, -1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 169.7468180854308, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15446630119316349, 0.15769106739761368, 0.18316672041277007, 0.1728474685585295, 0.1560786842953886, 0.17542728152208964]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 33.985257387161255 s
