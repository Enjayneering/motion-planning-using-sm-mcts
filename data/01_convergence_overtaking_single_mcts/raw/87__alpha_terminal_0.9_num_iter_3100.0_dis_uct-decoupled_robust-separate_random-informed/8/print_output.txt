Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 338, 'sum_payoffs': 103.40281002920823, 'action': [1.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 97.05638584672357, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 103.32091995494063, 'action': [0.0, -1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 105.84827778154046, 'action': [1.0, -1.5707963267948966]}, {'num_count': 313, 'sum_payoffs': 93.0718331428741, 'action': [0.0, 1.5707963267948966]}, {'num_count': 340, 'sum_payoffs': 104.06316765127202, 'action': [1.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 120.42158250330606, 'action': [2.0, 0.0]}, {'num_count': 356, 'sum_payoffs': 110.8586770508834, 'action': [2.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 116.29128780493023, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10899709771041599, 0.10415994840374072, 0.10899709771041599, 0.1109319574330861, 0.10093518219929055, 0.10964205095130602, 0.12221863914866173, 0.11480167687842631, 0.11899387294421154]
Actions to choose Agent 1: dict_values([{'num_count': 559, 'sum_payoffs': 175.83650992897495, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 145.25810490727267, 'action': [0.0, -1.5707963267948966]}, {'num_count': 537, 'sum_payoffs': 167.06781087950955, 'action': [1.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 139.83100510648433, 'action': [0.0, 1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 148.07108317599847, 'action': [0.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 178.23032827297584, 'action': [1.0, 0.0]}])
Weights num count: [0.18026443082876492, 0.15543373105449854, 0.17316994517897452, 0.1509190583682683, 0.15769106739761368, 0.18219929055143502]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.989542722702026 s
