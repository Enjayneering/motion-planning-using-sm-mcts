Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 321, 'sum_payoffs': 96.55950498650701, 'action': [0.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 113.63499366538205, 'action': [2.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 111.50266544597774, 'action': [1.0, -1.5707963267948966]}, {'num_count': 342, 'sum_payoffs': 105.20370112309695, 'action': [1.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 99.4872316754282, 'action': [0.0, -1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 110.57115056630434, 'action': [2.0, 1.5707963267948966]}, {'num_count': 329, 'sum_payoffs': 99.86810237681509, 'action': [0.0, 0.0]}, {'num_count': 340, 'sum_payoffs': 104.38663122973078, 'action': [1.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 115.31914676509281, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10351499516285069, 0.11673653660109642, 0.11512415349887133, 0.11028700419219607, 0.10577233150596582, 0.11447920025798129, 0.10609480812641084, 0.10964205095130602, 0.11802644308287649]
Actions to choose Agent 1: dict_values([{'num_count': 569, 'sum_payoffs': 179.24854382220866, 'action': [1.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 142.84574922463827, 'action': [0.0, -1.5707963267948966]}, {'num_count': 547, 'sum_payoffs': 170.43885367585213, 'action': [1.0, -1.5707963267948966]}, {'num_count': 525, 'sum_payoffs': 161.76833355610486, 'action': [1.0, 1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 145.1304194535776, 'action': [0.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 151.44215621702975, 'action': [0.0, 0.0]}])
Weights num count: [0.1834891970332151, 0.15382134795227345, 0.1763947113834247, 0.16930022573363432, 0.15575620767494355, 0.16091583360206385]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 31.646064043045044 s
