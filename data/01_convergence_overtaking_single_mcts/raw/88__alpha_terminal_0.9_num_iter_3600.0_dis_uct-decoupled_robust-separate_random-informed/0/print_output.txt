Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 404, 'sum_payoffs': 123.79388201570949, 'action': [2.0, -1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 126.6724033341031, 'action': [1.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 124.62527995931006, 'action': [2.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 108.87007350059594, 'action': [0.0, 1.5707963267948966]}, {'num_count': 388, 'sum_payoffs': 117.39007337070471, 'action': [0.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 118.95399664178572, 'action': [0.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 134.11346820117998, 'action': [2.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 125.52370580544586, 'action': [1.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 120.12155112017338, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1121910580394335, 0.11413496251041377, 0.11274645931685642, 0.10191613440710913, 0.10774784782004998, 0.10885865037489587, 0.11913357400722022, 0.11330186059427937, 0.10969175229103027]
Actions to choose Agent 1: dict_values([{'num_count': 549, 'sum_payoffs': 163.35556141335346, 'action': [0.0, -1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 196.65334579887963, 'action': [1.0, -1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 193.97706970224812, 'action': [1.0, 1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 171.52210621663147, 'action': [0.0, 1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 203.7225893084148, 'action': [1.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 169.5445134576182, 'action': [0.0, 0.0]}])
Weights num count: [0.1524576506525965, 0.17633990558178284, 0.17439600111080256, 0.15828936406553734, 0.18133851707858928, 0.15690086087198002]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 36.84766149520874 s
