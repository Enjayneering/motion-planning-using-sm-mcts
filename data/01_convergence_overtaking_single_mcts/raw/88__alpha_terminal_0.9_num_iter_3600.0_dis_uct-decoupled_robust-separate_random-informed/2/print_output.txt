Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 407, 'sum_payoffs': 125.24784213256189, 'action': [2.0, 1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 113.7528430774533, 'action': [0.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 119.93813101578071, 'action': [1.0, 0.0]}, {'num_count': 373, 'sum_payoffs': 111.40464524488819, 'action': [0.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 136.25169708325723, 'action': [2.0, 0.0]}, {'num_count': 383, 'sum_payoffs': 115.244687853798, 'action': [0.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 131.33928241354488, 'action': [2.0, -1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 126.4112551275658, 'action': [1.0, -1.5707963267948966]}, {'num_count': 398, 'sum_payoffs': 121.56618640800487, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1130241599555679, 0.10524854207164676, 0.1094140516523188, 0.10358233823937794, 0.12052207720077757, 0.10635934462649264, 0.11718966953623994, 0.11385726187170231, 0.11052485420716468]
Actions to choose Agent 1: dict_values([{'num_count': 546, 'sum_payoffs': 162.6957074800443, 'action': [0.0, 1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 212.15630919481208, 'action': [1.0, 0.0]}, {'num_count': 552, 'sum_payoffs': 164.95825711195164, 'action': [0.0, -1.5707963267948966]}, {'num_count': 622, 'sum_payoffs': 192.0807050667206, 'action': [1.0, -1.5707963267948966]}, {'num_count': 644, 'sum_payoffs': 200.7454473319632, 'action': [1.0, 1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 169.25918342349524, 'action': [0.0, 0.0]}])
Weights num count: [0.15162454873646208, 0.18689252985281865, 0.1532907525687309, 0.17272979727853374, 0.17883921133018607, 0.15634545959455706]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.431618452072144 s
