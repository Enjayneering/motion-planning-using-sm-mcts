Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 388, 'sum_payoffs': 117.03024230138023, 'action': [1.0, 1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 111.00205844530448, 'action': [0.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 139.90039409829808, 'action': [2.0, 0.0]}, {'num_count': 380, 'sum_payoffs': 113.82241741372435, 'action': [1.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 113.0208289537588, 'action': [0.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 126.29150117111713, 'action': [2.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 122.74322238404615, 'action': [1.0, -1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 134.12652185978138, 'action': [2.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 119.46603888930788, 'action': [0.0, 0.0]}])
Weights num count: [0.10774784782004998, 0.10358233823937794, 0.12329908358789225, 0.10552624271035824, 0.1049708414329353, 0.11413496251041377, 0.11163565676201055, 0.11941127464593168, 0.1094140516523188]
Actions to choose Agent 1: dict_values([{'num_count': 639, 'sum_payoffs': 198.3501229075857, 'action': [1.0, -1.5707963267948966]}, {'num_count': 575, 'sum_payoffs': 173.50576635777264, 'action': [0.0, 0.0]}, {'num_count': 658, 'sum_payoffs': 205.82124239802403, 'action': [1.0, 0.0]}, {'num_count': 639, 'sum_payoffs': 198.35361802676138, 'action': [1.0, 1.5707963267948966]}, {'num_count': 541, 'sum_payoffs': 160.46261839656725, 'action': [0.0, -1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 163.18757130107394, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17745070813662872, 0.1596778672590947, 0.18272702027214663, 0.17745070813662872, 0.15023604554290476, 0.15217995001388504]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.1991822719574 s
