Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 386, 'sum_payoffs': 116.24753850331004, 'action': [0.0, -1.5707963267948966]}, {'num_count': 371, 'sum_payoffs': 110.2451599972907, 'action': [0.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 118.31767028920994, 'action': [1.0, 0.0]}, {'num_count': 382, 'sum_payoffs': 114.72568754273601, 'action': [0.0, 0.0]}, {'num_count': 387, 'sum_payoffs': 116.75310846555602, 'action': [1.0, 1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 127.57847582525432, 'action': [2.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 131.63590380503254, 'action': [2.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 127.18962410745921, 'action': [1.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 135.06929124851527, 'action': [2.0, 0.0]}])
Weights num count: [0.10719244654262705, 0.10302693696195502, 0.10858094973618439, 0.10608164398778117, 0.10747014718133852, 0.11496806442654818, 0.11774507081366287, 0.11469036378783672, 0.11996667592335462]
Actions to choose Agent 1: dict_values([{'num_count': 637, 'sum_payoffs': 197.6377031871781, 'action': [1.0, -1.5707963267948966]}, {'num_count': 557, 'sum_payoffs': 166.65951979380327, 'action': [0.0, 1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 159.20073524852478, 'action': [0.0, -1.5707963267948966]}, {'num_count': 654, 'sum_payoffs': 204.17357287510777, 'action': [1.0, 1.5707963267948966]}, {'num_count': 647, 'sum_payoffs': 201.56468994723159, 'action': [1.0, 0.0]}, {'num_count': 567, 'sum_payoffs': 170.51300042042163, 'action': [0.0, 0.0]}])
Weights num count: [0.17689530685920576, 0.15467925576228825, 0.14940294362677034, 0.18161621771730074, 0.17967231324632046, 0.15745626214940295]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 36.17485284805298 s
