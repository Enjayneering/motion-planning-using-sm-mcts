Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 363, 'sum_payoffs': 106.94013127238752, 'action': [0.0, 1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 118.19804809054843, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 126.74914270956512, 'action': [2.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 112.84434925516715, 'action': [0.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 119.84300554358448, 'action': [1.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 121.07097048573176, 'action': [0.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 128.68859364247413, 'action': [2.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 125.46353007038178, 'action': [1.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 136.96296181211144, 'action': [2.0, 0.0]}])
Weights num count: [0.10080533185226326, 0.10858094973618439, 0.11441266314912524, 0.1049708414329353, 0.10969175229103027, 0.11052485420716468, 0.11580116634268259, 0.11357956123299083, 0.12135517911691197]
Actions to choose Agent 1: dict_values([{'num_count': 677, 'sum_payoffs': 214.20478927314718, 'action': [1.0, 0.0]}, {'num_count': 553, 'sum_payoffs': 165.79602219709125, 'action': [0.0, -1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 194.19858690654095, 'action': [1.0, 1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 175.8865376574014, 'action': [0.0, 0.0]}, {'num_count': 623, 'sum_payoffs': 193.02940128525282, 'action': [1.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 161.54972590471493, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18800333240766454, 0.1535684532074424, 0.17384059983337963, 0.16078866981394058, 0.1730074979172452, 0.15051374618161623]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 36.17369627952576 s
