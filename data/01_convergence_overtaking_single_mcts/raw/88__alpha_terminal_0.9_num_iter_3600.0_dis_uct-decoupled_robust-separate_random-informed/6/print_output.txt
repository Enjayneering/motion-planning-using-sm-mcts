Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 433, 'sum_payoffs': 136.09389137544622, 'action': [2.0, -1.5707963267948966]}, {'num_count': 396, 'sum_payoffs': 120.8915398475634, 'action': [1.0, -1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 135.58514105230017, 'action': [2.0, 0.0]}, {'num_count': 381, 'sum_payoffs': 114.71884429653528, 'action': [0.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 118.40565268289507, 'action': [1.0, 1.5707963267948966]}, {'num_count': 361, 'sum_payoffs': 106.74386219872845, 'action': [0.0, 1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 129.48344962461977, 'action': [2.0, 1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 114.35681587385253, 'action': [0.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 126.60523843778263, 'action': [1.0, 0.0]}])
Weights num count: [0.12024437656206609, 0.10996945292974174, 0.11996667592335462, 0.1058039433490697, 0.10830324909747292, 0.10024993057484032, 0.11580116634268259, 0.10552624271035824, 0.11385726187170231]
Actions to choose Agent 1: dict_values([{'num_count': 565, 'sum_payoffs': 169.61034446575871, 'action': [0.0, 0.0]}, {'num_count': 682, 'sum_payoffs': 215.20801536911893, 'action': [1.0, 0.0]}, {'num_count': 552, 'sum_payoffs': 164.65442338663493, 'action': [0.0, 1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 187.9149312194815, 'action': [1.0, -1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 197.18669871695167, 'action': [1.0, 1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 164.95605132068405, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15690086087198002, 0.1893918356012219, 0.1532907525687309, 0.16995279089141904, 0.1766176062204943, 0.1535684532074424]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.3983359336853 s
