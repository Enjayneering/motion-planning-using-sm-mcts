Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 371, 'sum_payoffs': 110.27853237340551, 'action': [0.0, -1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 109.87617239007422, 'action': [0.0, 1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 125.20073666744659, 'action': [2.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 139.15162863677887, 'action': [2.0, 0.0]}, {'num_count': 384, 'sum_payoffs': 115.56308061716517, 'action': [0.0, 0.0]}, {'num_count': 410, 'sum_payoffs': 126.05243758893374, 'action': [1.0, 0.0]}, {'num_count': 415, 'sum_payoffs': 128.0987151362102, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 127.73605099729807, 'action': [2.0, 1.5707963267948966]}, {'num_count': 386, 'sum_payoffs': 116.33103656908379, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10302693696195502, 0.10274923632324354, 0.11330186059427937, 0.12274368231046931, 0.10663704526520411, 0.11385726187170231, 0.11524576506525964, 0.11496806442654818, 0.10719244654262705]
Actions to choose Agent 1: dict_values([{'num_count': 643, 'sum_payoffs': 200.25325357269463, 'action': [1.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 160.34764815792988, 'action': [0.0, 1.5707963267948966]}, {'num_count': 637, 'sum_payoffs': 197.9623728661939, 'action': [1.0, 1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 197.59939910230406, 'action': [1.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 170.00033516175705, 'action': [0.0, -1.5707963267948966]}, {'num_count': 579, 'sum_payoffs': 175.3438026963558, 'action': [0.0, 0.0]}])
Weights num count: [0.17856151069147458, 0.14995834490419327, 0.17689530685920576, 0.1766176062204943, 0.15690086087198002, 0.16078866981394058]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.50274205207825 s
