Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 395, 'sum_payoffs': 120.03372744649592, 'action': [1.0, 0.0]}, {'num_count': 372, 'sum_payoffs': 110.65773108371847, 'action': [0.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 147.07922339473706, 'action': [2.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 123.37311463724723, 'action': [1.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 109.61917217929842, 'action': [0.0, 1.5707963267948966]}, {'num_count': 393, 'sum_payoffs': 119.24351205684853, 'action': [1.0, -1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 131.06189851220992, 'action': [2.0, -1.5707963267948966]}, {'num_count': 395, 'sum_payoffs': 120.06014279322436, 'action': [2.0, 1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 118.08700519465411, 'action': [0.0, 0.0]}])
Weights num count: [0.10969175229103027, 0.10330463760066648, 0.12801999444598722, 0.11191335740072202, 0.10247153568453207, 0.10913635101360733, 0.11718966953623994, 0.10969175229103027, 0.10830324909747292]
Actions to choose Agent 1: dict_values([{'num_count': 547, 'sum_payoffs': 163.26086243309214, 'action': [0.0, 1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 172.21328361886407, 'action': [0.0, 0.0]}, {'num_count': 636, 'sum_payoffs': 197.78039965734183, 'action': [1.0, 1.5707963267948966]}, {'num_count': 664, 'sum_payoffs': 208.91391278735247, 'action': [1.0, 0.0]}, {'num_count': 565, 'sum_payoffs': 170.30128773917414, 'action': [0.0, -1.5707963267948966]}, {'num_count': 618, 'sum_payoffs': 190.8005633880641, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15190224937517358, 0.15828936406553734, 0.1766176062204943, 0.18439322410441544, 0.15690086087198002, 0.17161899472368786]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.950621604919434 s
