Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 458, 'sum_payoffs': 139.28094964642514, 'action': [1.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 134.90735816657798, 'action': [0.0, 0.0]}, {'num_count': 488, 'sum_payoffs': 151.19916512657335, 'action': [2.0, 0.0]}, {'num_count': 445, 'sum_payoffs': 134.01361744470236, 'action': [1.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 121.2581009286221, 'action': [0.0, 1.5707963267948966]}, {'num_count': 433, 'sum_payoffs': 129.2097580584377, 'action': [0.0, -1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 139.66354630208124, 'action': [2.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 133.9874098594423, 'action': [1.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 160.8987918406546, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11168007802974884, 0.10899780541331383, 0.11899536698366252, 0.10851011948305292, 0.10070714459887832, 0.10558400390148744, 0.1119239209948793, 0.10851011948305292, 0.12484759814679347]
Actions to choose Agent 1: dict_values([{'num_count': 610, 'sum_payoffs': 179.805725377123, 'action': [0.0, -1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 222.38724244372992, 'action': [1.0, -1.5707963267948966]}, {'num_count': 732, 'sum_payoffs': 226.32509702308693, 'action': [1.0, 1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 183.57856668491277, 'action': [0.0, 1.5707963267948966]}, {'num_count': 765, 'sum_payoffs': 238.97492443513187, 'action': [1.0, 0.0]}, {'num_count': 651, 'sum_payoffs': 195.34308938683273, 'action': [0.0, 0.0]}])
Weights num count: [0.14874420872957816, 0.1760546208241892, 0.17849305047549377, 0.15118263838088272, 0.18653986832479882, 0.15874177029992684]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.445494413375854 s
