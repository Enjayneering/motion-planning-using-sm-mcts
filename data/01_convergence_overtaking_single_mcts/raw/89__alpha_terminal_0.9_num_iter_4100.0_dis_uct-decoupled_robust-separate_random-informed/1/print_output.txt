Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 429, 'sum_payoffs': 127.48768464256933, 'action': [0.0, -1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 135.80485628110756, 'action': [1.0, 0.0]}, {'num_count': 497, 'sum_payoffs': 154.60409005809635, 'action': [2.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 136.5870566345542, 'action': [1.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 149.41933192637526, 'action': [2.0, -1.5707963267948966]}, {'num_count': 418, 'sum_payoffs': 123.06498369813414, 'action': [0.0, 1.5707963267948966]}, {'num_count': 443, 'sum_payoffs': 133.02062425215746, 'action': [0.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 137.3858281403674, 'action': [1.0, 1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 144.99419490653162, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10460863204096561, 0.1097293343087052, 0.12118995366983662, 0.11021702023896611, 0.1180199951231407, 0.1019263594245306, 0.10802243355279201, 0.11070470616922702, 0.11533772250670568]
Actions to choose Agent 1: dict_values([{'num_count': 763, 'sum_payoffs': 238.73451220687647, 'action': [1.0, 0.0]}, {'num_count': 630, 'sum_payoffs': 187.73481093303283, 'action': [0.0, 1.5707963267948966]}, {'num_count': 630, 'sum_payoffs': 187.70250629099417, 'action': [0.0, -1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 222.54313015976373, 'action': [1.0, -1.5707963267948966]}, {'num_count': 665, 'sum_payoffs': 201.10004743199963, 'action': [0.0, 0.0]}, {'num_count': 691, 'sum_payoffs': 210.9115891515544, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1860521823945379, 0.15362106803218728, 0.15362106803218728, 0.17581077785905877, 0.16215557181175322, 0.1684954889051451]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.5259473323822 s
