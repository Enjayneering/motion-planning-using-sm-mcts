Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 451, 'sum_payoffs': 136.15610954632754, 'action': [1.0, 1.5707963267948966]}, {'num_count': 410, 'sum_payoffs': 119.92455870709412, 'action': [0.0, -1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 133.7487973729722, 'action': [0.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 147.70644590249518, 'action': [2.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 142.09287349497447, 'action': [2.0, 1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 141.72529956449125, 'action': [1.0, -1.5707963267948966]}, {'num_count': 444, 'sum_payoffs': 133.29322201113436, 'action': [1.0, 0.0]}, {'num_count': 422, 'sum_payoffs': 124.71790504504344, 'action': [0.0, 1.5707963267948966]}, {'num_count': 517, 'sum_payoffs': 162.669214665137, 'action': [2.0, 0.0]}])
Weights num count: [0.10997317727383565, 0.09997561570348695, 0.10851011948305292, 0.11704462326261887, 0.11363082175079249, 0.11338697878566203, 0.10826627651792246, 0.10290173128505242, 0.12606681297244574]
Actions to choose Agent 1: dict_values([{'num_count': 633, 'sum_payoffs': 187.90609132013378, 'action': [0.0, -1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 187.44966205417808, 'action': [0.0, 0.0]}, {'num_count': 755, 'sum_payoffs': 234.33483668501225, 'action': [1.0, 0.0]}, {'num_count': 729, 'sum_payoffs': 224.5007889668321, 'action': [1.0, -1.5707963267948966]}, {'num_count': 715, 'sum_payoffs': 219.15817451414003, 'action': [1.0, 1.5707963267948966]}, {'num_count': 636, 'sum_payoffs': 189.0651973968862, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15435259692757863, 0.1541087539624482, 0.18410143867349427, 0.17776152158010242, 0.17434772006827604, 0.15508412582297001]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.925599336624146 s
