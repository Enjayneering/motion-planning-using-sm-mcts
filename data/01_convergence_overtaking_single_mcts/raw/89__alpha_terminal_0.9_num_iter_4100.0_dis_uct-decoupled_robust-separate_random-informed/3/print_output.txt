Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 438, 'sum_payoffs': 131.43576310969652, 'action': [1.0, -1.5707963267948966]}, {'num_count': 466, 'sum_payoffs': 142.50039272639526, 'action': [1.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 148.1566512972501, 'action': [2.0, -1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 124.70352471310349, 'action': [0.0, 1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 125.84012524105142, 'action': [0.0, 0.0]}, {'num_count': 480, 'sum_payoffs': 148.16908237941786, 'action': [2.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 156.64279988953876, 'action': [2.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 140.16152123064452, 'action': [1.0, 0.0]}, {'num_count': 430, 'sum_payoffs': 128.1647891731519, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10680321872713973, 0.11363082175079249, 0.11704462326261887, 0.10265788831992197, 0.10338941721531333, 0.11704462326261887, 0.12216532553035844, 0.11216776396000976, 0.10485247500609607]
Actions to choose Agent 1: dict_values([{'num_count': 629, 'sum_payoffs': 187.20790702305237, 'action': [0.0, -1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 222.7012650670026, 'action': [1.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 185.55640657302678, 'action': [0.0, 1.5707963267948966]}, {'num_count': 756, 'sum_payoffs': 235.85401370599533, 'action': [1.0, 0.0]}, {'num_count': 671, 'sum_payoffs': 203.17850128513757, 'action': [0.0, 0.0]}, {'num_count': 697, 'sum_payoffs': 213.0455544862022, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1533772250670568, 0.1760546208241892, 0.152401853206535, 0.18434528163862474, 0.16361862960253595, 0.16995854669592783]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.585795402526855 s
