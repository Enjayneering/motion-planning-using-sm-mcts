Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 419, 'sum_payoffs': 123.62224497603667, 'action': [0.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 131.66947001677534, 'action': [1.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 156.37083900428019, 'action': [2.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 145.16988715090147, 'action': [2.0, 1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 143.56417122790845, 'action': [1.0, -1.5707963267948966]}, {'num_count': 472, 'sum_payoffs': 144.8070566488466, 'action': [2.0, -1.5707963267948966]}, {'num_count': 464, 'sum_payoffs': 141.6314147316623, 'action': [1.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 124.48323013030428, 'action': [0.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 132.78517679216353, 'action': [0.0, 0.0]}])
Weights num count: [0.10217020238966106, 0.10704706169227018, 0.12216532553035844, 0.11533772250670568, 0.11436235064618386, 0.11509387954157523, 0.11314313582053158, 0.10265788831992197, 0.10777859058766155]
Actions to choose Agent 1: dict_values([{'num_count': 613, 'sum_payoffs': 180.96702587520207, 'action': [0.0, -1.5707963267948966]}, {'num_count': 729, 'sum_payoffs': 225.12581846001876, 'action': [1.0, 1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 198.01740545791657, 'action': [0.0, 0.0]}, {'num_count': 719, 'sum_payoffs': 221.31227253072228, 'action': [1.0, -1.5707963267948966]}, {'num_count': 627, 'sum_payoffs': 186.26069027948049, 'action': [0.0, 1.5707963267948966]}, {'num_count': 754, 'sum_payoffs': 234.80959002629146, 'action': [1.0, 0.0]}])
Weights num count: [0.14947573762496952, 0.17776152158010242, 0.16044867105584004, 0.17532309192879786, 0.1528895391367959, 0.18385759570836382]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.71548247337341 s
