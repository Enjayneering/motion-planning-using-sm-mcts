Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 448, 'sum_payoffs': 135.12779775188184, 'action': [1.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 142.8116071236424, 'action': [2.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 151.96863653133764, 'action': [2.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 120.54847546064786, 'action': [0.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 137.0924398079682, 'action': [1.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 152.03095376264207, 'action': [2.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 143.59084408584448, 'action': [1.0, 0.0]}, {'num_count': 420, 'sum_payoffs': 124.06221145440595, 'action': [0.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 136.7266974059489, 'action': [0.0, 0.0]}])
Weights num count: [0.10924164837844429, 0.11387466471592295, 0.11948305291392343, 0.10021945866861741, 0.11046086320409657, 0.11948305291392343, 0.11436235064618386, 0.10241404535479151, 0.11021702023896611]
Actions to choose Agent 1: dict_values([{'num_count': 623, 'sum_payoffs': 184.0757359664029, 'action': [0.0, 1.5707963267948966]}, {'num_count': 633, 'sum_payoffs': 187.9779877743644, 'action': [0.0, -1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 221.45976990334236, 'action': [1.0, -1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 196.30401735742544, 'action': [0.0, 0.0]}, {'num_count': 754, 'sum_payoffs': 234.228911053268, 'action': [1.0, 0.0]}, {'num_count': 714, 'sum_payoffs': 218.85873032785324, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15191416727627408, 0.15435259692757863, 0.17581077785905877, 0.15971714216044866, 0.18385759570836382, 0.17410387710314557]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.10011386871338 s
