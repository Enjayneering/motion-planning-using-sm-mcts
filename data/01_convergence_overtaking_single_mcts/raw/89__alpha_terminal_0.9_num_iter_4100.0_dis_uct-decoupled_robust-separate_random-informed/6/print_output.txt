Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 443, 'sum_payoffs': 133.5879545333721, 'action': [1.0, 0.0]}, {'num_count': 469, 'sum_payoffs': 143.99712551166834, 'action': [2.0, 1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 164.90604560143026, 'action': [2.0, 0.0]}, {'num_count': 443, 'sum_payoffs': 133.53917517742153, 'action': [1.0, -1.5707963267948966]}, {'num_count': 430, 'sum_payoffs': 128.30422376988352, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 135.47238897145544, 'action': [1.0, 1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 153.94532497828186, 'action': [2.0, -1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 132.38757600437404, 'action': [0.0, 0.0]}, {'num_count': 412, 'sum_payoffs': 121.25001189392529, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10802243355279201, 0.11436235064618386, 0.12704218483296756, 0.10802243355279201, 0.10485247500609607, 0.10924164837844429, 0.12045842477444525, 0.10729090465740064, 0.10046330163374786]
Actions to choose Agent 1: dict_values([{'num_count': 643, 'sum_payoffs': 191.5872012365158, 'action': [0.0, -1.5707963267948966]}, {'num_count': 672, 'sum_payoffs': 202.65835445381072, 'action': [0.0, 0.0]}, {'num_count': 691, 'sum_payoffs': 209.84673615552956, 'action': [1.0, 1.5707963267948966]}, {'num_count': 728, 'sum_payoffs': 223.9437967519447, 'action': [1.0, -1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 179.93173352371437, 'action': [0.0, 1.5707963267948966]}, {'num_count': 754, 'sum_payoffs': 233.9315235031815, 'action': [1.0, 0.0]}])
Weights num count: [0.1567910265788832, 0.16386247256766642, 0.1684954889051451, 0.17751767861497195, 0.14923189465983908, 0.18385759570836382]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.774012327194214 s
