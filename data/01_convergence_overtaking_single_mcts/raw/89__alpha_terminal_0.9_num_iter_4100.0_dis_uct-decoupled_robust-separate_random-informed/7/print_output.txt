Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 454, 'sum_payoffs': 137.51728256303042, 'action': [1.0, -1.5707963267948966]}, {'num_count': 491, 'sum_payoffs': 152.33244014926532, 'action': [2.0, -1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 126.29985547309995, 'action': [0.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 127.22761760365186, 'action': [0.0, 1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 161.26267477481898, 'action': [2.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 143.8838575641564, 'action': [2.0, 1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 138.68167967152294, 'action': [1.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 131.9458054089997, 'action': [1.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 124.41443226116434, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.11070470616922702, 0.11972689587905389, 0.10387710314557425, 0.10436478907583516, 0.1250914411119239, 0.11460619361131431, 0.11143623506461839, 0.10729090465740064, 0.10265788831992197]
Actions to choose Agent 1: dict_values([{'num_count': 775, 'sum_payoffs': 243.51040173055148, 'action': [1.0, 0.0]}, {'num_count': 612, 'sum_payoffs': 181.0684258128645, 'action': [0.0, 1.5707963267948966]}, {'num_count': 730, 'sum_payoffs': 226.132636374269, 'action': [1.0, -1.5707963267948966]}, {'num_count': 722, 'sum_payoffs': 222.8935756801973, 'action': [1.0, 1.5707963267948966]}, {'num_count': 639, 'sum_payoffs': 191.26988914603527, 'action': [0.0, 0.0]}, {'num_count': 622, 'sum_payoffs': 184.73092216367507, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18897829797610338, 0.14923189465983908, 0.17800536454523286, 0.1760546208241892, 0.15581565471836137, 0.15167032431114363]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 45.12323617935181 s
