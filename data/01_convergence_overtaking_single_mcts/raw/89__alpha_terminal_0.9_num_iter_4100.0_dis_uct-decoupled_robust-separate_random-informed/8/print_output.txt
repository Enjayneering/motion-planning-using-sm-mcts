Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 492, 'sum_payoffs': 153.43995047302312, 'action': [2.0, 0.0]}, {'num_count': 444, 'sum_payoffs': 134.16956196588433, 'action': [0.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 141.62444144602648, 'action': [1.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 124.9379059649109, 'action': [0.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 134.85212752244217, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 142.50999086522236, 'action': [1.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 117.7819564118757, 'action': [0.0, 1.5707963267948966]}, {'num_count': 499, 'sum_payoffs': 156.2731040571383, 'action': [2.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 143.35354395339115, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11997073884418434, 0.10826627651792246, 0.11289929285540112, 0.10265788831992197, 0.10875396244818337, 0.11338697878566203, 0.09826871494757376, 0.12167763960009753, 0.11387466471592295]
Actions to choose Agent 1: dict_values([{'num_count': 627, 'sum_payoffs': 186.0685787975432, 'action': [0.0, -1.5707963267948966]}, {'num_count': 626, 'sum_payoffs': 185.66393308222322, 'action': [0.0, 1.5707963267948966]}, {'num_count': 663, 'sum_payoffs': 199.6905153722326, 'action': [0.0, 0.0]}, {'num_count': 709, 'sum_payoffs': 217.20844091158125, 'action': [1.0, 1.5707963267948966]}, {'num_count': 716, 'sum_payoffs': 219.91864645800618, 'action': [1.0, -1.5707963267948966]}, {'num_count': 759, 'sum_payoffs': 236.49498217907112, 'action': [1.0, 0.0]}])
Weights num count: [0.1528895391367959, 0.15264569617166546, 0.1616678858814923, 0.1728846622774933, 0.17459156303340648, 0.1850768105340161]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.825074434280396 s
