Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 435, 'sum_payoffs': 130.11757874336465, 'action': [0.0, -1.5707963267948966]}, {'num_count': 452, 'sum_payoffs': 136.9284857329114, 'action': [1.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 162.2839496008235, 'action': [2.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 149.0047456563103, 'action': [2.0, -1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 127.05675245659415, 'action': [0.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 131.7066316050613, 'action': [1.0, 1.5707963267948966]}, {'num_count': 438, 'sum_payoffs': 131.339578737319, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 140.8714614136685, 'action': [2.0, 1.5707963267948966]}, {'num_count': 450, 'sum_payoffs': 136.10809747809174, 'action': [1.0, 0.0]}])
Weights num count: [0.10607168983174835, 0.11021702023896611, 0.12557912704218482, 0.11753230919287978, 0.1041209461107047, 0.10704706169227018, 0.10680321872713973, 0.11265544989027067, 0.1097293343087052]
Actions to choose Agent 1: dict_values([{'num_count': 692, 'sum_payoffs': 211.04320207848937, 'action': [1.0, 1.5707963267948966]}, {'num_count': 733, 'sum_payoffs': 226.81672540249184, 'action': [1.0, -1.5707963267948966]}, {'num_count': 778, 'sum_payoffs': 244.20910808420166, 'action': [1.0, 0.0]}, {'num_count': 622, 'sum_payoffs': 184.3719198208523, 'action': [0.0, 1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 194.20321784157937, 'action': [0.0, 0.0]}, {'num_count': 627, 'sum_payoffs': 186.35537226026446, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.16873933187027554, 0.17873689344062424, 0.18970982687149476, 0.15167032431114363, 0.15801024140453548, 0.1528895391367959]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.52454423904419 s
