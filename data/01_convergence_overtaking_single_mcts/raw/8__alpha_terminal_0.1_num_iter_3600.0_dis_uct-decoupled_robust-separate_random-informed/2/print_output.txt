Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 483, 'sum_payoffs': 134.72263865820662, 'action': [2.0, 1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 102.06896550023066, 'action': [1.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 102.12893551521319, 'action': [1.0, 1.5707963267948966]}, {'num_count': 486, 'sum_payoffs': 135.8320839353831, 'action': [2.0, -1.5707963267948966]}, {'num_count': 481, 'sum_payoffs': 134.02298848341076, 'action': [2.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 76.85157420008528, 'action': [0.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 101.40929533542307, 'action': [1.0, 0.0]}, {'num_count': 323, 'sum_payoffs': 76.87156420507945, 'action': [0.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 77.20139928748326, 'action': [0.0, 0.0]}])
Weights num count: [0.13412940849763955, 0.1094140516523188, 0.1094140516523188, 0.13496251041377394, 0.13357400722021662, 0.0896973063038045, 0.10885865037489587, 0.0896973063038045, 0.08997500694251596]
Actions to choose Agent 1: dict_values([{'num_count': 521, 'sum_payoffs': 159.45027483599446, 'action': [0.0, 0.0]}, {'num_count': 677, 'sum_payoffs': 221.93903044776744, 'action': [1.0, 1.5707963267948966]}, {'num_count': 524, 'sum_payoffs': 160.63968013314752, 'action': [0.0, -1.5707963267948966]}, {'num_count': 671, 'sum_payoffs': 219.46026982849054, 'action': [1.0, 0.0]}, {'num_count': 685, 'sum_payoffs': 225.13743124683458, 'action': [1.0, -1.5707963267948966]}, {'num_count': 522, 'sum_payoffs': 159.86006993837478, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.14468203276867536, 0.18800333240766454, 0.14551513468480978, 0.18633712857539572, 0.19022493751735628, 0.14495973340738683]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.4286081790924072 s
