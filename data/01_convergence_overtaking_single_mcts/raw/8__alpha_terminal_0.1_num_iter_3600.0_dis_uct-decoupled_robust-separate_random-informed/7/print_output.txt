Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 396, 'sum_payoffs': 102.7886056800208, 'action': [1.0, -1.5707963267948966]}, {'num_count': 391, 'sum_payoffs': 101.01949023803675, 'action': [1.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 76.79160418510278, 'action': [0.0, -1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 101.40929533542307, 'action': [1.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 135.0524737406105, 'action': [2.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 134.39280357580293, 'action': [2.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 135.15242376558126, 'action': [2.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 77.1814092824891, 'action': [0.0, 1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 77.20139928748326, 'action': [0.0, 0.0]}])
Weights num count: [0.10996945292974174, 0.10858094973618439, 0.0896973063038045, 0.10885865037489587, 0.134407109136351, 0.13385170785892808, 0.134407109136351, 0.08997500694251596, 0.08997500694251596]
Actions to choose Agent 1: dict_values([{'num_count': 522, 'sum_payoffs': 159.84007993338076, 'action': [0.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 159.47026484098865, 'action': [0.0, 1.5707963267948966]}, {'num_count': 673, 'sum_payoffs': 220.2998500382455, 'action': [1.0, -1.5707963267948966]}, {'num_count': 682, 'sum_payoffs': 223.9080459396932, 'action': [1.0, 1.5707963267948966]}, {'num_count': 677, 'sum_payoffs': 221.9390304477676, 'action': [1.0, 0.0]}, {'num_count': 525, 'sum_payoffs': 161.02948523053374, 'action': [0.0, 0.0]}])
Weights num count: [0.14495973340738683, 0.14468203276867536, 0.18689252985281865, 0.1893918356012219, 0.18800333240766454, 0.14579283532352125]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.493419885635376 s
