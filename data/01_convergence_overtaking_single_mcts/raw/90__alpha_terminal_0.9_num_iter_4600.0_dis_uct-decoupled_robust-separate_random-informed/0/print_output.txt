Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 539, 'sum_payoffs': 165.43258243646636, 'action': [2.0, 1.5707963267948966]}, {'num_count': 509, 'sum_payoffs': 153.48477844496776, 'action': [1.0, -1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 143.7812202874455, 'action': [0.0, -1.5707963267948966]}, {'num_count': 498, 'sum_payoffs': 149.27649493013982, 'action': [0.0, 0.0]}, {'num_count': 486, 'sum_payoffs': 144.5448075574342, 'action': [1.0, 1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 134.85352178927005, 'action': [0.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 151.99181843688052, 'action': [1.0, 0.0]}, {'num_count': 570, 'sum_payoffs': 177.74255082154443, 'action': [2.0, 0.0]}, {'num_count': 548, 'sum_payoffs': 168.96215655607782, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11714844599000217, 0.11062812432079983, 0.10519452292979788, 0.10823733970875897, 0.10562921104107803, 0.10019560965007607, 0.10975874809823952, 0.1238861117148446, 0.11910454249076288]
Actions to choose Agent 1: dict_values([{'num_count': 861, 'sum_payoffs': 267.29219470024503, 'action': [1.0, 0.0]}, {'num_count': 689, 'sum_payoffs': 202.4740770519409, 'action': [0.0, -1.5707963267948966]}, {'num_count': 689, 'sum_payoffs': 202.52627780722418, 'action': [0.0, 1.5707963267948966]}, {'num_count': 719, 'sum_payoffs': 213.59093032252548, 'action': [0.0, 0.0]}, {'num_count': 812, 'sum_payoffs': 248.654818817774, 'action': [1.0, 1.5707963267948966]}, {'num_count': 830, 'sum_payoffs': 255.5143352030698, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18713323190610737, 0.1497500543360139, 0.1497500543360139, 0.15627037600521626, 0.17648337317974352, 0.18039556618126495]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 52.30287766456604 s
