Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 582, 'sum_payoffs': 182.70646208030522, 'action': [2.0, 0.0]}, {'num_count': 477, 'sum_payoffs': 141.25174856947805, 'action': [0.0, -1.5707963267948966]}, {'num_count': 526, 'sum_payoffs': 160.50582017088144, 'action': [1.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 146.36394046752238, 'action': [1.0, 1.5707963267948966]}, {'num_count': 459, 'sum_payoffs': 134.26393450756802, 'action': [0.0, 1.5707963267948966]}, {'num_count': 538, 'sum_payoffs': 165.2676223050519, 'action': [2.0, -1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 155.72481463121898, 'action': [1.0, -1.5707963267948966]}, {'num_count': 529, 'sum_payoffs': 161.6609636786763, 'action': [2.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 144.40879196760062, 'action': [0.0, 0.0]}])
Weights num count: [0.12649424038252555, 0.10367311454031732, 0.11432297326668116, 0.10649858726363834, 0.09976092153879591, 0.1169311019343621, 0.11171484459900022, 0.1149750054336014, 0.10541186698543795]
Actions to choose Agent 1: dict_values([{'num_count': 735, 'sum_payoffs': 220.19297375904029, 'action': [0.0, 0.0]}, {'num_count': 816, 'sum_payoffs': 250.75427932141397, 'action': [1.0, -1.5707963267948966]}, {'num_count': 804, 'sum_payoffs': 246.09152050647242, 'action': [1.0, 1.5707963267948966]}, {'num_count': 848, 'sum_payoffs': 262.84723689407474, 'action': [1.0, 0.0]}, {'num_count': 693, 'sum_payoffs': 204.37676218082004, 'action': [0.0, 1.5707963267948966]}, {'num_count': 704, 'sum_payoffs': 208.50604802055946, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15974788089545752, 0.17735274940230383, 0.1747446207346229, 0.18430775918278636, 0.1506194305585742, 0.15301021517061508]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.7997145652771 s
