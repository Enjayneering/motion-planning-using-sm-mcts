Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 537, 'sum_payoffs': 164.7429552296118, 'action': [2.0, -1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 137.29481843096085, 'action': [0.0, -1.5707963267948966]}, {'num_count': 563, 'sum_payoffs': 174.9446852855741, 'action': [2.0, 0.0]}, {'num_count': 509, 'sum_payoffs': 153.7146439211208, 'action': [1.0, -1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 132.89771149107594, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 163.9237966482309, 'action': [2.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 148.60515083197532, 'action': [1.0, 1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 165.52027007289183, 'action': [1.0, 0.0]}, {'num_count': 498, 'sum_payoffs': 149.3673751576633, 'action': [0.0, 0.0]}])
Weights num count: [0.11671375787872201, 0.10149967398391654, 0.12236470332536405, 0.11062812432079983, 0.09910888937187567, 0.11627906976744186, 0.10780265159747882, 0.11714844599000217, 0.10823733970875897]
Actions to choose Agent 1: dict_values([{'num_count': 737, 'sum_payoffs': 220.40721258202598, 'action': [0.0, 0.0]}, {'num_count': 865, 'sum_payoffs': 268.8759441904034, 'action': [1.0, 0.0]}, {'num_count': 695, 'sum_payoffs': 204.73109681104756, 'action': [0.0, -1.5707963267948966]}, {'num_count': 806, 'sum_payoffs': 246.4920899740816, 'action': [1.0, 1.5707963267948966]}, {'num_count': 698, 'sum_payoffs': 205.88390333775922, 'action': [0.0, 1.5707963267948966]}, {'num_count': 799, 'sum_payoffs': 243.79484246468783, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.16018256900673766, 0.18800260812866768, 0.15105411866985438, 0.17517930884590308, 0.1517061508367746, 0.17365790045642251]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.16167998313904 s
