Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 525, 'sum_payoffs': 159.93538522505494, 'action': [2.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 142.20186708439178, 'action': [0.0, 0.0]}, {'num_count': 574, 'sum_payoffs': 179.2809990291402, 'action': [2.0, 0.0]}, {'num_count': 496, 'sum_payoffs': 148.51930930298437, 'action': [1.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 143.8896240596706, 'action': [0.0, -1.5707963267948966]}, {'num_count': 513, 'sum_payoffs': 155.2059853814479, 'action': [1.0, -1.5707963267948966]}, {'num_count': 539, 'sum_payoffs': 165.42611660389073, 'action': [2.0, -1.5707963267948966]}, {'num_count': 471, 'sum_payoffs': 138.82747849729327, 'action': [0.0, 1.5707963267948966]}, {'num_count': 518, 'sum_payoffs': 157.174560708323, 'action': [1.0, 0.0]}])
Weights num count: [0.11410562921104107, 0.10432514670723755, 0.12475548793740492, 0.10780265159747882, 0.10519452292979788, 0.11149750054336013, 0.11714844599000217, 0.10236905020647685, 0.11258422082156053]
Actions to choose Agent 1: dict_values([{'num_count': 748, 'sum_payoffs': 224.42400550090775, 'action': [0.0, 0.0]}, {'num_count': 695, 'sum_payoffs': 204.60667727450095, 'action': [0.0, -1.5707963267948966]}, {'num_count': 821, 'sum_payoffs': 251.9210268448897, 'action': [1.0, 1.5707963267948966]}, {'num_count': 787, 'sum_payoffs': 239.0635994383288, 'action': [1.0, -1.5707963267948966]}, {'num_count': 842, 'sum_payoffs': 259.85661322638487, 'action': [1.0, 0.0]}, {'num_count': 707, 'sum_payoffs': 209.06299603594937, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16257335361877853, 0.15105411866985438, 0.17843946968050423, 0.1710497717887416, 0.18300369484894588, 0.1536622473375353]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.56858253479004 s
