Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 474, 'sum_payoffs': 139.18259106032713, 'action': [0.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 147.79140314809567, 'action': [1.0, 1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 167.0291328999751, 'action': [2.0, -1.5707963267948966]}, {'num_count': 585, 'sum_payoffs': 182.85909392669006, 'action': [2.0, 0.0]}, {'num_count': 539, 'sum_payoffs': 164.62759847337145, 'action': [2.0, 1.5707963267948966]}, {'num_count': 505, 'sum_payoffs': 151.39027145015746, 'action': [1.0, 0.0]}, {'num_count': 516, 'sum_payoffs': 155.6820779628713, 'action': [1.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 144.34201119174108, 'action': [0.0, 0.0]}, {'num_count': 453, 'sum_payoffs': 131.17736939873416, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10302108237339709, 0.10780265159747882, 0.11845251032384264, 0.12714627254944577, 0.11714844599000217, 0.10975874809823952, 0.11214953271028037, 0.1058465550967181, 0.09845685720495545]
Actions to choose Agent 1: dict_values([{'num_count': 695, 'sum_payoffs': 205.1507997054546, 'action': [0.0, 1.5707963267948966]}, {'num_count': 723, 'sum_payoffs': 215.62629859561457, 'action': [0.0, 0.0]}, {'num_count': 873, 'sum_payoffs': 272.40469997913533, 'action': [1.0, 0.0]}, {'num_count': 803, 'sum_payoffs': 245.70442865103516, 'action': [1.0, -1.5707963267948966]}, {'num_count': 807, 'sum_payoffs': 247.18285917533245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 699, 'sum_payoffs': 206.62561019238774, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15105411866985438, 0.15713975222777657, 0.18974136057378832, 0.17452727667898282, 0.17539665290154313, 0.1519234948924147]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 52.1314742565155 s
