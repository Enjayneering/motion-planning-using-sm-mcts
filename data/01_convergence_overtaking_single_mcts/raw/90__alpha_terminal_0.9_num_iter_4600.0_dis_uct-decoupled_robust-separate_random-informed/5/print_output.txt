Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 581, 'sum_payoffs': 181.73060843353477, 'action': [2.0, 0.0]}, {'num_count': 554, 'sum_payoffs': 171.03863539309774, 'action': [2.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 145.33115744147332, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 134.11722125098208, 'action': [0.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 144.61268931648263, 'action': [1.0, 1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 155.96056840081272, 'action': [1.0, 0.0]}, {'num_count': 474, 'sum_payoffs': 139.61990978326003, 'action': [0.0, -1.5707963267948966]}, {'num_count': 516, 'sum_payoffs': 155.9942880485491, 'action': [1.0, -1.5707963267948966]}, {'num_count': 523, 'sum_payoffs': 158.75171192113092, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.12627689632688546, 0.12040860682460335, 0.10628124320799825, 0.099978265594436, 0.1058465550967181, 0.11214953271028037, 0.10302108237339709, 0.11214953271028037, 0.11367094109976092]
Actions to choose Agent 1: dict_values([{'num_count': 711, 'sum_payoffs': 211.37043321240924, 'action': [0.0, -1.5707963267948966]}, {'num_count': 825, 'sum_payoffs': 254.41225887120214, 'action': [1.0, 1.5707963267948966]}, {'num_count': 792, 'sum_payoffs': 241.90640989945305, 'action': [1.0, -1.5707963267948966]}, {'num_count': 746, 'sum_payoffs': 224.53875972627486, 'action': [0.0, 0.0]}, {'num_count': 674, 'sum_payoffs': 197.61477534791638, 'action': [0.0, 1.5707963267948966]}, {'num_count': 852, 'sum_payoffs': 264.67510202334904, 'action': [1.0, 0.0]}])
Weights num count: [0.15453162356009564, 0.17930884590306456, 0.17213649206694198, 0.16213866550749836, 0.14648989350141273, 0.18517713540534667]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.510661363601685 s
