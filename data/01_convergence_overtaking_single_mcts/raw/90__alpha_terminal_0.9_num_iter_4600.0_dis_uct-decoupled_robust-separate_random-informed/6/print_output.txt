Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 540, 'sum_payoffs': 166.2605256708618, 'action': [1.0, 0.0]}, {'num_count': 481, 'sum_payoffs': 142.9501932163354, 'action': [0.0, -1.5707963267948966]}, {'num_count': 582, 'sum_payoffs': 182.9435615638291, 'action': [2.0, 0.0]}, {'num_count': 515, 'sum_payoffs': 156.39744826219535, 'action': [1.0, -1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 145.21127658373277, 'action': [1.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 137.89528690901065, 'action': [0.0, 0.0]}, {'num_count': 460, 'sum_payoffs': 134.85008832421366, 'action': [0.0, 1.5707963267948966]}, {'num_count': 535, 'sum_payoffs': 164.2290138840465, 'action': [2.0, -1.5707963267948966]}, {'num_count': 532, 'sum_payoffs': 163.11314898025373, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11736579004564225, 0.10454249076287764, 0.12649424038252555, 0.1119321886546403, 0.1058465550967181, 0.10171701803955661, 0.099978265594436, 0.11627906976744186, 0.11562703760052162]
Actions to choose Agent 1: dict_values([{'num_count': 872, 'sum_payoffs': 271.46332974811526, 'action': [1.0, 0.0]}, {'num_count': 699, 'sum_payoffs': 206.10808454026886, 'action': [0.0, 1.5707963267948966]}, {'num_count': 738, 'sum_payoffs': 220.67459720231807, 'action': [0.0, 0.0]}, {'num_count': 704, 'sum_payoffs': 208.0628655459586, 'action': [0.0, -1.5707963267948966]}, {'num_count': 781, 'sum_payoffs': 236.88082141734037, 'action': [1.0, 1.5707963267948966]}, {'num_count': 806, 'sum_payoffs': 246.34416850033443, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18952401651814824, 0.1519234948924147, 0.16039991306237775, 0.15301021517061508, 0.1697457074549011, 0.17517930884590308]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.48193383216858 s
