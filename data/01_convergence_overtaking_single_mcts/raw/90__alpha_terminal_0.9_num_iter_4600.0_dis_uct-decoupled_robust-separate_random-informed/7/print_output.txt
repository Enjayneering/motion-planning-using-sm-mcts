Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 553, 'sum_payoffs': 170.6206966721329, 'action': [2.0, -1.5707963267948966]}, {'num_count': 515, 'sum_payoffs': 155.6279527889005, 'action': [1.0, -1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 148.14508603892608, 'action': [1.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 132.60690700632256, 'action': [0.0, 1.5707963267948966]}, {'num_count': 502, 'sum_payoffs': 150.47948283912103, 'action': [0.0, 0.0]}, {'num_count': 508, 'sum_payoffs': 152.7322557340037, 'action': [1.0, 0.0]}, {'num_count': 529, 'sum_payoffs': 161.1260970191773, 'action': [2.0, 1.5707963267948966]}, {'num_count': 568, 'sum_payoffs': 176.52705821055832, 'action': [2.0, 0.0]}, {'num_count': 473, 'sum_payoffs': 139.14915293688946, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.12019126276896328, 0.1119321886546403, 0.10780265159747882, 0.09910888937187567, 0.10910671593131928, 0.11041078026515974, 0.1149750054336014, 0.12345142360356444, 0.102803738317757]
Actions to choose Agent 1: dict_values([{'num_count': 718, 'sum_payoffs': 213.62475261591433, 'action': [0.0, -1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 212.01612993494552, 'action': [0.0, 0.0]}, {'num_count': 861, 'sum_payoffs': 267.5901312984829, 'action': [1.0, 0.0]}, {'num_count': 805, 'sum_payoffs': 246.38776515611752, 'action': [1.0, 1.5707963267948966]}, {'num_count': 694, 'sum_payoffs': 204.63286474818358, 'action': [0.0, 1.5707963267948966]}, {'num_count': 808, 'sum_payoffs': 247.43717903130067, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15605303194957618, 0.15518365572701587, 0.18713323190610737, 0.174961964790263, 0.1508367746142143, 0.17561399695718322]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 53.94055485725403 s
