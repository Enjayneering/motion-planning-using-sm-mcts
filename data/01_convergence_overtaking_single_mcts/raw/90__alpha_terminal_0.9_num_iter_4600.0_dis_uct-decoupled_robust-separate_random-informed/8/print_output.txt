Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 495, 'sum_payoffs': 148.50790608325002, 'action': [1.0, 1.5707963267948966]}, {'num_count': 514, 'sum_payoffs': 156.07270196612131, 'action': [1.0, -1.5707963267948966]}, {'num_count': 521, 'sum_payoffs': 158.65809964377775, 'action': [1.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 137.1413722531754, 'action': [0.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 146.17091526571954, 'action': [0.0, 0.0]}, {'num_count': 577, 'sum_payoffs': 180.99671691206296, 'action': [2.0, 0.0]}, {'num_count': 456, 'sum_payoffs': 133.36243284931214, 'action': [0.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 172.25485570566804, 'action': [2.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 161.08568100725446, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10758530754183873, 0.11171484459900022, 0.11323625298848077, 0.10128232992827646, 0.10628124320799825, 0.12540752010432515, 0.09910888937187567, 0.12062595088024343, 0.11454031732232123]
Actions to choose Agent 1: dict_values([{'num_count': 713, 'sum_payoffs': 210.59486121697498, 'action': [0.0, 1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 243.15396526151557, 'action': [1.0, 1.5707963267948966]}, {'num_count': 681, 'sum_payoffs': 198.61688715246095, 'action': [0.0, -1.5707963267948966]}, {'num_count': 739, 'sum_payoffs': 220.26047052740165, 'action': [0.0, 0.0]}, {'num_count': 860, 'sum_payoffs': 265.77536941108906, 'action': [1.0, 0.0]}, {'num_count': 807, 'sum_payoffs': 245.7964579889253, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15496631167137578, 0.1738752445120626, 0.1480113018908933, 0.16061725711801783, 0.18691588785046728, 0.17539665290154313]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 57.72504711151123 s
