Searching game tree in timestep 0...
Max timehorizon: 6
Actions to choose Agent 0: dict_values([{'num_count': 495, 'sum_payoffs': 147.57811053863335, 'action': [1.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 137.0767986963087, 'action': [0.0, -1.5707963267948966]}, {'num_count': 527, 'sum_payoffs': 160.13269212322223, 'action': [1.0, -1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 168.39223678290398, 'action': [2.0, -1.5707963267948966]}, {'num_count': 507, 'sum_payoffs': 152.22050642802898, 'action': [1.0, 0.0]}, {'num_count': 495, 'sum_payoffs': 147.47534155617313, 'action': [0.0, 0.0]}, {'num_count': 566, 'sum_payoffs': 175.53174808103043, 'action': [2.0, 0.0]}, {'num_count': 463, 'sum_payoffs': 135.04276617918674, 'action': [0.0, 1.5707963267948966]}, {'num_count': 531, 'sum_payoffs': 161.7093334101414, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10758530754183873, 0.10171701803955661, 0.11454031732232123, 0.11910454249076288, 0.11019343620951967, 0.10758530754183873, 0.12301673549228428, 0.10063029776135622, 0.11540969354488155]
Actions to choose Agent 1: dict_values([{'num_count': 706, 'sum_payoffs': 209.0979803894088, 'action': [0.0, -1.5707963267948966]}, {'num_count': 800, 'sum_payoffs': 244.42234379673974, 'action': [1.0, -1.5707963267948966]}, {'num_count': 868, 'sum_payoffs': 270.3781967894984, 'action': [1.0, 0.0]}, {'num_count': 726, 'sum_payoffs': 216.58727088385572, 'action': [0.0, 0.0]}, {'num_count': 692, 'sum_payoffs': 203.97361044538027, 'action': [0.0, 1.5707963267948966]}, {'num_count': 808, 'sum_payoffs': 247.57679307050807, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15344490328189525, 0.1738752445120626, 0.18865464029558793, 0.1577917843946968, 0.15040208650293416, 0.17561399695718322]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 54.171616554260254 s
