Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 123, 'sum_payoffs': 36.29299408929815, 'action': [2.0, 1.5707963267948966]}, {'num_count': 114, 'sum_payoffs': 32.257773481197894, 'action': [0.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 34.40731533805128, 'action': [0.0, 1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 37.25465538802201, 'action': [2.0, -1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 37.78979525590744, 'action': [2.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 36.36952085897966, 'action': [1.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 36.4033172238166, 'action': [1.0, 1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 35.48399640131525, 'action': [0.0, 0.0]}, {'num_count': 126, 'sum_payoffs': 37.86955134058873, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11171662125340599, 0.10354223433242507, 0.1080835603996367, 0.11353315168029064, 0.11444141689373297, 0.11171662125340599, 0.11171662125340599, 0.10990009082652134, 0.11444141689373297]
Actions to choose Agent 1: dict_values([{'num_count': 198, 'sum_payoffs': 58.89838506012547, 'action': [1.0, 0.0]}, {'num_count': 193, 'sum_payoffs': 56.72096510408574, 'action': [1.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 52.01238066958824, 'action': [1.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 48.704396422761846, 'action': [0.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 50.6584535524934, 'action': [0.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 48.68470422084768, 'action': [0.0, 0.0]}])
Weights num count: [0.17983651226158037, 0.17529518619436876, 0.16530426884650318, 0.15803814713896458, 0.16257947320617622, 0.15803814713896458]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.799806594848633 s
