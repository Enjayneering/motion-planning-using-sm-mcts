Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 128, 'sum_payoffs': 38.45376893684049, 'action': [2.0, 0.0]}, {'num_count': 127, 'sum_payoffs': 38.03140891063169, 'action': [1.0, -1.5707963267948966]}, {'num_count': 118, 'sum_payoffs': 33.99695251125245, 'action': [0.0, -1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 34.90300936946695, 'action': [2.0, -1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 34.32654953083643, 'action': [0.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 36.15724682251361, 'action': [2.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 34.25969278265377, 'action': [0.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 34.93245459654872, 'action': [1.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 37.67238471861257, 'action': [1.0, 0.0]}])
Weights num count: [0.11625794732061762, 0.11534968210717529, 0.10717529518619437, 0.10899182561307902, 0.1080835603996367, 0.11171662125340599, 0.1080835603996367, 0.10899182561307902, 0.11444141689373297]
Actions to choose Agent 1: dict_values([{'num_count': 178, 'sum_payoffs': 50.659703482000026, 'action': [0.0, -1.5707963267948966]}, {'num_count': 193, 'sum_payoffs': 57.0146344308287, 'action': [1.0, -1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 57.460101620896424, 'action': [1.0, 0.0]}, {'num_count': 171, 'sum_payoffs': 47.70224443273504, 'action': [0.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 47.258383473438215, 'action': [0.0, 1.5707963267948966]}, {'num_count': 194, 'sum_payoffs': 57.555139589338765, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16167120799273388, 0.17529518619436876, 0.17620345140781107, 0.1553133514986376, 0.15440508628519528, 0.17620345140781107]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 15.91727614402771 s
