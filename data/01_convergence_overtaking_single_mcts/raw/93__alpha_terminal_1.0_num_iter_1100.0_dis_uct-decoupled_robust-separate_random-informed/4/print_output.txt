Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 119, 'sum_payoffs': 34.44218283581463, 'action': [0.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 34.49133597438521, 'action': [0.0, 1.5707963267948966]}, {'num_count': 123, 'sum_payoffs': 36.33762349524637, 'action': [2.0, 0.0]}, {'num_count': 123, 'sum_payoffs': 36.21332258582253, 'action': [1.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 36.720101344159694, 'action': [1.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 35.33861713859107, 'action': [2.0, 1.5707963267948966]}, {'num_count': 119, 'sum_payoffs': 34.34042339493552, 'action': [0.0, -1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 38.68468158230889, 'action': [1.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 36.77072360340607, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1080835603996367, 0.1080835603996367, 0.11171662125340599, 0.11171662125340599, 0.11262488646684832, 0.10990009082652134, 0.1080835603996367, 0.11625794732061762, 0.11262488646684832]
Actions to choose Agent 1: dict_values([{'num_count': 185, 'sum_payoffs': 53.44836394634282, 'action': [1.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 53.01598266751016, 'action': [1.0, 1.5707963267948966]}, {'num_count': 176, 'sum_payoffs': 49.48687140242956, 'action': [0.0, -1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 46.677703032314525, 'action': [0.0, 1.5707963267948966]}, {'num_count': 205, 'sum_payoffs': 61.98123743480016, 'action': [1.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 51.73478639593593, 'action': [0.0, 0.0]}])
Weights num count: [0.16802906448683017, 0.16712079927338783, 0.15985467756584923, 0.15349682107175294, 0.18619436875567666, 0.16439600363306087]
Selected final action: [1.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 15.41239047050476 s
