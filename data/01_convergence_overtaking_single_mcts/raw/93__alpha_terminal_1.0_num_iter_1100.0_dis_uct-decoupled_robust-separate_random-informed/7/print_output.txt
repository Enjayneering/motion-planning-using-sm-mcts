Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 122, 'sum_payoffs': 36.13637171986582, 'action': [1.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 38.821059668126864, 'action': [2.0, 1.5707963267948966]}, {'num_count': 120, 'sum_payoffs': 35.186778610907766, 'action': [0.0, -1.5707963267948966]}, {'num_count': 125, 'sum_payoffs': 37.45307055405787, 'action': [2.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 33.150885647574626, 'action': [0.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 37.03736115580646, 'action': [1.0, 1.5707963267948966]}, {'num_count': 116, 'sum_payoffs': 33.310898225585696, 'action': [0.0, 0.0]}, {'num_count': 128, 'sum_payoffs': 38.812027694190306, 'action': [2.0, -1.5707963267948966]}, {'num_count': 121, 'sum_payoffs': 35.691401516814835, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.11080835603996367, 0.11625794732061762, 0.10899182561307902, 0.11353315168029064, 0.10535876475930972, 0.11262488646684832, 0.10535876475930972, 0.11625794732061762, 0.10990009082652134]
Actions to choose Agent 1: dict_values([{'num_count': 172, 'sum_payoffs': 47.76990790676834, 'action': [0.0, -1.5707963267948966]}, {'num_count': 177, 'sum_payoffs': 49.86984366298309, 'action': [0.0, 1.5707963267948966]}, {'num_count': 201, 'sum_payoffs': 60.15427548508159, 'action': [1.0, 0.0]}, {'num_count': 169, 'sum_payoffs': 46.53997814182294, 'action': [0.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 55.88857939715582, 'action': [1.0, 1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 55.480955202379874, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15622161671207993, 0.16076294277929154, 0.18256130790190736, 0.15349682107175294, 0.1734786557674841, 0.17257039055404177]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 14.452669620513916 s
