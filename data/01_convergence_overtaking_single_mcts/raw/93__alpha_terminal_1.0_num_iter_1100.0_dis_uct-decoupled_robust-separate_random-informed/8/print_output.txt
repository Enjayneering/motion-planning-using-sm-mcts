Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 119, 'sum_payoffs': 34.78492873647278, 'action': [1.0, 1.5707963267948966]}, {'num_count': 132, 'sum_payoffs': 40.781483481760404, 'action': [2.0, 0.0]}, {'num_count': 121, 'sum_payoffs': 35.78312019607419, 'action': [1.0, -1.5707963267948966]}, {'num_count': 115, 'sum_payoffs': 32.923530630620256, 'action': [0.0, -1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 37.00623852780646, 'action': [1.0, 0.0]}, {'num_count': 116, 'sum_payoffs': 33.41367315853337, 'action': [0.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 34.78173098698757, 'action': [0.0, 1.5707963267948966]}, {'num_count': 128, 'sum_payoffs': 38.9763213781178, 'action': [2.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 38.04988576304302, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.1080835603996367, 0.11989100817438691, 0.10990009082652134, 0.1044504995458674, 0.11262488646684832, 0.10535876475930972, 0.1080835603996367, 0.11625794732061762, 0.11444141689373297]
Actions to choose Agent 1: dict_values([{'num_count': 191, 'sum_payoffs': 55.90262309266934, 'action': [1.0, -1.5707963267948966]}, {'num_count': 186, 'sum_payoffs': 53.71067609327878, 'action': [1.0, 1.5707963267948966]}, {'num_count': 199, 'sum_payoffs': 59.232749147163574, 'action': [1.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 51.12134139665238, 'action': [0.0, -1.5707963267948966]}, {'num_count': 174, 'sum_payoffs': 48.49141790317437, 'action': [0.0, 0.0]}, {'num_count': 170, 'sum_payoffs': 46.89126150949036, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1734786557674841, 0.16893732970027248, 0.1807447774750227, 0.16348773841961853, 0.15803814713896458, 0.15440508628519528]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 14.79020071029663 s
