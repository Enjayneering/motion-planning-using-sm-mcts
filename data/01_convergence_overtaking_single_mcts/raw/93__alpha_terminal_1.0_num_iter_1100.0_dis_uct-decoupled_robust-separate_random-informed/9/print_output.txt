Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 126, 'sum_payoffs': 37.90872179950047, 'action': [2.0, -1.5707963267948966]}, {'num_count': 129, 'sum_payoffs': 39.27187855501836, 'action': [2.0, 0.0]}, {'num_count': 118, 'sum_payoffs': 34.16577431041936, 'action': [0.0, 1.5707963267948966]}, {'num_count': 126, 'sum_payoffs': 37.90961933029448, 'action': [1.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 33.69238827007423, 'action': [1.0, 0.0]}, {'num_count': 119, 'sum_payoffs': 34.62279264899734, 'action': [0.0, -1.5707963267948966]}, {'num_count': 117, 'sum_payoffs': 33.693581906682844, 'action': [0.0, 0.0]}, {'num_count': 124, 'sum_payoffs': 36.825927427074824, 'action': [1.0, 1.5707963267948966]}, {'num_count': 124, 'sum_payoffs': 36.96340757398697, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11444141689373297, 0.11716621253405994, 0.10717529518619437, 0.11444141689373297, 0.10626702997275204, 0.1080835603996367, 0.10626702997275204, 0.11262488646684832, 0.11262488646684832]
Actions to choose Agent 1: dict_values([{'num_count': 176, 'sum_payoffs': 49.719166828875665, 'action': [0.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 51.44775968284838, 'action': [0.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 56.165251333604154, 'action': [1.0, -1.5707963267948966]}, {'num_count': 195, 'sum_payoffs': 57.87528602714234, 'action': [1.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 49.673868615422435, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 52.22868592118859, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15985467756584923, 0.16348773841961853, 0.1734786557674841, 0.1771117166212534, 0.15985467756584923, 0.16530426884650318]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 14.984322786331177 s
