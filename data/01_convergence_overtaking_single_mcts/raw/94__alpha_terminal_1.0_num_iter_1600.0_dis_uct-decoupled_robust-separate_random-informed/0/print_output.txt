Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 174, 'sum_payoffs': 50.87513859485852, 'action': [0.0, -1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 53.949842979550006, 'action': [1.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 49.04967522798533, 'action': [0.0, 0.0]}, {'num_count': 187, 'sum_payoffs': 56.664519594152196, 'action': [2.0, 0.0]}, {'num_count': 164, 'sum_payoffs': 46.58257192110516, 'action': [0.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 51.345261382617316, 'action': [2.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 53.13037564169881, 'action': [1.0, -1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 57.11832168781682, 'action': [2.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 54.45482947807708, 'action': [1.0, 0.0]}])
Weights num count: [0.10868207370393504, 0.11305434103685197, 0.1061836352279825, 0.11680199875078076, 0.10243597751405371, 0.10930668332292318, 0.11180512179887571, 0.1174266083697689, 0.1136789506558401]
Actions to choose Agent 1: dict_values([{'num_count': 261, 'sum_payoffs': 74.26195148946863, 'action': [0.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 84.81639793201258, 'action': [1.0, -1.5707963267948966]}, {'num_count': 256, 'sum_payoffs': 72.25131715390116, 'action': [0.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 67.02217584666158, 'action': [0.0, -1.5707963267948966]}, {'num_count': 289, 'sum_payoffs': 85.74330539064707, 'action': [1.0, 0.0]}, {'num_count': 264, 'sum_payoffs': 75.58658680860711, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16302311055590257, 0.17926296064959402, 0.1599000624609619, 0.15178013741411617, 0.18051217988757026, 0.16489693941286696]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 21.006792783737183 s
