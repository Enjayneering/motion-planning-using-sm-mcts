Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 176, 'sum_payoffs': 51.417465390466994, 'action': [1.0, 1.5707963267948966]}, {'num_count': 188, 'sum_payoffs': 56.80778551911846, 'action': [2.0, -1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 48.80846107891114, 'action': [0.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 52.81632147804273, 'action': [2.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 51.094684477554146, 'action': [0.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 51.43892087795802, 'action': [0.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 51.00456318208297, 'action': [1.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 53.661617548112815, 'action': [1.0, -1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 53.23694151946565, 'action': [2.0, 0.0]}])
Weights num count: [0.1099312929419113, 0.1174266083697689, 0.1061836352279825, 0.11180512179887571, 0.10930668332292318, 0.1099312929419113, 0.10930668332292318, 0.11305434103685197, 0.11242973141786383]
Actions to choose Agent 1: dict_values([{'num_count': 249, 'sum_payoffs': 69.40513161156763, 'action': [0.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 80.35540075569988, 'action': [1.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 85.25208695432684, 'action': [1.0, 1.5707963267948966]}, {'num_count': 269, 'sum_payoffs': 77.51363926069362, 'action': [1.0, 0.0]}, {'num_count': 265, 'sum_payoffs': 75.86993377330585, 'action': [0.0, 0.0]}, {'num_count': 253, 'sum_payoffs': 70.96513682526714, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15552779512804496, 0.17239225484072454, 0.17988757026858213, 0.1680199875078076, 0.1655215490318551, 0.1580262336039975]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.711949586868286 s
