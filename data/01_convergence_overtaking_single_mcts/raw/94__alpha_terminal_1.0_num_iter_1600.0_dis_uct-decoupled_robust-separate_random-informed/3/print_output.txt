Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 164, 'sum_payoffs': 46.1909644022177, 'action': [0.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 54.07866826766284, 'action': [1.0, 0.0]}, {'num_count': 173, 'sum_payoffs': 50.07235114861211, 'action': [1.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 53.137304089657796, 'action': [2.0, 0.0]}, {'num_count': 177, 'sum_payoffs': 51.84202486888332, 'action': [0.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 53.231617223703445, 'action': [2.0, 1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 54.868756813318974, 'action': [2.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 55.369536993105555, 'action': [1.0, -1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 51.053100913586846, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10243597751405371, 0.1136789506558401, 0.1080574640849469, 0.11242973141786383, 0.11055590256089944, 0.11242973141786383, 0.11492816989381636, 0.1155527795128045, 0.10930668332292318]
Actions to choose Agent 1: dict_values([{'num_count': 281, 'sum_payoffs': 82.79774177318419, 'action': [1.0, 0.0]}, {'num_count': 281, 'sum_payoffs': 82.72518781804187, 'action': [1.0, 1.5707963267948966]}, {'num_count': 257, 'sum_payoffs': 72.97724508619271, 'action': [0.0, -1.5707963267948966]}, {'num_count': 263, 'sum_payoffs': 75.46685482607955, 'action': [0.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 80.3834284178063, 'action': [1.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 67.45694089160568, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1755153029356652, 0.1755153029356652, 0.16052467207995003, 0.16427232979387882, 0.1717676452217364, 0.15178013741411617]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 22.934584856033325 s
