Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 177, 'sum_payoffs': 51.91928412011566, 'action': [1.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 48.97349174978712, 'action': [0.0, 1.5707963267948966]}, {'num_count': 189, 'sum_payoffs': 57.334886086661, 'action': [2.0, 0.0]}, {'num_count': 166, 'sum_payoffs': 47.128344747131074, 'action': [0.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 54.225219258753015, 'action': [1.0, -1.5707963267948966]}, {'num_count': 171, 'sum_payoffs': 49.396706630877084, 'action': [0.0, 0.0]}, {'num_count': 176, 'sum_payoffs': 51.585772579815114, 'action': [1.0, 0.0]}, {'num_count': 189, 'sum_payoffs': 57.26262202215242, 'action': [2.0, 1.5707963267948966]}, {'num_count': 180, 'sum_payoffs': 53.269433757505944, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11055590256089944, 0.1061836352279825, 0.11805121798875702, 0.10368519675202999, 0.1136789506558401, 0.10680824484697064, 0.1099312929419113, 0.11805121798875702, 0.11242973141786383]
Actions to choose Agent 1: dict_values([{'num_count': 279, 'sum_payoffs': 81.51168402997266, 'action': [1.0, -1.5707963267948966]}, {'num_count': 286, 'sum_payoffs': 84.32920432941982, 'action': [1.0, 0.0]}, {'num_count': 250, 'sum_payoffs': 69.83119312874338, 'action': [0.0, 1.5707963267948966]}, {'num_count': 261, 'sum_payoffs': 74.20311894098862, 'action': [0.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 80.62956806554769, 'action': [1.0, 1.5707963267948966]}, {'num_count': 247, 'sum_payoffs': 68.64223549303752, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17426608369768895, 0.17863835103060588, 0.1561524047470331, 0.16302311055590257, 0.17301686445971268, 0.15427857589006871]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 22.086090326309204 s
