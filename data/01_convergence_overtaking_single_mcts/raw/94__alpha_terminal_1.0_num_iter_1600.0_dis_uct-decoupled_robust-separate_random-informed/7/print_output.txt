Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 167, 'sum_payoffs': 47.68942255689023, 'action': [0.0, 1.5707963267948966]}, {'num_count': 169, 'sum_payoffs': 48.555036630431886, 'action': [0.0, -1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 54.25236476119649, 'action': [1.0, -1.5707963267948966]}, {'num_count': 184, 'sum_payoffs': 55.002469107340325, 'action': [2.0, 1.5707963267948966]}, {'num_count': 173, 'sum_payoffs': 50.25713682568627, 'action': [0.0, 0.0]}, {'num_count': 182, 'sum_payoffs': 54.23673283036903, 'action': [1.0, 1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 49.90109499083898, 'action': [1.0, 0.0]}, {'num_count': 191, 'sum_payoffs': 58.20822481504998, 'action': [2.0, 0.0]}, {'num_count': 180, 'sum_payoffs': 53.36236603920559, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10430980637101811, 0.10555902560899438, 0.1136789506558401, 0.11492816989381636, 0.1080574640849469, 0.1136789506558401, 0.10743285446595878, 0.1193004372267333, 0.11242973141786383]
Actions to choose Agent 1: dict_values([{'num_count': 273, 'sum_payoffs': 79.32347190606521, 'action': [1.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 87.10644686702165, 'action': [1.0, 0.0]}, {'num_count': 255, 'sum_payoffs': 71.95893670317889, 'action': [0.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 65.55303200043465, 'action': [0.0, 1.5707963267948966]}, {'num_count': 254, 'sum_payoffs': 71.58387229778829, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 85.07596638174263, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17051842598376016, 0.18238600874453467, 0.15927545284197375, 0.14928169893816365, 0.15865084322298564, 0.17926296064959402]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.811554431915283 s
