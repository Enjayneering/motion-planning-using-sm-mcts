Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 172, 'sum_payoffs': 49.99039537908233, 'action': [1.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 53.92454380574047, 'action': [2.0, 0.0]}, {'num_count': 181, 'sum_payoffs': 53.91488673215578, 'action': [1.0, 0.0]}, {'num_count': 172, 'sum_payoffs': 49.99134077199901, 'action': [0.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 50.83176733523239, 'action': [0.0, -1.5707963267948966]}, {'num_count': 190, 'sum_payoffs': 57.80181601409301, 'action': [2.0, -1.5707963267948966]}, {'num_count': 167, 'sum_payoffs': 47.88470857623506, 'action': [0.0, 1.5707963267948966]}, {'num_count': 181, 'sum_payoffs': 53.978777058813805, 'action': [2.0, 1.5707963267948966]}, {'num_count': 182, 'sum_payoffs': 54.44863244203238, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10743285446595878, 0.11305434103685197, 0.11305434103685197, 0.10743285446595878, 0.10868207370393504, 0.11867582760774516, 0.10430980637101811, 0.11305434103685197, 0.1136789506558401]
Actions to choose Agent 1: dict_values([{'num_count': 257, 'sum_payoffs': 72.10018541858896, 'action': [0.0, -1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 68.50906588742849, 'action': [0.0, 1.5707963267948966]}, {'num_count': 262, 'sum_payoffs': 74.05090738148786, 'action': [0.0, 0.0]}, {'num_count': 286, 'sum_payoffs': 83.7765700672232, 'action': [1.0, -1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 80.12517617701317, 'action': [1.0, 0.0]}, {'num_count': 270, 'sum_payoffs': 77.28749817773362, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.16052467207995003, 0.15490318550905685, 0.16364772017489068, 0.17863835103060588, 0.17301686445971268, 0.16864459712679575]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 20.31451439857483 s
