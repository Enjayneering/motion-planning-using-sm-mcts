Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 182, 'sum_payoffs': 54.1054155769138, 'action': [2.0, 1.5707963267948966]}, {'num_count': 175, 'sum_payoffs': 51.02219046610383, 'action': [1.0, 0.0]}, {'num_count': 174, 'sum_payoffs': 50.58194018106896, 'action': [0.0, -1.5707963267948966]}, {'num_count': 185, 'sum_payoffs': 55.382662822981366, 'action': [1.0, 1.5707963267948966]}, {'num_count': 179, 'sum_payoffs': 52.843228951544674, 'action': [2.0, 0.0]}, {'num_count': 179, 'sum_payoffs': 52.721698018823204, 'action': [2.0, -1.5707963267948966]}, {'num_count': 172, 'sum_payoffs': 49.67053063285459, 'action': [0.0, 1.5707963267948966]}, {'num_count': 170, 'sum_payoffs': 48.811684731699266, 'action': [0.0, 0.0]}, {'num_count': 184, 'sum_payoffs': 54.92123572825124, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1136789506558401, 0.10930668332292318, 0.10868207370393504, 0.1155527795128045, 0.11180512179887571, 0.11180512179887571, 0.10743285446595878, 0.1061836352279825, 0.11492816989381636]
Actions to choose Agent 1: dict_values([{'num_count': 251, 'sum_payoffs': 70.38951729455513, 'action': [0.0, 1.5707963267948966]}, {'num_count': 272, 'sum_payoffs': 78.79295930083691, 'action': [1.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 82.5120866213188, 'action': [1.0, -1.5707963267948966]}, {'num_count': 259, 'sum_payoffs': 73.54634828162449, 'action': [0.0, 0.0]}, {'num_count': 288, 'sum_payoffs': 85.26660535766537, 'action': [1.0, 0.0]}, {'num_count': 249, 'sum_payoffs': 69.4998545251242, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15677701436602123, 0.16989381636477202, 0.1755153029356652, 0.1617738913179263, 0.17988757026858213, 0.15552779512804496]
Selected final action: [1.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 21.810068607330322 s
