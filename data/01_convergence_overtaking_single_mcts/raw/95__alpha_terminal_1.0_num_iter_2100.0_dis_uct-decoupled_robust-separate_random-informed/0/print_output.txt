Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 239, 'sum_payoffs': 71.34002773771675, 'action': [2.0, 0.0]}, {'num_count': 239, 'sum_payoffs': 71.35069619950406, 'action': [2.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 69.61776546600136, 'action': [1.0, 1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 65.29854509586258, 'action': [0.0, 0.0]}, {'num_count': 247, 'sum_payoffs': 74.68551011940528, 'action': [2.0, 1.5707963267948966]}, {'num_count': 221, 'sum_payoffs': 63.677711198790654, 'action': [1.0, 0.0]}, {'num_count': 245, 'sum_payoffs': 73.88974405528624, 'action': [1.0, -1.5707963267948966]}, {'num_count': 226, 'sum_payoffs': 65.83996272448458, 'action': [0.0, -1.5707963267948966]}, {'num_count': 223, 'sum_payoffs': 64.59202519143754, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.11375535459305093, 0.11375535459305093, 0.11185149928605426, 0.10709186101856259, 0.11756306520704426, 0.10518800571156592, 0.11661113755354593, 0.10756782484531176, 0.10613993336506425]
Actions to choose Agent 1: dict_values([{'num_count': 333, 'sum_payoffs': 93.57651457801934, 'action': [0.0, 0.0]}, {'num_count': 327, 'sum_payoffs': 91.26351966663023, 'action': [0.0, -1.5707963267948966]}, {'num_count': 334, 'sum_payoffs': 94.00181791327238, 'action': [0.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 106.85266328744778, 'action': [1.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 106.89311361376899, 'action': [1.0, -1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 108.88601724167505, 'action': [1.0, 0.0]}])
Weights num count: [0.15849595430747263, 0.15564017134697763, 0.1589719181342218, 0.17467872441694432, 0.17467872441694432, 0.17705854355069015]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 27.021552085876465 s
