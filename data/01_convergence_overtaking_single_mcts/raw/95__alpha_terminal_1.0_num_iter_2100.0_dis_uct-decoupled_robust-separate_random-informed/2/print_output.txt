Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 234, 'sum_payoffs': 69.3230779147066, 'action': [1.0, -1.5707963267948966]}, {'num_count': 227, 'sum_payoffs': 66.26987016305344, 'action': [0.0, 0.0]}, {'num_count': 221, 'sum_payoffs': 63.73408333930264, 'action': [0.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 69.71283006077319, 'action': [1.0, 1.5707963267948966]}, {'num_count': 240, 'sum_payoffs': 71.9060498673956, 'action': [2.0, 0.0]}, {'num_count': 247, 'sum_payoffs': 74.79624776808954, 'action': [2.0, -1.5707963267948966]}, {'num_count': 233, 'sum_payoffs': 68.931029805949, 'action': [1.0, 0.0]}, {'num_count': 224, 'sum_payoffs': 65.12115314439855, 'action': [0.0, 1.5707963267948966]}, {'num_count': 239, 'sum_payoffs': 71.46802195013126, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.11137553545930509, 0.10804378867206092, 0.10518800571156592, 0.11185149928605426, 0.1142313184198001, 0.11756306520704426, 0.11089957163255593, 0.10661589719181343, 0.11375535459305093]
Actions to choose Agent 1: dict_values([{'num_count': 328, 'sum_payoffs': 91.58240016950099, 'action': [0.0, 1.5707963267948966]}, {'num_count': 374, 'sum_payoffs': 109.65466022161199, 'action': [1.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 95.48349932620842, 'action': [0.0, 0.0]}, {'num_count': 359, 'sum_payoffs': 103.71845239039185, 'action': [1.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 110.00293553380446, 'action': [1.0, 1.5707963267948966]}, {'num_count': 326, 'sum_payoffs': 90.7420286692885, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1561161351737268, 0.17801047120418848, 0.16087577344121848, 0.17087101380295097, 0.17848643503093764, 0.15516420752022847]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 26.850670337677002 s
