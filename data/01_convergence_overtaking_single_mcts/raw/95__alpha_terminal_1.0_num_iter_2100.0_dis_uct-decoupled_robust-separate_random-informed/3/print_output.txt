Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 226, 'sum_payoffs': 65.71912888099564, 'action': [1.0, 0.0]}, {'num_count': 223, 'sum_payoffs': 64.3676917787995, 'action': [0.0, 0.0]}, {'num_count': 235, 'sum_payoffs': 69.52908707913326, 'action': [1.0, -1.5707963267948966]}, {'num_count': 246, 'sum_payoffs': 74.21196418283192, 'action': [2.0, -1.5707963267948966]}, {'num_count': 235, 'sum_payoffs': 69.4230222840936, 'action': [1.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 72.86303513184157, 'action': [2.0, 0.0]}, {'num_count': 218, 'sum_payoffs': 62.33723243023769, 'action': [0.0, 1.5707963267948966]}, {'num_count': 231, 'sum_payoffs': 67.69505304220718, 'action': [0.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 72.92688210003453, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10756782484531176, 0.10613993336506425, 0.11185149928605426, 0.1170871013802951, 0.11185149928605426, 0.11565920990004759, 0.10376011423131842, 0.1099476439790576, 0.11565920990004759]
Actions to choose Agent 1: dict_values([{'num_count': 368, 'sum_payoffs': 107.71927076986225, 'action': [1.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 89.69200865432587, 'action': [0.0, -1.5707963267948966]}, {'num_count': 333, 'sum_payoffs': 93.90778710238112, 'action': [0.0, 1.5707963267948966]}, {'num_count': 325, 'sum_payoffs': 90.77969841104677, 'action': [0.0, 0.0]}, {'num_count': 378, 'sum_payoffs': 111.6497026929884, 'action': [1.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 109.98609884511791, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17515468824369348, 0.15326035221323178, 0.15849595430747263, 0.1546882436934793, 0.17991432651118516, 0.17801047120418848]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 28.38456702232361 s
