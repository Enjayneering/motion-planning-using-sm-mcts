Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 224, 'sum_payoffs': 64.80863547936114, 'action': [0.0, 0.0]}, {'num_count': 224, 'sum_payoffs': 64.78516637192658, 'action': [1.0, 0.0]}, {'num_count': 241, 'sum_payoffs': 71.95971782651357, 'action': [1.0, -1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 72.85627894671158, 'action': [2.0, -1.5707963267948966]}, {'num_count': 245, 'sum_payoffs': 73.55616808223913, 'action': [2.0, 1.5707963267948966]}, {'num_count': 248, 'sum_payoffs': 74.82407272187146, 'action': [2.0, 0.0]}, {'num_count': 229, 'sum_payoffs': 66.81313524044535, 'action': [1.0, 1.5707963267948966]}, {'num_count': 217, 'sum_payoffs': 61.770126652405935, 'action': [0.0, 1.5707963267948966]}, {'num_count': 229, 'sum_payoffs': 66.92301858326016, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10661589719181343, 0.10661589719181343, 0.11470728224654926, 0.11565920990004759, 0.11661113755354593, 0.11803902903379343, 0.10899571632555925, 0.10328415040456926, 0.10899571632555925]
Actions to choose Agent 1: dict_values([{'num_count': 366, 'sum_payoffs': 106.99594458877272, 'action': [1.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 95.20685427333133, 'action': [0.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 108.97935860803337, 'action': [1.0, -1.5707963267948966]}, {'num_count': 320, 'sum_payoffs': 88.90666143887, 'action': [0.0, 1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 90.1775034503158, 'action': [0.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 114.1066984545582, 'action': [1.0, 0.0]}])
Weights num count: [0.17420276059019515, 0.15992384578772012, 0.17658257972394098, 0.15230842455973345, 0.15373631603998097, 0.18277010947168015]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 29.046689987182617 s
