Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 236, 'sum_payoffs': 69.86938526813971, 'action': [2.0, 1.5707963267948966]}, {'num_count': 219, 'sum_payoffs': 62.722531711839586, 'action': [1.0, 0.0]}, {'num_count': 234, 'sum_payoffs': 68.92791335995226, 'action': [1.0, 1.5707963267948966]}, {'num_count': 243, 'sum_payoffs': 72.73393627484295, 'action': [2.0, -1.5707963267948966]}, {'num_count': 228, 'sum_payoffs': 66.43693101480972, 'action': [0.0, -1.5707963267948966]}, {'num_count': 249, 'sum_payoffs': 75.34705980908208, 'action': [2.0, 0.0]}, {'num_count': 251, 'sum_payoffs': 76.23817772047538, 'action': [1.0, -1.5707963267948966]}, {'num_count': 225, 'sum_payoffs': 65.13726485419092, 'action': [0.0, 1.5707963267948966]}, {'num_count': 215, 'sum_payoffs': 60.971681732624624, 'action': [0.0, 0.0]}])
Weights num count: [0.11232746311280342, 0.10423607805806759, 0.11137553545930509, 0.11565920990004759, 0.10851975249881009, 0.1185149928605426, 0.11946692051404094, 0.10709186101856259, 0.10233222275107091]
Actions to choose Agent 1: dict_values([{'num_count': 342, 'sum_payoffs': 97.46235010060845, 'action': [0.0, 0.0]}, {'num_count': 338, 'sum_payoffs': 95.92040907471014, 'action': [0.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 106.3910531711627, 'action': [1.0, 1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 110.40440091779264, 'action': [1.0, 0.0]}, {'num_count': 357, 'sum_payoffs': 103.30388425036767, 'action': [1.0, -1.5707963267948966]}, {'num_count': 323, 'sum_payoffs': 90.02165705681658, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16277962874821514, 0.16087577344121848, 0.173726796763446, 0.17848643503093764, 0.16991908614945264, 0.15373631603998097]
Selected final action: [1.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.22222222219629628, 0.2777777777453703]
Runtime: 27.276102304458618 s
