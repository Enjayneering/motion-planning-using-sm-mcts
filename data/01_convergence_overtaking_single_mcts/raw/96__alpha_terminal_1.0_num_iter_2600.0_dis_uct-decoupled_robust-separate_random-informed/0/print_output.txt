Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 303, 'sum_payoffs': 90.36000923911729, 'action': [2.0, 0.0]}, {'num_count': 276, 'sum_payoffs': 79.35770207014171, 'action': [0.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 81.75192121985256, 'action': [1.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 88.27406468658788, 'action': [1.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 92.48248575311017, 'action': [2.0, 1.5707963267948966]}, {'num_count': 291, 'sum_payoffs': 85.49579242252769, 'action': [1.0, 0.0]}, {'num_count': 261, 'sum_payoffs': 73.3119884309568, 'action': [0.0, 1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 80.88654478341377, 'action': [0.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 89.57829720063246, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.11649365628604383, 0.1061130334486736, 0.10841983852364476, 0.11457131872356786, 0.1184159938485198, 0.1118800461361015, 0.10034602076124567, 0.10765090349865436, 0.11572472126105345]
Actions to choose Agent 1: dict_values([{'num_count': 395, 'sum_payoffs': 110.28282709509405, 'action': [0.0, 1.5707963267948966]}, {'num_count': 427, 'sum_payoffs': 122.4728928825143, 'action': [0.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 118.71716423894564, 'action': [0.0, -1.5707963267948966]}, {'num_count': 441, 'sum_payoffs': 127.92549917445209, 'action': [1.0, -1.5707963267948966]}, {'num_count': 457, 'sum_payoffs': 133.98678723842886, 'action': [1.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 136.27887461765536, 'action': [1.0, 0.0]}])
Weights num count: [0.1518646674356017, 0.16416762783544792, 0.16032295271049596, 0.1695501730103806, 0.17570165321030373, 0.1780084582852749]
Selected final action: [2.0, 1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 33.20647072792053 s
