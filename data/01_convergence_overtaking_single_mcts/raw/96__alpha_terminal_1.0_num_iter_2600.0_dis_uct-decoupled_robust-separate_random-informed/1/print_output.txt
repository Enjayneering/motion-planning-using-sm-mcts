Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 315, 'sum_payoffs': 95.69194214497392, 'action': [2.0, 0.0]}, {'num_count': 294, 'sum_payoffs': 86.92020200092026, 'action': [2.0, 1.5707963267948966]}, {'num_count': 306, 'sum_payoffs': 91.86147697171108, 'action': [1.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 78.7958474084037, 'action': [0.0, 0.0]}, {'num_count': 287, 'sum_payoffs': 84.07016472625278, 'action': [1.0, 0.0]}, {'num_count': 273, 'sum_payoffs': 78.40289626798146, 'action': [0.0, 1.5707963267948966]}, {'num_count': 266, 'sum_payoffs': 75.3727934965628, 'action': [0.0, -1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 84.02403340925713, 'action': [1.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 88.57129796804534, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.12110726643598616, 0.11303344867358708, 0.11764705882352941, 0.1053440984236832, 0.11034217608612072, 0.104959630911188, 0.10226835832372165, 0.11034217608612072, 0.11457131872356786]
Actions to choose Agent 1: dict_values([{'num_count': 455, 'sum_payoffs': 132.14425968058222, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 115.80398509185319, 'action': [0.0, 1.5707963267948966]}, {'num_count': 467, 'sum_payoffs': 136.71566802755837, 'action': [1.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 129.90231299050728, 'action': [1.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 115.39680789293878, 'action': [0.0, 0.0]}, {'num_count': 406, 'sum_payoffs': 113.48895585585144, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17493271818531334, 0.15840061514801998, 0.17954632833525566, 0.17262591311034217, 0.1580161476355248, 0.15609381007304882]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.239869832992554 s
