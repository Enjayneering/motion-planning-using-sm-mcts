Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 309, 'sum_payoffs': 93.7358743150468, 'action': [2.0, 0.0]}, {'num_count': 312, 'sum_payoffs': 94.89125419024806, 'action': [2.0, -1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 90.78225670913314, 'action': [2.0, 1.5707963267948966]}, {'num_count': 275, 'sum_payoffs': 79.61321357896786, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 83.76978313284508, 'action': [1.0, 1.5707963267948966]}, {'num_count': 279, 'sum_payoffs': 81.22120211531248, 'action': [1.0, 0.0]}, {'num_count': 262, 'sum_payoffs': 74.36899949623087, 'action': [0.0, 1.5707963267948966]}, {'num_count': 302, 'sum_payoffs': 90.75270610640703, 'action': [1.0, -1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 79.162045351421, 'action': [0.0, 0.0]}])
Weights num count: [0.11880046136101499, 0.11995386389850057, 0.11610918877354863, 0.1057285659361784, 0.10957324106113034, 0.10726643598615918, 0.10073048827374087, 0.11610918877354863, 0.1053440984236832]
Actions to choose Agent 1: dict_values([{'num_count': 451, 'sum_payoffs': 130.6320988807331, 'action': [1.0, -1.5707963267948966]}, {'num_count': 384, 'sum_payoffs': 105.24109379515927, 'action': [0.0, -1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 134.82062491015165, 'action': [1.0, 0.0]}, {'num_count': 466, 'sum_payoffs': 136.3982960438148, 'action': [1.0, 1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 119.49500488071808, 'action': [0.0, 1.5707963267948966]}, {'num_count': 415, 'sum_payoffs': 116.93076404560595, 'action': [0.0, 0.0]}])
Weights num count: [0.17339484813533257, 0.14763552479815456, 0.1776239907727797, 0.17916186082276048, 0.16224529027297194, 0.15955401768550556]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.187793016433716 s
