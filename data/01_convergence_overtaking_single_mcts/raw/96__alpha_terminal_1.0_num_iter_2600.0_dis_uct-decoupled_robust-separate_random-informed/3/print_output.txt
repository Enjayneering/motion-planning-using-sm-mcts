Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 279, 'sum_payoffs': 81.10210404922034, 'action': [1.0, 0.0]}, {'num_count': 278, 'sum_payoffs': 80.74335962683136, 'action': [0.0, 0.0]}, {'num_count': 308, 'sum_payoffs': 93.15189755299271, 'action': [2.0, 0.0]}, {'num_count': 310, 'sum_payoffs': 93.917787182224, 'action': [2.0, -1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 86.53675161296829, 'action': [1.0, 1.5707963267948966]}, {'num_count': 274, 'sum_payoffs': 79.0637358267135, 'action': [0.0, -1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 83.62891511351546, 'action': [1.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 81.50221146643386, 'action': [0.0, 1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 87.34656285695702, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10726643598615918, 0.10688196847366398, 0.1184159938485198, 0.11918492887351019, 0.1122645136485967, 0.1053440984236832, 0.10957324106113034, 0.10765090349865436, 0.11303344867358708]
Actions to choose Agent 1: dict_values([{'num_count': 462, 'sum_payoffs': 135.92076480172523, 'action': [1.0, 0.0]}, {'num_count': 413, 'sum_payoffs': 116.98065260918524, 'action': [0.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 132.07109654467337, 'action': [1.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 131.63042908810732, 'action': [1.0, -1.5707963267948966]}, {'num_count': 416, 'sum_payoffs': 118.29526171287371, 'action': [0.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 114.32377303746757, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1776239907727797, 0.1587850826605152, 0.17377931564782775, 0.17339484813533257, 0.15993848519800077, 0.15609381007304882]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 35.57341742515564 s
