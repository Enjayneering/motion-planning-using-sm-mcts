Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 291, 'sum_payoffs': 85.99977455449182, 'action': [1.0, 1.5707963267948966]}, {'num_count': 300, 'sum_payoffs': 89.81953044568965, 'action': [1.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 89.31342097499426, 'action': [2.0, 1.5707963267948966]}, {'num_count': 277, 'sum_payoffs': 80.27035475319563, 'action': [0.0, 0.0]}, {'num_count': 309, 'sum_payoffs': 93.52485611568356, 'action': [2.0, -1.5707963267948966]}, {'num_count': 280, 'sum_payoffs': 81.5429747226986, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 77.8163152573333, 'action': [0.0, -1.5707963267948966]}, {'num_count': 270, 'sum_payoffs': 77.45924293773608, 'action': [0.0, 1.5707963267948966]}, {'num_count': 303, 'sum_payoffs': 91.01867522317869, 'action': [2.0, 0.0]}])
Weights num count: [0.1118800461361015, 0.11534025374855825, 0.11495578623606305, 0.10649750096116878, 0.11880046136101499, 0.10765090349865436, 0.10419069588619762, 0.10380622837370242, 0.11649365628604383]
Actions to choose Agent 1: dict_values([{'num_count': 472, 'sum_payoffs': 139.09942530329374, 'action': [1.0, 0.0]}, {'num_count': 407, 'sum_payoffs': 114.20881871036443, 'action': [0.0, 0.0]}, {'num_count': 408, 'sum_payoffs': 114.5294801335053, 'action': [0.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 113.8478395860441, 'action': [0.0, -1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 131.0308218854989, 'action': [1.0, 1.5707963267948966]}, {'num_count': 456, 'sum_payoffs': 132.8154309172465, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18146866589773164, 0.15647827758554403, 0.1568627450980392, 0.15609381007304882, 0.17339484813533257, 0.17531718569780855]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 32.83800745010376 s
