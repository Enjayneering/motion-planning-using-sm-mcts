Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 274, 'sum_payoffs': 78.8364792623096, 'action': [0.0, 0.0]}, {'num_count': 283, 'sum_payoffs': 82.45107377168792, 'action': [0.0, -1.5707963267948966]}, {'num_count': 299, 'sum_payoffs': 89.2173860341478, 'action': [2.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 92.92359353406002, 'action': [2.0, 0.0]}, {'num_count': 297, 'sum_payoffs': 88.39867670120543, 'action': [1.0, -1.5707963267948966]}, {'num_count': 288, 'sum_payoffs': 84.5778042037981, 'action': [1.0, 1.5707963267948966]}, {'num_count': 268, 'sum_payoffs': 76.44903007773286, 'action': [0.0, 1.5707963267948966]}, {'num_count': 298, 'sum_payoffs': 88.81800100396225, 'action': [2.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 83.45243984683616, 'action': [1.0, 0.0]}])
Weights num count: [0.1053440984236832, 0.10880430603613994, 0.11495578623606305, 0.1184159938485198, 0.11418685121107267, 0.11072664359861592, 0.10303729334871203, 0.11457131872356786, 0.10957324106113034]
Actions to choose Agent 1: dict_values([{'num_count': 452, 'sum_payoffs': 131.1702967081869, 'action': [1.0, 0.0]}, {'num_count': 453, 'sum_payoffs': 131.54470647018638, 'action': [1.0, -1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 118.49396533239606, 'action': [0.0, -1.5707963267948966]}, {'num_count': 397, 'sum_payoffs': 110.26197053848479, 'action': [0.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 115.91855813588127, 'action': [0.0, 0.0]}, {'num_count': 467, 'sum_payoffs': 136.93551148889145, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.17377931564782775, 0.17416378316032297, 0.16109188773548636, 0.15263360246059207, 0.15840061514801998, 0.17954632833525566]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 33.13020300865173 s
