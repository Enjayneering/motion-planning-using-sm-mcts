Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 273, 'sum_payoffs': 78.03508631252204, 'action': [0.0, 0.0]}, {'num_count': 315, 'sum_payoffs': 95.2690487593125, 'action': [2.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 84.68543879300749, 'action': [1.0, 0.0]}, {'num_count': 290, 'sum_payoffs': 85.02827387275157, 'action': [1.0, 1.5707963267948966]}, {'num_count': 267, 'sum_payoffs': 75.72505427842776, 'action': [0.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 85.90101339429984, 'action': [2.0, -1.5707963267948966]}, {'num_count': 304, 'sum_payoffs': 90.77403391713217, 'action': [2.0, 1.5707963267948966]}, {'num_count': 276, 'sum_payoffs': 79.25500342183854, 'action': [0.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 86.73700021960366, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.104959630911188, 0.12110726643598616, 0.1111111111111111, 0.1114955786236063, 0.10265282583621683, 0.1122645136485967, 0.11687812379853903, 0.1061130334486736, 0.11303344867358708]
Actions to choose Agent 1: dict_values([{'num_count': 403, 'sum_payoffs': 113.02070601865366, 'action': [0.0, 1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 142.62923455234744, 'action': [1.0, 0.0]}, {'num_count': 397, 'sum_payoffs': 110.80567490032853, 'action': [0.0, -1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 112.67490430747677, 'action': [0.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 136.46958115233832, 'action': [1.0, -1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 132.5814060298928, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.15494040753556323, 0.1845444059976932, 0.15263360246059207, 0.15455594002306805, 0.17839292579777008, 0.17454825067281815]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.30989122390747 s
