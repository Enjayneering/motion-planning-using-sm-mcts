Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 279, 'sum_payoffs': 80.89311678911895, 'action': [1.0, 1.5707963267948966]}, {'num_count': 292, 'sum_payoffs': 86.18212437340783, 'action': [1.0, -1.5707963267948966]}, {'num_count': 308, 'sum_payoffs': 92.8478712557654, 'action': [2.0, -1.5707963267948966]}, {'num_count': 307, 'sum_payoffs': 92.41831347246253, 'action': [2.0, 0.0]}, {'num_count': 289, 'sum_payoffs': 85.01736655327541, 'action': [2.0, 1.5707963267948966]}, {'num_count': 281, 'sum_payoffs': 81.70731757251718, 'action': [0.0, 1.5707963267948966]}, {'num_count': 285, 'sum_payoffs': 83.34058962859895, 'action': [1.0, 0.0]}, {'num_count': 277, 'sum_payoffs': 80.02510050361413, 'action': [0.0, -1.5707963267948966]}, {'num_count': 282, 'sum_payoffs': 82.17368201128795, 'action': [0.0, 0.0]}])
Weights num count: [0.10726643598615918, 0.1122645136485967, 0.1184159938485198, 0.11803152633602461, 0.1111111111111111, 0.10803537101114956, 0.10957324106113034, 0.10649750096116878, 0.10841983852364476]
Actions to choose Agent 1: dict_values([{'num_count': 462, 'sum_payoffs': 136.093692929176, 'action': [1.0, -1.5707963267948966]}, {'num_count': 401, 'sum_payoffs': 112.6810145645584, 'action': [0.0, 0.0]}, {'num_count': 459, 'sum_payoffs': 134.92076460685564, 'action': [1.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 116.72880549563205, 'action': [0.0, -1.5707963267948966]}, {'num_count': 408, 'sum_payoffs': 115.34106752684204, 'action': [0.0, 1.5707963267948966]}, {'num_count': 458, 'sum_payoffs': 134.55056681674466, 'action': [1.0, 0.0]}])
Weights num count: [0.1776239907727797, 0.15417147251057287, 0.17647058823529413, 0.15840061514801998, 0.1568627450980392, 0.17608612072279892]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 34.46897268295288 s
