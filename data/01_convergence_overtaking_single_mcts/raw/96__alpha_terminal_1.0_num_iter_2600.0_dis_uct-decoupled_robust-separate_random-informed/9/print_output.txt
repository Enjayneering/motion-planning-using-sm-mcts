Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 275, 'sum_payoffs': 79.69543769836385, 'action': [0.0, 0.0]}, {'num_count': 301, 'sum_payoffs': 90.31806204062283, 'action': [2.0, -1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 94.11533683343211, 'action': [2.0, 0.0]}, {'num_count': 275, 'sum_payoffs': 79.6200473793212, 'action': [0.0, -1.5707963267948966]}, {'num_count': 294, 'sum_payoffs': 87.46035122650294, 'action': [1.0, -1.5707963267948966]}, {'num_count': 297, 'sum_payoffs': 88.63491302955696, 'action': [2.0, 1.5707963267948966]}, {'num_count': 290, 'sum_payoffs': 85.77090876911645, 'action': [1.0, 1.5707963267948966]}, {'num_count': 287, 'sum_payoffs': 84.44935583647357, 'action': [1.0, 0.0]}, {'num_count': 271, 'sum_payoffs': 77.98569467264166, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1057285659361784, 0.11572472126105345, 0.11918492887351019, 0.1057285659361784, 0.11303344867358708, 0.11418685121107267, 0.1114955786236063, 0.11034217608612072, 0.10419069588619762]
Actions to choose Agent 1: dict_values([{'num_count': 383, 'sum_payoffs': 105.17863122025241, 'action': [0.0, 1.5707963267948966]}, {'num_count': 468, 'sum_payoffs': 137.55175948705988, 'action': [1.0, 0.0]}, {'num_count': 470, 'sum_payoffs': 138.22850400353101, 'action': [1.0, 1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 113.08789814917455, 'action': [0.0, -1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 116.49677423344951, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 135.20345252488605, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.14725105728565935, 0.17993079584775087, 0.18069973087274124, 0.15532487504805845, 0.1587850826605152, 0.1776239907727797]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 37.11362051963806 s
