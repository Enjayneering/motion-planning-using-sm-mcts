Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 336, 'sum_payoffs': 97.27483781363937, 'action': [1.0, 0.0]}, {'num_count': 354, 'sum_payoffs': 104.49617959005992, 'action': [1.0, -1.5707963267948966]}, {'num_count': 357, 'sum_payoffs': 105.74808223867863, 'action': [2.0, 1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 97.32825405632272, 'action': [0.0, 0.0]}, {'num_count': 327, 'sum_payoffs': 93.76977750344011, 'action': [0.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 94.13677188442492, 'action': [0.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 97.27361761783868, 'action': [1.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 107.63259524618357, 'action': [2.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 108.5191759753841, 'action': [2.0, 0.0]}])
Weights num count: [0.10835214446952596, 0.11415672363753628, 0.11512415349887133, 0.10835214446952596, 0.1054498548855208, 0.10577233150596582, 0.10835214446952596, 0.11673653660109642, 0.11738148984198646]
Actions to choose Agent 1: dict_values([{'num_count': 553, 'sum_payoffs': 161.1078997756898, 'action': [1.0, 0.0]}, {'num_count': 544, 'sum_payoffs': 157.80827347933504, 'action': [1.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 137.59749751521727, 'action': [0.0, -1.5707963267948966]}, {'num_count': 548, 'sum_payoffs': 159.28071281987226, 'action': [1.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 133.8749082218964, 'action': [0.0, 1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 135.7685525967319, 'action': [0.0, 0.0]}])
Weights num count: [0.17832957110609482, 0.17542728152208964, 0.1580135440180587, 0.1767171880038697, 0.1547887778136085, 0.15640116091583361]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.65447473526001 s
