Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 350, 'sum_payoffs': 103.06791956015013, 'action': [2.0, 1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 103.88137993746369, 'action': [1.0, 1.5707963267948966]}, {'num_count': 310, 'sum_payoffs': 87.12481806247432, 'action': [0.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 95.79735237133326, 'action': [0.0, 0.0]}, {'num_count': 353, 'sum_payoffs': 104.21105245900624, 'action': [1.0, -1.5707963267948966]}, {'num_count': 330, 'sum_payoffs': 94.99121536340283, 'action': [0.0, -1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 112.31417329145822, 'action': [2.0, 0.0]}, {'num_count': 362, 'sum_payoffs': 107.87577748377755, 'action': [2.0, -1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 98.27585461708348, 'action': [1.0, 0.0]}])
Weights num count: [0.11286681715575621, 0.11351177039664624, 0.0999677523379555, 0.10706223798774589, 0.11383424701709126, 0.10641728474685586, 0.12028377942599161, 0.11673653660109642, 0.10899709771041599]
Actions to choose Agent 1: dict_values([{'num_count': 551, 'sum_payoffs': 160.23150318311153, 'action': [1.0, 1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 161.34199835810378, 'action': [1.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 159.49278809912144, 'action': [1.0, -1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 134.347487172295, 'action': [0.0, -1.5707963267948966]}, {'num_count': 480, 'sum_payoffs': 133.6272120692858, 'action': [0.0, 1.5707963267948966]}, {'num_count': 484, 'sum_payoffs': 135.19183944710812, 'action': [0.0, 0.0]}])
Weights num count: [0.17768461786520479, 0.17865204772653984, 0.17703966462431472, 0.15543373105449854, 0.1547887778136085, 0.1560786842953886]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.461376905441284 s
