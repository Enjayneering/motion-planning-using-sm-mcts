Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 339, 'sum_payoffs': 98.69841539149147, 'action': [1.0, 1.5707963267948966]}, {'num_count': 360, 'sum_payoffs': 107.25640214268896, 'action': [1.0, -1.5707963267948966]}, {'num_count': 336, 'sum_payoffs': 97.57284613551694, 'action': [1.0, 0.0]}, {'num_count': 379, 'sum_payoffs': 114.89365292539071, 'action': [2.0, 0.0]}, {'num_count': 339, 'sum_payoffs': 98.71026127467465, 'action': [0.0, 1.5707963267948966]}, {'num_count': 353, 'sum_payoffs': 104.41156327129605, 'action': [2.0, -1.5707963267948966]}, {'num_count': 318, 'sum_payoffs': 90.35770720586737, 'action': [0.0, 0.0]}, {'num_count': 325, 'sum_payoffs': 93.09297215092418, 'action': [0.0, -1.5707963267948966]}, {'num_count': 351, 'sum_payoffs': 103.54397768899076, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.10931957433086101, 0.11609158336020639, 0.10835214446952596, 0.12221863914866173, 0.10931957433086101, 0.11383424701709126, 0.10254756530151564, 0.10480490164463076, 0.11318929377620122]
Actions to choose Agent 1: dict_values([{'num_count': 536, 'sum_payoffs': 155.06790013559893, 'action': [1.0, 1.5707963267948966]}, {'num_count': 567, 'sum_payoffs': 166.77500747816856, 'action': [1.0, 0.0]}, {'num_count': 490, 'sum_payoffs': 137.77835193955357, 'action': [0.0, -1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 137.84445977351496, 'action': [0.0, 0.0]}, {'num_count': 483, 'sum_payoffs': 135.26406186645198, 'action': [0.0, 1.5707963267948966]}, {'num_count': 534, 'sum_payoffs': 154.20784658621034, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1728474685585295, 0.18284424379232506, 0.1580135440180587, 0.1580135440180587, 0.15575620767494355, 0.17220251531763947]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 38.46762180328369 s
