Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 317, 'sum_payoffs': 89.80603313671601, 'action': [0.0, 1.5707963267948966]}, {'num_count': 370, 'sum_payoffs': 110.95712953995827, 'action': [2.0, -1.5707963267948966]}, {'num_count': 324, 'sum_payoffs': 92.47398880883732, 'action': [0.0, 0.0]}, {'num_count': 350, 'sum_payoffs': 102.97422001940636, 'action': [1.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 103.68934540683792, 'action': [2.0, 1.5707963267948966]}, {'num_count': 346, 'sum_payoffs': 101.30723729099905, 'action': [1.0, 1.5707963267948966]}, {'num_count': 338, 'sum_payoffs': 98.03726668067722, 'action': [1.0, 0.0]}, {'num_count': 328, 'sum_payoffs': 94.10906897122325, 'action': [0.0, -1.5707963267948966]}, {'num_count': 375, 'sum_payoffs': 113.00513523933273, 'action': [2.0, 0.0]}])
Weights num count: [0.10222508868107062, 0.11931634956465656, 0.10448242502418574, 0.11286681715575621, 0.11351177039664624, 0.11157691067397614, 0.10899709771041599, 0.10577233150596582, 0.12092873266688164]
Actions to choose Agent 1: dict_values([{'num_count': 534, 'sum_payoffs': 154.43988014388285, 'action': [1.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 135.01472592578253, 'action': [0.0, 0.0]}, {'num_count': 474, 'sum_payoffs': 131.9703139882905, 'action': [0.0, -1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 137.25891666655411, 'action': [0.0, 1.5707963267948966]}, {'num_count': 545, 'sum_payoffs': 158.48609290720177, 'action': [1.0, -1.5707963267948966]}, {'num_count': 577, 'sum_payoffs': 170.63357235840462, 'action': [1.0, 0.0]}])
Weights num count: [0.17220251531763947, 0.15543373105449854, 0.1528539180909384, 0.15736859077716867, 0.17574975814253466, 0.18606900999677523]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.049837589263916 s
