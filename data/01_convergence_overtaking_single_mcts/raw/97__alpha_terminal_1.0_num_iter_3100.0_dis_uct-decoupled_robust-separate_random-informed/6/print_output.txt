Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 345, 'sum_payoffs': 101.06303399534995, 'action': [2.0, -1.5707963267948966]}, {'num_count': 348, 'sum_payoffs': 102.28788035730919, 'action': [1.0, 0.0]}, {'num_count': 327, 'sum_payoffs': 93.95132814164899, 'action': [0.0, 0.0]}, {'num_count': 331, 'sum_payoffs': 95.49651623254354, 'action': [0.0, -1.5707963267948966]}, {'num_count': 358, 'sum_payoffs': 106.37754436804084, 'action': [2.0, 1.5707963267948966]}, {'num_count': 332, 'sum_payoffs': 95.8849223204474, 'action': [0.0, 1.5707963267948966]}, {'num_count': 337, 'sum_payoffs': 97.92189824270041, 'action': [1.0, 1.5707963267948966]}, {'num_count': 355, 'sum_payoffs': 105.11158189242228, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 109.91813505856508, 'action': [2.0, 0.0]}])
Weights num count: [0.11125443405353112, 0.11222186391486617, 0.1054498548855208, 0.10673976136730087, 0.11544663011931634, 0.10706223798774589, 0.10867462108997097, 0.11447920025798129, 0.11834891970332151]
Actions to choose Agent 1: dict_values([{'num_count': 537, 'sum_payoffs': 155.36668242515344, 'action': [1.0, -1.5707963267948966]}, {'num_count': 550, 'sum_payoffs': 160.23295054204507, 'action': [1.0, 1.5707963267948966]}, {'num_count': 490, 'sum_payoffs': 137.74228780498942, 'action': [0.0, 0.0]}, {'num_count': 560, 'sum_payoffs': 163.91608433339215, 'action': [1.0, 0.0]}, {'num_count': 474, 'sum_payoffs': 131.85004201420693, 'action': [0.0, -1.5707963267948966]}, {'num_count': 489, 'sum_payoffs': 137.41357255043735, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17316994517897452, 0.17736214124475974, 0.1580135440180587, 0.18058690744920994, 0.1528539180909384, 0.15769106739761368]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 41.27501463890076 s
