Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 328, 'sum_payoffs': 94.15734922540574, 'action': [0.0, 0.0]}, {'num_count': 370, 'sum_payoffs': 110.9020108190835, 'action': [2.0, 0.0]}, {'num_count': 334, 'sum_payoffs': 96.43607271466965, 'action': [1.0, 1.5707963267948966]}, {'num_count': 359, 'sum_payoffs': 106.50965794537461, 'action': [2.0, -1.5707963267948966]}, {'num_count': 350, 'sum_payoffs': 102.91265463206247, 'action': [1.0, 0.0]}, {'num_count': 322, 'sum_payoffs': 91.68675070134778, 'action': [0.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 108.89736965600504, 'action': [2.0, 1.5707963267948966]}, {'num_count': 328, 'sum_payoffs': 94.15782795649777, 'action': [0.0, 1.5707963267948966]}, {'num_count': 344, 'sum_payoffs': 100.53846461037097, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10577233150596582, 0.11931634956465656, 0.10770719122863592, 0.11576910673976137, 0.11286681715575621, 0.1038374717832957, 0.11770396646243148, 0.10577233150596582, 0.1109319574330861]
Actions to choose Agent 1: dict_values([{'num_count': 497, 'sum_payoffs': 140.455886646596, 'action': [0.0, 0.0]}, {'num_count': 542, 'sum_payoffs': 157.31824101586383, 'action': [1.0, 1.5707963267948966]}, {'num_count': 487, 'sum_payoffs': 136.7887962904494, 'action': [0.0, -1.5707963267948966]}, {'num_count': 540, 'sum_payoffs': 156.59457048787374, 'action': [1.0, -1.5707963267948966]}, {'num_count': 570, 'sum_payoffs': 167.87510701336834, 'action': [1.0, 0.0]}, {'num_count': 464, 'sum_payoffs': 128.28364264638483, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.16027088036117382, 0.1747823282811996, 0.15704611415672365, 0.17413737504030957, 0.1838116736536601, 0.14962915188648823]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 40.20588040351868 s
