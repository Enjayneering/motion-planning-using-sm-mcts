Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 326, 'sum_payoffs': 93.06290403948557, 'action': [0.0, 1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 107.56244672674556, 'action': [2.0, 0.0]}, {'num_count': 352, 'sum_payoffs': 103.58628259533361, 'action': [1.0, 0.0]}, {'num_count': 324, 'sum_payoffs': 92.36417606229685, 'action': [0.0, -1.5707963267948966]}, {'num_count': 322, 'sum_payoffs': 91.52575142419336, 'action': [0.0, 0.0]}, {'num_count': 361, 'sum_payoffs': 107.12248302178709, 'action': [2.0, -1.5707963267948966]}, {'num_count': 352, 'sum_payoffs': 103.57603563951271, 'action': [1.0, -1.5707963267948966]}, {'num_count': 362, 'sum_payoffs': 107.5748353634347, 'action': [2.0, 1.5707963267948966]}, {'num_count': 339, 'sum_payoffs': 98.2273562634778, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10512737826507579, 0.11673653660109642, 0.11351177039664624, 0.10448242502418574, 0.1038374717832957, 0.11641405998065141, 0.11351177039664624, 0.11673653660109642, 0.10931957433086101]
Actions to choose Agent 1: dict_values([{'num_count': 491, 'sum_payoffs': 138.52609195416164, 'action': [0.0, 0.0]}, {'num_count': 537, 'sum_payoffs': 155.72042123132795, 'action': [1.0, 1.5707963267948966]}, {'num_count': 493, 'sum_payoffs': 139.2494055635555, 'action': [0.0, 1.5707963267948966]}, {'num_count': 572, 'sum_payoffs': 168.96670481416743, 'action': [1.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 134.05106998763637, 'action': [0.0, -1.5707963267948966]}, {'num_count': 528, 'sum_payoffs': 152.44900264620642, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15833602063850372, 0.17316994517897452, 0.15898097387939375, 0.18445662689455014, 0.15446630119316349, 0.17026765559496937]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 41.40113949775696 s
