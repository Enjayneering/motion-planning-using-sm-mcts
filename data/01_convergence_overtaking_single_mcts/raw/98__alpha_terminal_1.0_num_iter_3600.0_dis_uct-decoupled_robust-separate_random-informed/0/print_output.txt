Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 381, 'sum_payoffs': 109.14109731888297, 'action': [1.0, 1.5707963267948966]}, {'num_count': 382, 'sum_payoffs': 109.45363088979526, 'action': [0.0, 1.5707963267948966]}, {'num_count': 411, 'sum_payoffs': 120.87798318054456, 'action': [2.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 120.09710289411194, 'action': [2.0, 1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 105.9709206456474, 'action': [0.0, -1.5707963267948966]}, {'num_count': 385, 'sum_payoffs': 110.72907543411148, 'action': [0.0, 0.0]}, {'num_count': 402, 'sum_payoffs': 117.37003326192455, 'action': [1.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 135.40154184785908, 'action': [2.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 120.06905332475739, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1058039433490697, 0.10608164398778117, 0.11413496251041377, 0.11357956123299083, 0.10358233823937794, 0.10691474590391557, 0.11163565676201055, 0.12440988614273812, 0.11357956123299083]
Actions to choose Agent 1: dict_values([{'num_count': 621, 'sum_payoffs': 177.66198806753584, 'action': [1.0, 1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 188.06308279099306, 'action': [1.0, 0.0]}, {'num_count': 591, 'sum_payoffs': 166.62688233613684, 'action': [0.0, 0.0]}, {'num_count': 626, 'sum_payoffs': 179.50332294963562, 'action': [1.0, -1.5707963267948966]}, {'num_count': 569, 'sum_payoffs': 158.76849733117473, 'action': [0.0, -1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 149.6182764038682, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17245209663982228, 0.1802277145237434, 0.1641210774784782, 0.17384059983337963, 0.15801166342682588, 0.15106914745903915]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.35418725013733 s
