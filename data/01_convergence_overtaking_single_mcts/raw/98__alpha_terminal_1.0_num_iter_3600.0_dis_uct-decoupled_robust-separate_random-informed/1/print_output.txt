Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 396, 'sum_payoffs': 114.67978743630475, 'action': [1.0, 0.0]}, {'num_count': 381, 'sum_payoffs': 108.74669460502409, 'action': [0.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 121.68599324424936, 'action': [2.0, 1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 119.63364534311178, 'action': [1.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 129.96189909393405, 'action': [2.0, 0.0]}, {'num_count': 426, 'sum_payoffs': 126.4253869567615, 'action': [2.0, -1.5707963267948966]}, {'num_count': 379, 'sum_payoffs': 107.98538762547638, 'action': [0.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 107.21923645728792, 'action': [0.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 109.4571585134669, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10996945292974174, 0.1058039433490697, 0.11496806442654818, 0.11357956123299083, 0.12079977783948903, 0.11830047209108581, 0.10524854207164676, 0.10469314079422383, 0.10635934462649264]
Actions to choose Agent 1: dict_values([{'num_count': 553, 'sum_payoffs': 153.84055854701003, 'action': [0.0, -1.5707963267948966]}, {'num_count': 557, 'sum_payoffs': 155.3156528424299, 'action': [0.0, 1.5707963267948966]}, {'num_count': 658, 'sum_payoffs': 192.46420152222788, 'action': [1.0, 0.0]}, {'num_count': 640, 'sum_payoffs': 185.82810718441206, 'action': [1.0, 1.5707963267948966]}, {'num_count': 568, 'sum_payoffs': 159.29871181037828, 'action': [0.0, 0.0]}, {'num_count': 624, 'sum_payoffs': 179.82209122130425, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1535684532074424, 0.15467925576228825, 0.18272702027214663, 0.17772840877534019, 0.1577339627881144, 0.17328519855595667]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.488016843795776 s
