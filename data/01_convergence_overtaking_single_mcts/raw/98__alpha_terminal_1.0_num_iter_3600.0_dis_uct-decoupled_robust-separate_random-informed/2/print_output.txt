Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 404, 'sum_payoffs': 117.27041945925608, 'action': [1.0, 1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 108.97338206127698, 'action': [0.0, -1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 120.37746679134831, 'action': [2.0, 1.5707963267948966]}, {'num_count': 423, 'sum_payoffs': 124.63774602603658, 'action': [2.0, 0.0]}, {'num_count': 381, 'sum_payoffs': 108.24671612858683, 'action': [0.0, 0.0]}, {'num_count': 398, 'sum_payoffs': 114.92943303897147, 'action': [1.0, 0.0]}, {'num_count': 374, 'sum_payoffs': 105.575309566564, 'action': [0.0, 1.5707963267948966]}, {'num_count': 419, 'sum_payoffs': 123.15347800309499, 'action': [2.0, -1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 117.9544082820416, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1121910580394335, 0.10635934462649264, 0.11441266314912524, 0.1174673701749514, 0.1058039433490697, 0.11052485420716468, 0.10386003887808942, 0.11635656762010553, 0.11274645931685642]
Actions to choose Agent 1: dict_values([{'num_count': 628, 'sum_payoffs': 181.64843414985813, 'action': [1.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 150.06569538594098, 'action': [0.0, -1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 189.32378817111766, 'action': [1.0, 0.0]}, {'num_count': 584, 'sum_payoffs': 165.32899172426949, 'action': [0.0, 0.0]}, {'num_count': 630, 'sum_payoffs': 182.23178465593324, 'action': [1.0, 1.5707963267948966]}, {'num_count': 567, 'sum_payoffs': 159.09667032725122, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17439600111080256, 0.15051374618161623, 0.1802277145237434, 0.16217717300749793, 0.17495140238822549, 0.15745626214940295]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.495421171188354 s
