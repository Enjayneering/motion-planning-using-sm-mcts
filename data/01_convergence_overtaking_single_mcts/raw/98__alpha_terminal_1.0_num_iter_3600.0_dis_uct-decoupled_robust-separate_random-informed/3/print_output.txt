Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 443, 'sum_payoffs': 133.45508363363038, 'action': [2.0, 0.0]}, {'num_count': 396, 'sum_payoffs': 114.91938038584546, 'action': [1.0, 0.0]}, {'num_count': 411, 'sum_payoffs': 120.80707703234413, 'action': [2.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 104.04564247053614, 'action': [0.0, 0.0]}, {'num_count': 391, 'sum_payoffs': 112.90216521937593, 'action': [0.0, -1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 127.57520409518258, 'action': [2.0, -1.5707963267948966]}, {'num_count': 404, 'sum_payoffs': 118.06137795209249, 'action': [1.0, -1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 103.63521373424192, 'action': [0.0, 1.5707963267948966]}, {'num_count': 392, 'sum_payoffs': 113.34535680511408, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.12302138294918079, 0.10996945292974174, 0.11413496251041377, 0.10219383504582061, 0.10858094973618439, 0.11885587336850875, 0.1121910580394335, 0.10191613440710913, 0.10885865037489587]
Actions to choose Agent 1: dict_values([{'num_count': 674, 'sum_payoffs': 197.40331402078354, 'action': [1.0, 0.0]}, {'num_count': 574, 'sum_payoffs': 160.69557313932614, 'action': [0.0, 0.0]}, {'num_count': 619, 'sum_payoffs': 177.1594335265165, 'action': [1.0, 1.5707963267948966]}, {'num_count': 544, 'sum_payoffs': 149.75333666793085, 'action': [0.0, -1.5707963267948966]}, {'num_count': 564, 'sum_payoffs': 156.98176558174455, 'action': [0.0, 1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 179.29365893082127, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.18717023049153014, 0.15940016662038323, 0.17189669536239932, 0.15106914745903915, 0.15662316023326853, 0.17356289919466814]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 48.37456917762756 s
