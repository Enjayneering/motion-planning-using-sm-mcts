Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 397, 'sum_payoffs': 114.57540059879388, 'action': [1.0, 0.0]}, {'num_count': 371, 'sum_payoffs': 104.39891113043942, 'action': [0.0, 1.5707963267948966]}, {'num_count': 381, 'sum_payoffs': 108.38775293834557, 'action': [0.0, -1.5707963267948966]}, {'num_count': 417, 'sum_payoffs': 122.40988413641959, 'action': [2.0, -1.5707963267948966]}, {'num_count': 394, 'sum_payoffs': 113.47190365287665, 'action': [0.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 116.92445049211459, 'action': [1.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 111.8257045455791, 'action': [1.0, 1.5707963267948966]}, {'num_count': 413, 'sum_payoffs': 120.85442339293378, 'action': [2.0, 1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 129.02876050129186, 'action': [2.0, 0.0]}])
Weights num count: [0.1102471535684532, 0.10302693696195502, 0.1058039433490697, 0.11580116634268259, 0.1094140516523188, 0.11191335740072202, 0.10830324909747292, 0.11469036378783672, 0.12052207720077757]
Actions to choose Agent 1: dict_values([{'num_count': 623, 'sum_payoffs': 179.5777285260058, 'action': [1.0, -1.5707963267948966]}, {'num_count': 669, 'sum_payoffs': 196.58556911147008, 'action': [1.0, 0.0]}, {'num_count': 567, 'sum_payoffs': 158.9665798487358, 'action': [0.0, 1.5707963267948966]}, {'num_count': 581, 'sum_payoffs': 164.14807253836864, 'action': [0.0, 0.0]}, {'num_count': 555, 'sum_payoffs': 154.58618428855462, 'action': [0.0, -1.5707963267948966]}, {'num_count': 605, 'sum_payoffs': 172.83062864163557, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.1730074979172452, 0.1857817272979728, 0.15745626214940295, 0.1613440710913635, 0.15412385448486532, 0.16800888642043876]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.47178387641907 s
