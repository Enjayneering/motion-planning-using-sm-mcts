Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 370, 'sum_payoffs': 104.76963152503949, 'action': [0.0, 1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 124.37137367647645, 'action': [2.0, -1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 107.93751026992673, 'action': [0.0, 0.0]}, {'num_count': 386, 'sum_payoffs': 110.95116420613982, 'action': [0.0, -1.5707963267948966]}, {'num_count': 407, 'sum_payoffs': 119.27635516744931, 'action': [2.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 127.99412885420182, 'action': [2.0, 0.0]}, {'num_count': 409, 'sum_payoffs': 119.97887261069847, 'action': [1.0, 0.0]}, {'num_count': 395, 'sum_payoffs': 114.49853470438727, 'action': [1.0, 1.5707963267948966]}, {'num_count': 406, 'sum_payoffs': 118.75332867693969, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.10274923632324354, 0.116634268258817, 0.1049708414329353, 0.10719244654262705, 0.1130241599555679, 0.11913357400722022, 0.11357956123299083, 0.10969175229103027, 0.11274645931685642]
Actions to choose Agent 1: dict_values([{'num_count': 571, 'sum_payoffs': 159.40960001089445, 'action': [0.0, -1.5707963267948966]}, {'num_count': 632, 'sum_payoffs': 181.74311085316054, 'action': [1.0, -1.5707963267948966]}, {'num_count': 628, 'sum_payoffs': 180.2535818750343, 'action': [1.0, 1.5707963267948966]}, {'num_count': 641, 'sum_payoffs': 185.06970675049777, 'action': [1.0, 0.0]}, {'num_count': 558, 'sum_payoffs': 154.6974457410679, 'action': [0.0, 0.0]}, {'num_count': 570, 'sum_payoffs': 159.06088508131734, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15856706470424883, 0.17550680366564844, 0.17439600111080256, 0.17800610941405165, 0.1549569564009997, 0.15828936406553734]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.48599982261658 s
