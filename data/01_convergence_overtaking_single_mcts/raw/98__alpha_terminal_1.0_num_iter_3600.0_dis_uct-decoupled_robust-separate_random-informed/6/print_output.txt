Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 393, 'sum_payoffs': 113.5467562036563, 'action': [1.0, 0.0]}, {'num_count': 377, 'sum_payoffs': 107.26002982000266, 'action': [0.0, 1.5707963267948966]}, {'num_count': 372, 'sum_payoffs': 105.31834428064819, 'action': [0.0, -1.5707963267948966]}, {'num_count': 373, 'sum_payoffs': 105.62135970181549, 'action': [0.0, 0.0]}, {'num_count': 403, 'sum_payoffs': 117.42836403612787, 'action': [1.0, -1.5707963267948966]}, {'num_count': 414, 'sum_payoffs': 121.66490631030818, 'action': [1.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 131.55003192546081, 'action': [2.0, 0.0]}, {'num_count': 417, 'sum_payoffs': 122.95578233091825, 'action': [2.0, 1.5707963267948966]}, {'num_count': 412, 'sum_payoffs': 120.93909331615454, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10913635101360733, 0.10469314079422383, 0.10330463760066648, 0.10358233823937794, 0.11191335740072202, 0.11496806442654818, 0.1219105803943349, 0.11580116634268259, 0.11441266314912524]
Actions to choose Agent 1: dict_values([{'num_count': 636, 'sum_payoffs': 183.8563395882497, 'action': [1.0, 1.5707963267948966]}, {'num_count': 550, 'sum_payoffs': 152.3666732484483, 'action': [0.0, 1.5707963267948966]}, {'num_count': 584, 'sum_payoffs': 164.70835747325737, 'action': [0.0, 0.0]}, {'num_count': 563, 'sum_payoffs': 157.0458551531692, 'action': [0.0, -1.5707963267948966]}, {'num_count': 601, 'sum_payoffs': 170.9730498056677, 'action': [1.0, -1.5707963267948966]}, {'num_count': 666, 'sum_payoffs': 194.96666817842868, 'action': [1.0, 0.0]}])
Weights num count: [0.1766176062204943, 0.15273535129130797, 0.16217717300749793, 0.15634545959455706, 0.16689808386559288, 0.18494862538183837]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.942819118499756 s
