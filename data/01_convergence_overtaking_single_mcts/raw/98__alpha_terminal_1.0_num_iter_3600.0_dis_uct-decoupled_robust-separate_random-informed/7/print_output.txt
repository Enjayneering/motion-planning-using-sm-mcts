Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 113.70690795159743, 'action': [1.0, 0.0]}, {'num_count': 421, 'sum_payoffs': 124.33596944170165, 'action': [2.0, -1.5707963267948966]}, {'num_count': 380, 'sum_payoffs': 108.1921878247499, 'action': [0.0, -1.5707963267948966]}, {'num_count': 390, 'sum_payoffs': 112.19510466894579, 'action': [0.0, 0.0]}, {'num_count': 432, 'sum_payoffs': 128.67200756598064, 'action': [2.0, 0.0]}, {'num_count': 381, 'sum_payoffs': 108.63601314077567, 'action': [1.0, 1.5707963267948966]}, {'num_count': 421, 'sum_payoffs': 124.27872449486667, 'action': [2.0, 1.5707963267948966]}, {'num_count': 378, 'sum_payoffs': 107.51749316827122, 'action': [0.0, 1.5707963267948966]}, {'num_count': 403, 'sum_payoffs': 117.18804675896988, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1094140516523188, 0.11691196889752846, 0.10552624271035824, 0.10830324909747292, 0.11996667592335462, 0.1058039433490697, 0.11691196889752846, 0.1049708414329353, 0.11191335740072202]
Actions to choose Agent 1: dict_values([{'num_count': 635, 'sum_payoffs': 183.7893440762757, 'action': [1.0, -1.5707963267948966]}, {'num_count': 542, 'sum_payoffs': 149.67406120813393, 'action': [0.0, -1.5707963267948966]}, {'num_count': 625, 'sum_payoffs': 179.97174777978574, 'action': [1.0, 1.5707963267948966]}, {'num_count': 574, 'sum_payoffs': 161.34862461477297, 'action': [0.0, 0.0]}, {'num_count': 576, 'sum_payoffs': 162.04352346218144, 'action': [0.0, 1.5707963267948966]}, {'num_count': 648, 'sum_payoffs': 188.58465419438727, 'action': [1.0, 0.0]}])
Weights num count: [0.17633990558178284, 0.15051374618161623, 0.17356289919466814, 0.15940016662038323, 0.15995556789780616, 0.17995001388503193]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 46.82332515716553 s
