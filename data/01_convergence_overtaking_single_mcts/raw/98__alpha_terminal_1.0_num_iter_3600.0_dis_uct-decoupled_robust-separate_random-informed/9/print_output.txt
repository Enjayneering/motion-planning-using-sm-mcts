Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 390, 'sum_payoffs': 112.25486238087761, 'action': [0.0, -1.5707963267948966]}, {'num_count': 399, 'sum_payoffs': 115.85344859656098, 'action': [1.0, -1.5707963267948966]}, {'num_count': 377, 'sum_payoffs': 107.22286167754504, 'action': [0.0, 1.5707963267948966]}, {'num_count': 402, 'sum_payoffs': 117.02237157504553, 'action': [1.0, 0.0]}, {'num_count': 414, 'sum_payoffs': 121.72233802595608, 'action': [2.0, 1.5707963267948966]}, {'num_count': 426, 'sum_payoffs': 126.50367329503058, 'action': [2.0, 0.0]}, {'num_count': 419, 'sum_payoffs': 123.64660903229897, 'action': [2.0, -1.5707963267948966]}, {'num_count': 383, 'sum_payoffs': 109.60527266699593, 'action': [0.0, 0.0]}, {'num_count': 390, 'sum_payoffs': 112.3647259901368, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10830324909747292, 0.11080255484587614, 0.10469314079422383, 0.11163565676201055, 0.11496806442654818, 0.11830047209108581, 0.11635656762010553, 0.10635934462649264, 0.10830324909747292]
Actions to choose Agent 1: dict_values([{'num_count': 554, 'sum_payoffs': 154.53587672021231, 'action': [0.0, 1.5707963267948966]}, {'num_count': 674, 'sum_payoffs': 198.7817670013638, 'action': [1.0, 0.0]}, {'num_count': 634, 'sum_payoffs': 183.93455574536895, 'action': [1.0, 1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 178.69126460438392, 'action': [1.0, -1.5707963267948966]}, {'num_count': 562, 'sum_payoffs': 157.4044191437601, 'action': [0.0, 0.0]}, {'num_count': 556, 'sum_payoffs': 155.25876739911376, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.15384615384615385, 0.18717023049153014, 0.17606220494307137, 0.1721743960011108, 0.1560677589558456, 0.15440155512357678]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 44.471535205841064 s
