Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 513, 'sum_payoffs': 153.62719562795974, 'action': [2.0, 0.0]}, {'num_count': 428, 'sum_payoffs': 120.84512467242646, 'action': [0.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 136.1472985248988, 'action': [1.0, -1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 117.74163947870015, 'action': [0.0, -1.5707963267948966]}, {'num_count': 424, 'sum_payoffs': 119.23548124796525, 'action': [0.0, 1.5707963267948966]}, {'num_count': 436, 'sum_payoffs': 123.93323900412265, 'action': [1.0, 1.5707963267948966]}, {'num_count': 462, 'sum_payoffs': 133.88797294110586, 'action': [2.0, 1.5707963267948966]}, {'num_count': 476, 'sum_payoffs': 139.27225583112897, 'action': [2.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 138.13281321547905, 'action': [1.0, 0.0]}])
Weights num count: [0.1250914411119239, 0.10436478907583516, 0.1141185076810534, 0.10241404535479151, 0.10338941721531333, 0.1063155327968788, 0.11265544989027067, 0.11606925140209705, 0.11533772250670568]
Actions to choose Agent 1: dict_values([{'num_count': 698, 'sum_payoffs': 198.79320196240377, 'action': [1.0, 1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 210.79672208423725, 'action': [1.0, -1.5707963267948966]}, {'num_count': 612, 'sum_payoffs': 167.84114349456362, 'action': [0.0, -1.5707963267948966]}, {'num_count': 653, 'sum_payoffs': 182.56637582153115, 'action': [0.0, 0.0]}, {'num_count': 652, 'sum_payoffs': 182.2074704905617, 'action': [0.0, 1.5707963267948966]}, {'num_count': 754, 'sum_payoffs': 219.08691113504062, 'action': [1.0, 0.0]}])
Weights num count: [0.17020238966105827, 0.17824920751036333, 0.14923189465983908, 0.15922945623018775, 0.1589856132650573, 0.18385759570836382]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.525620460510254 s
