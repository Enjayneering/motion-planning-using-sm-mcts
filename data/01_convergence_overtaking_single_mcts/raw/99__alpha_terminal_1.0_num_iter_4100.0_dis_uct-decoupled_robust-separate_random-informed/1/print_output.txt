Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 472, 'sum_payoffs': 137.6575986559104, 'action': [1.0, -1.5707963267948966]}, {'num_count': 420, 'sum_payoffs': 117.76554275040696, 'action': [0.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 134.18340711472646, 'action': [1.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 141.520074370632, 'action': [2.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 128.06498349474595, 'action': [1.0, 1.5707963267948966]}, {'num_count': 440, 'sum_payoffs': 125.22366039757047, 'action': [0.0, -1.5707963267948966]}, {'num_count': 437, 'sum_payoffs': 124.10900283567112, 'action': [0.0, 0.0]}, {'num_count': 462, 'sum_payoffs': 133.73181188160217, 'action': [2.0, 1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 139.60004216362546, 'action': [2.0, 0.0]}])
Weights num count: [0.11509387954157523, 0.10241404535479151, 0.11289929285540112, 0.11753230919287978, 0.10899780541331383, 0.10729090465740064, 0.10655937576200927, 0.11265544989027067, 0.1163130943672275]
Actions to choose Agent 1: dict_values([{'num_count': 646, 'sum_payoffs': 179.59752112787967, 'action': [0.0, 1.5707963267948966]}, {'num_count': 745, 'sum_payoffs': 215.34813066210037, 'action': [1.0, 0.0]}, {'num_count': 636, 'sum_payoffs': 175.96541990274804, 'action': [0.0, -1.5707963267948966]}, {'num_count': 701, 'sum_payoffs': 199.37741245461606, 'action': [1.0, 1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 180.58839800999885, 'action': [0.0, 0.0]}, {'num_count': 723, 'sum_payoffs': 207.36540719766847, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.15752255547427457, 0.1816630090221897, 0.15508412582297001, 0.17093391855644965, 0.15825408436966593, 0.17629846378931968]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.764217138290405 s
