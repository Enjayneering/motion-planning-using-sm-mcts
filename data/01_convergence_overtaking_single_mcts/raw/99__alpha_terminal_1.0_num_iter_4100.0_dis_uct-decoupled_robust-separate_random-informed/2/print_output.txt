Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 434, 'sum_payoffs': 123.21287979716008, 'action': [0.0, 0.0]}, {'num_count': 451, 'sum_payoffs': 129.68714490295397, 'action': [1.0, 1.5707963267948966]}, {'num_count': 488, 'sum_payoffs': 144.01395529948184, 'action': [2.0, 0.0]}, {'num_count': 452, 'sum_payoffs': 130.00390236487664, 'action': [1.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 135.07754950911306, 'action': [2.0, 1.5707963267948966]}, {'num_count': 432, 'sum_payoffs': 122.41975753096021, 'action': [0.0, -1.5707963267948966]}, {'num_count': 477, 'sum_payoffs': 139.6671504320577, 'action': [1.0, 0.0]}, {'num_count': 479, 'sum_payoffs': 140.48593906483367, 'action': [2.0, -1.5707963267948966]}, {'num_count': 422, 'sum_payoffs': 118.58031754068209, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10582784686661789, 0.10997317727383565, 0.11899536698366252, 0.11021702023896611, 0.11338697878566203, 0.10534016093635698, 0.1163130943672275, 0.11680078029748842, 0.10290173128505242]
Actions to choose Agent 1: dict_values([{'num_count': 634, 'sum_payoffs': 175.41648800799447, 'action': [0.0, -1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 206.6853724301911, 'action': [1.0, 1.5707963267948966]}, {'num_count': 629, 'sum_payoffs': 173.6296630167726, 'action': [0.0, 1.5707963267948966]}, {'num_count': 660, 'sum_payoffs': 184.63769849828262, 'action': [0.0, 0.0]}, {'num_count': 734, 'sum_payoffs': 211.4864136237451, 'action': [1.0, 0.0]}, {'num_count': 722, 'sum_payoffs': 207.12435755821048, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1545964398927091, 0.17581077785905877, 0.1533772250670568, 0.16093635698610095, 0.17898073640575468, 0.1760546208241892]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.7890727519989 s
