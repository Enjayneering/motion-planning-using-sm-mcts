Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 442, 'sum_payoffs': 125.96252680825084, 'action': [0.0, 0.0]}, {'num_count': 501, 'sum_payoffs': 148.69812389153037, 'action': [2.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 128.27141046843346, 'action': [1.0, 1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 121.78324974347139, 'action': [0.0, -1.5707963267948966]}, {'num_count': 465, 'sum_payoffs': 134.77978087860623, 'action': [2.0, 1.5707963267948966]}, {'num_count': 453, 'sum_payoffs': 130.15193715720545, 'action': [1.0, -1.5707963267948966]}, {'num_count': 485, 'sum_payoffs': 142.4433842937065, 'action': [2.0, -1.5707963267948966]}, {'num_count': 428, 'sum_payoffs': 120.56549611811911, 'action': [0.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 127.8202631217778, 'action': [1.0, 0.0]}])
Weights num count: [0.10777859058766155, 0.12216532553035844, 0.10924164837844429, 0.10509631797122652, 0.11338697878566203, 0.11046086320409657, 0.11826383808827115, 0.10436478907583516, 0.10899780541331383]
Actions to choose Agent 1: dict_values([{'num_count': 721, 'sum_payoffs': 206.95574177213842, 'action': [1.0, -1.5707963267948966]}, {'num_count': 724, 'sum_payoffs': 208.10060920189318, 'action': [1.0, 1.5707963267948966]}, {'num_count': 740, 'sum_payoffs': 213.91332946770984, 'action': [1.0, 0.0]}, {'num_count': 632, 'sum_payoffs': 174.89402903186786, 'action': [0.0, 1.5707963267948966]}, {'num_count': 642, 'sum_payoffs': 178.4572767158773, 'action': [0.0, 0.0]}, {'num_count': 641, 'sum_payoffs': 178.0229805792406, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.17581077785905877, 0.17654230675445012, 0.18044379419653744, 0.1541087539624482, 0.15654718361375275, 0.15630334064862228]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.19021940231323 s
