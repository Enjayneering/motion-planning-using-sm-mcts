Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 448, 'sum_payoffs': 128.5282960301198, 'action': [1.0, 1.5707963267948966]}, {'num_count': 425, 'sum_payoffs': 119.6598933853345, 'action': [0.0, 1.5707963267948966]}, {'num_count': 463, 'sum_payoffs': 134.27115798968356, 'action': [2.0, 1.5707963267948966]}, {'num_count': 482, 'sum_payoffs': 141.58206198755974, 'action': [2.0, 0.0]}, {'num_count': 485, 'sum_payoffs': 142.68185603774856, 'action': [2.0, -1.5707963267948966]}, {'num_count': 461, 'sum_payoffs': 133.51555130822678, 'action': [1.0, 0.0]}, {'num_count': 429, 'sum_payoffs': 121.18242885657295, 'action': [0.0, 0.0]}, {'num_count': 472, 'sum_payoffs': 137.6939068082924, 'action': [1.0, -1.5707963267948966]}, {'num_count': 435, 'sum_payoffs': 123.50048218602906, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.10924164837844429, 0.10363326018044379, 0.11289929285540112, 0.11753230919287978, 0.11826383808827115, 0.11241160692514021, 0.10460863204096561, 0.11509387954157523, 0.10607168983174835]
Actions to choose Agent 1: dict_values([{'num_count': 714, 'sum_payoffs': 204.42237450536726, 'action': [1.0, -1.5707963267948966]}, {'num_count': 706, 'sum_payoffs': 201.5153678653939, 'action': [1.0, 1.5707963267948966]}, {'num_count': 635, 'sum_payoffs': 175.93071163213452, 'action': [0.0, -1.5707963267948966]}, {'num_count': 657, 'sum_payoffs': 183.83303736579205, 'action': [0.0, 0.0]}, {'num_count': 760, 'sum_payoffs': 221.1938322304313, 'action': [1.0, 0.0]}, {'num_count': 628, 'sum_payoffs': 173.3699962131141, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.17410387710314557, 0.17215313338210192, 0.15484028285783955, 0.16020482809070957, 0.18532065349914656, 0.15313338210192637]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.36416244506836 s
