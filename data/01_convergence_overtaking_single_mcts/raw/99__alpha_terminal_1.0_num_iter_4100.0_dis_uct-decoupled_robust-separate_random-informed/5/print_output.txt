Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 442, 'sum_payoffs': 126.03281437343146, 'action': [0.0, 0.0]}, {'num_count': 484, 'sum_payoffs': 142.14847273876092, 'action': [2.0, -1.5707963267948966]}, {'num_count': 494, 'sum_payoffs': 146.1039565846268, 'action': [2.0, 0.0]}, {'num_count': 435, 'sum_payoffs': 123.45315077139537, 'action': [0.0, -1.5707963267948966]}, {'num_count': 473, 'sum_payoffs': 137.92647140600943, 'action': [2.0, 1.5707963267948966]}, {'num_count': 429, 'sum_payoffs': 121.08281924044272, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 128.3881747274335, 'action': [1.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 127.66189982601803, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 128.71332035377543, 'action': [1.0, 0.0]}])
Weights num count: [0.10777859058766155, 0.1180199951231407, 0.12045842477444525, 0.10607168983174835, 0.11533772250670568, 0.10460863204096561, 0.10924164837844429, 0.10875396244818337, 0.10948549134357474]
Actions to choose Agent 1: dict_values([{'num_count': 640, 'sum_payoffs': 177.51735943769066, 'action': [0.0, 0.0]}, {'num_count': 642, 'sum_payoffs': 178.36777169696592, 'action': [0.0, -1.5707963267948966]}, {'num_count': 714, 'sum_payoffs': 204.25999048905376, 'action': [1.0, 1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 202.49969508467933, 'action': [1.0, -1.5707963267948966]}, {'num_count': 759, 'sum_payoffs': 220.69354449825846, 'action': [1.0, 0.0]}, {'num_count': 636, 'sum_payoffs': 176.1572855217204, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.15605949768349184, 0.15654718361375275, 0.17410387710314557, 0.1728846622774933, 0.1850768105340161, 0.15508412582297001]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.213817834854126 s
