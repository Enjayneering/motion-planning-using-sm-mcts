Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 421, 'sum_payoffs': 118.26506321744964, 'action': [0.0, 1.5707963267948966]}, {'num_count': 475, 'sum_payoffs': 138.91675558672867, 'action': [2.0, 1.5707963267948966]}, {'num_count': 439, 'sum_payoffs': 125.04311661456619, 'action': [0.0, -1.5707963267948966]}, {'num_count': 455, 'sum_payoffs': 131.21201140451015, 'action': [1.0, -1.5707963267948966]}, {'num_count': 512, 'sum_payoffs': 153.25572352796013, 'action': [2.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 125.12626388323746, 'action': [1.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 128.89432681835913, 'action': [1.0, 0.0]}, {'num_count': 439, 'sum_payoffs': 125.1629608607584, 'action': [0.0, 0.0]}, {'num_count': 471, 'sum_payoffs': 137.390575264463, 'action': [2.0, -1.5707963267948966]}])
Weights num count: [0.10265788831992197, 0.11582540843696659, 0.10704706169227018, 0.11094854913435748, 0.12484759814679347, 0.10704706169227018, 0.10948549134357474, 0.10704706169227018, 0.11485003657644477]
Actions to choose Agent 1: dict_values([{'num_count': 635, 'sum_payoffs': 176.17706166662643, 'action': [0.0, 1.5707963267948966]}, {'num_count': 695, 'sum_payoffs': 197.86425835047967, 'action': [1.0, -1.5707963267948966]}, {'num_count': 773, 'sum_payoffs': 226.30508774808771, 'action': [1.0, 0.0]}, {'num_count': 627, 'sum_payoffs': 173.41691974133664, 'action': [0.0, -1.5707963267948966]}, {'num_count': 709, 'sum_payoffs': 202.87339126538657, 'action': [1.0, 1.5707963267948966]}, {'num_count': 661, 'sum_payoffs': 185.5188274024572, 'action': [0.0, 0.0]}])
Weights num count: [0.15484028285783955, 0.16947086076566692, 0.18849061204584247, 0.1528895391367959, 0.1728846622774933, 0.1611801999512314]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 51.92883062362671 s
