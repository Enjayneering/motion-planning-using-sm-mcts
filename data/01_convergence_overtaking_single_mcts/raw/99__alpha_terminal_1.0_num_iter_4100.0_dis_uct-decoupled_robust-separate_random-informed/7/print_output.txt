Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 447, 'sum_payoffs': 128.5816879265245, 'action': [1.0, 1.5707963267948966]}, {'num_count': 454, 'sum_payoffs': 131.30748670305658, 'action': [1.0, 0.0]}, {'num_count': 478, 'sum_payoffs': 140.50287701288167, 'action': [2.0, 1.5707963267948966]}, {'num_count': 501, 'sum_payoffs': 149.4343548888907, 'action': [2.0, 0.0]}, {'num_count': 454, 'sum_payoffs': 131.2864891594494, 'action': [1.0, -1.5707963267948966]}, {'num_count': 483, 'sum_payoffs': 142.4712419191657, 'action': [2.0, -1.5707963267948966]}, {'num_count': 434, 'sum_payoffs': 123.67065849065747, 'action': [0.0, 0.0]}, {'num_count': 440, 'sum_payoffs': 125.9104126036889, 'action': [0.0, -1.5707963267948966]}, {'num_count': 409, 'sum_payoffs': 114.06055347685523, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.10899780541331383, 0.11070470616922702, 0.11655693733235796, 0.12216532553035844, 0.11070470616922702, 0.11777615215801024, 0.10582784686661789, 0.10729090465740064, 0.0997317727383565]
Actions to choose Agent 1: dict_values([{'num_count': 641, 'sum_payoffs': 176.92752539097313, 'action': [0.0, 1.5707963267948966]}, {'num_count': 721, 'sum_payoffs': 205.72538710156667, 'action': [1.0, 1.5707963267948966]}, {'num_count': 649, 'sum_payoffs': 179.83778278565126, 'action': [0.0, 0.0]}, {'num_count': 703, 'sum_payoffs': 199.25470700261374, 'action': [1.0, -1.5707963267948966]}, {'num_count': 620, 'sum_payoffs': 169.53596136422638, 'action': [0.0, -1.5707963267948966]}, {'num_count': 766, 'sum_payoffs': 222.04074416988496, 'action': [1.0, 0.0]}])
Weights num count: [0.15630334064862228, 0.17581077785905877, 0.15825408436966593, 0.17142160448671057, 0.15118263838088272, 0.1867837112899293]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 50.510979413986206 s
