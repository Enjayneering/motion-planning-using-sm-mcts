Searching game tree in timestep 0...
Max timehorizon: 7
Actions to choose Agent 0: dict_values([{'num_count': 426, 'sum_payoffs': 120.07412970134871, 'action': [0.0, 1.5707963267948966]}, {'num_count': 496, 'sum_payoffs': 146.96946335680857, 'action': [2.0, 0.0]}, {'num_count': 468, 'sum_payoffs': 136.1716746066966, 'action': [1.0, -1.5707963267948966]}, {'num_count': 469, 'sum_payoffs': 136.57633041716693, 'action': [2.0, 1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 122.01212526296534, 'action': [0.0, -1.5707963267948966]}, {'num_count': 442, 'sum_payoffs': 126.15103693983538, 'action': [0.0, 0.0]}, {'num_count': 455, 'sum_payoffs': 131.1750390558809, 'action': [1.0, 0.0]}, {'num_count': 482, 'sum_payoffs': 141.51339622223625, 'action': [2.0, -1.5707963267948966]}, {'num_count': 431, 'sum_payoffs': 121.99066550819512, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.10387710314557425, 0.12094611070470616, 0.1141185076810534, 0.11436235064618386, 0.10509631797122652, 0.10777859058766155, 0.11094854913435748, 0.11753230919287978, 0.10509631797122652]
Actions to choose Agent 1: dict_values([{'num_count': 634, 'sum_payoffs': 175.40099710065402, 'action': [0.0, 1.5707963267948966]}, {'num_count': 655, 'sum_payoffs': 182.8733650806656, 'action': [0.0, 0.0]}, {'num_count': 767, 'sum_payoffs': 223.38087456582744, 'action': [1.0, 0.0]}, {'num_count': 690, 'sum_payoffs': 195.53479392946855, 'action': [1.0, 1.5707963267948966]}, {'num_count': 731, 'sum_payoffs': 210.26644546019205, 'action': [1.0, -1.5707963267948966]}, {'num_count': 623, 'sum_payoffs': 171.4268291601847, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1545964398927091, 0.15971714216044866, 0.18702755425505974, 0.16825164594001463, 0.17824920751036333, 0.15191416727627408]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 49.80560040473938 s
