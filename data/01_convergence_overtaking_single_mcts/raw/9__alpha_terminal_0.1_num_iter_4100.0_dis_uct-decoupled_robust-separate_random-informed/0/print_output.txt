Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 554, 'sum_payoffs': 154.3628185649779, 'action': [2.0, 1.5707963267948966]}, {'num_count': 556, 'sum_payoffs': 155.08245874476805, 'action': [2.0, -1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 153.25337328780142, 'action': [2.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 116.06196899614955, 'action': [1.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 87.29635180953899, 'action': [0.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 115.3623188213536, 'action': [1.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 116.12193901113206, 'action': [1.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 87.66616690193115, 'action': [0.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 86.69665165971391, 'action': [0.0, 0.0]}])
Weights num count: [0.1350890026822726, 0.13557668861253352, 0.13435747378688126, 0.10924164837844429, 0.08924652523774688, 0.10875396244818337, 0.10924164837844429, 0.08949036820287734, 0.08875883930748597]
Actions to choose Agent 1: dict_values([{'num_count': 591, 'sum_payoffs': 181.05947023469176, 'action': [0.0, 0.0]}, {'num_count': 597, 'sum_payoffs': 183.39830081900948, 'action': [0.0, 1.5707963267948966]}, {'num_count': 768, 'sum_payoffs': 251.05447272177503, 'action': [1.0, -1.5707963267948966]}, {'num_count': 598, 'sum_payoffs': 183.7081458964191, 'action': [0.0, -1.5707963267948966]}, {'num_count': 770, 'sum_payoffs': 251.874062926536, 'action': [1.0, 1.5707963267948966]}, {'num_count': 776, 'sum_payoffs': 254.25287352084237, 'action': [1.0, 0.0]}])
Weights num count: [0.1441111923920995, 0.14557425018288223, 0.1872713972201902, 0.14581809314801267, 0.18775908315045112, 0.18922214094123385]
Selected final action: [2.0, -1.5707963267948966, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5462043285369873 s
