Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 445, 'sum_payoffs': 115.05247374394396, 'action': [1.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 154.73263365737012, 'action': [2.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 116.45177409353585, 'action': [1.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 87.3563218245215, 'action': [0.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 87.33633181952733, 'action': [0.0, 1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 86.67666165471972, 'action': [0.0, -1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 154.01299347757995, 'action': [2.0, 1.5707963267948966]}, {'num_count': 451, 'sum_payoffs': 117.13143426333764, 'action': [1.0, 0.0]}, {'num_count': 551, 'sum_payoffs': 153.25337328780145, 'action': [2.0, 0.0]}])
Weights num count: [0.10851011948305292, 0.13533284564740308, 0.10948549134357474, 0.08924652523774688, 0.08924652523774688, 0.08875883930748597, 0.13484515971714217, 0.10997317727383565, 0.13435747378688126]
Actions to choose Agent 1: dict_values([{'num_count': 760, 'sum_payoffs': 247.8560719227081, 'action': [1.0, -1.5707963267948966]}, {'num_count': 592, 'sum_payoffs': 181.38930531709553, 'action': [0.0, 1.5707963267948966]}, {'num_count': 598, 'sum_payoffs': 183.72813590141345, 'action': [0.0, 0.0]}, {'num_count': 598, 'sum_payoffs': 183.72813590141322, 'action': [0.0, -1.5707963267948966]}, {'num_count': 779, 'sum_payoffs': 255.50224883297784, 'action': [1.0, 1.5707963267948966]}, {'num_count': 773, 'sum_payoffs': 253.1234382386714, 'action': [1.0, 0.0]}])
Weights num count: [0.18532065349914656, 0.14435503535722993, 0.14581809314801267, 0.14581809314801267, 0.1899536698366252, 0.18849061204584247]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5865030288696289 s
