Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 557, 'sum_payoffs': 155.47226384215438, 'action': [2.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 87.37631182951567, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 115.34232881635943, 'action': [1.0, 1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 154.2828585450013, 'action': [2.0, 1.5707963267948966]}, {'num_count': 445, 'sum_payoffs': 115.05247374394396, 'action': [1.0, 0.0]}, {'num_count': 367, 'sum_payoffs': 87.72613691691366, 'action': [0.0, 0.0]}, {'num_count': 554, 'sum_payoffs': 154.34282855998373, 'action': [2.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 115.38230882634777, 'action': [1.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 86.96651672713519, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.135820531577664, 0.08924652523774688, 0.10875396244818337, 0.1350890026822726, 0.10851011948305292, 0.08949036820287734, 0.1350890026822726, 0.10875396244818337, 0.08900268227261643]
Actions to choose Agent 1: dict_values([{'num_count': 766, 'sum_payoffs': 250.21489251201993, 'action': [1.0, 1.5707963267948966]}, {'num_count': 594, 'sum_payoffs': 182.18890551686238, 'action': [0.0, 1.5707963267948966]}, {'num_count': 593, 'sum_payoffs': 181.83908042946436, 'action': [0.0, 0.0]}, {'num_count': 773, 'sum_payoffs': 253.08345822868327, 'action': [1.0, 0.0]}, {'num_count': 776, 'sum_payoffs': 254.25287352084243, 'action': [1.0, -1.5707963267948966]}, {'num_count': 598, 'sum_payoffs': 183.7281359014135, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1867837112899293, 0.14484272128749084, 0.1445988783223604, 0.18849061204584247, 0.18922214094123385, 0.14581809314801267]
Selected final action: [2.0, 0.0, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5726900100708008 s
