Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 552, 'sum_payoffs': 153.6631683901819, 'action': [2.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 116.45177409353586, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 115.77211392373408, 'action': [1.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 87.72613691691366, 'action': [0.0, 1.5707963267948966]}, {'num_count': 369, 'sum_payoffs': 88.38580708172125, 'action': [0.0, -1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 88.01599198932912, 'action': [0.0, 0.0]}, {'num_count': 448, 'sum_payoffs': 116.14192901612623, 'action': [1.0, 0.0]}, {'num_count': 552, 'sum_payoffs': 153.6831583951761, 'action': [2.0, 0.0]}, {'num_count': 548, 'sum_payoffs': 152.20389802560769, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.1346013167520117, 0.10948549134357474, 0.10899780541331383, 0.08949036820287734, 0.08997805413313825, 0.0897342111680078, 0.10924164837844429, 0.1346013167520117, 0.13362594489148988]
Actions to choose Agent 1: dict_values([{'num_count': 606, 'sum_payoffs': 186.88655669049226, 'action': [0.0, 1.5707963267948966]}, {'num_count': 601, 'sum_payoffs': 184.93753120356064, 'action': [0.0, 0.0]}, {'num_count': 768, 'sum_payoffs': 251.09445273176317, 'action': [1.0, 0.0]}, {'num_count': 771, 'sum_payoffs': 252.3038480339109, 'action': [1.0, 1.5707963267948966]}, {'num_count': 756, 'sum_payoffs': 246.2968515331628, 'action': [1.0, -1.5707963267948966]}, {'num_count': 598, 'sum_payoffs': 183.76811591140154, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.14776883686905634, 0.14654962204340405, 0.1872713972201902, 0.18800292611558156, 0.18434528163862474, 0.14581809314801267]
Selected final action: [2.0, -1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.538611888885498 s
