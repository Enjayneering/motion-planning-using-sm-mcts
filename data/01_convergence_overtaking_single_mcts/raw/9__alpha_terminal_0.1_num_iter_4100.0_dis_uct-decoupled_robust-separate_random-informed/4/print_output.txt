Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 366, 'sum_payoffs': 87.39630183450984, 'action': [0.0, -1.5707963267948966]}, {'num_count': 557, 'sum_payoffs': 155.49225384714848, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 115.38230882634777, 'action': [1.0, -1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 87.31634181453316, 'action': [0.0, 1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 115.71214390875157, 'action': [1.0, 0.0]}, {'num_count': 552, 'sum_payoffs': 153.62318838019365, 'action': [2.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 86.94652672214102, 'action': [0.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 115.44227884133028, 'action': [1.0, 1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 154.69265364738172, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.08924652523774688, 0.135820531577664, 0.10875396244818337, 0.08924652523774688, 0.10899780541331383, 0.1346013167520117, 0.08900268227261643, 0.10875396244818337, 0.13533284564740308]
Actions to choose Agent 1: dict_values([{'num_count': 772, 'sum_payoffs': 252.65367312130894, 'action': [1.0, 1.5707963267948966]}, {'num_count': 589, 'sum_payoffs': 180.21989002493663, 'action': [0.0, -1.5707963267948966]}, {'num_count': 594, 'sum_payoffs': 182.18890551686232, 'action': [0.0, 0.0]}, {'num_count': 776, 'sum_payoffs': 254.2728635258364, 'action': [1.0, 0.0]}, {'num_count': 770, 'sum_payoffs': 251.85407292154167, 'action': [1.0, -1.5707963267948966]}, {'num_count': 599, 'sum_payoffs': 184.1779110137823, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.18824676908071203, 0.14362350646183858, 0.14484272128749084, 0.18922214094123385, 0.18775908315045112, 0.14606193611314314]
Selected final action: [2.0, 0.0, 1.0, 0.0]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.531341552734375 s
