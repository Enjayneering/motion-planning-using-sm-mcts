Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 368, 'sum_payoffs': 88.0359819943233, 'action': [0.0, -1.5707963267948966]}, {'num_count': 553, 'sum_payoffs': 154.01299347758004, 'action': [2.0, 1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 86.96651672713519, 'action': [0.0, 1.5707963267948966]}, {'num_count': 556, 'sum_payoffs': 155.10244874976212, 'action': [2.0, 0.0]}, {'num_count': 446, 'sum_payoffs': 115.44227884133028, 'action': [1.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 87.33633181952733, 'action': [0.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 115.73213391374574, 'action': [1.0, 0.0]}, {'num_count': 552, 'sum_payoffs': 153.663168390182, 'action': [2.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 115.79210392872825, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.0897342111680078, 0.13484515971714217, 0.08900268227261643, 0.13557668861253352, 0.10875396244818337, 0.08924652523774688, 0.10899780541331383, 0.1346013167520117, 0.10899780541331383]
Actions to choose Agent 1: dict_values([{'num_count': 766, 'sum_payoffs': 250.2548725220083, 'action': [1.0, 0.0]}, {'num_count': 773, 'sum_payoffs': 253.12343823867175, 'action': [1.0, 1.5707963267948966]}, {'num_count': 597, 'sum_payoffs': 183.31834079903294, 'action': [0.0, -1.5707963267948966]}, {'num_count': 594, 'sum_payoffs': 182.20889552185648, 'action': [0.0, 0.0]}, {'num_count': 772, 'sum_payoffs': 252.67366312630273, 'action': [1.0, -1.5707963267948966]}, {'num_count': 598, 'sum_payoffs': 183.7481259064075, 'action': [0.0, 1.5707963267948966]}])
Weights num count: [0.1867837112899293, 0.18849061204584247, 0.14557425018288223, 0.14484272128749084, 0.18824676908071203, 0.14581809314801267]
Selected final action: [2.0, 0.0, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.49138855934143066 s
