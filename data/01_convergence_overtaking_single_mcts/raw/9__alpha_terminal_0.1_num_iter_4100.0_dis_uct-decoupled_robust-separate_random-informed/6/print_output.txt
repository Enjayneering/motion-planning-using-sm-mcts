Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 368, 'sum_payoffs': 88.0359819943233, 'action': [0.0, 1.5707963267948966]}, {'num_count': 366, 'sum_payoffs': 87.37631182951569, 'action': [0.0, -1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 115.44227884133028, 'action': [1.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 115.73213391374574, 'action': [1.0, -1.5707963267948966]}, {'num_count': 365, 'sum_payoffs': 86.98650673212936, 'action': [0.0, 0.0]}, {'num_count': 555, 'sum_payoffs': 154.77261366735837, 'action': [2.0, 1.5707963267948966]}, {'num_count': 554, 'sum_payoffs': 154.3428285599838, 'action': [2.0, -1.5707963267948966]}, {'num_count': 552, 'sum_payoffs': 153.68315839517624, 'action': [2.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 115.73213391374574, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.0897342111680078, 0.08924652523774688, 0.10875396244818337, 0.10899780541331383, 0.08900268227261643, 0.13533284564740308, 0.1350890026822726, 0.1346013167520117, 0.10899780541331383]
Actions to choose Agent 1: dict_values([{'num_count': 770, 'sum_payoffs': 251.87406292653614, 'action': [1.0, 0.0]}, {'num_count': 597, 'sum_payoffs': 183.3783108140154, 'action': [0.0, 0.0]}, {'num_count': 776, 'sum_payoffs': 254.27286352583636, 'action': [1.0, -1.5707963267948966]}, {'num_count': 595, 'sum_payoffs': 182.59870061924272, 'action': [0.0, -1.5707963267948966]}, {'num_count': 598, 'sum_payoffs': 183.68815589142525, 'action': [0.0, 1.5707963267948966]}, {'num_count': 764, 'sum_payoffs': 249.49525233223, 'action': [1.0, 1.5707963267948966]}])
Weights num count: [0.18775908315045112, 0.14557425018288223, 0.18922214094123385, 0.14508656425262131, 0.14581809314801267, 0.18629602535966838]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.49593424797058105 s
