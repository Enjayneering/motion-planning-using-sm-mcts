Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 552, 'sum_payoffs': 153.62318838019357, 'action': [2.0, 1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 116.51174410851837, 'action': [1.0, 0.0]}, {'num_count': 549, 'sum_payoffs': 152.57371311799977, 'action': [2.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 87.39630183450986, 'action': [0.0, 0.0]}, {'num_count': 368, 'sum_payoffs': 88.05597199931745, 'action': [0.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 115.38230882634777, 'action': [1.0, 1.5707963267948966]}, {'num_count': 367, 'sum_payoffs': 87.74612692190782, 'action': [0.0, -1.5707963267948966]}, {'num_count': 556, 'sum_payoffs': 155.04247873477948, 'action': [2.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 115.79210392872825, 'action': [1.0, -1.5707963267948966]}])
Weights num count: [0.1346013167520117, 0.10948549134357474, 0.13386978785662035, 0.08924652523774688, 0.0897342111680078, 0.10875396244818337, 0.08949036820287734, 0.13557668861253352, 0.10899780541331383]
Actions to choose Agent 1: dict_values([{'num_count': 762, 'sum_payoffs': 248.67566212746885, 'action': [1.0, 0.0]}, {'num_count': 603, 'sum_payoffs': 185.75712140832164, 'action': [0.0, 0.0]}, {'num_count': 771, 'sum_payoffs': 252.26386802392233, 'action': [1.0, -1.5707963267948966]}, {'num_count': 596, 'sum_payoffs': 182.94852570664048, 'action': [0.0, 1.5707963267948966]}, {'num_count': 768, 'sum_payoffs': 251.0344827167809, 'action': [1.0, 1.5707963267948966]}, {'num_count': 600, 'sum_payoffs': 184.56771611116858, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18580833942940747, 0.14703730797366496, 0.18800292611558156, 0.14533040721775176, 0.1872713972201902, 0.14630577907827358]
Selected final action: [2.0, -1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5048065185546875 s
