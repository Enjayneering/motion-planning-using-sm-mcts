Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 554, 'sum_payoffs': 154.36281856497786, 'action': [2.0, 0.0]}, {'num_count': 555, 'sum_payoffs': 154.69265364738172, 'action': [2.0, 1.5707963267948966]}, {'num_count': 446, 'sum_payoffs': 115.34232881635943, 'action': [1.0, 1.5707963267948966]}, {'num_count': 368, 'sum_payoffs': 88.07596200431162, 'action': [0.0, 1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 86.32683656732175, 'action': [0.0, -1.5707963267948966]}, {'num_count': 449, 'sum_payoffs': 116.43178408854169, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 115.7521239187399, 'action': [1.0, 0.0]}, {'num_count': 554, 'sum_payoffs': 154.30284854999536, 'action': [2.0, -1.5707963267948966]}, {'num_count': 364, 'sum_payoffs': 86.6966516597139, 'action': [0.0, 0.0]}])
Weights num count: [0.1350890026822726, 0.13533284564740308, 0.10875396244818337, 0.0897342111680078, 0.08851499634235552, 0.10948549134357474, 0.10899780541331383, 0.1350890026822726, 0.08875883930748597]
Actions to choose Agent 1: dict_values([{'num_count': 768, 'sum_payoffs': 251.09445273176334, 'action': [1.0, 0.0]}, {'num_count': 594, 'sum_payoffs': 182.16891551186805, 'action': [0.0, 1.5707963267948966]}, {'num_count': 599, 'sum_payoffs': 184.11794099879964, 'action': [0.0, 0.0]}, {'num_count': 778, 'sum_payoffs': 255.05247372060896, 'action': [1.0, 1.5707963267948966]}, {'num_count': 770, 'sum_payoffs': 251.8340829165477, 'action': [1.0, -1.5707963267948966]}, {'num_count': 591, 'sum_payoffs': 181.03948022969755, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.1872713972201902, 0.14484272128749084, 0.14606193611314314, 0.18970982687149476, 0.18775908315045112, 0.1441111923920995]
Selected final action: [2.0, 1.5707963267948966, 1.0, 1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.48998069763183594 s
