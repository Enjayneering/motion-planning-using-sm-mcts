Searching game tree in timestep 0...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 367, 'sum_payoffs': 87.72613691691365, 'action': [0.0, 0.0]}, {'num_count': 366, 'sum_payoffs': 87.31634181453316, 'action': [0.0, 1.5707963267948966]}, {'num_count': 448, 'sum_payoffs': 116.08195900114372, 'action': [1.0, -1.5707963267948966]}, {'num_count': 447, 'sum_payoffs': 115.79210392872825, 'action': [1.0, 0.0]}, {'num_count': 447, 'sum_payoffs': 115.75212391873991, 'action': [1.0, 1.5707963267948966]}, {'num_count': 363, 'sum_payoffs': 86.34682657231592, 'action': [0.0, -1.5707963267948966]}, {'num_count': 555, 'sum_payoffs': 154.7326336573701, 'action': [2.0, -1.5707963267948966]}, {'num_count': 551, 'sum_payoffs': 153.27336329279566, 'action': [2.0, 0.0]}, {'num_count': 556, 'sum_payoffs': 155.02248872978544, 'action': [2.0, 1.5707963267948966]}])
Weights num count: [0.08949036820287734, 0.08924652523774688, 0.10924164837844429, 0.10899780541331383, 0.10899780541331383, 0.08851499634235552, 0.13533284564740308, 0.13435747378688126, 0.13557668861253352]
Actions to choose Agent 1: dict_values([{'num_count': 770, 'sum_payoffs': 251.87406292653608, 'action': [1.0, 0.0]}, {'num_count': 594, 'sum_payoffs': 182.18890551686226, 'action': [0.0, 0.0]}, {'num_count': 591, 'sum_payoffs': 180.99950021970932, 'action': [0.0, 1.5707963267948966]}, {'num_count': 765, 'sum_payoffs': 249.9050474346105, 'action': [1.0, 1.5707963267948966]}, {'num_count': 780, 'sum_payoffs': 255.8320839153815, 'action': [1.0, -1.5707963267948966]}, {'num_count': 600, 'sum_payoffs': 184.52773610118004, 'action': [0.0, -1.5707963267948966]}])
Weights num count: [0.18775908315045112, 0.14484272128749084, 0.1441111923920995, 0.18653986832479882, 0.19019751280175567, 0.14630577907827358]
Selected final action: [2.0, 1.5707963267948966, 1.0, -1.5707963267948966]
Total payoff list: [0.2777777777453703, 0.22222222219629628]
Runtime: 0.5193197727203369 s
