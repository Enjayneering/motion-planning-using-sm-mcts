Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1093, 'sum_payoffs': 221.51978285874154, 'action': [1.0, 0]}, {'num_count': 507, 'sum_payoffs': 75.15224998778754, 'action': [0.0, 0]}])
Weights num count: [0.6826983135540288, 0.31667707682698315]
Actions to choose Agent 1: dict_values([{'num_count': 511, 'sum_payoffs': 76.05787498764039, 'action': [0.0, 0]}, {'num_count': 1089, 'sum_payoffs': 220.61415785888855, 'action': [1.0, 0]}])
Weights num count: [0.31917551530293564, 0.6801998750780762]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 958, 'sum_payoffs': 205.32374996663629, 'action': [1.0, 0]}, {'num_count': 642, 'sum_payoffs': 119.92499998051126, 'action': [0.0, 0]}])
Weights num count: [0.5983760149906309, 0.400999375390381]
Actions to choose Agent 1: dict_values([{'num_count': 639, 'sum_payoffs': 119.21624998062646, 'action': [0.0, 0]}, {'num_count': 961, 'sum_payoffs': 206.23499996648826, 'action': [1.0, 0]}])
Weights num count: [0.39912554653341664, 0.6002498438475953]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2661159038543701 s
