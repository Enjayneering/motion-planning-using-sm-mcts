Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1574, 'sum_payoffs': 601.4249997895025, 'action': [1.0, 0]}, {'num_count': 526, 'sum_payoffs': 163.1137499429104, 'action': [0.0, 0]}])
Weights num count: [0.7491670633031889, 0.25035697287006187]
Actions to choose Agent 1: dict_values([{'num_count': 560, 'sum_payoffs': 177.5114999378716, 'action': [0.0, 0]}, {'num_count': 1540, 'sum_payoffs': 589.2142497937753, 'action': [1.0, 0]}])
Weights num count: [0.26653974297953353, 0.7329842931937173]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1353, 'sum_payoffs': 565.3799998021211, 'action': [1.0, 0]}, {'num_count': 747, 'sum_payoffs': 284.71499990035335, 'action': [0.0, 0]}])
Weights num count: [0.643979057591623, 0.3555449785816278]
Actions to choose Agent 1: dict_values([{'num_count': 1343, 'sum_payoffs': 561.329999803539, 'action': [1.0, 0]}, {'num_count': 757, 'sum_payoffs': 289.57499989865244, 'action': [0.0, 0]}])
Weights num count: [0.6392194193241314, 0.36030461684911946]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.33774328231811523 s
