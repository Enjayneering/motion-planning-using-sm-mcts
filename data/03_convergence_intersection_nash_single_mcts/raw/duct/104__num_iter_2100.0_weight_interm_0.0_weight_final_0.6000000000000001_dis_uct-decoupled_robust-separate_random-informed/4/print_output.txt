Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 544, 'sum_payoffs': 170.2214999546073, 'action': [0.0, 0]}, {'num_count': 1556, 'sum_payoffs': 593.7704998416763, 'action': [1.0, 0]}])
Weights num count: [0.2589243217515469, 0.7405997144217039]
Actions to choose Agent 1: dict_values([{'num_count': 560, 'sum_payoffs': 177.6937499526144, 'action': [0.0, 0]}, {'num_count': 1540, 'sum_payoffs': 589.578749842794, 'action': [1.0, 0]}])
Weights num count: [0.26653974297953353, 0.7329842931937173]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 738, 'sum_payoffs': 280.46249992521143, 'action': [0.0, 0]}, {'num_count': 1362, 'sum_payoffs': 569.4299998481636, 'action': [1.0, 0]}])
Weights num count: [0.3512613041408853, 0.6482627320323655]
Actions to choose Agent 1: dict_values([{'num_count': 745, 'sum_payoffs': 284.1074999242396, 'action': [0.0, 0]}, {'num_count': 1355, 'sum_payoffs': 566.9999998488117, 'action': [1.0, 0]}])
Weights num count: [0.3545930509281295, 0.6449309852451214]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3318634033203125 s
