Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1531, 'sum_payoffs': 583.7467498443482, 'action': [1.0, 0]}, {'num_count': 569, 'sum_payoffs': 180.4274999518855, 'action': [0.0, 0]}])
Weights num count: [0.7287006187529748, 0.27082341742027605]
Actions to choose Agent 1: dict_values([{'num_count': 583, 'sum_payoffs': 186.80624995018445, 'action': [0.0, 0]}, {'num_count': 1517, 'sum_payoffs': 579.9194998453685, 'action': [1.0, 0]}])
Weights num count: [0.2774869109947644, 0.7220371251784864]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1033, 'sum_payoffs': 417.352499888716, 'action': [0.0, 0]}, {'num_count': 1067, 'sum_payoffs': 433.14749988450507, 'action': [1.0, 0]}])
Weights num count: [0.49167063303188957, 0.5078534031413613]
Actions to choose Agent 1: dict_values([{'num_count': 1033, 'sum_payoffs': 417.352499888716, 'action': [0.0, 0]}, {'num_count': 1067, 'sum_payoffs': 433.14749988450507, 'action': [1.0, 0]}])
Weights num count: [0.49167063303188957, 0.5078534031413613]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.36688780784606934 s
