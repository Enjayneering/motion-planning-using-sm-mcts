Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 653, 'sum_payoffs': 97.88287496574128, 'action': [0.0, 0]}, {'num_count': 1447, 'sum_payoffs': 289.68690779334656, 'action': [1.0, 0]}])
Weights num count: [0.3108043788672061, 0.6887196573060448]
Actions to choose Agent 1: dict_values([{'num_count': 1447, 'sum_payoffs': 289.9141577932671, 'action': [1.0, 0]}, {'num_count': 653, 'sum_payoffs': 98.02012496569323, 'action': [0.0, 0]}])
Weights num count: [0.6887196573060448, 0.3108043788672061]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 751, 'sum_payoffs': 135.01124995274805, 'action': [0.0, 0]}, {'num_count': 1349, 'sum_payoffs': 291.3862498980153, 'action': [1.0, 0]}])
Weights num count: [0.3574488338886245, 0.6420752022846263]
Actions to choose Agent 1: dict_values([{'num_count': 750, 'sum_payoffs': 134.9099999527835, 'action': [0.0, 0]}, {'num_count': 1350, 'sum_payoffs': 291.8924998978381, 'action': [1.0, 0]}])
Weights num count: [0.3569728700618753, 0.6425511661113755]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.36652445793151855 s
