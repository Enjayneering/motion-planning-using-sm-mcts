Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 672, 'sum_payoffs': 102.45321706940467, 'action': [0.0, 0]}, {'num_count': 1428, 'sum_payoffs': 285.36749990012027, 'action': [1.0, 0]}])
Weights num count: [0.31984769157544024, 0.6796763445978106]
Actions to choose Agent 1: dict_values([{'num_count': 683, 'sum_payoffs': 105.1054341737396, 'action': [0.0, 0]}, {'num_count': 1417, 'sum_payoffs': 282.8975327957214, 'action': [1.0, 0]}])
Weights num count: [0.3250832936696811, 0.6744407425035698]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 813, 'sum_payoffs': 151.00874994714843, 'action': [0.0, 0]}, {'num_count': 1287, 'sum_payoffs': 275.3887499036164, 'action': [1.0, 0]}])
Weights num count: [0.3869585911470728, 0.612565445026178]
Actions to choose Agent 1: dict_values([{'num_count': 1289, 'sum_payoffs': 276.09749990336826, 'action': [1.0, 0]}, {'num_count': 811, 'sum_payoffs': 150.50249994732562, 'action': [0.0, 0]}])
Weights num count: [0.6135173726796763, 0.3860066634935745]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3502767086029053 s
