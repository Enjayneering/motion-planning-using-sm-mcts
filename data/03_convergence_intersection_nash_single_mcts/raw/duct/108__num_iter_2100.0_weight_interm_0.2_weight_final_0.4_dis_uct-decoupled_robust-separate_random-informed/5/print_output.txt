Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 598, 'sum_payoffs': 120.7469999677997, 'action': [0.0, 0]}, {'num_count': 1502, 'sum_payoffs': 392.3201051585348, 'action': [1.0, 0]}])
Weights num count: [0.2846263683960019, 0.714897667777249]
Actions to choose Agent 1: dict_values([{'num_count': 611, 'sum_payoffs': 125.2424999666008, 'action': [0.0, 0]}, {'num_count': 1489, 'sum_payoffs': 390.01160515915006, 'action': [1.0, 0]}])
Weights num count: [0.2908138981437411, 0.7087101380295098]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1329, 'sum_payoffs': 375.629999899836, 'action': [1.0, 0]}, {'num_count': 771, 'sum_payoffs': 191.99999994879843, 'action': [0.0, 0]}])
Weights num count: [0.632555925749643, 0.36696811042360783]
Actions to choose Agent 1: dict_values([{'num_count': 1326, 'sum_payoffs': 374.95499990001593, 'action': [1.0, 0]}, {'num_count': 774, 'sum_payoffs': 193.2149999484744, 'action': [0.0, 0]}])
Weights num count: [0.6311280342693956, 0.3683960019038553]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.33724236488342285 s
