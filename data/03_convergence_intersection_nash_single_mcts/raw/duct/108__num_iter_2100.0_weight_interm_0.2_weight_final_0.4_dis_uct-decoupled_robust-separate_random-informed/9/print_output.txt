Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1471, 'sum_payoffs': 382.91699989788265, 'action': [1.0, 0]}, {'num_count': 629, 'sum_payoffs': 129.83249996537668, 'action': [0.0, 0]}])
Weights num count: [0.7001427891480247, 0.29938124702522606]
Actions to choose Agent 1: dict_values([{'num_count': 628, 'sum_payoffs': 130.14939470213432, 'action': [0.0, 0]}, {'num_count': 1472, 'sum_payoffs': 384.54410516060653, 'action': [1.0, 0]}])
Weights num count: [0.2989052831984769, 0.7006187529747739]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 795, 'sum_payoffs': 199.69499994674655, 'action': [0.0, 0]}, {'num_count': 1305, 'sum_payoffs': 367.7999999019234, 'action': [1.0, 0]}])
Weights num count: [0.3783912422655878, 0.6211327939076631]
Actions to choose Agent 1: dict_values([{'num_count': 800, 'sum_payoffs': 201.5849999462425, 'action': [0.0, 0]}, {'num_count': 1300, 'sum_payoffs': 366.4499999022833, 'action': [1.0, 0]}])
Weights num count: [0.38077106139933364, 0.6187529747739172]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3909487724304199 s
