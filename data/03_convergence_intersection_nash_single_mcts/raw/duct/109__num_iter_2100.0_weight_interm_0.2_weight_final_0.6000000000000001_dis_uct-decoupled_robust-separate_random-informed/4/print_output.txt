Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1513, 'sum_payoffs': 440.49851634824836, 'action': [1.0, 0]}, {'num_count': 587, 'sum_payoffs': 135.00618746962417, 'action': [0.0, 0]}])
Weights num count: [0.7201332698714897, 0.2793907663017611]
Actions to choose Agent 1: dict_values([{'num_count': 597, 'sum_payoffs': 139.15518746869031, 'action': [0.0, 0]}, {'num_count': 1503, 'sum_payoffs': 439.3566413485053, 'action': [1.0, 0]}])
Weights num count: [0.2841504045692527, 0.7153736316039981]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 752, 'sum_payoffs': 211.02749995251736, 'action': [0.0, 0]}, {'num_count': 1348, 'sum_payoffs': 427.2187499038737, 'action': [1.0, 0]}])
Weights num count: [0.3579247977153736, 0.6415992384578773]
Actions to choose Agent 1: dict_values([{'num_count': 1347, 'sum_payoffs': 427.37062490383954, 'action': [1.0, 0]}, {'num_count': 753, 'sum_payoffs': 211.7868749523465, 'action': [0.0, 0]}])
Weights num count: [0.6411232746311281, 0.3584007615421228]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.336944580078125 s
