Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 565, 'sum_payoffs': 138.3191999723356, 'action': [0.0, 0]}, {'num_count': 1535, 'sum_payoffs': 475.0872630628841, 'action': [1.0, 0]}])
Weights num count: [0.2689195621132794, 0.7306044740599714]
Actions to choose Agent 1: dict_values([{'num_count': 560, 'sum_payoffs': 137.02319997259497, 'action': [0.0, 0]}, {'num_count': 1540, 'sum_payoffs': 477.84126306233384, 'action': [1.0, 0]}])
Weights num count: [0.26653974297953353, 0.7329842931937173]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 760, 'sum_payoffs': 229.24799995415253, 'action': [0.0, 0]}, {'num_count': 1340, 'sum_payoffs': 451.2059999097671, 'action': [1.0, 0]}])
Weights num count: [0.361732508329367, 0.6377915278438838]
Actions to choose Agent 1: dict_values([{'num_count': 765, 'sum_payoffs': 231.515999953699, 'action': [0.0, 0]}, {'num_count': 1335, 'sum_payoffs': 449.90999991002633, 'action': [1.0, 0]}])
Weights num count: [0.3641123274631128, 0.6354117087101381]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.35404133796691895 s
