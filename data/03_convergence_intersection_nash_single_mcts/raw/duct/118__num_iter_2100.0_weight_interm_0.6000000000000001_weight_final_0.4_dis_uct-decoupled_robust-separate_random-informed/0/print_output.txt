Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 738, 'sum_payoffs': 91.50029998169917, 'action': [0.0, 0]}, {'num_count': 1362, 'sum_payoffs': 220.705389429546, 'action': [1.0, 0]}])
Weights num count: [0.3512613041408853, 0.6482627320323655]
Actions to choose Agent 1: dict_values([{'num_count': 720, 'sum_payoffs': 88.26299998234666, 'action': [0.0, 0]}, {'num_count': 1380, 'sum_payoffs': 225.10908942866556, 'action': [1.0, 0]}])
Weights num count: [0.3426939552594003, 0.6568300809138505]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 864, 'sum_payoffs': 129.49199997410315, 'action': [0.0, 0]}, {'num_count': 1236, 'sum_payoffs': 212.16599995756994, 'action': [1.0, 0]}])
Weights num count: [0.4112327463112803, 0.5882912898619705]
Actions to choose Agent 1: dict_values([{'num_count': 866, 'sum_payoffs': 129.97799997400594, 'action': [0.0, 0]}, {'num_count': 1234, 'sum_payoffs': 211.84199995763473, 'action': [1.0, 0]}])
Weights num count: [0.4121846739647787, 0.5873393622084722]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3415238857269287 s
