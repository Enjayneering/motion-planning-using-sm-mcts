Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 710, 'sum_payoffs': 86.07599998278424, 'action': [0.0, 0]}, {'num_count': 1390, 'sum_payoffs': 226.71288942834437, 'action': [1.0, 0]}])
Weights num count: [0.3379343169919086, 0.6615897191813422]
Actions to choose Agent 1: dict_values([{'num_count': 713, 'sum_payoffs': 86.73209998265293, 'action': [0.0, 0]}, {'num_count': 1387, 'sum_payoffs': 226.20258942844657, 'action': [1.0, 0]}])
Weights num count: [0.3393622084721561, 0.6601618277010948]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 888, 'sum_payoffs': 134.75699997304977, 'action': [0.0, 0]}, {'num_count': 1212, 'sum_payoffs': 206.90099995862278, 'action': [1.0, 0]}])
Weights num count: [0.42265587815326033, 0.5768681580199905]
Actions to choose Agent 1: dict_values([{'num_count': 1211, 'sum_payoffs': 206.81999995863896, 'action': [1.0, 0]}, {'num_count': 889, 'sum_payoffs': 135.1619999729688, 'action': [0.0, 0]}])
Weights num count: [0.5763921941932413, 0.4231318419800095]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.34392523765563965 s
