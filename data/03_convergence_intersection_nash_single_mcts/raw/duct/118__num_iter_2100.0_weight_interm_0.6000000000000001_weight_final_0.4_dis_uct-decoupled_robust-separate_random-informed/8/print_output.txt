Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1383, 'sum_payoffs': 225.10908942866536, 'action': [1.0, 0]}, {'num_count': 717, 'sum_payoffs': 87.30989998253737, 'action': [0.0, 0]}])
Weights num count: [0.658257972394098, 0.3412660637791528]
Actions to choose Agent 1: dict_values([{'num_count': 730, 'sum_payoffs': 89.5751999820842, 'action': [0.0, 0]}, {'num_count': 1370, 'sum_payoffs': 221.67738942935162, 'action': [1.0, 0]}])
Weights num count: [0.347453593526892, 0.6520704426463588]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 842, 'sum_payoffs': 124.63199997507513, 'action': [0.0, 0]}, {'num_count': 1258, 'sum_payoffs': 217.10699995658197, 'action': [1.0, 0]}])
Weights num count: [0.4007615421227987, 0.5987624940504521]
Actions to choose Agent 1: dict_values([{'num_count': 844, 'sum_payoffs': 125.19899997496175, 'action': [0.0, 0]}, {'num_count': 1256, 'sum_payoffs': 216.86399995663055, 'action': [1.0, 0]}])
Weights num count: [0.401713469776297, 0.5978105663969538]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.33576369285583496 s
