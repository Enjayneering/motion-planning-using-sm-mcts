Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1422, 'sum_payoffs': 283.23112494807333, 'action': [1.0, 0]}, {'num_count': 678, 'sum_payoffs': 103.4105920863041, 'action': [0.0, 0]}])
Weights num count: [0.6768205616373155, 0.3227034745359353]
Actions to choose Agent 1: dict_values([{'num_count': 673, 'sum_payoffs': 102.76312498115966, 'action': [0.0, 0]}, {'num_count': 1427, 'sum_payoffs': 285.3365920529505, 'action': [1.0, 0]}])
Weights num count: [0.32032365540218943, 0.6792003807710614]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1279, 'sum_payoffs': 273.1612499499172, 'action': [1.0, 0]}, {'num_count': 821, 'sum_payoffs': 153.0337499719444, 'action': [0.0, 0]}])
Weights num count: [0.6087577344121847, 0.39076630176106614]
Actions to choose Agent 1: dict_values([{'num_count': 824, 'sum_payoffs': 154.04624997175875, 'action': [0.0, 0]}, {'num_count': 1276, 'sum_payoffs': 272.75624994999146, 'action': [1.0, 0]}])
Weights num count: [0.39219419324131366, 0.6073298429319371]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4038228988647461 s
