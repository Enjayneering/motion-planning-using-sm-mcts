Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1448, 'sum_payoffs': 289.37303284168445, 'action': [1.0, 0]}, {'num_count': 652, 'sum_payoffs': 97.38674998214535, 'action': [0.0, 0]}])
Weights num count: [0.6891956211327939, 0.31032841504045694]
Actions to choose Agent 1: dict_values([{'num_count': 648, 'sum_payoffs': 96.75449998226127, 'action': [0.0, 0]}, {'num_count': 1452, 'sum_payoffs': 290.9165328414015, 'action': [1.0, 0]}])
Weights num count: [0.3084245597334603, 0.6910994764397905]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 805, 'sum_payoffs': 148.98374997268715, 'action': [0.0, 0]}, {'num_count': 1295, 'sum_payoffs': 277.51499994911904, 'action': [1.0, 0]}])
Weights num count: [0.3831508805330795, 0.6163731556401714]
Actions to choose Agent 1: dict_values([{'num_count': 1297, 'sum_payoffs': 278.1224999490077, 'action': [1.0, 0]}, {'num_count': 803, 'sum_payoffs': 148.57874997276141, 'action': [0.0, 0]}])
Weights num count: [0.6173250832936696, 0.38219895287958117]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.36111927032470703 s
