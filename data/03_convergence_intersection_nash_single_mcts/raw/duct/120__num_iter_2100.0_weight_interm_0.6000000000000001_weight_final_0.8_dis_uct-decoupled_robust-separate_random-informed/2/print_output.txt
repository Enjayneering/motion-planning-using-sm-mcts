Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 652, 'sum_payoffs': 114.49542855180093, 'action': [0.0, 0]}, {'num_count': 1448, 'sum_payoffs': 327.333992425081, 'action': [1.0, 0]}])
Weights num count: [0.31032841504045694, 0.6891956211327939]
Actions to choose Agent 1: dict_values([{'num_count': 1450, 'sum_payoffs': 329.1391352819141, 'action': [1.0, 0]}, {'num_count': 650, 'sum_payoffs': 114.56485712321738, 'action': [0.0, 0]}])
Weights num count: [0.6901475487862923, 0.3093764873869586]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 801, 'sum_payoffs': 170.94857139926808, 'action': [0.0, 0]}, {'num_count': 1299, 'sum_payoffs': 315.86142851728596, 'action': [1.0, 0]}])
Weights num count: [0.38124702522608284, 0.618277010947168]
Actions to choose Agent 1: dict_values([{'num_count': 1301, 'sum_payoffs': 316.6714285171471, 'action': [1.0, 0]}, {'num_count': 799, 'sum_payoffs': 170.60142854218472, 'action': [0.0, 0]}])
Weights num count: [0.6192289386006663, 0.3802950975725845]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3573172092437744 s
