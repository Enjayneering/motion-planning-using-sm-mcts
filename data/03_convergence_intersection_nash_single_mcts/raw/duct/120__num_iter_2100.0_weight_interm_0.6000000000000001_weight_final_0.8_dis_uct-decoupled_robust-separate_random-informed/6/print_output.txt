Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1459, 'sum_payoffs': 329.97227813891385, 'action': [1.0, 0]}, {'num_count': 641, 'sum_payoffs': 111.50999998088392, 'action': [0.0, 0]}])
Weights num count: [0.6944312232270348, 0.3050928129462161]
Actions to choose Agent 1: dict_values([{'num_count': 1465, 'sum_payoffs': 332.3675638527889, 'action': [1.0, 0]}, {'num_count': 635, 'sum_payoffs': 110.364428552509, 'action': [0.0, 0]}])
Weights num count: [0.6972870061875297, 0.3022370299857211]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1264, 'sum_payoffs': 305.9099999475628, 'action': [1.0, 0]}, {'num_count': 836, 'sum_payoffs': 181.13142854037997, 'action': [0.0, 0]}])
Weights num count: [0.6016182770109472, 0.39790575916230364]
Actions to choose Agent 1: dict_values([{'num_count': 1261, 'sum_payoffs': 305.2157142333961, 'action': [1.0, 0]}, {'num_count': 839, 'sum_payoffs': 182.05714282593559, 'action': [0.0, 0]}])
Weights num count: [0.6001903855306997, 0.39933365064255116]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.33740830421447754 s
