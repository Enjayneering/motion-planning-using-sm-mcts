Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 827, 'sum_payoffs': 50.514749989896764, 'action': [0.0, 0]}, {'num_count': 1273, 'sum_payoffs': 111.38305260930348, 'action': [1.0, 0]}])
Weights num count: [0.3936220847215612, 0.6059019514516897]
Actions to choose Agent 1: dict_values([{'num_count': 848, 'sum_payoffs': 52.86689998942629, 'action': [0.0, 0]}, {'num_count': 1252, 'sum_payoffs': 107.93740260999246, 'action': [1.0, 0]}])
Weights num count: [0.4036173250832937, 0.5959067110899572]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1061, 'sum_payoffs': 87.66449998246867, 'action': [1.0, 0]}, {'num_count': 1039, 'sum_payoffs': 84.55499998309048, 'action': [0.0, 0]}])
Weights num count: [0.5049976201808662, 0.49452641599238456]
Actions to choose Agent 1: dict_values([{'num_count': 1039, 'sum_payoffs': 84.59549998308238, 'action': [0.0, 0]}, {'num_count': 1061, 'sum_payoffs': 87.70499998246056, 'action': [1.0, 0]}])
Weights num count: [0.49452641599238456, 0.5049976201808662]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.33673882484436035 s
