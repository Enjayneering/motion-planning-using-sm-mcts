Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1325, 'sum_payoffs': 181.21721049309036, 'action': [1.0, 0]}, {'num_count': 775, 'sum_payoffs': 80.33024998527256, 'action': [0.0, 0]}])
Weights num count: [0.6306520704426464, 0.3688719657306045]
Actions to choose Agent 1: dict_values([{'num_count': 768, 'sum_payoffs': 79.48649998542722, 'action': [0.0, 0]}, {'num_count': 1332, 'sum_payoffs': 183.1544604927351, 'action': [1.0, 0]}])
Weights num count: [0.3655402189433603, 0.6339838172298905]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1054, 'sum_payoffs': 143.48999997369492, 'action': [1.0, 0]}, {'num_count': 1046, 'sum_payoffs': 141.87749997399038, 'action': [0.0, 0]}])
Weights num count: [0.5016658733936221, 0.49785816277962874]
Actions to choose Agent 1: dict_values([{'num_count': 1054, 'sum_payoffs': 143.48999997369492, 'action': [1.0, 0]}, {'num_count': 1046, 'sum_payoffs': 141.87749997399038, 'action': [0.0, 0]}])
Weights num count: [0.5016658733936221, 0.49785816277962874]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3427736759185791 s
