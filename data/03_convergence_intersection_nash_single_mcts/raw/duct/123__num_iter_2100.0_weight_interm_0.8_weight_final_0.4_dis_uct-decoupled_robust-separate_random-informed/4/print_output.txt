Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1334, 'sum_payoffs': 183.34346049270079, 'action': [1.0, 0]}, {'num_count': 766, 'sum_payoffs': 78.99978945920077, 'action': [0.0, 0]}])
Weights num count: [0.6349357448833889, 0.364588291289862]
Actions to choose Agent 1: dict_values([{'num_count': 1340, 'sum_payoffs': 185.09846049237896, 'action': [1.0, 0]}, {'num_count': 760, 'sum_payoffs': 78.33828945932204, 'action': [0.0, 0]}])
Weights num count: [0.6377915278438838, 0.361732508329367]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1254, 'sum_payoffs': 182.43749996655828, 'action': [1.0, 0]}, {'num_count': 846, 'sum_payoffs': 102.72749998116672, 'action': [0.0, 0]}])
Weights num count: [0.5968586387434555, 0.40266539742979535]
Actions to choose Agent 1: dict_values([{'num_count': 845, 'sum_payoffs': 102.59249998119145, 'action': [0.0, 0]}, {'num_count': 1255, 'sum_payoffs': 182.7074999665088, 'action': [1.0, 0]}])
Weights num count: [0.40218943360304615, 0.5973346025702047]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.33850741386413574 s
