Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1345, 'sum_payoffs': 184.49846049248873, 'action': [1.0, 0]}, {'num_count': 755, 'sum_payoffs': 76.63199998595051, 'action': [0.0, 0]}])
Weights num count: [0.6401713469776297, 0.35935268919562113]
Actions to choose Agent 1: dict_values([{'num_count': 1360, 'sum_payoffs': 187.77146049188846, 'action': [1.0, 0]}, {'num_count': 740, 'sum_payoffs': 74.20949998639459, 'action': [0.0, 0]}])
Weights num count: [0.6473108043788672, 0.35221323179438363]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1157, 'sum_payoffs': 163.67249996999664, 'action': [1.0, 0]}, {'num_count': 943, 'sum_payoffs': 121.6949999776907, 'action': [0.0, 0]}])
Weights num count: [0.5506901475487863, 0.44883388862446455]
Actions to choose Agent 1: dict_values([{'num_count': 1156, 'sum_payoffs': 163.46999997003377, 'action': [1.0, 0]}, {'num_count': 944, 'sum_payoffs': 121.89749997765358, 'action': [0.0, 0]}])
Weights num count: [0.5502141837220371, 0.4493098524512137]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.33971285820007324 s
