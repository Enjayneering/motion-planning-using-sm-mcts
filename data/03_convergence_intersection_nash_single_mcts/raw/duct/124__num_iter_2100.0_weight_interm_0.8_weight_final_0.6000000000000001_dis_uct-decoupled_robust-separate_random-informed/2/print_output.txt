Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1413, 'sum_payoffs': 245.74528755185204, 'action': [1.0, 0]}, {'num_count': 687, 'sum_payoffs': 88.35589284199638, 'action': [0.0, 0]}])
Weights num count: [0.672536887196573, 0.3269871489766778]
Actions to choose Agent 1: dict_values([{'num_count': 1411, 'sum_payoffs': 245.66718040900827, 'action': [1.0, 0]}, {'num_count': 689, 'sum_payoffs': 89.05885712759017, 'action': [0.0, 0]}])
Weights num count: [0.6715849595430747, 0.32793907663017613]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 788, 'sum_payoffs': 121.92749997909822, 'action': [0.0, 0]}, {'num_count': 1312, 'sum_payoffs': 244.11535710100492, 'action': [1.0, 0]}])
Weights num count: [0.37505949547834366, 0.6244645406949072]
Actions to choose Agent 1: dict_values([{'num_count': 1312, 'sum_payoffs': 244.28892852954658, 'action': [1.0, 0]}, {'num_count': 788, 'sum_payoffs': 121.92749997909822, 'action': [0.0, 0]}])
Weights num count: [0.6244645406949072, 0.37505949547834366]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.34101247787475586 s
