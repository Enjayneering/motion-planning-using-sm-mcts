Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1460, 'sum_payoffs': 292.4752499524772, 'action': [1.0, 0]}, {'num_count': 640, 'sum_payoffs': 94.78462498459723, 'action': [0.0, 0]}])
Weights num count: [0.694907187053784, 0.3046168491194669]
Actions to choose Agent 1: dict_values([{'num_count': 644, 'sum_payoffs': 96.24262498436019, 'action': [0.0, 0]}, {'num_count': 1456, 'sum_payoffs': 292.83974995241783, 'action': [1.0, 0]}])
Weights num count: [0.30652070442646356, 0.6930033317467873]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 805, 'sum_payoffs': 148.98374997578972, 'action': [0.0, 0]}, {'num_count': 1295, 'sum_payoffs': 277.6162499548887, 'action': [1.0, 0]}])
Weights num count: [0.3831508805330795, 0.6163731556401714]
Actions to choose Agent 1: dict_values([{'num_count': 806, 'sum_payoffs': 149.38874997572393, 'action': [0.0, 0]}, {'num_count': 1294, 'sum_payoffs': 277.4137499549217, 'action': [1.0, 0]}])
Weights num count: [0.38362684435982863, 0.6158971918134222]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3534882068634033 s
