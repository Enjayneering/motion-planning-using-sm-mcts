Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 651, 'sum_payoffs': 204.66674987719796, 'action': [0.0, 0]}, {'num_count': 1949, 'sum_payoffs': 741.2107495552602, 'action': [1.0, 0]}])
Weights num count: [0.2502883506343714, 0.7493271818531334]
Actions to choose Agent 1: dict_values([{'num_count': 668, 'sum_payoffs': 212.50349987249564, 'action': [0.0, 0]}, {'num_count': 1932, 'sum_payoffs': 736.289999558213, 'action': [1.0, 0]}])
Weights num count: [0.2568242983467897, 0.7427912341407151]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1705, 'sum_payoffs': 711.5849995730712, 'action': [1.0, 0]}, {'num_count': 895, 'sum_payoffs': 340.60499979564077, 'action': [0.0, 0]}])
Weights num count: [0.655517108804306, 0.34409842368319876]
Actions to choose Agent 1: dict_values([{'num_count': 893, 'sum_payoffs': 340.1999997958837, 'action': [0.0, 0]}, {'num_count': 1707, 'sum_payoffs': 713.2049995720992, 'action': [1.0, 0]}])
Weights num count: [0.3433294886582084, 0.6562860438292965]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.39281535148620605 s
