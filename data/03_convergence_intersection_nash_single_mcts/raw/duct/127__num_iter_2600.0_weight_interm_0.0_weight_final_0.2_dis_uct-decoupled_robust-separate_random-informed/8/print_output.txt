Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1939, 'sum_payoffs': 737.7479995573362, 'action': [1.0, 0]}, {'num_count': 661, 'sum_payoffs': 208.85849987468276, 'action': [0.0, 0]}])
Weights num count: [0.7454825067281815, 0.25413302575932334]
Actions to choose Agent 1: dict_values([{'num_count': 672, 'sum_payoffs': 213.77924987172995, 'action': [0.0, 0]}, {'num_count': 1928, 'sum_payoffs': 734.6497495591966, 'action': [1.0, 0]}])
Weights num count: [0.25836216839677045, 0.7412533640907343]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1681, 'sum_payoffs': 700.6499995796319, 'action': [1.0, 0]}, {'num_count': 919, 'sum_payoffs': 351.7424997889585, 'action': [0.0, 0]}])
Weights num count: [0.6462898885044214, 0.3533256439830834]
Actions to choose Agent 1: dict_values([{'num_count': 931, 'sum_payoffs': 357.6149997854353, 'action': [0.0, 0]}, {'num_count': 1669, 'sum_payoffs': 695.9924995824261, 'action': [1.0, 0]}])
Weights num count: [0.35793925413302574, 0.641676278354479]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4114573001861572 s
