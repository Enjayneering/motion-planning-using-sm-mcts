Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 658, 'sum_payoffs': 207.7649999272829, 'action': [0.0, 0]}, {'num_count': 1942, 'sum_payoffs': 739.0237497413543, 'action': [1.0, 0]}])
Weights num count: [0.25297962322183776, 0.746635909265667]
Actions to choose Agent 1: dict_values([{'num_count': 1923, 'sum_payoffs': 732.644999743586, 'action': [1.0, 0]}, {'num_count': 677, 'sum_payoffs': 215.9662499244127, 'action': [0.0, 0]}])
Weights num count: [0.7393310265282583, 0.26028450595924646]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 901, 'sum_payoffs': 343.64249987972994, 'action': [0.0, 0]}, {'num_count': 1699, 'sum_payoffs': 709.1549997517849, 'action': [1.0, 0]}])
Weights num count: [0.3464052287581699, 0.6532103037293349]
Actions to choose Agent 1: dict_values([{'num_count': 1697, 'sum_payoffs': 708.7499997519266, 'action': [1.0, 0]}, {'num_count': 903, 'sum_payoffs': 344.8574998793047, 'action': [0.0, 0]}])
Weights num count: [0.6524413687043444, 0.34717416378316035]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4058394432067871 s
