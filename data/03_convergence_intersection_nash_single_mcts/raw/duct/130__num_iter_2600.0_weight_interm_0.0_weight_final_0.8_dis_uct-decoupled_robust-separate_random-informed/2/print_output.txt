Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 657, 'sum_payoffs': 207.0359999534182, 'action': [0.0, 0]}, {'num_count': 1943, 'sum_payoffs': 738.6592498337817, 'action': [1.0, 0]}])
Weights num count: [0.25259515570934254, 0.7470203767781622]
Actions to choose Agent 1: dict_values([{'num_count': 1931, 'sum_payoffs': 737.0189998341505, 'action': [1.0, 0]}, {'num_count': 669, 'sum_payoffs': 213.05024995206514, 'action': [0.0, 0]}])
Weights num count: [0.7424067666282199, 0.25720876585928487]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 895, 'sum_payoffs': 340.80749992331624, 'action': [0.0, 0]}, {'num_count': 1705, 'sum_payoffs': 711.5849998398984, 'action': [1.0, 0]}])
Weights num count: [0.34409842368319876, 0.655517108804306]
Actions to choose Agent 1: dict_values([{'num_count': 1707, 'sum_payoffs': 713.0024998395795, 'action': [1.0, 0]}, {'num_count': 893, 'sum_payoffs': 340.199999923453, 'action': [0.0, 0]}])
Weights num count: [0.6562860438292965, 0.3433294886582084]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.41501903533935547 s
