Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 675, 'sum_payoffs': 214.50824995173707, 'action': [0.0, 0]}, {'num_count': 1925, 'sum_payoffs': 731.5514998353825, 'action': [1.0, 0]}])
Weights num count: [0.25951557093425603, 0.7400999615532488]
Actions to choose Agent 1: dict_values([{'num_count': 693, 'sum_payoffs': 222.7094999498919, 'action': [0.0, 0]}, {'num_count': 1907, 'sum_payoffs': 726.9952498364086, 'action': [1.0, 0]}])
Weights num count: [0.2664359861591695, 0.7331795463283353]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 893, 'sum_payoffs': 339.9974999234984, 'action': [0.0, 0]}, {'num_count': 1707, 'sum_payoffs': 712.5974998396705, 'action': [1.0, 0]}])
Weights num count: [0.3433294886582084, 0.6562860438292965]
Actions to choose Agent 1: dict_values([{'num_count': 1704, 'sum_payoffs': 711.7874998398527, 'action': [1.0, 0]}, {'num_count': 896, 'sum_payoffs': 341.61749992313383, 'action': [0.0, 0]}])
Weights num count: [0.6551326412918108, 0.344482891195694]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.41053247451782227 s
