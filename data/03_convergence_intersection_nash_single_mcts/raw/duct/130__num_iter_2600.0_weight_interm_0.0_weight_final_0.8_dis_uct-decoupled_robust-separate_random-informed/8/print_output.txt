Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1945, 'sum_payoffs': 740.1172498334543, 'action': [1.0, 0]}, {'num_count': 655, 'sum_payoffs': 206.4892499535413, 'action': [0.0, 0]}])
Weights num count: [0.7477893118031527, 0.2518262206843522]
Actions to choose Agent 1: dict_values([{'num_count': 1926, 'sum_payoffs': 734.1029998348076, 'action': [1.0, 0]}, {'num_count': 674, 'sum_payoffs': 214.69049995169613, 'action': [0.0, 0]}])
Weights num count: [0.740484429065744, 0.2591311034217609]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 893, 'sum_payoffs': 339.5924999235897, 'action': [0.0, 0]}, {'num_count': 1707, 'sum_payoffs': 712.1924998397618, 'action': [1.0, 0]}])
Weights num count: [0.3433294886582084, 0.6562860438292965]
Actions to choose Agent 1: dict_values([{'num_count': 894, 'sum_payoffs': 340.8074999233162, 'action': [0.0, 0]}, {'num_count': 1706, 'sum_payoffs': 712.5974998396707, 'action': [1.0, 0]}])
Weights num count: [0.34371395617070355, 0.6559015763168012]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.40544843673706055 s
