Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 818, 'sum_payoffs': 125.4572170613538, 'action': [0.0, 0]}, {'num_count': 1782, 'sum_payoffs': 353.3832827710565, 'action': [1.0, 0]}])
Weights num count: [0.3144944252210688, 0.6851211072664359]
Actions to choose Agent 1: dict_values([{'num_count': 1797, 'sum_payoffs': 357.38715776965586, 'action': [1.0, 0]}, {'num_count': 803, 'sum_payoffs': 122.3645920624359, 'action': [0.0, 0]}])
Weights num count: [0.6908881199538639, 0.3087274125336409]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1065, 'sum_payoffs': 203.45624992879334, 'action': [0.0, 0]}, {'num_count': 1535, 'sum_payoffs': 324.39374988645955, 'action': [1.0, 0]}])
Weights num count: [0.40945790080738176, 0.590157631680123]
Actions to choose Agent 1: dict_values([{'num_count': 1067, 'sum_payoffs': 204.0637499285807, 'action': [0.0, 0]}, {'num_count': 1533, 'sum_payoffs': 323.98874988660134, 'action': [1.0, 0]}])
Weights num count: [0.4102268358323722, 0.5893886966551326]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.45577192306518555 s
