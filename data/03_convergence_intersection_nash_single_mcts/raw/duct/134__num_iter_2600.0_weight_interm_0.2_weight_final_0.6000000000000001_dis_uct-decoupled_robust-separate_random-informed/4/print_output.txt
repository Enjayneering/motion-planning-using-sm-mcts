Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 704, 'sum_payoffs': 163.0096874633225, 'action': [0.0, 0]}, {'num_count': 1896, 'sum_payoffs': 549.8001413236537, 'action': [1.0, 0]}])
Weights num count: [0.2706651287966167, 0.7289504036908881]
Actions to choose Agent 1: dict_values([{'num_count': 1887, 'sum_payoffs': 547.9573913240683, 'action': [1.0, 0]}, {'num_count': 713, 'sum_payoffs': 166.21931246260044, 'action': [0.0, 0]}])
Weights num count: [0.7254901960784313, 0.27412533640907344]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1688, 'sum_payoffs': 533.3793748799909, 'action': [1.0, 0]}, {'num_count': 912, 'sum_payoffs': 256.4381249422996, 'action': [0.0, 0]}])
Weights num count: [0.6489811610918877, 0.3506343713956171]
Actions to choose Agent 1: dict_values([{'num_count': 921, 'sum_payoffs': 259.9312499415137, 'action': [0.0, 0]}, {'num_count': 1679, 'sum_payoffs': 530.7974998805713, 'action': [1.0, 0]}])
Weights num count: [0.35409457900807384, 0.645520953479431]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3978002071380615 s
