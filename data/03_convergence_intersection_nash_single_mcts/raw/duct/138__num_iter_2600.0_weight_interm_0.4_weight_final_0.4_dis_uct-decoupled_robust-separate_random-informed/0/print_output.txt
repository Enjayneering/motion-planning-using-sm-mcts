Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 824, 'sum_payoffs': 126.82409207672899, 'action': [0.0, 0]}, {'num_count': 1776, 'sum_payoffs': 352.10190781550705, 'action': [1.0, 0]}])
Weights num count: [0.31680123029603996, 0.6828143021914648]
Actions to choose Agent 1: dict_values([{'num_count': 819, 'sum_payoffs': 126.18621707687237, 'action': [0.0, 0]}, {'num_count': 1781, 'sum_payoffs': 354.1977828150345, 'action': [1.0, 0]}])
Weights num count: [0.314878892733564, 0.6847366397539408]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1043, 'sum_payoffs': 197.78624995549555, 'action': [0.0, 0]}, {'num_count': 1557, 'sum_payoffs': 330.06374992573626, 'action': [1.0, 0]}])
Weights num count: [0.40099961553248753, 0.5986159169550173]
Actions to choose Agent 1: dict_values([{'num_count': 1045, 'sum_payoffs': 198.39374995535883, 'action': [0.0, 0]}, {'num_count': 1555, 'sum_payoffs': 329.65874992582735, 'action': [1.0, 0]}])
Weights num count: [0.4017685505574779, 0.5978469819300269]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4349331855773926 s
