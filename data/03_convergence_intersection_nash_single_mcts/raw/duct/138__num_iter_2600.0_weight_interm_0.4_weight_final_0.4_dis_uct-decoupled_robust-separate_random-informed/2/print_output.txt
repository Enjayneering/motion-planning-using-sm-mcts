Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1768, 'sum_payoffs': 349.5155328160888, 'action': [1.0, 0]}, {'num_count': 832, 'sum_payoffs': 128.54137497107948, 'action': [0.0, 0]}])
Weights num count: [0.6797385620915033, 0.31987697039600155]
Actions to choose Agent 1: dict_values([{'num_count': 1774, 'sum_payoffs': 352.1075328155053, 'action': [1.0, 0]}, {'num_count': 826, 'sum_payoffs': 127.77187497125261, 'action': [0.0, 0]}])
Weights num count: [0.6820453671664745, 0.3175701653210304]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1626, 'sum_payoffs': 347.68124992177343, 'action': [1.0, 0]}, {'num_count': 974, 'sum_payoffs': 180.16874995946083, 'action': [0.0, 0]}])
Weights num count: [0.6251441753171857, 0.3744713571703191]
Actions to choose Agent 1: dict_values([{'num_count': 970, 'sum_payoffs': 179.2574999596659, 'action': [0.0, 0]}, {'num_count': 1630, 'sum_payoffs': 348.7949999215229, 'action': [1.0, 0]}])
Weights num count: [0.37293348712033836, 0.6266820453671664]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4154784679412842 s
