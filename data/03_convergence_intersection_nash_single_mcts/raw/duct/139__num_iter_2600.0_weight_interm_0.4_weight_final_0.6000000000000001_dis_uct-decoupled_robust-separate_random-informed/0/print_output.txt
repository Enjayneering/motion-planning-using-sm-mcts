Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 764, 'sum_payoffs': 140.49722365611123, 'action': [0.0, 0]}, {'num_count': 1836, 'sum_payoffs': 431.4586262294889, 'action': [1.0, 0]}])
Weights num count: [0.2937331795463283, 0.7058823529411765]
Actions to choose Agent 1: dict_values([{'num_count': 773, 'sum_payoffs': 143.4496736555207, 'action': [0.0, 0]}, {'num_count': 1827, 'sum_payoffs': 430.25577622972946, 'action': [1.0, 0]}])
Weights num count: [0.29719338715878507, 0.7024221453287197]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1630, 'sum_payoffs': 414.3059999171306, 'action': [1.0, 0]}, {'num_count': 970, 'sum_payoffs': 218.33099995633015, 'action': [0.0, 0]}])
Weights num count: [0.6266820453671664, 0.37293348712033836]
Actions to choose Agent 1: dict_values([{'num_count': 1626, 'sum_payoffs': 413.4554999173007, 'action': [1.0, 0]}, {'num_count': 974, 'sum_payoffs': 219.66749995606284, 'action': [0.0, 0]}])
Weights num count: [0.6251441753171857, 0.3744713571703191]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.42333078384399414 s
