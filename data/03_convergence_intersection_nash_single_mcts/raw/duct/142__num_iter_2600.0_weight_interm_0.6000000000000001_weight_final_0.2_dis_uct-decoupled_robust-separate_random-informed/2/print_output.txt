Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1613, 'sum_payoffs': 167.60161180439243, 'action': [1.0, 0]}, {'num_count': 987, 'sum_payoffs': 75.39749998303598, 'action': [0.0, 0]}])
Weights num count: [0.6201460976547482, 0.37946943483275664]
Actions to choose Agent 1: dict_values([{'num_count': 975, 'sum_payoffs': 74.10656248332648, 'action': [0.0, 0]}, {'num_count': 1625, 'sum_payoffs': 169.98604930385582, 'action': [1.0, 0]}])
Weights num count: [0.3748558246828143, 0.6247597078046905]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1480, 'sum_payoffs': 160.81874996381572, 'action': [1.0, 0]}, {'num_count': 1120, 'sum_payoffs': 104.45624997649627, 'action': [0.0, 0]}])
Weights num count: [0.5690119184928873, 0.43060361399461744]
Actions to choose Agent 1: dict_values([{'num_count': 1120, 'sum_payoffs': 104.45624997649627, 'action': [0.0, 0]}, {'num_count': 1480, 'sum_payoffs': 160.8187499638157, 'action': [1.0, 0]}])
Weights num count: [0.43060361399461744, 0.5690119184928873]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.43486762046813965 s
