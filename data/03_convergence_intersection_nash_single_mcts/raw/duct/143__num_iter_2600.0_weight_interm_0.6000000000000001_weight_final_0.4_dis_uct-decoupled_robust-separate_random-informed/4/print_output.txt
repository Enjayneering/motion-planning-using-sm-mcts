Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1741, 'sum_payoffs': 281.67948941735324, 'action': [1.0, 0]}, {'num_count': 859, 'sum_payoffs': 104.37389997912416, 'action': [0.0, 0]}])
Weights num count: [0.669357939254133, 0.3302575932333718]
Actions to choose Agent 1: dict_values([{'num_count': 1728, 'sum_payoffs': 278.66088941795687, 'action': [1.0, 0]}, {'num_count': 872, 'sum_payoffs': 106.66349997866621, 'action': [0.0, 0]}])
Weights num count: [0.6643598615916955, 0.3352556708958093]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1580, 'sum_payoffs': 272.0249999455975, 'action': [1.0, 0]}, {'num_count': 1020, 'sum_payoffs': 150.7139999698582, 'action': [0.0, 0]}])
Weights num count: [0.6074586697424068, 0.39215686274509803]
Actions to choose Agent 1: dict_values([{'num_count': 1019, 'sum_payoffs': 150.55199996989063, 'action': [0.0, 0]}, {'num_count': 1581, 'sum_payoffs': 272.3489999455327, 'action': [1.0, 0]}])
Weights num count: [0.3917723952326028, 0.6078431372549019]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4304988384246826 s
