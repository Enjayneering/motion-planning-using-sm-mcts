Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1726, 'sum_payoffs': 278.07029994439034, 'action': [1.0, 0]}, {'num_count': 874, 'sum_payoffs': 106.97939997860307, 'action': [0.0, 0]}])
Weights num count: [0.6635909265667052, 0.3360246059207997]
Actions to choose Agent 1: dict_values([{'num_count': 882, 'sum_payoffs': 108.87479997822385, 'action': [0.0, 0]}, {'num_count': 1718, 'sum_payoffs': 277.19549994456526, 'action': [1.0, 0]}])
Weights num count: [0.3391003460207612, 0.6605151864667436]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1042, 'sum_payoffs': 155.4929999689024, 'action': [0.0, 0]}, {'num_count': 1558, 'sum_payoffs': 267.3269999465379, 'action': [1.0, 0]}])
Weights num count: [0.4006151480199923, 0.5990003844675125]
Actions to choose Agent 1: dict_values([{'num_count': 1559, 'sum_payoffs': 267.5699999464893, 'action': [1.0, 0]}, {'num_count': 1041, 'sum_payoffs': 155.24999996895102, 'action': [0.0, 0]}])
Weights num count: [0.5993848519800077, 0.4002306805074971]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4319913387298584 s
