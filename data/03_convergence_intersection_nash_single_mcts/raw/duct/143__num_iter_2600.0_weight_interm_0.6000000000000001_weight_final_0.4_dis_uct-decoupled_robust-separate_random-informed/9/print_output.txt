Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 900, 'sum_payoffs': 112.61699997747549, 'action': [0.0, 0]}, {'num_count': 1700, 'sum_payoffs': 273.9520894188986, 'action': [1.0, 0]}])
Weights num count: [0.3460207612456747, 0.6535947712418301]
Actions to choose Agent 1: dict_values([{'num_count': 1696, 'sum_payoffs': 273.4417894190009, 'action': [1.0, 0]}, {'num_count': 904, 'sum_payoffs': 113.56469997728598, 'action': [0.0, 0]}])
Weights num count: [0.6520569011918493, 0.3475586312956555]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 965, 'sum_payoffs': 138.8879999722243, 'action': [0.0, 0]}, {'num_count': 1635, 'sum_payoffs': 283.6889999432628, 'action': [1.0, 0]}])
Weights num count: [0.37101114955786235, 0.6286043829296425]
Actions to choose Agent 1: dict_values([{'num_count': 966, 'sum_payoffs': 139.2119999721595, 'action': [0.0, 0]}, {'num_count': 1634, 'sum_payoffs': 283.68899994326284, 'action': [1.0, 0]}])
Weights num count: [0.37139561707035756, 0.6282199154171473]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.43845248222351074 s
