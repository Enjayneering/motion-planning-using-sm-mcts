Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 795, 'sum_payoffs': 120.24899997795382, 'action': [0.0, 0]}, {'num_count': 1805, 'sum_payoffs': 358.6685328289834, 'action': [1.0, 0]}])
Weights num count: [0.30565167243367936, 0.6939638600538255]
Actions to choose Agent 1: dict_values([{'num_count': 792, 'sum_payoffs': 120.0723749779862, 'action': [0.0, 0]}, {'num_count': 1808, 'sum_payoffs': 360.1209078287167, 'action': [1.0, 0]}])
Weights num count: [0.3044982698961938, 0.695117262591311]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 955, 'sum_payoffs': 175.40999996784115, 'action': [0.0, 0]}, {'num_count': 1645, 'sum_payoffs': 352.4399999353811, 'action': [1.0, 0]}])
Weights num count: [0.3671664744329104, 0.6324490580545944]
Actions to choose Agent 1: dict_values([{'num_count': 1645, 'sum_payoffs': 352.5412499353625, 'action': [1.0, 0]}, {'num_count': 955, 'sum_payoffs': 175.51124996782258, 'action': [0.0, 0]}])
Weights num count: [0.6324490580545944, 0.3671664744329104]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.43637704849243164 s
