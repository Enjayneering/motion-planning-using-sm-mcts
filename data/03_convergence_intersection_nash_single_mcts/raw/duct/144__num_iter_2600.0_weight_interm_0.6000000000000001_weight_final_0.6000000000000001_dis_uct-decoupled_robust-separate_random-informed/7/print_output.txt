Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1810, 'sum_payoffs': 360.30315782868325, 'action': [1.0, 0]}, {'num_count': 790, 'sum_payoffs': 119.3883749781116, 'action': [0.0, 0]}])
Weights num count: [0.6958861976163014, 0.3037293348712034]
Actions to choose Agent 1: dict_values([{'num_count': 803, 'sum_payoffs': 122.156999977604, 'action': [0.0, 0]}, {'num_count': 1797, 'sum_payoffs': 356.98778282929123, 'action': [1.0, 0]}])
Weights num count: [0.3087274125336409, 0.6908881199538639]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 975, 'sum_payoffs': 180.472499966913, 'action': [0.0, 0]}, {'num_count': 1625, 'sum_payoffs': 347.2762499363276, 'action': [1.0, 0]}])
Weights num count: [0.3748558246828143, 0.6247597078046905]
Actions to choose Agent 1: dict_values([{'num_count': 1622, 'sum_payoffs': 346.668749936439, 'action': [1.0, 0]}, {'num_count': 978, 'sum_payoffs': 181.28249996676448, 'action': [0.0, 0]}])
Weights num count: [0.623606305267205, 0.3760092272202999]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4178445339202881 s
