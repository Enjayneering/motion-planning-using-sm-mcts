Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1006, 'sum_payoffs': 60.319799987935404, 'action': [0.0, 0]}, {'num_count': 1594, 'sum_payoffs': 136.50610260427987, 'action': [1.0, 0]}])
Weights num count: [0.38677431757016534, 0.6128412149173394]
Actions to choose Agent 1: dict_values([{'num_count': 1602, 'sum_payoffs': 137.91865260399734, 'action': [1.0, 0]}, {'num_count': 998, 'sum_payoffs': 59.49044998810128, 'action': [0.0, 0]}])
Weights num count: [0.615916955017301, 0.38369857747020375]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1074, 'sum_payoffs': 75.5639999848881, 'action': [0.0, 0]}, {'num_count': 1526, 'sum_payoffs': 137.19599997256103, 'action': [1.0, 0]}])
Weights num count: [0.4129181084198385, 0.5866974240676662]
Actions to choose Agent 1: dict_values([{'num_count': 1527, 'sum_payoffs': 137.35799997252863, 'action': [1.0, 0]}, {'num_count': 1073, 'sum_payoffs': 75.4829999849043, 'action': [0.0, 0]}])
Weights num count: [0.5870818915801614, 0.41253364090734335]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5113027095794678 s
