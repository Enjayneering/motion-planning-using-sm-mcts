Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1775, 'sum_payoffs': 305.8784999475553, 'action': [1.0, 0]}, {'num_count': 825, 'sum_payoffs': 105.96635524499258, 'action': [0.0, 0]}])
Weights num count: [0.6824298346789697, 0.31718569780853517]
Actions to choose Agent 1: dict_values([{'num_count': 836, 'sum_payoffs': 108.62199810168029, 'action': [0.0, 0]}, {'num_count': 1764, 'sum_payoffs': 304.6287856620554, 'action': [1.0, 0]}])
Weights num count: [0.32141484044598234, 0.6782006920415224]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1530, 'sum_payoffs': 278.9164285236074, 'action': [1.0, 0]}, {'num_count': 1070, 'sum_payoffs': 173.8253571130568, 'action': [0.0, 0]}])
Weights num count: [0.5882352941176471, 0.41138023836985776]
Actions to choose Agent 1: dict_values([{'num_count': 1531, 'sum_payoffs': 279.35035709496157, 'action': [1.0, 0]}, {'num_count': 1069, 'sum_payoffs': 173.73857139878598, 'action': [0.0, 0]}])
Weights num count: [0.5886197616301423, 0.41099577085736255]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.46367359161376953 s
