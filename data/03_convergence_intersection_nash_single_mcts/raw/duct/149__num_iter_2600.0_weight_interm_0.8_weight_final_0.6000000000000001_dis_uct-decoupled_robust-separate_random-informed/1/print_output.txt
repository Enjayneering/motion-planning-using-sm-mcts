Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1757, 'sum_payoffs': 301.6757875422609, 'action': [1.0, 0]}, {'num_count': 843, 'sum_payoffs': 109.3667142669663, 'action': [0.0, 0]}])
Weights num count: [0.6755094194540562, 0.32410611303344866]
Actions to choose Agent 1: dict_values([{'num_count': 836, 'sum_payoffs': 108.51332140996976, 'action': [0.0, 0]}, {'num_count': 1764, 'sum_payoffs': 304.40375182750734, 'action': [1.0, 0]}])
Weights num count: [0.32141484044598234, 0.6782006920415224]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 997, 'sum_payoffs': 157.33607140159842, 'action': [0.0, 0]}, {'num_count': 1603, 'sum_payoffs': 295.4924999493378, 'action': [1.0, 0]}])
Weights num count: [0.3833141099577086, 0.6163014225297963]
Actions to choose Agent 1: dict_values([{'num_count': 999, 'sum_payoffs': 157.76999997295258, 'action': [0.0, 0]}, {'num_count': 1601, 'sum_payoffs': 295.0585713779837, 'action': [1.0, 0]}])
Weights num count: [0.38408304498269896, 0.6155324875048058]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4178812503814697 s
