Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 815, 'sum_payoffs': 124.8097499797181, 'action': [0.0, 0]}, {'num_count': 1785, 'sum_payoffs': 353.838907837246, 'action': [1.0, 0]}])
Weights num count: [0.3133410226835832, 0.6862745098039216]
Actions to choose Agent 1: dict_values([{'num_count': 803, 'sum_payoffs': 122.35499998011682, 'action': [0.0, 0]}, {'num_count': 1797, 'sum_payoffs': 357.20490783669914, 'action': [1.0, 0]}])
Weights num count: [0.3087274125336409, 0.6908881199538639]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1024, 'sum_payoffs': 192.92624996864996, 'action': [0.0, 0]}, {'num_count': 1576, 'sum_payoffs': 334.82249994558805, 'action': [1.0, 0]}])
Weights num count: [0.39369473279507883, 0.605920799692426]
Actions to choose Agent 1: dict_values([{'num_count': 1026, 'sum_payoffs': 193.53374996855126, 'action': [0.0, 0]}, {'num_count': 1574, 'sum_payoffs': 334.4174999456539, 'action': [1.0, 0]}])
Weights num count: [0.3944636678200692, 0.6051518646674356]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.45886874198913574 s
