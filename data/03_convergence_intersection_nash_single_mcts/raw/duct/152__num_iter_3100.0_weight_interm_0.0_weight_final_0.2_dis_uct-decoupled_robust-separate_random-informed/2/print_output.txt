Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2323, 'sum_payoffs': 881.3609994711641, 'action': [1.0, 0]}, {'num_count': 777, 'sum_payoffs': 247.67774985138996, 'action': [0.0, 0]}])
Weights num count: [0.7491131892937762, 0.2505643340857788]
Actions to choose Agent 1: dict_values([{'num_count': 2309, 'sum_payoffs': 877.3514994735708, 'action': [1.0, 0]}, {'num_count': 791, 'sum_payoffs': 253.8742498476718, 'action': [0.0, 0]}])
Weights num count: [0.744598516607546, 0.255079006772009]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1093, 'sum_payoffs': 419.98499974801666, 'action': [0.0, 0]}, {'num_count': 2007, 'sum_payoffs': 835.1099994989631, 'action': [1.0, 0]}])
Weights num count: [0.3524669461464044, 0.6472105772331506]
Actions to choose Agent 1: dict_values([{'num_count': 1995, 'sum_payoffs': 830.2499995018788, 'action': [1.0, 0]}, {'num_count': 1105, 'sum_payoffs': 425.65499974461494, 'action': [0.0, 0]}])
Weights num count: [0.6433408577878104, 0.3563366655917446]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.49334239959716797 s
