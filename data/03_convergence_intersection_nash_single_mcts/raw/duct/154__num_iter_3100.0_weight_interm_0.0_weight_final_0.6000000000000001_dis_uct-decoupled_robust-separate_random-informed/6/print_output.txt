Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2327, 'sum_payoffs': 882.8189997646115, 'action': [1.0, 0]}, {'num_count': 773, 'sum_payoffs': 246.03749993438882, 'action': [0.0, 0]}])
Weights num count: [0.7504030957755563, 0.2492744276039987]
Actions to choose Agent 1: dict_values([{'num_count': 798, 'sum_payoffs': 256.79024993152103, 'action': [0.0, 0]}, {'num_count': 2302, 'sum_payoffs': 874.9822497667009, 'action': [1.0, 0]}])
Weights num count: [0.25733634311512416, 0.7423411802644309]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1105, 'sum_payoffs': 425.4524998865555, 'action': [0.0, 0]}, {'num_count': 1995, 'sum_payoffs': 829.6424997787544, 'action': [1.0, 0]}])
Weights num count: [0.3563366655917446, 0.6433408577878104]
Actions to choose Agent 1: dict_values([{'num_count': 1118, 'sum_payoffs': 431.7299998848818, 'action': [0.0, 0]}, {'num_count': 1982, 'sum_payoffs': 824.5799997801048, 'action': [1.0, 0]}])
Weights num count: [0.36052886165752984, 0.6391486617220251]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.49739766120910645 s
