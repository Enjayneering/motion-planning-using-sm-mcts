Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 787, 'sum_payoffs': 251.6872499328818, 'action': [0.0, 0]}, {'num_count': 2313, 'sum_payoffs': 877.8982497659234, 'action': [1.0, 0]}])
Weights num count: [0.25378910029022894, 0.745888423089326]
Actions to choose Agent 1: dict_values([{'num_count': 2301, 'sum_payoffs': 874.0709997669438, 'action': [1.0, 0]}, {'num_count': 799, 'sum_payoffs': 256.97249993147227, 'action': [0.0, 0]}])
Weights num count: [0.7420187036439858, 0.25765881973556914]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1101, 'sum_payoffs': 423.42749988709545, 'action': [0.0, 0]}, {'num_count': 1999, 'sum_payoffs': 831.4649997782682, 'action': [1.0, 0]}])
Weights num count: [0.3550467591099645, 0.6446307642695904]
Actions to choose Agent 1: dict_values([{'num_count': 1995, 'sum_payoffs': 830.4524997785381, 'action': [1.0, 0]}, {'num_count': 1105, 'sum_payoffs': 426.0599998863936, 'action': [0.0, 0]}])
Weights num count: [0.6433408577878104, 0.3563366655917446]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.47045183181762695 s
