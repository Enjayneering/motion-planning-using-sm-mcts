Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 757, 'sum_payoffs': 239.29424994616076, 'action': [0.0, 0]}, {'num_count': 2343, 'sum_payoffs': 889.1977497998945, 'action': [1.0, 0]}])
Weights num count: [0.24411480167687843, 0.7555627217026766]
Actions to choose Agent 1: dict_values([{'num_count': 2319, 'sum_payoffs': 881.9077498015358, 'action': [1.0, 0]}, {'num_count': 781, 'sum_payoffs': 249.86474994378275, 'action': [0.0, 0]}])
Weights num count: [0.7478232828119962, 0.25185424056755884]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1572, 'sum_payoffs': 637.8749998564775, 'action': [1.0, 0]}, {'num_count': 1528, 'sum_payoffs': 617.8274998609866, 'action': [0.0, 0]}])
Weights num count: [0.5069332473395679, 0.4927442760399871]
Actions to choose Agent 1: dict_values([{'num_count': 1528, 'sum_payoffs': 617.8274998609866, 'action': [0.0, 0]}, {'num_count': 1572, 'sum_payoffs': 637.8749998564775, 'action': [1.0, 0]}])
Weights num count: [0.4927442760399871, 0.5069332473395679]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.49162912368774414 s
