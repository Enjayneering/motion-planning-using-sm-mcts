Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2165, 'sum_payoffs': 427.5534077451017, 'action': [1.0, 0]}, {'num_count': 935, 'sum_payoffs': 142.6196249500835, 'action': [0.0, 0]}])
Weights num count: [0.6981618832634634, 0.3015156401160916]
Actions to choose Agent 1: dict_values([{'num_count': 949, 'sum_payoffs': 146.17349994883966, 'action': [0.0, 0]}, {'num_count': 2151, 'sum_payoffs': 425.45753274583615, 'action': [1.0, 0]}])
Weights num count: [0.30603031280232185, 0.6936472105772331]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1116, 'sum_payoffs': 204.97499992826116, 'action': [0.0, 0]}, {'num_count': 1984, 'sum_payoffs': 424.1249998515441, 'action': [1.0, 0]}])
Weights num count: [0.3598839084166398, 0.6397936149629152]
Actions to choose Agent 1: dict_values([{'num_count': 1987, 'sum_payoffs': 424.9349998512605, 'action': [1.0, 0]}, {'num_count': 1113, 'sum_payoffs': 204.36749992847376, 'action': [0.0, 0]}])
Weights num count: [0.6407610448242502, 0.35891647855530473]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.49303150177001953 s
