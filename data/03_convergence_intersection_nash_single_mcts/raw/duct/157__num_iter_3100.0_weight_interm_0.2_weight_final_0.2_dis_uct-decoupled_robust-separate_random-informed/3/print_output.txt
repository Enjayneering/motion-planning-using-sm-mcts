Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2100, 'sum_payoffs': 412.47728275037974, 'action': [1.0, 0]}, {'num_count': 1000, 'sum_payoffs': 157.15912494499486, 'action': [0.0, 0]}])
Weights num count: [0.6772009029345373, 0.32247662044501774]
Actions to choose Agent 1: dict_values([{'num_count': 991, 'sum_payoffs': 155.47837494558314, 'action': [0.0, 0]}, {'num_count': 2109, 'sum_payoffs': 415.43378274934514, 'action': [1.0, 0]}])
Weights num count: [0.31957433086101256, 0.6801031925185425]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1937, 'sum_payoffs': 412.17749985572647, 'action': [1.0, 0]}, {'num_count': 1163, 'sum_payoffs': 216.71999992415013, 'action': [0.0, 0]}])
Weights num count: [0.6246372138019993, 0.37504030957755563]
Actions to choose Agent 1: dict_values([{'num_count': 1167, 'sum_payoffs': 217.83374992376037, 'action': [0.0, 0]}, {'num_count': 1933, 'sum_payoffs': 411.4687498559746, 'action': [1.0, 0]}])
Weights num count: [0.3763302160593357, 0.6233473073202193]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5353872776031494 s
