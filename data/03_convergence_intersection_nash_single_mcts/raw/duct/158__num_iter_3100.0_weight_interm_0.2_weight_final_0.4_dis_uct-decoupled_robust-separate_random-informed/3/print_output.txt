Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 854, 'sum_payoffs': 175.81649995311307, 'action': [0.0, 0]}, {'num_count': 2246, 'sum_payoffs': 580.5881051083381, 'action': [1.0, 0]}])
Weights num count: [0.27539503386004516, 0.7242824895195098]
Actions to choose Agent 1: dict_values([{'num_count': 874, 'sum_payoffs': 181.7399999515333, 'action': [0.0, 0]}, {'num_count': 2226, 'sum_payoffs': 575.8796051095927, 'action': [1.0, 0]}])
Weights num count: [0.2818445662689455, 0.7178329571106095]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1983, 'sum_payoffs': 557.2049998514276, 'action': [1.0, 0]}, {'num_count': 1117, 'sum_payoffs': 280.4249999252187, 'action': [0.0, 0]}])
Weights num count: [0.6394711383424702, 0.3602063850370848]
Actions to choose Agent 1: dict_values([{'num_count': 1979, 'sum_payoffs': 556.2599998516795, 'action': [1.0, 0]}, {'num_count': 1121, 'sum_payoffs': 281.9099999248228, 'action': [0.0, 0]}])
Weights num count: [0.6381812318606901, 0.36149629151886487]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4957125186920166 s
