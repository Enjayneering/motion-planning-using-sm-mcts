Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2284, 'sum_payoffs': 659.2693912990242, 'action': [1.0, 0]}, {'num_count': 816, 'sum_payoffs': 189.43368745737635, 'action': [0.0, 0]}])
Weights num count: [0.7365366010964205, 0.26314092228313446]
Actions to choose Agent 1: dict_values([{'num_count': 821, 'sum_payoffs': 191.55487495689889, 'action': [0.0, 0]}, {'num_count': 2279, 'sum_payoffs': 659.3352037990097, 'action': [1.0, 0]}])
Weights num count: [0.26475330538535957, 0.7349242179941954]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1991, 'sum_payoffs': 626.6306248590253, 'action': [1.0, 0]}, {'num_count': 1109, 'sum_payoffs': 315.06187492910993, 'action': [0.0, 0]}])
Weights num count: [0.6420509513060303, 0.35762657207352466]
Actions to choose Agent 1: dict_values([{'num_count': 1118, 'sum_payoffs': 318.70687492828984, 'action': [0.0, 0]}, {'num_count': 1982, 'sum_payoffs': 624.2006248595716, 'action': [1.0, 0]}])
Weights num count: [0.36052886165752984, 0.6391486617220251]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.48269176483154297 s
