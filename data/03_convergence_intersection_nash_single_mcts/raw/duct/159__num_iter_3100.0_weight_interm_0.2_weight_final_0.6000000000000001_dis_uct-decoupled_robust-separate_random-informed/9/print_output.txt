Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 821, 'sum_payoffs': 191.25056245696726, 'action': [0.0, 0]}, {'num_count': 2279, 'sum_payoffs': 658.466703799205, 'action': [1.0, 0]}])
Weights num count: [0.26475330538535957, 0.7349242179941954]
Actions to choose Agent 1: dict_values([{'num_count': 835, 'sum_payoffs': 196.03462495589073, 'action': [0.0, 0]}, {'num_count': 2265, 'sum_payoffs': 655.5962662998511, 'action': [1.0, 0]}])
Weights num count: [0.2692679780715898, 0.7304095453079652]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1797, 'sum_payoffs': 558.438749874357, 'action': [1.0, 0]}, {'num_count': 1303, 'sum_payoffs': 383.40562491373146, 'action': [0.0, 0]}])
Weights num count: [0.5794904869396968, 0.4201870364398581]
Actions to choose Agent 1: dict_values([{'num_count': 1305, 'sum_payoffs': 384.3168749135264, 'action': [0.0, 0]}, {'num_count': 1795, 'sum_payoffs': 558.1349998744253, 'action': [1.0, 0]}])
Weights num count: [0.42083198968074814, 0.5788455336988069]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5081532001495361 s
