Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 824, 'sum_payoffs': 206.764199958645, 'action': [0.0, 0]}, {'num_count': 2276, 'sum_payoffs': 698.0597998604014, 'action': [1.0, 0]}])
Weights num count: [0.2657207352466946, 0.7339567881328604]
Actions to choose Agent 1: dict_values([{'num_count': 2282, 'sum_payoffs': 702.304199859552, 'action': [1.0, 0]}, {'num_count': 818, 'sum_payoffs': 205.72739995885215, 'action': [0.0, 0]}])
Weights num count: [0.7358916478555305, 0.2637858755240245]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1099, 'sum_payoffs': 333.7379999332566, 'action': [0.0, 0]}, {'num_count': 2001, 'sum_payoffs': 670.5539998658779, 'action': [1.0, 0]}])
Weights num count: [0.3544018058690745, 0.6452757175104805]
Actions to choose Agent 1: dict_values([{'num_count': 1105, 'sum_payoffs': 336.4919999327059, 'action': [0.0, 0]}, {'num_count': 1995, 'sum_payoffs': 669.0959998661698, 'action': [1.0, 0]}])
Weights num count: [0.3563366655917446, 0.6433408577878104]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4731884002685547 s
