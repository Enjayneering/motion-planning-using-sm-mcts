Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1042, 'sum_payoffs': 104.25974997219556, 'action': [0.0, 0]}, {'num_count': 2058, 'sum_payoffs': 279.6937104517322, 'action': [1.0, 0]}])
Weights num count: [0.3360206385037085, 0.6636568848758465]
Actions to choose Agent 1: dict_values([{'num_count': 1044, 'sum_payoffs': 104.74574997206591, 'action': [0.0, 0]}, {'num_count': 2056, 'sum_payoffs': 279.5722104517642, 'action': [1.0, 0]}])
Weights num count: [0.33666559174459854, 0.6630119316349564]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1814, 'sum_payoffs': 259.2524999308729, 'action': [1.0, 0]}, {'num_count': 1286, 'sum_payoffs': 161.0474999570541, 'action': [0.0, 0]}])
Weights num count: [0.5849725894872622, 0.4147049338922928]
Actions to choose Agent 1: dict_values([{'num_count': 1285, 'sum_payoffs': 160.91249995709012, 'action': [0.0, 0]}, {'num_count': 1815, 'sum_payoffs': 259.5224999308009, 'action': [1.0, 0]}])
Weights num count: [0.4143824572718478, 0.5852950661077072]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5036571025848389 s
