Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 967, 'sum_payoffs': 150.0007499662509, 'action': [0.0, 0]}, {'num_count': 2133, 'sum_payoffs': 420.7190328000594, 'action': [1.0, 0]}])
Weights num count: [0.31183489197033215, 0.6878426314092229]
Actions to choose Agent 1: dict_values([{'num_count': 2143, 'sum_payoffs': 423.36165779946407, 'action': [1.0, 0]}, {'num_count': 957, 'sum_payoffs': 147.90487496672213, 'action': [0.0, 0]}])
Weights num count: [0.691067397613673, 0.308610125765882]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1094, 'sum_payoffs': 199.40624995513224, 'action': [0.0, 0]}, {'num_count': 2006, 'sum_payoffs': 429.288749903418, 'action': [1.0, 0]}])
Weights num count: [0.3527894227668494, 0.6468881006127056]
Actions to choose Agent 1: dict_values([{'num_count': 1096, 'sum_payoffs': 200.11499995497275, 'action': [0.0, 0]}, {'num_count': 2004, 'sum_payoffs': 429.18749990344077, 'action': [1.0, 0]}])
Weights num count: [0.3534343760077394, 0.6462431473718155]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.48351263999938965 s
