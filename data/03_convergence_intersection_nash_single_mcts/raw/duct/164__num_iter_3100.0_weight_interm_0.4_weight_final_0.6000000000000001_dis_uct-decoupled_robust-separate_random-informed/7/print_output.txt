Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 924, 'sum_payoffs': 173.5460999652912, 'action': [0.0, 0]}, {'num_count': 2176, 'sum_payoffs': 508.765026214022, 'action': [1.0, 0]}])
Weights num count: [0.2979683972911964, 0.7017091260883586]
Actions to choose Agent 1: dict_values([{'num_count': 926, 'sum_payoffs': 174.4208999651168, 'action': [0.0, 0]}, {'num_count': 2174, 'sum_payoffs': 508.98372621397806, 'action': [1.0, 0]}])
Weights num count: [0.29861335053208643, 0.7010641728474686]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1849, 'sum_payoffs': 464.7284999070445, 'action': [1.0, 0]}, {'num_count': 1251, 'sum_payoffs': 289.2869999421371, 'action': [0.0, 0]}])
Weights num count: [0.5962592712028378, 0.4034182521767172]
Actions to choose Agent 1: dict_values([{'num_count': 1848, 'sum_payoffs': 464.7284999070445, 'action': [1.0, 0]}, {'num_count': 1252, 'sum_payoffs': 289.77299994203986, 'action': [0.0, 0]}])
Weights num count: [0.5959367945823928, 0.4037407287971622]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.47171545028686523 s
