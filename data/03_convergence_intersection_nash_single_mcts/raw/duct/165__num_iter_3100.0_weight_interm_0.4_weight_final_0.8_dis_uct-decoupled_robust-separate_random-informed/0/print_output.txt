Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2274, 'sum_payoffs': 588.1211051553294, 'action': [1.0, 0]}, {'num_count': 826, 'sum_payoffs': 167.83499996923004, 'action': [0.0, 0]}])
Weights num count: [0.7333118348919704, 0.26636568848758463]
Actions to choose Agent 1: dict_values([{'num_count': 2255, 'sum_payoffs': 583.5341051561699, 'action': [1.0, 0]}, {'num_count': 845, 'sum_payoffs': 173.39399996821066, 'action': [0.0, 0]}])
Weights num count: [0.727184779103515, 0.27249274427604]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1980, 'sum_payoffs': 556.2599998980465, 'action': [1.0, 0]}, {'num_count': 1120, 'sum_payoffs': 281.36999994841943, 'action': [0.0, 0]}])
Weights num count: [0.6385037084811351, 0.3611738148984199]
Actions to choose Agent 1: dict_values([{'num_count': 1976, 'sum_payoffs': 555.3149998982198, 'action': [1.0, 0]}, {'num_count': 1124, 'sum_payoffs': 282.85499994814734, 'action': [0.0, 0]}])
Weights num count: [0.6372138019993551, 0.36246372138019994]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5260226726531982 s
