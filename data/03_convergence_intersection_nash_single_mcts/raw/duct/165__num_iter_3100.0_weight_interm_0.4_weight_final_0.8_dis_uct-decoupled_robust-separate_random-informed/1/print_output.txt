Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 864, 'sum_payoffs': 178.48949996727634, 'action': [0.0, 0]}, {'num_count': 2236, 'sum_payoffs': 577.3376051573067, 'action': [1.0, 0]}])
Weights num count: [0.2786198000644953, 0.7210577233150597]
Actions to choose Agent 1: dict_values([{'num_count': 882, 'sum_payoffs': 184.29449996621227, 'action': [0.0, 0]}, {'num_count': 2218, 'sum_payoffs': 574.2056051578804, 'action': [1.0, 0]}])
Weights num count: [0.28442437923250563, 0.7152531441470493]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1120, 'sum_payoffs': 281.36999994841943, 'action': [0.0, 0]}, {'num_count': 1980, 'sum_payoffs': 556.2599998980465, 'action': [1.0, 0]}])
Weights num count: [0.3611738148984199, 0.6385037084811351]
Actions to choose Agent 1: dict_values([{'num_count': 1124, 'sum_payoffs': 282.85499994814734, 'action': [0.0, 0]}, {'num_count': 1976, 'sum_payoffs': 555.3149998982198, 'action': [1.0, 0]}])
Weights num count: [0.36246372138019994, 0.6372138019993551]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4884822368621826 s
