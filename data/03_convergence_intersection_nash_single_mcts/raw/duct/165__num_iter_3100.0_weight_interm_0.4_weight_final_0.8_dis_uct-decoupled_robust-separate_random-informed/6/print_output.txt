Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2200, 'sum_payoffs': 566.9801051592025, 'action': [1.0, 0]}, {'num_count': 900, 'sum_payoffs': 188.573999965428, 'action': [0.0, 0]}])
Weights num count: [0.709448564979039, 0.29022895840051594]
Actions to choose Agent 1: dict_values([{'num_count': 2183, 'sum_payoffs': 563.9996051597485, 'action': [1.0, 0]}, {'num_count': 917, 'sum_payoffs': 194.22749996439168, 'action': [0.0, 0]}])
Weights num count: [0.7039664624314738, 0.29571106094808125]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1086, 'sum_payoffs': 270.70499995037284, 'action': [0.0, 0]}, {'num_count': 2014, 'sum_payoffs': 567.0599998960676, 'action': [1.0, 0]}])
Weights num count: [0.35020960980328925, 0.6494679135762658]
Actions to choose Agent 1: dict_values([{'num_count': 2011, 'sum_payoffs': 566.3849998961914, 'action': [1.0, 0]}, {'num_count': 1089, 'sum_payoffs': 271.6499999501997, 'action': [0.0, 0]}])
Weights num count: [0.6485004837149306, 0.3511770396646243]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5172054767608643 s
