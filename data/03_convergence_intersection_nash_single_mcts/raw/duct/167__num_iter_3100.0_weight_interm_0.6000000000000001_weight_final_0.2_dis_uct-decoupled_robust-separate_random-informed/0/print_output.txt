Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1965, 'sum_payoffs': 204.7502367960316, 'action': [1.0, 0]}, {'num_count': 1135, 'sum_payoffs': 85.84649998068458, 'action': [0.0, 0]}])
Weights num count: [0.6336665591744598, 0.36601096420509516]
Actions to choose Agent 1: dict_values([{'num_count': 1952, 'sum_payoffs': 202.50248679653768, 'action': [1.0, 0]}, {'num_count': 1148, 'sum_payoffs': 87.45637498032234, 'action': [0.0, 0]}])
Weights num count: [0.6294743631086747, 0.37020316027088035]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1442, 'sum_payoffs': 141.4124999681821, 'action': [0.0, 0]}, {'num_count': 1658, 'sum_payoffs': 174.43687496075287, 'action': [1.0, 0]}])
Weights num count: [0.4650112866817156, 0.5346662366978394]
Actions to choose Agent 1: dict_values([{'num_count': 1657, 'sum_payoffs': 174.33562496077565, 'action': [1.0, 0]}, {'num_count': 1443, 'sum_payoffs': 141.61499996813654, 'action': [0.0, 0]}])
Weights num count: [0.5343437600773944, 0.4653337633021606]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.538712739944458 s
