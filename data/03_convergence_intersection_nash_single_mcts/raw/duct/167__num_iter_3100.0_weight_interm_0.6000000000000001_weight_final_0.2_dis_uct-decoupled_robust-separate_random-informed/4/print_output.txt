Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1177, 'sum_payoffs': 91.92824997931652, 'action': [0.0, 0]}, {'num_count': 1923, 'sum_payoffs': 199.15448679728956, 'action': [1.0, 0]}])
Weights num count: [0.37955498226378587, 0.6201225411157691]
Actions to choose Agent 1: dict_values([{'num_count': 1167, 'sum_payoffs': 90.84318747956043, 'action': [0.0, 0]}, {'num_count': 1933, 'sum_payoffs': 201.15079929683978, 'action': [1.0, 0]}])
Weights num count: [0.3763302160593357, 0.6233473073202193]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1683, 'sum_payoffs': 178.23374995989863, 'action': [1.0, 0]}, {'num_count': 1417, 'sum_payoffs': 137.66624996902496, 'action': [0.0, 0]}])
Weights num count: [0.5427281522089649, 0.45694937117059015]
Actions to choose Agent 1: dict_values([{'num_count': 1682, 'sum_payoffs': 178.08187495993278, 'action': [1.0, 0]}, {'num_count': 1418, 'sum_payoffs': 137.81812496899076, 'action': [0.0, 0]}])
Weights num count: [0.5424056755885198, 0.45727184779103514]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.49165987968444824 s
