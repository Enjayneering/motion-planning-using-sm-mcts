Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2087, 'sum_payoffs': 334.057499933193, 'action': [1.0, 0]}, {'num_count': 1013, 'sum_payoffs': 123.44939997530857, 'action': [0.0, 0]}])
Weights num count: [0.673008706868752, 0.326668816510803]
Actions to choose Agent 1: dict_values([{'num_count': 2070, 'sum_payoffs': 330.7283999338589, 'action': [1.0, 0]}, {'num_count': 1030, 'sum_payoffs': 126.63269997467201, 'action': [0.0, 0]}])
Weights num count: [0.6675266043211867, 0.33215091905836824]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1949, 'sum_payoffs': 336.4199999327097, 'action': [1.0, 0]}, {'num_count': 1151, 'sum_payoffs': 167.156999966571, 'action': [0.0, 0]}])
Weights num count: [0.6285069332473395, 0.3711705901322154]
Actions to choose Agent 1: dict_values([{'num_count': 1152, 'sum_payoffs': 167.48099996650618, 'action': [0.0, 0]}, {'num_count': 1948, 'sum_payoffs': 336.41999993270974, 'action': [1.0, 0]}])
Weights num count: [0.3714930667526604, 0.6281844566268946]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4875824451446533 s
