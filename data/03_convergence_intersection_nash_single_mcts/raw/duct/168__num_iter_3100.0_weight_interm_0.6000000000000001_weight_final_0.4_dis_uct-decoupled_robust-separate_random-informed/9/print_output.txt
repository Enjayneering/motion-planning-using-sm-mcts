Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1007, 'sum_payoffs': 122.13719997557078, 'action': [0.0, 0]}, {'num_count': 2093, 'sum_payoffs': 335.24819993295466, 'action': [1.0, 0]}])
Weights num count: [0.3247339567881329, 0.6749435665914221]
Actions to choose Agent 1: dict_values([{'num_count': 1003, 'sum_payoffs': 122.1614999755661, 'action': [0.0, 0]}, {'num_count': 2097, 'sum_payoffs': 337.4108999325222, 'action': [1.0, 0]}])
Weights num count: [0.3234440503063528, 0.6762334730732021]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1934, 'sum_payoffs': 333.42299993330994, 'action': [1.0, 0]}, {'num_count': 1166, 'sum_payoffs': 170.3969999659219, 'action': [0.0, 0]}])
Weights num count: [0.6236697839406643, 0.37600773943889065]
Actions to choose Agent 1: dict_values([{'num_count': 1165, 'sum_payoffs': 170.23499996595427, 'action': [0.0, 0]}, {'num_count': 1935, 'sum_payoffs': 333.74699993324504, 'action': [1.0, 0]}])
Weights num count: [0.37568526281844566, 0.6239922605611093]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.48567914962768555 s
