Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2112, 'sum_payoffs': 415.0344078186511, 'action': [1.0, 0]}, {'num_count': 988, 'sum_payoffs': 154.29321707697537, 'action': [0.0, 0]}])
Weights num count: [0.6810706223798775, 0.31860690099967753]
Actions to choose Agent 1: dict_values([{'num_count': 2117, 'sum_payoffs': 417.5898749234448, 'action': [1.0, 0]}, {'num_count': 983, 'sum_payoffs': 153.74249997181317, 'action': [0.0, 0]}])
Weights num count: [0.6826830054821026, 0.3169945178974524]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1164, 'sum_payoffs': 216.82124996024774, 'action': [0.0, 0]}, {'num_count': 1936, 'sum_payoffs': 411.8737499244826, 'action': [1.0, 0]}])
Weights num count: [0.3753627861980006, 0.6243147371815544]
Actions to choose Agent 1: dict_values([{'num_count': 1167, 'sum_payoffs': 217.83374996006208, 'action': [0.0, 0]}, {'num_count': 1933, 'sum_payoffs': 411.46874992455685, 'action': [1.0, 0]}])
Weights num count: [0.3763302160593357, 0.6233473073202193]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.49139976501464844 s
