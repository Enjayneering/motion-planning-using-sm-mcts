Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2197, 'sum_payoffs': 490.79970668276263, 'action': [1.0, 0]}, {'num_count': 903, 'sum_payoffs': 158.40899997284285, 'action': [0.0, 0]}])
Weights num count: [0.7084811351177039, 0.291196388261851]
Actions to choose Agent 1: dict_values([{'num_count': 2195, 'sum_payoffs': 492.01856382541104, 'action': [1.0, 0]}, {'num_count': 905, 'sum_payoffs': 159.68957140119474, 'action': [0.0, 0]}])
Weights num count: [0.707836181876814, 0.29184134150274105]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1924, 'sum_payoffs': 464.5542856346552, 'action': [1.0, 0]}, {'num_count': 1176, 'sum_payoffs': 253.91571424219012, 'action': [0.0, 0]}])
Weights num count: [0.6204450177362141, 0.3792325056433409]
Actions to choose Agent 1: dict_values([{'num_count': 1926, 'sum_payoffs': 465.3642856345163, 'action': [1.0, 0]}, {'num_count': 1174, 'sum_payoffs': 253.56857138510676, 'action': [0.0, 0]}])
Weights num count: [0.6210899709771042, 0.37858755240245084]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.48192286491394043 s
