Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 877, 'sum_payoffs': 152.26071425961126, 'action': [0.0, 0]}, {'num_count': 2223, 'sum_payoffs': 497.9547066815363, 'action': [1.0, 0]}])
Weights num count: [0.2828119961302806, 0.7168655272492744]
Actions to choose Agent 1: dict_values([{'num_count': 889, 'sum_payoffs': 155.21142854481937, 'action': [0.0, 0]}, {'num_count': 2211, 'sum_payoffs': 494.7957066820777, 'action': [1.0, 0]}])
Weights num count: [0.2866817155756208, 0.7129958078039342]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1984, 'sum_payoffs': 481.2171427746557, 'action': [1.0, 0]}, {'num_count': 1116, 'sum_payoffs': 237.25285710218887, 'action': [0.0, 0]}])
Weights num count: [0.6397936149629152, 0.3598839084166398]
Actions to choose Agent 1: dict_values([{'num_count': 1119, 'sum_payoffs': 238.06285710205003, 'action': [0.0, 0]}, {'num_count': 1981, 'sum_payoffs': 480.6385713461834, 'action': [1.0, 0]}])
Weights num count: [0.36085133827797483, 0.6388261851015802]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5061831474304199 s
