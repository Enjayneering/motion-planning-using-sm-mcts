Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1818, 'sum_payoffs': 152.02480260117645, 'action': [1.0, 0]}, {'num_count': 1282, 'sum_payoffs': 84.16934998316633, 'action': [0.0, 0]}])
Weights num count: [0.5862624959690422, 0.4134150274105127]
Actions to choose Agent 1: dict_values([{'num_count': 1282, 'sum_payoffs': 84.16934998316633, 'action': [0.0, 0]}, {'num_count': 1818, 'sum_payoffs': 152.02480260117653, 'action': [1.0, 0]}])
Weights num count: [0.4134150274105127, 0.5862624959690422]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1761, 'sum_payoffs': 154.52999996909222, 'action': [1.0, 0]}, {'num_count': 1339, 'sum_payoffs': 98.68949998026258, 'action': [0.0, 0]}])
Weights num count: [0.5678813286036762, 0.43179619477587877]
Actions to choose Agent 1: dict_values([{'num_count': 1760, 'sum_payoffs': 154.4489999691084, 'action': [1.0, 0]}, {'num_count': 1340, 'sum_payoffs': 98.85149998023019, 'action': [0.0, 0]}])
Weights num count: [0.5675588519832312, 0.43211867139632376]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.481856107711792 s
