Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 964, 'sum_payoffs': 124.48703569294587, 'action': [0.0, 0]}, {'num_count': 2136, 'sum_payoffs': 366.42082324544333, 'action': [1.0, 0]}])
Weights num count: [0.3108674621089971, 0.6888100612705579]
Actions to choose Agent 1: dict_values([{'num_count': 2132, 'sum_payoffs': 366.0302875312249, 'action': [1.0, 0]}, {'num_count': 968, 'sum_payoffs': 125.50242854991457, 'action': [0.0, 0]}])
Weights num count: [0.6875201547887778, 0.3121573685907772]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1932, 'sum_payoffs': 355.2878570819402, 'action': [1.0, 0]}, {'num_count': 1168, 'sum_payoffs': 184.3264285398278, 'action': [0.0, 0]}])
Weights num count: [0.6230248306997742, 0.37665269267978074]
Actions to choose Agent 1: dict_values([{'num_count': 1167, 'sum_payoffs': 184.1528571112861, 'action': [0.0, 0]}, {'num_count': 1933, 'sum_payoffs': 355.63499993902354, 'action': [1.0, 0]}])
Weights num count: [0.3763302160593357, 0.6233473073202193]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5266940593719482 s
