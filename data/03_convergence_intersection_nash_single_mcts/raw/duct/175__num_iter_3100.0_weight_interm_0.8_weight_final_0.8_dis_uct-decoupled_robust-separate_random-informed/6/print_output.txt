Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2186, 'sum_payoffs': 431.75078282459, 'action': [1.0, 0]}, {'num_count': 914, 'sum_payoffs': 137.57287497764443, 'action': [0.0, 0]}])
Weights num count: [0.7049338922928088, 0.2947436310867462]
Actions to choose Agent 1: dict_values([{'num_count': 2179, 'sum_payoffs': 430.7022828247604, 'action': [1.0, 0]}, {'num_count': 921, 'sum_payoffs': 139.5326249773261, 'action': [0.0, 0]}])
Weights num count: [0.7026765559496937, 0.2970009674298613]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1949, 'sum_payoffs': 415.2149999325171, 'action': [1.0, 0]}, {'num_count': 1151, 'sum_payoffs': 213.68249996527754, 'action': [0.0, 0]}])
Weights num count: [0.6285069332473395, 0.3711705901322154]
Actions to choose Agent 1: dict_values([{'num_count': 1944, 'sum_payoffs': 414.2024999326818, 'action': [1.0, 0]}, {'num_count': 1156, 'sum_payoffs': 215.09999996504723, 'action': [0.0, 0]}])
Weights num count: [0.6268945501451145, 0.3727829732344405]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4768693447113037 s
