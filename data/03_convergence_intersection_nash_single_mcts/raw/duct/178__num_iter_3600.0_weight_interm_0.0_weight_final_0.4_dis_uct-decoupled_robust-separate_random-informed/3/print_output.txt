Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2724, 'sum_payoffs': 1031.1704996391256, 'action': [1.0, 0]}, {'num_count': 876, 'sum_payoffs': 279.7537499020875, 'action': [0.0, 0]}])
Weights num count: [0.7564565398500417, 0.24326575951124688]
Actions to choose Agent 1: dict_values([{'num_count': 896, 'sum_payoffs': 288.3194998990894, 'action': [0.0, 0]}, {'num_count': 2704, 'sum_payoffs': 1025.5207496411058, 'action': [1.0, 0]}])
Weights num count: [0.24881977228547625, 0.7509025270758123]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1264, 'sum_payoffs': 487.0124998295534, 'action': [0.0, 0]}, {'num_count': 2336, 'sum_payoffs': 970.379999660328, 'action': [1.0, 0]}])
Weights num count: [0.3510136073312969, 0.6487086920299917]
Actions to choose Agent 1: dict_values([{'num_count': 1267, 'sum_payoffs': 488.63249982898645, 'action': [0.0, 0]}, {'num_count': 2333, 'sum_payoffs': 969.5699996606118, 'action': [1.0, 0]}])
Weights num count: [0.3518467092474313, 0.6478755901138573]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5652525424957275 s
