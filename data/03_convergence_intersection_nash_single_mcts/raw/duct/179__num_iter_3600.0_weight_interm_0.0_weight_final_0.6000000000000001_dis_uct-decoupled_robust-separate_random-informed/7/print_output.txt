Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2678, 'sum_payoffs': 1012.3987497300634, 'action': [1.0, 0]}, {'num_count': 922, 'sum_payoffs': 297.9787499205395, 'action': [0.0, 0]}])
Weights num count: [0.7436823104693141, 0.25603998889197443]
Actions to choose Agent 1: dict_values([{'num_count': 949, 'sum_payoffs': 309.46049991747833, 'action': [0.0, 0]}, {'num_count': 2651, 'sum_payoffs': 1004.5619997321528, 'action': [1.0, 0]}])
Weights num count: [0.26353790613718414, 0.7361843932241044]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1802, 'sum_payoffs': 730.0124998053303, 'action': [0.0, 0]}, {'num_count': 1798, 'sum_payoffs': 728.1899998058165, 'action': [1.0, 0]}])
Weights num count: [0.5004165509580673, 0.49930574840322134]
Actions to choose Agent 1: dict_values([{'num_count': 1802, 'sum_payoffs': 730.0124998053303, 'action': [0.0, 0]}, {'num_count': 1798, 'sum_payoffs': 728.1899998058165, 'action': [1.0, 0]}])
Weights num count: [0.5004165509580673, 0.49930574840322134]
Selected final action: [0.0, 0, 0.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 1.0, 1.5707963267948966, 1.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5368499755859375 s
