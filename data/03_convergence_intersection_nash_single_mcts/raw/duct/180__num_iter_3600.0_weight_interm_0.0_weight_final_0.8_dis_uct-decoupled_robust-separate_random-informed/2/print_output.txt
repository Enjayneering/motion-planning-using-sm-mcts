Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 867, 'sum_payoffs': 275.926499937919, 'action': [0.0, 0]}, {'num_count': 2733, 'sum_payoffs': 1034.2687497672393, 'action': [1.0, 0]}])
Weights num count: [0.24076645376284364, 0.7589558455984449]
Actions to choose Agent 1: dict_values([{'num_count': 897, 'sum_payoffs': 288.8662499350075, 'action': [0.0, 0]}, {'num_count': 2703, 'sum_payoffs': 1025.3384997692465, 'action': [1.0, 0]}])
Weights num count: [0.2490974729241877, 0.7506248264371008]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2330, 'sum_payoffs': 967.9499997822371, 'action': [1.0, 0]}, {'num_count': 1270, 'sum_payoffs': 489.8474998897756, 'action': [0.0, 0]}])
Weights num count: [0.6470424881977228, 0.35267981116356567]
Actions to choose Agent 1: dict_values([{'num_count': 2327, 'sum_payoffs': 967.1399997824193, 'action': [1.0, 0]}, {'num_count': 1273, 'sum_payoffs': 491.467499889411, 'action': [0.0, 0]}])
Weights num count: [0.6462093862815884, 0.35351291307970006]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5889043807983398 s
