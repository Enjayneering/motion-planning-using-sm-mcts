Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 910, 'sum_payoffs': 293.4224999339822, 'action': [0.0, 0]}, {'num_count': 2690, 'sum_payoffs': 1018.0484997708844, 'action': [1.0, 0]}])
Weights num count: [0.2527075812274368, 0.7470147181338517]
Actions to choose Agent 1: dict_values([{'num_count': 982, 'sum_payoffs': 322.40024992746226, 'action': [0.0, 0]}, {'num_count': 2618, 'sum_payoffs': 990.8932497769977, 'action': [1.0, 0]}])
Weights num count: [0.2727020272146626, 0.7270202721466259]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2339, 'sum_payoffs': 971.7974997813718, 'action': [1.0, 0]}, {'num_count': 1261, 'sum_payoffs': 485.59499989073197, 'action': [0.0, 0]}])
Weights num count: [0.6495417939461261, 0.35018050541516244]
Actions to choose Agent 1: dict_values([{'num_count': 1259, 'sum_payoffs': 485.1899998908231, 'action': [0.0, 0]}, {'num_count': 2341, 'sum_payoffs': 973.4174997810073, 'action': [1.0, 0]}])
Weights num count: [0.3496251041377395, 0.650097195223549]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5588123798370361 s
