Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2536, 'sum_payoffs': 498.05887482569295, 'action': [1.0, 0]}, {'num_count': 1064, 'sum_payoffs': 162.3487499431783, 'action': [0.0, 0]}])
Weights num count: [0.7042488197722855, 0.29547347958900305]
Actions to choose Agent 1: dict_values([{'num_count': 1066, 'sum_payoffs': 163.44787494279382, 'action': [0.0, 0]}, {'num_count': 2534, 'sum_payoffs': 499.14674982531204, 'action': [1.0, 0]}])
Weights num count: [0.296028880866426, 0.7036934184948626]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2320, 'sum_payoffs': 494.69624982683706, 'action': [1.0, 0]}, {'num_count': 1280, 'sum_payoffs': 235.65374991752387, 'action': [0.0, 0]}])
Weights num count: [0.6442654818106082, 0.35545681755068037]
Actions to choose Agent 1: dict_values([{'num_count': 2319, 'sum_payoffs': 494.5949998268725, 'action': [1.0, 0]}, {'num_count': 1281, 'sum_payoffs': 235.95749991741755, 'action': [0.0, 0]}])
Weights num count: [0.6439877811718967, 0.35573451818939184]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5578465461730957 s
