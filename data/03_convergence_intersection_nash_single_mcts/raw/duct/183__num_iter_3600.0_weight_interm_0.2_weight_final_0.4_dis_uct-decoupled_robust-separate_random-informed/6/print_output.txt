Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2586, 'sum_payoffs': 664.7606050859081, 'action': [1.0, 0]}, {'num_count': 1014, 'sum_payoffs': 212.45249994334256, 'action': [0.0, 0]}])
Weights num count: [0.7181338517078589, 0.2815884476534296]
Actions to choose Agent 1: dict_values([{'num_count': 1010, 'sum_payoffs': 211.90949994348725, 'action': [0.0, 0]}, {'num_count': 2590, 'sum_payoffs': 667.2476050852451, 'action': [1.0, 0]}])
Weights num count: [0.28047764509858375, 0.7192446542627048]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1427, 'sum_payoffs': 368.0399999018597, 'action': [0.0, 0]}, {'num_count': 2173, 'sum_payoffs': 604.5899998387948, 'action': [1.0, 0]}])
Weights num count: [0.3962788114412663, 0.6034434879200222]
Actions to choose Agent 1: dict_values([{'num_count': 2169, 'sum_payoffs': 603.6449998390467, 'action': [1.0, 0]}, {'num_count': 1431, 'sum_payoffs': 369.5249999014638, 'action': [0.0, 0]}])
Weights num count: [0.6023326853651764, 0.39738961399611217]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5545976161956787 s
