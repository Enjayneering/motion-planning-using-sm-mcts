Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 966, 'sum_payoffs': 228.1162499486718, 'action': [0.0, 0]}, {'num_count': 2634, 'sum_payoffs': 757.2560623296102, 'action': [1.0, 0]}])
Weights num count: [0.2682588169952791, 0.7314634823660094]
Actions to choose Agent 1: dict_values([{'num_count': 979, 'sum_payoffs': 232.78612494762066, 'action': [0.0, 0]}, {'num_count': 2621, 'sum_payoffs': 755.0465623301068, 'action': [1.0, 0]}])
Weights num count: [0.2718689252985282, 0.7278533740627603]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1294, 'sum_payoffs': 369.73687491680874, 'action': [0.0, 0]}, {'num_count': 2306, 'sum_payoffs': 723.9824998371377, 'action': [1.0, 0]}])
Weights num count: [0.35934462649264093, 0.6403776728686476]
Actions to choose Agent 1: dict_values([{'num_count': 1301, 'sum_payoffs': 372.47062491619363, 'action': [0.0, 0]}, {'num_count': 2299, 'sum_payoffs': 722.1599998375474, 'action': [1.0, 0]}])
Weights num count: [0.36128853096362123, 0.6384337683976673]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5798203945159912 s
