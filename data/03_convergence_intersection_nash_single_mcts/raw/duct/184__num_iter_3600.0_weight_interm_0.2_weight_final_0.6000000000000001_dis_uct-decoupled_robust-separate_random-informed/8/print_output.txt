Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2656, 'sum_payoffs': 764.7437662752923, 'action': [1.0, 0]}, {'num_count': 944, 'sum_payoffs': 221.50687495015882, 'action': [0.0, 0]}])
Weights num count: [0.7375728964176618, 0.26214940294362676]
Actions to choose Agent 1: dict_values([{'num_count': 2640, 'sum_payoffs': 761.3923912760475, 'action': [1.0, 0]}, {'num_count': 960, 'sum_payoffs': 226.77187494897402, 'action': [0.0, 0]}])
Weights num count: [0.7331296861982782, 0.26659261316301025]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2300, 'sum_payoffs': 722.0081248375817, 'action': [1.0, 0]}, {'num_count': 1300, 'sum_payoffs': 371.86312491633043, 'action': [0.0, 0]}])
Weights num count: [0.6387114690363788, 0.36101083032490977]
Actions to choose Agent 1: dict_values([{'num_count': 2295, 'sum_payoffs': 720.641249837889, 'action': [1.0, 0]}, {'num_count': 1305, 'sum_payoffs': 373.8374999158862, 'action': [0.0, 0]}])
Weights num count: [0.6373229658428214, 0.3623993335184671]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5505986213684082 s
