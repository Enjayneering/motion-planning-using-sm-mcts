Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2711, 'sum_payoffs': 830.1102629918926, 'action': [1.0, 0]}, {'num_count': 889, 'sum_payoffs': 220.49753679800278, 'action': [0.0, 0]}])
Weights num count: [0.7528464315467925, 0.24687586781449597]
Actions to choose Agent 1: dict_values([{'num_count': 918, 'sum_payoffs': 230.55773679599054, 'action': [0.0, 0]}, {'num_count': 2682, 'sum_payoffs': 822.3828629934378, 'action': [1.0, 0]}])
Weights num count: [0.2549291863371286, 0.74479311302416]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2323, 'sum_payoffs': 776.8259998446092, 'action': [1.0, 0]}, {'num_count': 1277, 'sum_payoffs': 389.7899999220471, 'action': [0.0, 0]}])
Weights num count: [0.6450985837267426, 0.354623715634546]
Actions to choose Agent 1: dict_values([{'num_count': 2311, 'sum_payoffs': 773.2619998453225, 'action': [1.0, 0]}, {'num_count': 1289, 'sum_payoffs': 394.32599992114007, 'action': [0.0, 0]}])
Weights num count: [0.641766176062205, 0.3579561232990836]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5600333213806152 s
