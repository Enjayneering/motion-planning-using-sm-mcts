Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2635, 'sum_payoffs': 805.3242629968438, 'action': [1.0, 0]}, {'num_count': 965, 'sum_payoffs': 245.35079995092613, 'action': [0.0, 0]}])
Weights num count: [0.7317411830047209, 0.26798111635656763]
Actions to choose Agent 1: dict_values([{'num_count': 980, 'sum_payoffs': 251.0369999497891, 'action': [0.0, 0]}, {'num_count': 2620, 'sum_payoffs': 802.8456629973408, 'action': [1.0, 0]}])
Weights num count: [0.27214662593723965, 0.7275756734240488]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1293, 'sum_payoffs': 395.6219999208807, 'action': [0.0, 0]}, {'num_count': 2307, 'sum_payoffs': 771.1559998457441, 'action': [1.0, 0]}])
Weights num count: [0.35906692585392946, 0.640655373507359]
Actions to choose Agent 1: dict_values([{'num_count': 2305, 'sum_payoffs': 770.8319998458088, 'action': [1.0, 0]}, {'num_count': 1295, 'sum_payoffs': 396.59399992068626, 'action': [0.0, 0]}])
Weights num count: [0.6400999722299361, 0.3596223271313524]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5444416999816895 s
