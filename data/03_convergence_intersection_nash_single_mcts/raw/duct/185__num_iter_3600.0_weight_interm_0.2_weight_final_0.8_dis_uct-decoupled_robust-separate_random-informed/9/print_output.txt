Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2666, 'sum_payoffs': 815.5302629948039, 'action': [1.0, 0]}, {'num_count': 934, 'sum_payoffs': 235.14479995296776, 'action': [0.0, 0]}])
Weights num count: [0.7403499028047764, 0.25937239655651206]
Actions to choose Agent 1: dict_values([{'num_count': 961, 'sum_payoffs': 244.62179995107277, 'action': [0.0, 0]}, {'num_count': 2639, 'sum_payoffs': 808.6776629961765, 'action': [1.0, 0]}])
Weights num count: [0.26687031380172177, 0.7328519855595668]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1827, 'sum_payoffs': 593.4419998813117, 'action': [1.0, 0]}, {'num_count': 1773, 'sum_payoffs': 573.3359998853358, 'action': [0.0, 0]}])
Weights num count: [0.507359066925854, 0.4923632324354346]
Actions to choose Agent 1: dict_values([{'num_count': 1827, 'sum_payoffs': 593.4419998813117, 'action': [1.0, 0]}, {'num_count': 1773, 'sum_payoffs': 573.3359998853358, 'action': [0.0, 0]}])
Weights num count: [0.507359066925854, 0.4923632324354346]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5944855213165283 s
