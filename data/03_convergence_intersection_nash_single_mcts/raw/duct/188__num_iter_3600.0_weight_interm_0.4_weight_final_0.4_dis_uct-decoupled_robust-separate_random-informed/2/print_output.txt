Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2524, 'sum_payoffs': 495.95737488838637, 'action': [1.0, 0]}, {'num_count': 1076, 'sum_payoffs': 165.36037496279414, 'action': [0.0, 0]}])
Weights num count: [0.7009164121077478, 0.2988058872535407]
Actions to choose Agent 1: dict_values([{'num_count': 1083, 'sum_payoffs': 166.9499999624364, 'action': [0.0, 0]}, {'num_count': 2517, 'sum_payoffs': 494.5499998887034, 'action': [1.0, 0]}])
Weights num count: [0.300749791724521, 0.6989725076367675]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2233, 'sum_payoffs': 473.1299998935576, 'action': [1.0, 0]}, {'num_count': 1367, 'sum_payoffs': 257.1187499421436, 'action': [0.0, 0]}])
Weights num count: [0.6201055262427103, 0.37961677311857817]
Actions to choose Agent 1: dict_values([{'num_count': 1370, 'sum_payoffs': 257.9287499419614, 'action': [0.0, 0]}, {'num_count': 2230, 'sum_payoffs': 472.72499989364877, 'action': [1.0, 0]}])
Weights num count: [0.38044987503471256, 0.6192724243265759]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5445008277893066 s
