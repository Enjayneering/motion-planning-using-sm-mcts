Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2597, 'sum_payoffs': 605.9771761945723, 'action': [1.0, 0]}, {'num_count': 1003, 'sum_payoffs': 185.46929996290717, 'action': [0.0, 0]}])
Weights num count: [0.721188558733685, 0.27853374062760344]
Actions to choose Agent 1: dict_values([{'num_count': 2568, 'sum_payoffs': 599.1655261959344, 'action': [1.0, 0]}, {'num_count': 1032, 'sum_payoffs': 193.15574996137056, 'action': [0.0, 0]}])
Weights num count: [0.7131352402110525, 0.286587059150236]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1366, 'sum_payoffs': 312.1289999375681, 'action': [0.0, 0]}, {'num_count': 2234, 'sum_payoffs': 563.7509998872378, 'action': [1.0, 0]}])
Weights num count: [0.3793390724798667, 0.6203832268814218]
Actions to choose Agent 1: dict_values([{'num_count': 2235, 'sum_payoffs': 564.2369998871407, 'action': [1.0, 0]}, {'num_count': 1365, 'sum_payoffs': 311.8859999376167, 'action': [0.0, 0]}])
Weights num count: [0.6206609275201332, 0.37906137184115524]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5457696914672852 s
