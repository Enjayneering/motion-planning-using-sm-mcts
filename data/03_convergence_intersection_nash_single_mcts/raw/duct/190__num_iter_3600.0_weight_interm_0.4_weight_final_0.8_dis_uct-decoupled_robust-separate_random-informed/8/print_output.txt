Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 987, 'sum_payoffs': 204.67649996247593, 'action': [0.0, 0]}, {'num_count': 2613, 'sum_payoffs': 671.9861051399655, 'action': [1.0, 0]}])
Weights num count: [0.27409053040821996, 0.7256317689530686]
Actions to choose Agent 1: dict_values([{'num_count': 992, 'sum_payoffs': 206.95799996205818, 'action': [0.0, 0]}, {'num_count': 2608, 'sum_payoffs': 672.620605139849, 'action': [1.0, 0]}])
Weights num count: [0.2754790336017773, 0.7242432657595113]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2346, 'sum_payoffs': 658.8599998792464, 'action': [1.0, 0]}, {'num_count': 1254, 'sum_payoffs': 313.63499994250725, 'action': [0.0, 0]}])
Weights num count: [0.6514856984171064, 0.3482366009441822]
Actions to choose Agent 1: dict_values([{'num_count': 1257, 'sum_payoffs': 314.84999994228457, 'action': [0.0, 0]}, {'num_count': 2343, 'sum_payoffs': 658.4549998793206, 'action': [1.0, 0]}])
Weights num count: [0.3490697028603166, 0.650652596500972]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5520064830780029 s
