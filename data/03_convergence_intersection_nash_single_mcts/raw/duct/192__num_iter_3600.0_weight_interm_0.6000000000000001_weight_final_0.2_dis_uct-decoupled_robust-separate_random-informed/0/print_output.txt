Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2280, 'sum_payoffs': 235.46779928911562, 'action': [1.0, 0]}, {'num_count': 1320, 'sum_payoffs': 101.13187497724526, 'action': [0.0, 0]}])
Weights num count: [0.6331574562621494, 0.3665648430991391]
Actions to choose Agent 1: dict_values([{'num_count': 1308, 'sum_payoffs': 99.77343747755094, 'action': [0.0, 0]}, {'num_count': 2292, 'sum_payoffs': 237.55523678864571, 'action': [1.0, 0]}])
Weights num count: [0.3632324354346015, 0.636489863926687]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1546, 'sum_payoffs': 145.36124996729217, 'action': [0.0, 0]}, {'num_count': 2054, 'sum_payoffs': 221.1131249502539, 'action': [1.0, 0]}])
Weights num count: [0.42932518744793113, 0.5703971119133574]
Actions to choose Agent 1: dict_values([{'num_count': 2051, 'sum_payoffs': 220.75874995033362, 'action': [1.0, 0]}, {'num_count': 1549, 'sum_payoffs': 145.81687496718968, 'action': [0.0, 0]}])
Weights num count: [0.569564009997223, 0.4301582893640655]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5659275054931641 s
