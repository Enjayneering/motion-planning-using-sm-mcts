Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2316, 'sum_payoffs': 239.18029928828173, 'action': [1.0, 0]}, {'num_count': 1284, 'sum_payoffs': 95.5597499784993, 'action': [0.0, 0]}])
Weights num count: [0.6431546792557623, 0.35656762010552623]
Actions to choose Agent 1: dict_values([{'num_count': 2298, 'sum_payoffs': 236.45498678889507, 'action': [1.0, 0]}, {'num_count': 1302, 'sum_payoffs': 97.82943747798859, 'action': [0.0, 0]}])
Weights num count: [0.6381560677589558, 0.3615662316023327]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1974, 'sum_payoffs': 209.21624995293007, 'action': [1.0, 0]}, {'num_count': 1626, 'sum_payoffs': 157.2581249646173, 'action': [0.0, 0]}])
Weights num count: [0.5481810608164399, 0.4515412385448487]
Actions to choose Agent 1: dict_values([{'num_count': 1627, 'sum_payoffs': 157.46062496457176, 'action': [0.0, 0]}, {'num_count': 1973, 'sum_payoffs': 209.11499995295284, 'action': [1.0, 0]}])
Weights num count: [0.45181893918356014, 0.5479033601777284]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5517473220825195 s
