Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2270, 'sum_payoffs': 234.0772992894287, 'action': [1.0, 0]}, {'num_count': 1330, 'sum_payoffs': 102.45318747694787, 'action': [0.0, 0]}])
Weights num count: [0.6303804498750347, 0.3693418494862538]
Actions to choose Agent 1: dict_values([{'num_count': 1322, 'sum_payoffs': 101.84568747708404, 'action': [0.0, 0]}, {'num_count': 2278, 'sum_payoffs': 235.86942428902438, 'action': [1.0, 0]}])
Weights num count: [0.36712024437656204, 0.6326020549847264]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2091, 'sum_payoffs': 226.58062494902353, 'action': [1.0, 0]}, {'num_count': 1509, 'sum_payoffs': 139.8431249685336, 'action': [0.0, 0]}])
Weights num count: [0.5806720355456818, 0.4190502638156068]
Actions to choose Agent 1: dict_values([{'num_count': 1511, 'sum_payoffs': 140.19749996845388, 'action': [0.0, 0]}, {'num_count': 2089, 'sum_payoffs': 226.4287499490577, 'action': [1.0, 0]}])
Weights num count: [0.4196056650930297, 0.5801166342682588]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5515186786651611 s
