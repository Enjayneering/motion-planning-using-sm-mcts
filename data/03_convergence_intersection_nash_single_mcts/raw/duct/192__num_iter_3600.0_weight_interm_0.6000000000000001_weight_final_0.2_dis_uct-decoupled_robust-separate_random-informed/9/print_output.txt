Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2255, 'sum_payoffs': 231.3671742900386, 'action': [1.0, 0]}, {'num_count': 1345, 'sum_payoffs': 104.22337497655057, 'action': [0.0, 0]}])
Weights num count: [0.6262149402943626, 0.37350735906692584]
Actions to choose Agent 1: dict_values([{'num_count': 1343, 'sum_payoffs': 104.01918747659589, 'action': [0.0, 0]}, {'num_count': 2257, 'sum_payoffs': 231.75361178995075, 'action': [1.0, 0]}])
Weights num count: [0.3729519577895029, 0.6267703415717856]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1499, 'sum_payoffs': 138.37499996886456, 'action': [0.0, 0]}, {'num_count': 2101, 'sum_payoffs': 228.04874994869343, 'action': [1.0, 0]}])
Weights num count: [0.4162732574284921, 0.5834490419327965]
Actions to choose Agent 1: dict_values([{'num_count': 2098, 'sum_payoffs': 227.74499994876174, 'action': [1.0, 0]}, {'num_count': 1502, 'sum_payoffs': 138.88124996875067, 'action': [0.0, 0]}])
Weights num count: [0.5826159400166621, 0.4171063593446265]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5699570178985596 s
