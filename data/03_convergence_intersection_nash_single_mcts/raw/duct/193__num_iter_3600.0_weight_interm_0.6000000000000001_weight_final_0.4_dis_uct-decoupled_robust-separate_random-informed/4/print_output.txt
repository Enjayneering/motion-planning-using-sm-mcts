Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2478, 'sum_payoffs': 396.71568939434695, 'action': [1.0, 0]}, {'num_count': 1122, 'sum_payoffs': 135.25919997294704, 'action': [0.0, 0]}])
Weights num count: [0.6881421827270203, 0.31158011663426827]
Actions to choose Agent 1: dict_values([{'num_count': 1136, 'sum_payoffs': 137.986199972402, 'action': [0.0, 0]}, {'num_count': 2464, 'sum_payoffs': 394.28028939483397, 'action': [1.0, 0]}])
Weights num count: [0.3154679255762288, 0.6842543737850597]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2257, 'sum_payoffs': 387.85499992241597, 'action': [1.0, 0]}, {'num_count': 1343, 'sum_payoffs': 196.8029999606421, 'action': [0.0, 0]}])
Weights num count: [0.6267703415717856, 0.3729519577895029]
Actions to choose Agent 1: dict_values([{'num_count': 1346, 'sum_payoffs': 197.5319999604963, 'action': [0.0, 0]}, {'num_count': 2254, 'sum_payoffs': 387.449999922497, 'action': [1.0, 0]}])
Weights num count: [0.3737850597056373, 0.6259372396556512]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5500853061676025 s
