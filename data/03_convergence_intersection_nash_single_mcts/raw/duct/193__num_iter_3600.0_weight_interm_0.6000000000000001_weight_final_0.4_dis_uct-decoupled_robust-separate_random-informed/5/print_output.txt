Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2463, 'sum_payoffs': 391.8259893953262, 'action': [1.0, 0]}, {'num_count': 1137, 'sum_payoffs': 137.18429997256243, 'action': [0.0, 0]}])
Weights num count: [0.6839766731463482, 0.3157456262149403]
Actions to choose Agent 1: dict_values([{'num_count': 1125, 'sum_payoffs': 135.57509997288383, 'action': [0.0, 0]}, {'num_count': 2475, 'sum_payoffs': 395.76798939453715, 'action': [1.0, 0]}])
Weights num count: [0.31241321855040266, 0.6873090808108858]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1362, 'sum_payoffs': 200.77199995984793, 'action': [0.0, 0]}, {'num_count': 2238, 'sum_payoffs': 383.8859999232107, 'action': [1.0, 0]}])
Weights num count: [0.37822826992502084, 0.6214940294362677]
Actions to choose Agent 1: dict_values([{'num_count': 2234, 'sum_payoffs': 383.2379999233404, 'action': [1.0, 0]}, {'num_count': 1366, 'sum_payoffs': 201.74399995965354, 'action': [0.0, 0]}])
Weights num count: [0.6203832268814218, 0.3793390724798667]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5577876567840576 s
