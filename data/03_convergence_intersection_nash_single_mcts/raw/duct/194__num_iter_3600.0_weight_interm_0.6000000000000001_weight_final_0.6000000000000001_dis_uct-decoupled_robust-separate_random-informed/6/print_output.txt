Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1092, 'sum_payoffs': 168.68137496907428, 'action': [0.0, 0]}, {'num_count': 2508, 'sum_payoffs': 492.0294078045382, 'action': [1.0, 0]}])
Weights num count: [0.30324909747292417, 0.6964732018883644]
Actions to choose Agent 1: dict_values([{'num_count': 2500, 'sum_payoffs': 491.6142828046132, 'action': [1.0, 0]}, {'num_count': 1100, 'sum_payoffs': 171.10124996863067, 'action': [0.0, 0]}])
Weights num count: [0.6942515967786725, 0.30547070258261594]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2214, 'sum_payoffs': 468.4724999141042, 'action': [1.0, 0]}, {'num_count': 1386, 'sum_payoffs': 261.6749999520229, 'action': [0.0, 0]}])
Weights num count: [0.6148292141071925, 0.3848930852540961]
Actions to choose Agent 1: dict_values([{'num_count': 1387, 'sum_payoffs': 262.0799999519487, 'action': [0.0, 0]}, {'num_count': 2213, 'sum_payoffs': 468.4724999141042, 'action': [1.0, 0]}])
Weights num count: [0.38517078589280757, 0.614551513468481]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5744121074676514 s
