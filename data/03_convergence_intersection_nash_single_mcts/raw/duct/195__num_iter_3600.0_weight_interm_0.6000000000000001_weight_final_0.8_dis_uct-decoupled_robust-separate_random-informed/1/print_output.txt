Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1042, 'sum_payoffs': 184.47557139694464, 'action': [0.0, 0]}, {'num_count': 2558, 'sum_payoffs': 569.3967066692844, 'action': [1.0, 0]}])
Weights num count: [0.2893640655373507, 0.7103582338239378]
Actions to choose Agent 1: dict_values([{'num_count': 1043, 'sum_payoffs': 185.20457139682, 'action': [0.0, 0]}, {'num_count': 2557, 'sum_payoffs': 570.1257066691596, 'action': [1.0, 0]}])
Weights num count: [0.2896417661760622, 0.7100805331852263]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2231, 'sum_payoffs': 536.9914284793756, 'action': [1.0, 0]}, {'num_count': 1369, 'sum_payoffs': 297.07714280621923, 'action': [0.0, 0]}])
Weights num count: [0.6195501249652874, 0.3801721743960011]
Actions to choose Agent 1: dict_values([{'num_count': 2227, 'sum_payoffs': 536.1814284795147, 'action': [1.0, 0]}, {'num_count': 1373, 'sum_payoffs': 298.34999994885817, 'action': [0.0, 0]}])
Weights num count: [0.6184393224104415, 0.381282976950847]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5616304874420166 s
