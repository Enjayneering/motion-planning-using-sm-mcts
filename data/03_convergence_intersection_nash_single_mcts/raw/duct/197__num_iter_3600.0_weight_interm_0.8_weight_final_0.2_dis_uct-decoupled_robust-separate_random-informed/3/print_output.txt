Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2185, 'sum_payoffs': 182.65180259504967, 'action': [1.0, 0]}, {'num_count': 1415, 'sum_payoffs': 88.55144998229011, 'action': [0.0, 0]}])
Weights num count: [0.6067758955845598, 0.3929464037767287]
Actions to choose Agent 1: dict_values([{'num_count': 1414, 'sum_payoffs': 88.4794499823045, 'action': [0.0, 0]}, {'num_count': 2186, 'sum_payoffs': 182.94250259499162, 'action': [1.0, 0]}])
Weights num count: [0.3926687031380172, 0.6070535962232713]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1460, 'sum_payoffs': 103.14449997937227, 'action': [0.0, 0]}, {'num_count': 2140, 'sum_payoffs': 190.61549996187082, 'action': [1.0, 0]}])
Weights num count: [0.4054429325187448, 0.5942793668425438]
Actions to choose Agent 1: dict_values([{'num_count': 2141, 'sum_payoffs': 190.77749996183843, 'action': [1.0, 0]}, {'num_count': 1459, 'sum_payoffs': 103.06349997938847, 'action': [0.0, 0]}])
Weights num count: [0.5945570674812553, 0.40516523188003334]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5556964874267578 s
