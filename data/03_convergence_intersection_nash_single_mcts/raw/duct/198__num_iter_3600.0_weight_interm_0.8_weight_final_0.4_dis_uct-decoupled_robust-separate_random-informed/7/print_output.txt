Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2431, 'sum_payoffs': 329.3189604659411, 'action': [1.0, 0]}, {'num_count': 1169, 'sum_payoffs': 115.92374997874717, 'action': [0.0, 0]}])
Weights num count: [0.6750902527075813, 0.3246320466537073]
Actions to choose Agent 1: dict_values([{'num_count': 1177, 'sum_payoffs': 117.3209999784911, 'action': [0.0, 0]}, {'num_count': 2423, 'sum_payoffs': 328.28621046613046, 'action': [1.0, 0]}])
Weights num count: [0.32685365176339903, 0.6728686475978894]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2280, 'sum_payoffs': 331.00499993933255, 'action': [1.0, 0]}, {'num_count': 1320, 'sum_payoffs': 156.4574999713198, 'action': [0.0, 0]}])
Weights num count: [0.6331574562621494, 0.3665648430991391]
Actions to choose Agent 1: dict_values([{'num_count': 1321, 'sum_payoffs': 156.79499997125794, 'action': [0.0, 0]}, {'num_count': 2279, 'sum_payoffs': 331.07249993932015, 'action': [1.0, 0]}])
Weights num count: [0.3668425437378506, 0.6328797556234379]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5526943206787109 s
