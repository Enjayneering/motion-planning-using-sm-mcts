Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1187, 'sum_payoffs': 118.48949997827687, 'action': [0.0, 0]}, {'num_count': 2413, 'sum_payoffs': 325.7347104665972, 'action': [1.0, 0]}])
Weights num count: [0.32963065815051373, 0.6700916412107748]
Actions to choose Agent 1: dict_values([{'num_count': 2412, 'sum_payoffs': 325.4384604666516, 'action': [1.0, 0]}, {'num_count': 1188, 'sum_payoffs': 118.66424997824454, 'action': [0.0, 0]}])
Weights num count: [0.6698139405720633, 0.3299083587892252]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2234, 'sum_payoffs': 322.83749994082945, 'action': [1.0, 0]}, {'num_count': 1366, 'sum_payoffs': 164.75999996979806, 'action': [0.0, 0]}])
Weights num count: [0.6203832268814218, 0.3793390724798667]
Actions to choose Agent 1: dict_values([{'num_count': 1366, 'sum_payoffs': 164.89499996977332, 'action': [0.0, 0]}, {'num_count': 2234, 'sum_payoffs': 322.9724999408047, 'action': [1.0, 0]}])
Weights num count: [0.3793390724798667, 0.6203832268814218]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5525531768798828 s
