Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2526, 'sum_payoffs': 495.7655328141915, 'action': [1.0, 0]}, {'num_count': 1074, 'sum_payoffs': 164.68146707850318, 'action': [0.0, 0]}])
Weights num count: [0.7014718133851708, 0.29825048597611775]
Actions to choose Agent 1: dict_values([{'num_count': 1069, 'sum_payoffs': 164.3675920785544, 'action': [0.0, 0]}, {'num_count': 2531, 'sum_payoffs': 498.8131578136962, 'action': [1.0, 0]}])
Weights num count: [0.2968619827825604, 0.7028603165787282]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1331, 'sum_payoffs': 248.107499959684, 'action': [0.0, 0]}, {'num_count': 2269, 'sum_payoffs': 481.93874992166866, 'action': [1.0, 0]}])
Weights num count: [0.3696195501249653, 0.6301027492363233]
Actions to choose Agent 1: dict_values([{'num_count': 1335, 'sum_payoffs': 249.32249995948658, 'action': [0.0, 0]}, {'num_count': 2265, 'sum_payoffs': 481.33124992176744, 'action': [1.0, 0]}])
Weights num count: [0.37073035267981114, 0.6289919466814774]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5349264144897461 s
