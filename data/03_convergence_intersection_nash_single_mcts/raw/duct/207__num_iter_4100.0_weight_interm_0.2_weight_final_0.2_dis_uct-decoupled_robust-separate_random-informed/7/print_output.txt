Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2901, 'sum_payoffs': 567.7092826960411, 'action': [1.0, 0]}, {'num_count': 1199, 'sum_payoffs': 184.0967170408291, 'action': [0.0, 0]}])
Weights num count: [0.7073884418434528, 0.29236771519141674]
Actions to choose Agent 1: dict_values([{'num_count': 1214, 'sum_payoffs': 187.8328420395219, 'action': [0.0, 0]}, {'num_count': 2886, 'sum_payoffs': 565.2489076969044, 'action': [1.0, 0]}])
Weights num count: [0.29602535966837357, 0.703730797366496]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1555, 'sum_payoffs': 293.56874989725185, 'action': [0.0, 0]}, {'num_count': 2545, 'sum_payoffs': 537.9299998117073, 'action': [1.0, 0]}])
Weights num count: [0.37917581077785906, 0.6205803462570105]
Actions to choose Agent 1: dict_values([{'num_count': 2542, 'sum_payoffs': 537.3224998119198, 'action': [1.0, 0]}, {'num_count': 1558, 'sum_payoffs': 294.3787498969684, 'action': [0.0, 0]}])
Weights num count: [0.6198488173616191, 0.3799073396732504]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7021031379699707 s
