Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1052, 'sum_payoffs': 215.5544999425151, 'action': [0.0, 0]}, {'num_count': 3048, 'sum_payoffs': 783.2501050543285, 'action': [1.0, 0]}])
Weights num count: [0.2565227993172397, 0.7432333577176299]
Actions to choose Agent 1: dict_values([{'num_count': 3032, 'sum_payoffs': 779.4836050553321, 'action': [1.0, 0]}, {'num_count': 1068, 'sum_payoffs': 220.29299994125122, 'action': [0.0, 0]}])
Weights num count: [0.7393318702755426, 0.260424286759327]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1723, 'sum_payoffs': 450.92999987976134, 'action': [0.0, 0]}, {'num_count': 2377, 'sum_payoffs': 656.8349998248667, 'action': [1.0, 0]}])
Weights num count: [0.4201414289197757, 0.5796147281150938]
Actions to choose Agent 1: dict_values([{'num_count': 1724, 'sum_payoffs': 451.4699998796174, 'action': [0.0, 0]}, {'num_count': 2376, 'sum_payoffs': 656.8349998248667, 'action': [1.0, 0]}])
Weights num count: [0.4203852718849061, 0.5793708851499634]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6744129657745361 s
