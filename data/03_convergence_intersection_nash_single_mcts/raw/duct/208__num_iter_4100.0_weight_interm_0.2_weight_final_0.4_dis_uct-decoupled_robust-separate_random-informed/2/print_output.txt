Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3013, 'sum_payoffs': 773.3171050569767, 'action': [1.0, 0]}, {'num_count': 1087, 'sum_payoffs': 225.26699993992472, 'action': [0.0, 0]}])
Weights num count: [0.7346988539380639, 0.26505730309680564]
Actions to choose Agent 1: dict_values([{'num_count': 1076, 'sum_payoffs': 222.93149994054767, 'action': [0.0, 0]}, {'num_count': 3024, 'sum_payoffs': 778.3256050556402, 'action': [1.0, 0]}])
Weights num count: [0.26237503048037064, 0.737381126554499]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2652, 'sum_payoffs': 742.5599998020102, 'action': [1.0, 0]}, {'num_count': 1448, 'sum_payoffs': 364.93499990268816, 'action': [0.0, 0]}])
Weights num count: [0.6466715435259692, 0.35308461350890025]
Actions to choose Agent 1: dict_values([{'num_count': 2642, 'sum_payoffs': 739.994999802694, 'action': [1.0, 0]}, {'num_count': 1458, 'sum_payoffs': 368.3099999017884, 'action': [0.0, 0]}])
Weights num count: [0.6442331138746648, 0.35552304316020483]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6046814918518066 s
