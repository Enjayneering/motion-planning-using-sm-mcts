Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1119, 'sum_payoffs': 234.1364999375596, 'action': [0.0, 0]}, {'num_count': 2981, 'sum_payoffs': 764.5391050593145, 'action': [1.0, 0]}])
Weights num count: [0.2728602779809802, 0.7268958790538893]
Actions to choose Agent 1: dict_values([{'num_count': 2973, 'sum_payoffs': 764.3531050593638, 'action': [1.0, 0]}, {'num_count': 1127, 'sum_payoffs': 236.99549993679707, 'action': [0.0, 0]}])
Weights num count: [0.7249451353328457, 0.27481102170202387]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1501, 'sum_payoffs': 381.40499989829726, 'action': [0.0, 0]}, {'num_count': 2599, 'sum_payoffs': 726.0899998064011, 'action': [1.0, 0]}])
Weights num count: [0.3660082906608144, 0.6337478663740551]
Actions to choose Agent 1: dict_values([{'num_count': 1506, 'sum_payoffs': 383.1599998978294, 'action': [0.0, 0]}, {'num_count': 2594, 'sum_payoffs': 724.874999806725, 'action': [1.0, 0]}])
Weights num count: [0.3672275054864667, 0.6325286515484029]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6424996852874756 s
