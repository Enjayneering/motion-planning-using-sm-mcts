Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3010, 'sum_payoffs': 863.4096412530945, 'action': [1.0, 0]}, {'num_count': 1090, 'sum_payoffs': 258.96431244173004, 'action': [0.0, 0]}])
Weights num count: [0.7339673250426725, 0.26578883199219705]
Actions to choose Agent 1: dict_values([{'num_count': 1107, 'sum_payoffs': 264.68268744044326, 'action': [0.0, 0]}, {'num_count': 2993, 'sum_payoffs': 859.8782662538897, 'action': [1.0, 0]}])
Weights num count: [0.26993416239941476, 0.7298219946354547]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1446, 'sum_payoffs': 413.02124990707, 'action': [0.0, 0]}, {'num_count': 2654, 'sum_payoffs': 832.5731248127237, 'action': [1.0, 0]}])
Weights num count: [0.35259692757863936, 0.6471592294562302]
Actions to choose Agent 1: dict_values([{'num_count': 1450, 'sum_payoffs': 414.69187490669407, 'action': [0.0, 0]}, {'num_count': 2650, 'sum_payoffs': 831.8137498128943, 'action': [1.0, 0]}])
Weights num count: [0.3535722994391612, 0.6461838575957084]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6498372554779053 s
