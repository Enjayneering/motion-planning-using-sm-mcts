Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1088, 'sum_payoffs': 278.06039994438515, 'action': [0.0, 0]}, {'num_count': 3012, 'sum_payoffs': 918.6108629741886, 'action': [1.0, 0]}])
Weights num count: [0.2653011460619361, 0.7344550109729334]
Actions to choose Agent 1: dict_values([{'num_count': 1095, 'sum_payoffs': 281.12219994377324, 'action': [0.0, 0]}, {'num_count': 3005, 'sum_payoffs': 918.4650629742187, 'action': [1.0, 0]}])
Weights num count: [0.2670080468178493, 0.7327481102170202]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1424, 'sum_payoffs': 434.0159999132047, 'action': [0.0, 0]}, {'num_count': 2676, 'sum_payoffs': 894.1139998211358, 'action': [1.0, 0]}])
Weights num count: [0.3472323823457693, 0.6525237746891002]
Actions to choose Agent 1: dict_values([{'num_count': 1423, 'sum_payoffs': 434.17799991317236, 'action': [0.0, 0]}, {'num_count': 2677, 'sum_payoffs': 895.247999820909, 'action': [1.0, 0]}])
Weights num count: [0.3469885393806389, 0.6527676176542306]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6606695652008057 s
