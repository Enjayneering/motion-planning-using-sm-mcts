Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1243, 'sum_payoffs': 193.47749995646842, 'action': [0.0, 0]}, {'num_count': 2857, 'sum_payoffs': 557.2749077693328, 'action': [1.0, 0]}])
Weights num count: [0.3030968056571568, 0.6966593513777127]
Actions to choose Agent 1: dict_values([{'num_count': 1238, 'sum_payoffs': 192.98137495658017, 'action': [0.0, 0]}, {'num_count': 2862, 'sum_payoffs': 559.7757827687705, 'action': [1.0, 0]}])
Weights num count: [0.3018775908315045, 0.6978785662033651]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2485, 'sum_payoffs': 523.2487498822819, 'action': [1.0, 0]}, {'num_count': 1615, 'sum_payoffs': 308.24999993064216, 'action': [0.0, 0]}])
Weights num count: [0.6059497683491831, 0.3938063886856864]
Actions to choose Agent 1: dict_values([{'num_count': 2483, 'sum_payoffs': 522.843749882373, 'action': [1.0, 0]}, {'num_count': 1617, 'sum_payoffs': 308.8574999305055, 'action': [0.0, 0]}])
Weights num count: [0.6054620824189222, 0.39429407461594734]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6062748432159424 s
