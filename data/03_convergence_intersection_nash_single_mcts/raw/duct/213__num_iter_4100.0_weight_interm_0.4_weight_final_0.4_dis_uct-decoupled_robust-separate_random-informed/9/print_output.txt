Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2881, 'sum_payoffs': 563.8764077678462, 'action': [1.0, 0]}, {'num_count': 1219, 'sum_payoffs': 188.81999995751548, 'action': [0.0, 0]}])
Weights num count: [0.7025115825408437, 0.29724457449402586]
Actions to choose Agent 1: dict_values([{'num_count': 1228, 'sum_payoffs': 191.0981249570026, 'action': [0.0, 0]}, {'num_count': 2872, 'sum_payoffs': 562.6917827681133, 'action': [1.0, 0]}])
Weights num count: [0.2994391611802, 0.7003169958546696]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1494, 'sum_payoffs': 278.5837499373165, 'action': [0.0, 0]}, {'num_count': 2606, 'sum_payoffs': 552.7124998756474, 'action': [1.0, 0]}])
Weights num count: [0.36430138990490124, 0.6354547671299683]
Actions to choose Agent 1: dict_values([{'num_count': 1499, 'sum_payoffs': 280.0012499369976, 'action': [0.0, 0]}, {'num_count': 2601, 'sum_payoffs': 551.9024998758298, 'action': [1.0, 0]}])
Weights num count: [0.36552060473055353, 0.634235552304316]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6324663162231445 s
