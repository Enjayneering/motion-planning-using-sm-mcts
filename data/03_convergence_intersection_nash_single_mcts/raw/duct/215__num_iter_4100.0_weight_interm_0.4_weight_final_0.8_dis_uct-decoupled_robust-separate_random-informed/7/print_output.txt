Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3007, 'sum_payoffs': 772.2506051215918, 'action': [1.0, 0]}, {'num_count': 1093, 'sum_payoffs': 227.15399995835494, 'action': [0.0, 0]}])
Weights num count: [0.7332357961472812, 0.2665203608875884]
Actions to choose Agent 1: dict_values([{'num_count': 2983, 'sum_payoffs': 766.3616051226704, 'action': [1.0, 0]}, {'num_count': 1117, 'sum_payoffs': 234.0149999570971, 'action': [0.0, 0]}])
Weights num count: [0.7273835649841502, 0.27237259205071934]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2620, 'sum_payoffs': 732.704999865715, 'action': [1.0, 0]}, {'num_count': 1480, 'sum_payoffs': 374.92499993127547, 'action': [0.0, 0]}])
Weights num count: [0.6388685686417946, 0.3608875883930749]
Actions to choose Agent 1: dict_values([{'num_count': 2614, 'sum_payoffs': 731.3549998659623, 'action': [1.0, 0]}, {'num_count': 1486, 'sum_payoffs': 377.0849999308797, 'action': [0.0, 0]}])
Weights num count: [0.6374055108510119, 0.3623506461838576]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6400458812713623 s
