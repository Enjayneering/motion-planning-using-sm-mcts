Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2594, 'sum_payoffs': 265.17454928243177, 'action': [1.0, 0]}, {'num_count': 1506, 'sum_payoffs': 116.24343747384594, 'action': [0.0, 0]}])
Weights num count: [0.6325286515484029, 0.3672275054864667]
Actions to choose Agent 1: dict_values([{'num_count': 1491, 'sum_payoffs': 114.70274997419234, 'action': [0.0, 0]}, {'num_count': 2609, 'sum_payoffs': 267.89986178181886, 'action': [1.0, 0]}])
Weights num count: [0.3635698610095099, 0.6361862960253597]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2431, 'sum_payoffs': 264.1949999405612, 'action': [1.0, 0]}, {'num_count': 1669, 'sum_payoffs': 152.80312496561803, 'action': [0.0, 0]}])
Weights num count: [0.5927822482321385, 0.40697390880273104]
Actions to choose Agent 1: dict_values([{'num_count': 1670, 'sum_payoffs': 153.05624996556108, 'action': [0.0, 0]}, {'num_count': 2430, 'sum_payoffs': 264.1443749405726, 'action': [1.0, 0]}])
Weights num count: [0.4072177517678615, 0.5925384052670081]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6352934837341309 s
