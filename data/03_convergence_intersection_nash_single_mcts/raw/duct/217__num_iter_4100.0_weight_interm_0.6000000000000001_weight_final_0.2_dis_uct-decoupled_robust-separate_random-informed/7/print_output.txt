Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2518, 'sum_payoffs': 254.58886178481066, 'action': [1.0, 0]}, {'num_count': 1582, 'sum_payoffs': 126.19124997160783, 'action': [0.0, 0]}])
Weights num count: [0.6139965861984882, 0.38575957083638135]
Actions to choose Agent 1: dict_values([{'num_count': 1554, 'sum_payoffs': 123.31406247225517, 'action': [0.0, 0]}, {'num_count': 2546, 'sum_payoffs': 259.7441742836508, 'action': [1.0, 0]}])
Weights num count: [0.3789319678127286, 0.6208241892221409]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2364, 'sum_payoffs': 254.52562494273894, 'action': [1.0, 0]}, {'num_count': 1736, 'sum_payoffs': 162.6243749634094, 'action': [0.0, 0]}])
Weights num count: [0.576444769568398, 0.42331138746647157]
Actions to choose Agent 1: dict_values([{'num_count': 2361, 'sum_payoffs': 254.12062494283003, 'action': [1.0, 0]}, {'num_count': 1739, 'sum_payoffs': 163.0293749633183, 'action': [0.0, 0]}])
Weights num count: [0.5757132406730066, 0.424042916361863]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6338851451873779 s
