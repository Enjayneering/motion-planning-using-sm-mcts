Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1187, 'sum_payoffs': 181.49512496672506, 'action': [0.0, 0]}, {'num_count': 2913, 'sum_payoffs': 570.1640327902255, 'action': [1.0, 0]}])
Weights num count: [0.2894415996098513, 0.7103145574250183]
Actions to choose Agent 1: dict_values([{'num_count': 1191, 'sum_payoffs': 182.86199996647446, 'action': [0.0, 0]}, {'num_count': 2909, 'sum_payoffs': 570.4374077901746, 'action': [1.0, 0]}])
Weights num count: [0.2904169714703731, 0.7093391855644965]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2509, 'sum_payoffs': 529.1212499029888, 'action': [1.0, 0]}, {'num_count': 1591, 'sum_payoffs': 302.4787499445424, 'action': [0.0, 0]}])
Weights num count: [0.6118019995123141, 0.38795415752255547]
Actions to choose Agent 1: dict_values([{'num_count': 1597, 'sum_payoffs': 303.89624994428254, 'action': [0.0, 0]}, {'num_count': 2503, 'sum_payoffs': 527.9062499032113, 'action': [1.0, 0]}])
Weights num count: [0.3894172153133382, 0.6103389417215314]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6464920043945312 s
