Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2937, 'sum_payoffs': 651.8778495122822, 'action': [1.0, 0]}, {'num_count': 1163, 'sum_payoffs': 206.55385710744454, 'action': [0.0, 0]}])
Weights num count: [0.7161667885881492, 0.28358936844672034]
Actions to choose Agent 1: dict_values([{'num_count': 1167, 'sum_payoffs': 207.69942853581986, 'action': [0.0, 0]}, {'num_count': 2933, 'sum_payoffs': 651.5654209409068, 'action': [1.0, 0]}])
Weights num count: [0.28456474030724216, 0.7151914167276274]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1504, 'sum_payoffs': 324.5014285158037, 'action': [0.0, 0]}, {'num_count': 2596, 'sum_payoffs': 625.3971427499164, 'action': [1.0, 0]}])
Weights num count: [0.3667398195562058, 0.6330163374786637]
Actions to choose Agent 1: dict_values([{'num_count': 1507, 'sum_payoffs': 325.4271428013593, 'action': [0.0, 0]}, {'num_count': 2593, 'sum_payoffs': 624.9342856071387, 'action': [1.0, 0]}])
Weights num count: [0.3674713484515972, 0.6322848085832724]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6231462955474854 s
