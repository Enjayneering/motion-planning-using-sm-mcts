Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1221, 'sum_payoffs': 220.64785710502758, 'action': [0.0, 0]}, {'num_count': 2879, 'sum_payoffs': 637.6102780861573, 'action': [1.0, 0]}])
Weights num count: [0.29773226042428674, 0.7020238966105827]
Actions to choose Agent 1: dict_values([{'num_count': 2873, 'sum_payoffs': 637.2978495147825, 'action': [1.0, 0]}, {'num_count': 1227, 'sum_payoffs': 222.62657139040334, 'action': [0.0, 0]}])
Weights num count: [0.7005608388198, 0.2991953182150695]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1448, 'sum_payoffs': 309.2271428041365, 'action': [0.0, 0]}, {'num_count': 2652, 'sum_payoffs': 640.5557141758856, 'action': [1.0, 0]}])
Weights num count: [0.35308461350890025, 0.6466715435259692]
Actions to choose Agent 1: dict_values([{'num_count': 1453, 'sum_payoffs': 310.7314285181644, 'action': [0.0, 0]}, {'num_count': 2647, 'sum_payoffs': 639.5142856046357, 'action': [1.0, 0]}])
Weights num count: [0.35430382833455254, 0.645452328700317]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.602461576461792 s
