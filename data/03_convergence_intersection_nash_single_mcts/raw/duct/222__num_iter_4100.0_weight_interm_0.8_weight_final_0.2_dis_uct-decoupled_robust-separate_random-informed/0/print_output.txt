Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1531, 'sum_payoffs': 91.95929998160881, 'action': [0.0, 0]}, {'num_count': 2569, 'sum_payoffs': 215.37490258850497, 'action': [1.0, 0]}])
Weights num count: [0.3733235796147281, 0.6264325774201415]
Actions to choose Agent 1: dict_values([{'num_count': 1547, 'sum_payoffs': 93.66344998126806, 'action': [0.0, 0]}, {'num_count': 2553, 'sum_payoffs': 213.16045258894792, 'action': [1.0, 0]}])
Weights num count: [0.3772250670568154, 0.6225310899780542]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2018, 'sum_payoffs': 163.04399996738925, 'action': [0.0, 0]}, {'num_count': 2082, 'sum_payoffs': 171.1349999657699, 'action': [1.0, 0]}])
Weights num count: [0.4920751036332602, 0.5076810534016094]
Actions to choose Agent 1: dict_values([{'num_count': 2018, 'sum_payoffs': 163.08449996738116, 'action': [0.0, 0]}, {'num_count': 2082, 'sum_payoffs': 171.17549996576182, 'action': [1.0, 0]}])
Weights num count: [0.4920751036332602, 0.5076810534016094]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6394734382629395 s
