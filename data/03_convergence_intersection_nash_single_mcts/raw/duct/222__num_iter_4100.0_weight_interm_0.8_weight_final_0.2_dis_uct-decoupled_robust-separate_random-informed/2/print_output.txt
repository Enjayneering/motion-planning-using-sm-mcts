Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2568, 'sum_payoffs': 215.8852025884029, 'action': [1.0, 0]}, {'num_count': 1532, 'sum_payoffs': 92.4524999815103, 'action': [0.0, 0]}])
Weights num count: [0.626188734455011, 0.3735674225798586]
Actions to choose Agent 1: dict_values([{'num_count': 1533, 'sum_payoffs': 92.67119998146653, 'action': [0.0, 0]}, {'num_count': 2567, 'sum_payoffs': 215.95810258838827, 'action': [1.0, 0]}])
Weights num count: [0.373811265544989, 0.6259448914898805]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2313, 'sum_payoffs': 200.3354999599246, 'action': [1.0, 0]}, {'num_count': 1787, 'sum_payoffs': 133.88399997322486, 'action': [0.0, 0]}])
Weights num count: [0.5640087783467447, 0.43574737868812485]
Actions to choose Agent 1: dict_values([{'num_count': 2312, 'sum_payoffs': 200.25449995994077, 'action': [1.0, 0]}, {'num_count': 1788, 'sum_payoffs': 134.04599997319244, 'action': [0.0, 0]}])
Weights num count: [0.5637649353816142, 0.4359912216532553]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6362085342407227 s
