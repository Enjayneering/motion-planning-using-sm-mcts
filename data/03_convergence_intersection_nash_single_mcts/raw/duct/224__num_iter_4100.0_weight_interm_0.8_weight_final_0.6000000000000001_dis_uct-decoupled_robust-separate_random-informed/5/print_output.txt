Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1264, 'sum_payoffs': 166.1904642572228, 'action': [0.0, 0]}, {'num_count': 2836, 'sum_payoffs': 481.0040017972224, 'action': [1.0, 0]}])
Weights num count: [0.3082175079248964, 0.6915386491099732]
Actions to choose Agent 1: dict_values([{'num_count': 1270, 'sum_payoffs': 167.75260711409769, 'action': [0.0, 0]}, {'num_count': 2830, 'sum_payoffs': 480.8477875115352, 'action': [1.0, 0]}])
Weights num count: [0.3096805657156791, 0.6900755913191905]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1626, 'sum_payoffs': 263.99571424045115, 'action': [0.0, 0]}, {'num_count': 2474, 'sum_payoffs': 449.1032142087099, 'action': [1.0, 0]}])
Weights num count: [0.39648866130212146, 0.6032674957327481]
Actions to choose Agent 1: dict_values([{'num_count': 2472, 'sum_payoffs': 448.7560713516266, 'action': [1.0, 0]}, {'num_count': 1628, 'sum_payoffs': 264.51642852607614, 'action': [0.0, 0]}])
Weights num count: [0.6027798098024872, 0.39697634723238234]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6081793308258057 s
