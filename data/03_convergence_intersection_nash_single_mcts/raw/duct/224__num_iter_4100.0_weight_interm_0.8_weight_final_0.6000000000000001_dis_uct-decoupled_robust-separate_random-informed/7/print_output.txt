Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2869, 'sum_payoffs': 487.2525732247224, 'action': [1.0, 0]}, {'num_count': 1231, 'sum_payoffs': 159.66771425834156, 'action': [0.0, 0]}])
Weights num count: [0.6995854669592783, 0.30017069007559133]
Actions to choose Agent 1: dict_values([{'num_count': 1253, 'sum_payoffs': 163.85303568619523, 'action': [0.0, 0]}, {'num_count': 2847, 'sum_payoffs': 483.06725179686885, 'action': [1.0, 0]}])
Weights num count: [0.30553523530846133, 0.6942209217264081]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1732, 'sum_payoffs': 287.0807142364922, 'action': [0.0, 0]}, {'num_count': 2368, 'sum_payoffs': 426.10499992693804, 'action': [1.0, 0]}])
Weights num count: [0.42233601560594974, 0.5774201414289197]
Actions to choose Agent 1: dict_values([{'num_count': 1733, 'sum_payoffs': 287.3410713793047, 'action': [0.0, 0]}, {'num_count': 2367, 'sum_payoffs': 426.0182142126672, 'action': [1.0, 0]}])
Weights num count: [0.4225798585710802, 0.5771762984637894]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6068503856658936 s
