Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1158, 'sum_payoffs': 377.8042498992569, 'action': [0.0, 0]}, {'num_count': 3442, 'sum_payoffs': 1298.1667496538735, 'action': [1.0, 0]}])
Weights num count: [0.2516844164312106, 0.7480982395131494]
Actions to choose Agent 1: dict_values([{'num_count': 3417, 'sum_payoffs': 1290.1477496560115, 'action': [1.0, 0]}, {'num_count': 1183, 'sum_payoffs': 388.37474989643806, 'action': [0.0, 0]}])
Weights num count: [0.7426646381221473, 0.25711801782221255]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1641, 'sum_payoffs': 637.0649998301213, 'action': [0.0, 0]}, {'num_count': 2959, 'sum_payoffs': 1225.3274996732093, 'action': [1.0, 0]}])
Weights num count: [0.3566615953053684, 0.6431210606389915]
Actions to choose Agent 1: dict_values([{'num_count': 1663, 'sum_payoffs': 647.3924998273665, 'action': [0.0, 0]}, {'num_count': 2937, 'sum_payoffs': 1216.619999675532, 'action': [1.0, 0]}])
Weights num count: [0.36144316452945013, 0.6383394914149098]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6702706813812256 s
