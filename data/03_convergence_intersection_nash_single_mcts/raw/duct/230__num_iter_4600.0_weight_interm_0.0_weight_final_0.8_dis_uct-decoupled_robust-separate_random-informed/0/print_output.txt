Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1132, 'sum_payoffs': 367.7804999172524, 'action': [0.0, 0]}, {'num_count': 3468, 'sum_payoffs': 1308.190499705646, 'action': [1.0, 0]}])
Weights num count: [0.24603347098456857, 0.7537491849597914]
Actions to choose Agent 1: dict_values([{'num_count': 3474, 'sum_payoffs': 1311.6532497048674, 'action': [1.0, 0]}, {'num_count': 1126, 'sum_payoffs': 365.77574991770354, 'action': [0.0, 0]}])
Weights num count: [0.7550532492936318, 0.2447294066507281]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1627, 'sum_payoffs': 630.5849998581167, 'action': [0.0, 0]}, {'num_count': 2973, 'sum_payoffs': 1231.1999997229725, 'action': [1.0, 0]}])
Weights num count: [0.35361877852640733, 0.6461638774179527]
Actions to choose Agent 1: dict_values([{'num_count': 1638, 'sum_payoffs': 636.2549998568414, 'action': [0.0, 0]}, {'num_count': 2962, 'sum_payoffs': 1227.5549997237933, 'action': [1.0, 0]}])
Weights num count: [0.35600956313844817, 0.6437730928059118]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7374680042266846 s
