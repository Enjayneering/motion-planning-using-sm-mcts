Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1132, 'sum_payoffs': 367.4159999173345, 'action': [0.0, 0]}, {'num_count': 3468, 'sum_payoffs': 1308.1904997056458, 'action': [1.0, 0]}])
Weights num count: [0.24603347098456857, 0.7537491849597914]
Actions to choose Agent 1: dict_values([{'num_count': 3428, 'sum_payoffs': 1293.974999708843, 'action': [1.0, 0]}, {'num_count': 1172, 'sum_payoffs': 383.8184999136432, 'action': [0.0, 0]}])
Weights num count: [0.7450554227341882, 0.2547272332101717]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1633, 'sum_payoffs': 633.6224998574335, 'action': [0.0, 0]}, {'num_count': 2967, 'sum_payoffs': 1228.9724997234741, 'action': [1.0, 0]}])
Weights num count: [0.3549228428602478, 0.6448598130841121]
Actions to choose Agent 1: dict_values([{'num_count': 1639, 'sum_payoffs': 636.4574998567957, 'action': [0.0, 0]}, {'num_count': 2961, 'sum_payoffs': 1226.9474997239302, 'action': [1.0, 0]}])
Weights num count: [0.3562269071940882, 0.6435557487502717]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6739017963409424 s
