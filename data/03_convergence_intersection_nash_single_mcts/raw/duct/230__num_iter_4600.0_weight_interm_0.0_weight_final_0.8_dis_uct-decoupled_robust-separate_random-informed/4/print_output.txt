Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1118, 'sum_payoffs': 361.94849991856444, 'action': [0.0, 0]}, {'num_count': 3482, 'sum_payoffs': 1313.1112497045403, 'action': [1.0, 0]}])
Weights num count: [0.24299065420560748, 0.7567920017387525]
Actions to choose Agent 1: dict_values([{'num_count': 3466, 'sum_payoffs': 1309.6484997053185, 'action': [1.0, 0]}, {'num_count': 1134, 'sum_payoffs': 369.05624991696465, 'action': [0.0, 0]}])
Weights num count: [0.7533144968485112, 0.24646815909584874]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2969, 'sum_payoffs': 1229.7824997232922, 'action': [1.0, 0]}, {'num_count': 1631, 'sum_payoffs': 632.6099998576614, 'action': [0.0, 0]}])
Weights num count: [0.6452945011953923, 0.3544881547489676]
Actions to choose Agent 1: dict_values([{'num_count': 2954, 'sum_payoffs': 1223.9099997246144, 'action': [1.0, 0]}, {'num_count': 1646, 'sum_payoffs': 639.6974998560671, 'action': [0.0, 0]}])
Weights num count: [0.6420343403607911, 0.3577483155835688]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6693599224090576 s
