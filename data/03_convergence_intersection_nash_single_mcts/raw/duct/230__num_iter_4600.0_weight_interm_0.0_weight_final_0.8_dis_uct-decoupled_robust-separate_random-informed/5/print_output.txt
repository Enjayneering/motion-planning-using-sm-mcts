Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1129, 'sum_payoffs': 366.3224999175796, 'action': [0.0, 0]}, {'num_count': 3471, 'sum_payoffs': 1308.7372497055221, 'action': [1.0, 0]}])
Weights num count: [0.24538143881764835, 0.7544012171267116]
Actions to choose Agent 1: dict_values([{'num_count': 3431, 'sum_payoffs': 1295.4329997085142, 'action': [1.0, 0]}, {'num_count': 1169, 'sum_payoffs': 382.9072499138479, 'action': [0.0, 0]}])
Weights num count: [0.7457074549011085, 0.2540752010432515]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1624, 'sum_payoffs': 629.36999985839, 'action': [0.0, 0]}, {'num_count': 2976, 'sum_payoffs': 1232.6174997226535, 'action': [1.0, 0]}])
Weights num count: [0.35296674635948705, 0.6468159095848729]
Actions to choose Agent 1: dict_values([{'num_count': 1642, 'sum_payoffs': 638.0774998564315, 'action': [0.0, 0]}, {'num_count': 2958, 'sum_payoffs': 1225.9349997241586, 'action': [1.0, 0]}])
Weights num count: [0.35687893936100845, 0.6429037165833514]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7398202419281006 s
