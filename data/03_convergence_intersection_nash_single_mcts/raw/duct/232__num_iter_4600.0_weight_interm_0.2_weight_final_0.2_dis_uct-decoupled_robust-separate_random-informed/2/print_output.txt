Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1314, 'sum_payoffs': 201.2343749295677, 'action': [0.0, 0]}, {'num_count': 3286, 'sum_payoffs': 640.2447826706427, 'action': [1.0, 0]}])
Weights num count: [0.2855900891110628, 0.7141925668332971]
Actions to choose Agent 1: dict_values([{'num_count': 3281, 'sum_payoffs': 639.8802826707705, 'action': [1.0, 0]}, {'num_count': 1319, 'sum_payoffs': 202.69237492905754, 'action': [0.0, 0]}])
Weights num count: [0.7131058465550967, 0.2866768093892632]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1725, 'sum_payoffs': 325.76624988598013, 'action': [0.0, 0]}, {'num_count': 2875, 'sum_payoffs': 606.98249978755, 'action': [1.0, 0]}])
Weights num count: [0.37491849597913496, 0.6248641599652249]
Actions to choose Agent 1: dict_values([{'num_count': 2873, 'sum_payoffs': 606.7799997876207, 'action': [1.0, 0]}, {'num_count': 1727, 'sum_payoffs': 326.3737498857674, 'action': [0.0, 0]}])
Weights num count: [0.6244294718539448, 0.3753531840904151]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6783583164215088 s
