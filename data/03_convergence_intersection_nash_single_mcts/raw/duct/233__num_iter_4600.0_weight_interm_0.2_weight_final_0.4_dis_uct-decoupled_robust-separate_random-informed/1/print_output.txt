Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1204, 'sum_payoffs': 250.51199993319207, 'action': [0.0, 0]}, {'num_count': 3396, 'sum_payoffs': 869.3666050313794, 'action': [1.0, 0]}])
Weights num count: [0.2616822429906542, 0.7381004129537058]
Actions to choose Agent 1: dict_values([{'num_count': 3377, 'sum_payoffs': 865.9271050322965, 'action': [1.0, 0]}, {'num_count': 1223, 'sum_payoffs': 256.3814999316269, 'action': [0.0, 0]}])
Weights num count: [0.7339708758965442, 0.2658117800478157]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2973, 'sum_payoffs': 830.9849997784353, 'action': [1.0, 0]}, {'num_count': 1627, 'sum_payoffs': 411.64499989023545, 'action': [0.0, 0]}])
Weights num count: [0.6461638774179527, 0.35361877852640733]
Actions to choose Agent 1: dict_values([{'num_count': 1634, 'sum_payoffs': 414.0749998895876, 'action': [0.0, 0]}, {'num_count': 2966, 'sum_payoffs': 829.3649997788672, 'action': [1.0, 0]}])
Weights num count: [0.35514018691588783, 0.6446424690284721]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7779703140258789 s
