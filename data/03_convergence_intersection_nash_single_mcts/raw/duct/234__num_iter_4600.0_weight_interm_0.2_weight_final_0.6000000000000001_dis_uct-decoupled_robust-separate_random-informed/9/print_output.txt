Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3445, 'sum_payoffs': 986.8384537253182, 'action': [1.0, 0]}, {'num_count': 1155, 'sum_payoffs': 272.08124993877857, 'action': [0.0, 0]}])
Weights num count: [0.7487502716800696, 0.25103238426429036]
Actions to choose Agent 1: dict_values([{'num_count': 1182, 'sum_payoffs': 280.8292499368096, 'action': [0.0, 0]}, {'num_count': 3418, 'sum_payoffs': 980.2774537267936, 'action': [1.0, 0]}])
Weights num count: [0.2569006737665725, 0.7428819821777874]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1634, 'sum_payoffs': 468.9112498944953, 'action': [0.0, 0]}, {'num_count': 2966, 'sum_payoffs': 928.861874791075, 'action': [1.0, 0]}])
Weights num count: [0.35514018691588783, 0.6446424690284721]
Actions to choose Agent 1: dict_values([{'num_count': 1644, 'sum_payoffs': 472.55624989367516, 'action': [0.0, 0]}, {'num_count': 2956, 'sum_payoffs': 925.8243747917579, 'action': [1.0, 0]}])
Weights num count: [0.3573136274722886, 0.6424690284720713]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.684206485748291 s
