Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3389, 'sum_payoffs': 1031.460062951627, 'action': [1.0, 0]}, {'num_count': 1211, 'sum_payoffs': 310.99499993780074, 'action': [0.0, 0]}])
Weights num count: [0.7365790045642252, 0.26320365138013474]
Actions to choose Agent 1: dict_values([{'num_count': 1222, 'sum_payoffs': 315.3689999369256, 'action': [0.0, 0]}, {'num_count': 3378, 'sum_payoffs': 1030.0020629519158, 'action': [1.0, 0]}])
Weights num count: [0.26559443599217564, 0.7341882199521843]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2003, 'sum_payoffs': 636.5159998726908, 'action': [0.0, 0]}, {'num_count': 2597, 'sum_payoffs': 854.2619998291095, 'action': [1.0, 0]}])
Weights num count: [0.4353401434470767, 0.5644425124972832]
Actions to choose Agent 1: dict_values([{'num_count': 2004, 'sum_payoffs': 637.1639998725611, 'action': [0.0, 0]}, {'num_count': 2596, 'sum_payoffs': 854.2619998291095, 'action': [1.0, 0]}])
Weights num count: [0.4355574875027168, 0.5642251684416432]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6926798820495605 s
