Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1165, 'sum_payoffs': 295.9775999408027, 'action': [0.0, 0]}, {'num_count': 3435, 'sum_payoffs': 1045.4406629488228, 'action': [1.0, 0]}])
Weights num count: [0.25320582482069115, 0.7465768311236688]
Actions to choose Agent 1: dict_values([{'num_count': 1197, 'sum_payoffs': 307.47959993850304, 'action': [0.0, 0]}, {'num_count': 3403, 'sum_payoffs': 1038.0210629503094, 'action': [1.0, 0]}])
Weights num count: [0.2601608346011737, 0.7396218213431862]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2973, 'sum_payoffs': 991.1519998017146, 'action': [1.0, 0]}, {'num_count': 1627, 'sum_payoffs': 499.30199990014773, 'action': [0.0, 0]}])
Weights num count: [0.6461638774179527, 0.35361877852640733]
Actions to choose Agent 1: dict_values([{'num_count': 2955, 'sum_payoffs': 985.3199998028819, 'action': [1.0, 0]}, {'num_count': 1645, 'sum_payoffs': 506.10599989878705, 'action': [0.0, 0]}])
Weights num count: [0.6422516844164312, 0.35753097152792873]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7069213390350342 s
