Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3401, 'sum_payoffs': 1035.542462950805, 'action': [1.0, 0]}, {'num_count': 1199, 'sum_payoffs': 307.3157999385354, 'action': [0.0, 0]}])
Weights num count: [0.7391871332319061, 0.26059552271245384]
Actions to choose Agent 1: dict_values([{'num_count': 3399, 'sum_payoffs': 1035.9798629507184, 'action': [1.0, 0]}, {'num_count': 1201, 'sum_payoffs': 308.33639993833157, 'action': [0.0, 0]}])
Weights num count: [0.738752445120626, 0.26103021082373395]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1597, 'sum_payoffs': 488.2859999023503, 'action': [0.0, 0]}, {'num_count': 3003, 'sum_payoffs': 1001.8439997995747, 'action': [1.0, 0]}])
Weights num count: [0.347098456857205, 0.6526841990871549]
Actions to choose Agent 1: dict_values([{'num_count': 1605, 'sum_payoffs': 491.68799990166997, 'action': [0.0, 0]}, {'num_count': 2995, 'sum_payoffs': 999.7379997999963, 'action': [1.0, 0]}])
Weights num count: [0.3488372093023256, 0.6509454466420344]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6824896335601807 s
