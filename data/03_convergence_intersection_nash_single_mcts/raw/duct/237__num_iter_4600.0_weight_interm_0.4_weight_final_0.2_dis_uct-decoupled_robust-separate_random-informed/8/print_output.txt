Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1466, 'sum_payoffs': 146.4877499609348, 'action': [0.0, 0]}, {'num_count': 3134, 'sum_payoffs': 419.47196041447995, 'action': [1.0, 0]}])
Weights num count: [0.3186263855683547, 0.6811562703760052]
Actions to choose Agent 1: dict_values([{'num_count': 3121, 'sum_payoffs': 418.0139604148697, 'action': [1.0, 0]}, {'num_count': 1479, 'sum_payoffs': 148.9177499602871, 'action': [0.0, 0]}])
Weights num count: [0.6783307976526842, 0.3214518582916757]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2770, 'sum_payoffs': 394.859999894717, 'action': [1.0, 0]}, {'num_count': 1830, 'sum_payoffs': 227.93999993922202, 'action': [0.0, 0]}])
Weights num count: [0.6020430341230167, 0.3977396218213432]
Actions to choose Agent 1: dict_values([{'num_count': 2770, 'sum_payoffs': 394.859999894717, 'action': [1.0, 0]}, {'num_count': 1830, 'sum_payoffs': 227.93999993922202, 'action': [0.0, 0]}])
Weights num count: [0.6020430341230167, 0.3977396218213432]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6988067626953125 s
