Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1401, 'sum_payoffs': 220.6327499503575, 'action': [0.0, 0]}, {'num_count': 3199, 'sum_payoffs': 622.4697827546696, 'action': [1.0, 0]}])
Weights num count: [0.3044990219517496, 0.6952836339926103]
Actions to choose Agent 1: dict_values([{'num_count': 3193, 'sum_payoffs': 621.9230327547929, 'action': [1.0, 0]}, {'num_count': 1407, 'sum_payoffs': 222.27299994998825, 'action': [0.0, 0]}])
Weights num count: [0.6939795696587698, 0.3058030862855901]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1692, 'sum_payoffs': 317.76749992850193, 'action': [0.0, 0]}, {'num_count': 2908, 'sum_payoffs': 614.9812498616269, 'action': [1.0, 0]}])
Weights num count: [0.3677461421430124, 0.6320365138013475]
Actions to choose Agent 1: dict_values([{'num_count': 2906, 'sum_payoffs': 614.7787498616725, 'action': [1.0, 0]}, {'num_count': 1694, 'sum_payoffs': 318.3749999283653, 'action': [0.0, 0]}])
Weights num count: [0.6316018256900674, 0.36818083025429255]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7113237380981445 s
