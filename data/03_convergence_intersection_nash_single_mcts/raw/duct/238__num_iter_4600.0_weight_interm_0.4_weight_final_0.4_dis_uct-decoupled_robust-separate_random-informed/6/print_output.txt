Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1329, 'sum_payoffs': 205.1313749538455, 'action': [0.0, 0]}, {'num_count': 3271, 'sum_payoffs': 638.690032751024, 'action': [1.0, 0]}])
Weights num count: [0.288850249945664, 0.7109324059986959]
Actions to choose Agent 1: dict_values([{'num_count': 3259, 'sum_payoffs': 636.8675327514334, 'action': [1.0, 0]}, {'num_count': 1341, 'sum_payoffs': 208.04737495318875, 'action': [0.0, 0]}])
Weights num count: [0.708324277331015, 0.29145837861334495]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2914, 'sum_payoffs': 616.3987498613071, 'action': [1.0, 0]}, {'num_count': 1686, 'sum_payoffs': 316.2487499288433, 'action': [0.0, 0]}])
Weights num count: [0.633340578135188, 0.36644207780917193]
Actions to choose Agent 1: dict_values([{'num_count': 1690, 'sum_payoffs': 317.36249992859274, 'action': [0.0, 0]}, {'num_count': 2910, 'sum_payoffs': 615.6899998614667, 'action': [1.0, 0]}])
Weights num count: [0.3673114540317322, 0.6324712019126277]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6850082874298096 s
