Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2990, 'sum_payoffs': 306.35381243106474, 'action': [1.0, 0]}, {'num_count': 1610, 'sum_payoffs': 121.03593747276646, 'action': [0.0, 0]}])
Weights num count: [0.6498587263638339, 0.349923929580526]
Actions to choose Agent 1: dict_values([{'num_count': 1623, 'sum_payoffs': 122.60024997241418, 'action': [0.0, 0]}, {'num_count': 2977, 'sum_payoffs': 304.2427499315394, 'action': [1.0, 0]}])
Weights num count: [0.352749402303847, 0.6470332536405129]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1898, 'sum_payoffs': 175.98937496040367, 'action': [0.0, 0]}, {'num_count': 2702, 'sum_payoffs': 291.7856249343487, 'action': [1.0, 0]}])
Weights num count: [0.41251901760486853, 0.5872636383394915]
Actions to choose Agent 1: dict_values([{'num_count': 2703, 'sum_payoffs': 291.988124934303, 'action': [1.0, 0]}, {'num_count': 1897, 'sum_payoffs': 175.88812496042644, 'action': [0.0, 0]}])
Weights num count: [0.5874809823951315, 0.4123016735492284]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6839840412139893 s
