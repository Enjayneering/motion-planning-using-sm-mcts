Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3198, 'sum_payoffs': 506.0359893724847, 'action': [1.0, 0]}, {'num_count': 1402, 'sum_payoffs': 169.8326999660341, 'action': [0.0, 0]}])
Weights num count: [0.6950662899369702, 0.3047163660073897]
Actions to choose Agent 1: dict_values([{'num_count': 1417, 'sum_payoffs': 172.57859996548493, 'action': [0.0, 0]}, {'num_count': 3183, 'sum_payoffs': 503.29008937303223, 'action': [1.0, 0]}])
Weights num count: [0.3079765268419909, 0.691806129102369]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2887, 'sum_payoffs': 493.0739999013583, 'action': [1.0, 0]}, {'num_count': 1713, 'sum_payoffs': 253.5839999492861, 'action': [0.0, 0]}])
Weights num count: [0.6274722886329059, 0.372310367311454]
Actions to choose Agent 1: dict_values([{'num_count': 1716, 'sum_payoffs': 254.1509999491727, 'action': [0.0, 0]}, {'num_count': 2884, 'sum_payoffs': 492.6689999014394, 'action': [1.0, 0]}])
Weights num count: [0.3729623994783743, 0.6268202564659856]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7247545719146729 s
