Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3314, 'sum_payoffs': 733.1401352126327, 'action': [1.0, 0]}, {'num_count': 1286, 'sum_payoffs': 228.9484285321761, 'action': [0.0, 0]}])
Weights num count: [0.7202782003912193, 0.27950445555314063]
Actions to choose Agent 1: dict_values([{'num_count': 1293, 'sum_payoffs': 231.2704285317782, 'action': [0.0, 0]}, {'num_count': 3307, 'sum_payoffs': 733.3175637840309, 'action': [1.0, 0]}])
Weights num count: [0.2810258639426212, 0.7187567920017387]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1724, 'sum_payoffs': 375.2999999356662, 'action': [0.0, 0]}, {'num_count': 2876, 'sum_payoffs': 690.3128570244884, 'action': [1.0, 0]}])
Weights num count: [0.3747011519234949, 0.625081504020865]
Actions to choose Agent 1: dict_values([{'num_count': 1728, 'sum_payoffs': 376.57285707830516, 'action': [0.0, 0]}, {'num_count': 2872, 'sum_payoffs': 689.5028570246275, 'action': [1.0, 0]}])
Weights num count: [0.3755705281460552, 0.6242121277983047]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6741814613342285 s
