Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1995, 'sum_payoffs': 136.52054997269778, 'action': [0.0, 0]}, {'num_count': 2605, 'sum_payoffs': 208.19425258994323, 'action': [1.0, 0]}])
Weights num count: [0.4336013910019561, 0.5661812649424038]
Actions to choose Agent 1: dict_values([{'num_count': 2606, 'sum_payoffs': 208.44940258989212, 'action': [1.0, 0]}, {'num_count': 1994, 'sum_payoffs': 136.48409997270502, 'action': [0.0, 0]}])
Weights num count: [0.566398608998044, 0.433384046946316]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2114, 'sum_payoffs': 164.29949996713842, 'action': [0.0, 0]}, {'num_count': 2486, 'sum_payoffs': 210.46049995789977, 'action': [1.0, 0]}])
Weights num count: [0.4594653336231254, 0.5403173223212345]
Actions to choose Agent 1: dict_values([{'num_count': 2486, 'sum_payoffs': 210.46049995789977, 'action': [1.0, 0]}, {'num_count': 2114, 'sum_payoffs': 164.2994999671384, 'action': [0.0, 0]}])
Weights num count: [0.5403173223212345, 0.4594653336231254]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7727005481719971 s
