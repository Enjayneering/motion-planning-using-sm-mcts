Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2797, 'sum_payoffs': 230.5381025854753, 'action': [1.0, 0]}, {'num_count': 1803, 'sum_payoffs': 114.24959997715158, 'action': [0.0, 0]}])
Weights num count: [0.6079113236252989, 0.39187133231906107]
Actions to choose Agent 1: dict_values([{'num_count': 1798, 'sum_payoffs': 113.73929997725365, 'action': [0.0, 0]}, {'num_count': 2802, 'sum_payoffs': 231.26710258533012, 'action': [1.0, 0]}])
Weights num count: [0.3907846120408607, 0.6089980439034992]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2749, 'sum_payoffs': 242.86049995141542, 'action': [1.0, 0]}, {'num_count': 1851, 'sum_payoffs': 131.81849997363736, 'action': [0.0, 0]}])
Weights num count: [0.5974788089545751, 0.40230384698978483]
Actions to choose Agent 1: dict_values([{'num_count': 1850, 'sum_payoffs': 131.73749997365357, 'action': [0.0, 0]}, {'num_count': 2750, 'sum_payoffs': 243.022499951383, 'action': [1.0, 0]}])
Weights num count: [0.4020865029341448, 0.5976961530102152]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7077794075012207 s
