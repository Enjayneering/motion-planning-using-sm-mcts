Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3039, 'sum_payoffs': 403.9882104522568, 'action': [1.0, 0]}, {'num_count': 1561, 'sum_payoffs': 161.4854999703937, 'action': [0.0, 0]}])
Weights num count: [0.6605085850901978, 0.3392740708541621]
Actions to choose Agent 1: dict_values([{'num_count': 1563, 'sum_payoffs': 162.46499997021388, 'action': [0.0, 0]}, {'num_count': 3037, 'sum_payoffs': 404.8312104521023, 'action': [1.0, 0]}])
Weights num count: [0.3397087589654423, 0.6600738969789176]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1760, 'sum_payoffs': 215.51999996049753, 'action': [0.0, 0]}, {'num_count': 2840, 'sum_payoffs': 407.0774999253914, 'action': [1.0, 0]}])
Weights num count: [0.3825255379265377, 0.6172571180178222]
Actions to choose Agent 1: dict_values([{'num_count': 1762, 'sum_payoffs': 216.05999996039856, 'action': [0.0, 0]}, {'num_count': 2838, 'sum_payoffs': 406.94249992541614, 'action': [1.0, 0]}])
Weights num count: [0.3829602260378179, 0.616822429906542]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6863870620727539 s
