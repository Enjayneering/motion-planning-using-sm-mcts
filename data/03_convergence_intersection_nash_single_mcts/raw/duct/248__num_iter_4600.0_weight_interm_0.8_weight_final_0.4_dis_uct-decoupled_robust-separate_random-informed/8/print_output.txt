Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3101, 'sum_payoffs': 414.63149992399156, 'action': [1.0, 0]}, {'num_count': 1499, 'sum_payoffs': 151.83374997216333, 'action': [0.0, 0]}])
Weights num count: [0.6739839165398827, 0.3257987394044773]
Actions to choose Agent 1: dict_values([{'num_count': 3087, 'sum_payoffs': 411.8437499245016, 'action': [1.0, 0]}, {'num_count': 1513, 'sum_payoffs': 153.8924999717857, 'action': [0.0, 0]}])
Weights num count: [0.6709410997609215, 0.3288415561834384]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2846, 'sum_payoffs': 408.1574999251946, 'action': [1.0, 0]}, {'num_count': 1754, 'sum_payoffs': 214.50749996068265, 'action': [0.0, 0]}])
Weights num count: [0.6185611823516627, 0.38122147359269726]
Actions to choose Agent 1: dict_values([{'num_count': 2843, 'sum_payoffs': 407.7524999252688, 'action': [1.0, 0]}, {'num_count': 1757, 'sum_payoffs': 215.18249996055897, 'action': [0.0, 0]}])
Weights num count: [0.6179091501847425, 0.3818735057596175]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7106161117553711 s
