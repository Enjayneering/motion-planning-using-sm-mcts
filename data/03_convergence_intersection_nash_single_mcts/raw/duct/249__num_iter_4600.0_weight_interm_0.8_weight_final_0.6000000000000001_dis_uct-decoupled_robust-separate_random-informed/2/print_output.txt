Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1380, 'sum_payoffs': 180.2098928262472, 'action': [0.0, 0]}, {'num_count': 3220, 'sum_payoffs': 543.3659660722451, 'action': [1.0, 0]}])
Weights num count: [0.299934796783308, 0.699847859161052]
Actions to choose Agent 1: dict_values([{'num_count': 1367, 'sum_payoffs': 178.21157139801872, 'action': [0.0, 0]}, {'num_count': 3233, 'sum_payoffs': 546.9264303573483, 'action': [1.0, 0]}])
Weights num count: [0.29710932405998697, 0.702673331884373]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1792, 'sum_payoffs': 290.2049999502423, 'action': [0.0, 0]}, {'num_count': 2808, 'sum_payoffs': 509.59285705547956, 'action': [1.0, 0]}])
Weights num count: [0.38948054770702023, 0.6103021082373397]
Actions to choose Agent 1: dict_values([{'num_count': 2805, 'sum_payoffs': 509.15892848412534, 'action': [1.0, 0]}, {'num_count': 1795, 'sum_payoffs': 290.9860713786798, 'action': [0.0, 0]}])
Weights num count: [0.6096500760704194, 0.39013257987394045]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.7184658050537109 s
