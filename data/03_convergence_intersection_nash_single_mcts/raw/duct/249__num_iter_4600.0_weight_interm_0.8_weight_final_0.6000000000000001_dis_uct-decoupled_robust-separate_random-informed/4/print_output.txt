Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1390, 'sum_payoffs': 182.5264285401359, 'action': [0.0, 0]}, {'num_count': 3210, 'sum_payoffs': 542.5524303580994, 'action': [1.0, 0]}])
Weights num count: [0.30210823733970876, 0.6976744186046512]
Actions to choose Agent 1: dict_values([{'num_count': 3197, 'sum_payoffs': 539.9806803585402, 'action': [1.0, 0]}, {'num_count': 1403, 'sum_payoffs': 184.9419642540072, 'action': [0.0, 0]}])
Weights num count: [0.6948489458813302, 0.3049337100630298]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2899, 'sum_payoffs': 529.0328570521452, 'action': [1.0, 0]}, {'num_count': 1701, 'sum_payoffs': 270.6782142393061, 'action': [0.0, 0]}])
Weights num count: [0.6300804173005868, 0.3697022386437731]
Actions to choose Agent 1: dict_values([{'num_count': 2896, 'sum_payoffs': 528.6857141950619, 'action': [1.0, 0]}, {'num_count': 1704, 'sum_payoffs': 271.5460713820144, 'action': [0.0, 0]}])
Weights num count: [0.6294283851336666, 0.37035427081069333]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6983239650726318 s
