Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1392, 'sum_payoffs': 182.85046237466645, 'action': [0.0, 0]}, {'num_count': 3208, 'sum_payoffs': 542.1618946438798, 'action': [1.0, 0]}])
Weights num count: [0.30254292545098893, 0.697239730493371]
Actions to choose Agent 1: dict_values([{'num_count': 3191, 'sum_payoffs': 539.148180358682, 'action': [1.0, 0]}, {'num_count': 1409, 'sum_payoffs': 186.33281951692643, 'action': [0.0, 0]}])
Weights num count: [0.6935448815474897, 0.3062377743968702]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2900, 'sum_payoffs': 529.2064284806871, 'action': [1.0, 0]}, {'num_count': 1700, 'sum_payoffs': 270.5046428107647, 'action': [0.0, 0]}])
Weights num count: [0.630297761356227, 0.369484894588133]
Actions to choose Agent 1: dict_values([{'num_count': 1703, 'sum_payoffs': 271.28571423920226, 'action': [0.0, 0]}, {'num_count': 2897, 'sum_payoffs': 528.772499909333, 'action': [1.0, 0]}])
Weights num count: [0.3701369267550533, 0.6296457291893067]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6833865642547607 s
