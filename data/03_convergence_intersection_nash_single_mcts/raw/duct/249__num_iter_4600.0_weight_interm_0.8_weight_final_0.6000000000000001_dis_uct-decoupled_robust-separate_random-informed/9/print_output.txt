Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1361, 'sum_payoffs': 176.7731785411227, 'action': [0.0, 0]}, {'num_count': 3239, 'sum_payoffs': 547.6293946429427, 'action': [1.0, 0]}])
Weights num count: [0.29580525972614646, 0.7039773962182134]
Actions to choose Agent 1: dict_values([{'num_count': 1374, 'sum_payoffs': 179.58503568349772, 'action': [0.0, 0]}, {'num_count': 3226, 'sum_payoffs': 545.91103750038, 'action': [1.0, 0]}])
Weights num count: [0.29863073244946753, 0.7011519234948924]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2870, 'sum_payoffs': 522.8710713389167, 'action': [1.0, 0]}, {'num_count': 1730, 'sum_payoffs': 276.9267856668057, 'action': [0.0, 0]}])
Weights num count: [0.6237774396870246, 0.37600521625733535]
Actions to choose Agent 1: dict_values([{'num_count': 2869, 'sum_payoffs': 522.7842856246459, 'action': [1.0, 0]}, {'num_count': 1731, 'sum_payoffs': 277.1871428096182, 'action': [0.0, 0]}])
Weights num count: [0.6235600956313845, 0.37622256031297546]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6929242610931396 s
