Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 198, 'sum_payoffs': 39.16349998955641, 'action': [0.0, 0]}, {'num_count': 402, 'sum_payoffs': 109.95410523383619, 'action': [1.0, 0]}])
Weights num count: [0.32945091514143093, 0.6688851913477537]
Actions to choose Agent 1: dict_values([{'num_count': 402, 'sum_payoffs': 110.2616052337542, 'action': [1.0, 0]}, {'num_count': 198, 'sum_payoffs': 39.09899998957361, 'action': [0.0, 0]}])
Weights num count: [0.6688851913477537, 0.32945091514143093]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 253, 'sum_payoffs': 63.74999998299974, 'action': [0.0, 0]}, {'num_count': 347, 'sum_payoffs': 98.87999997363204, 'action': [1.0, 0]}])
Weights num count: [0.4209650582362729, 0.5773710482529119]
Actions to choose Agent 1: dict_values([{'num_count': 255, 'sum_payoffs': 64.82999998271175, 'action': [0.0, 0]}, {'num_count': 345, 'sum_payoffs': 98.60999997370403, 'action': [1.0, 0]}])
Weights num count: [0.4242928452579035, 0.5740432612312812]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.15580439567565918 s
