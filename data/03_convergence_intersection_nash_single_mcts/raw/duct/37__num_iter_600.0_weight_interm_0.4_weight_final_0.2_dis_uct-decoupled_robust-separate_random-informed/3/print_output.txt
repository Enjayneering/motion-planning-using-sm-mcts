Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 345, 'sum_payoffs': 51.20546051266077, 'action': [1.0, 0]}, {'num_count': 255, 'sum_payoffs': 29.840999992042402, 'action': [0.0, 0]}])
Weights num count: [0.5740432612312812, 0.4242928452579035]
Actions to choose Agent 1: dict_values([{'num_count': 254, 'sum_payoffs': 29.415749992155806, 'action': [0.0, 0]}, {'num_count': 346, 'sum_payoffs': 51.14471051267698, 'action': [1.0, 0]}])
Weights num count: [0.4226289517470882, 0.5757071547420965]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 264, 'sum_payoffs': 32.72999999127189, 'action': [0.0, 0]}, {'num_count': 336, 'sum_payoffs': 50.00249998666605, 'action': [1.0, 0]}])
Weights num count: [0.43926788685524126, 0.5590682196339434]
Actions to choose Agent 1: dict_values([{'num_count': 265, 'sum_payoffs': 32.93249999121789, 'action': [0.0, 0]}, {'num_count': 335, 'sum_payoffs': 49.934999986684055, 'action': [1.0, 0]}])
Weights num count: [0.44093178036605657, 0.5574043261231281]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.15812373161315918 s
