Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 221, 'sum_payoffs': 41.67404999166513, 'action': [0.0, 0]}, {'num_count': 379, 'sum_payoffs': 93.02037629718534, 'action': [1.0, 0]}])
Weights num count: [0.36772046589018303, 0.6306156405990017]
Actions to choose Agent 1: dict_values([{'num_count': 221, 'sum_payoffs': 42.03809999159232, 'action': [0.0, 0]}, {'num_count': 379, 'sum_payoffs': 93.96852629699569, 'action': [1.0, 0]}])
Weights num count: [0.36772046589018303, 0.6306156405990017]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 338, 'sum_payoffs': 86.62049998267555, 'action': [1.0, 0]}, {'num_count': 262, 'sum_payoffs': 60.137999987972464, 'action': [0.0, 0]}])
Weights num count: [0.562396006655574, 0.43594009983361065]
Actions to choose Agent 1: dict_values([{'num_count': 262, 'sum_payoffs': 60.25949998794817, 'action': [0.0, 0]}, {'num_count': 338, 'sum_payoffs': 86.74199998265124, 'action': [1.0, 0]}])
Weights num count: [0.43594009983361065, 0.562396006655574]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.14782142639160156 s
