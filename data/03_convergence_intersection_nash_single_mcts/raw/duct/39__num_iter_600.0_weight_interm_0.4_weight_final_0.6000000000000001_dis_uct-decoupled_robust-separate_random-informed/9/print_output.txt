Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 387, 'sum_payoffs': 96.07812629657339, 'action': [1.0, 0]}, {'num_count': 213, 'sum_payoffs': 39.46187367631813, 'action': [0.0, 0]}])
Weights num count: [0.6439267886855241, 0.3544093178036606]
Actions to choose Agent 1: dict_values([{'num_count': 385, 'sum_payoffs': 95.75007629663901, 'action': [1.0, 0]}, {'num_count': 215, 'sum_payoffs': 40.22732367616503, 'action': [0.0, 0]}])
Weights num count: [0.6405990016638935, 0.3577371048252912]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 256, 'sum_payoffs': 58.07249998838546, 'action': [0.0, 0]}, {'num_count': 344, 'sum_payoffs': 88.56449998228679, 'action': [1.0, 0]}])
Weights num count: [0.4259567387687188, 0.5723793677204659]
Actions to choose Agent 1: dict_values([{'num_count': 258, 'sum_payoffs': 58.92299998821536, 'action': [0.0, 0]}, {'num_count': 342, 'sum_payoffs': 88.1999999823597, 'action': [1.0, 0]}])
Weights num count: [0.4292845257903494, 0.5690515806988353]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.14543724060058594 s
