Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 394, 'sum_payoffs': 107.3756052434718, 'action': [1.0, 0]}, {'num_count': 206, 'sum_payoffs': 41.893499992319555, 'action': [0.0, 0]}])
Weights num count: [0.6555740432612313, 0.3427620632279534]
Actions to choose Agent 1: dict_values([{'num_count': 201, 'sum_payoffs': 40.40849999259182, 'action': [0.0, 0]}, {'num_count': 399, 'sum_payoffs': 109.58960524306582, 'action': [1.0, 0]}])
Weights num count: [0.33444259567387685, 0.6638935108153078]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 383, 'sum_payoffs': 112.10999997944636, 'action': [1.0, 0]}, {'num_count': 217, 'sum_payoffs': 50.51999999073794, 'action': [0.0, 0]}])
Weights num count: [0.6372712146422629, 0.3610648918469218]
Actions to choose Agent 1: dict_values([{'num_count': 217, 'sum_payoffs': 50.654999990713186, 'action': [0.0, 0]}, {'num_count': 383, 'sum_payoffs': 112.5149999793721, 'action': [1.0, 0]}])
Weights num count: [0.3610648918469218, 0.6372712146422629]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.154008150100708 s
