Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 384, 'sum_payoffs': 81.73965787975094, 'action': [1.0, 0]}, {'num_count': 216, 'sum_payoffs': 32.819624993983, 'action': [0.0, 0]}])
Weights num count: [0.6389351081530782, 0.3594009983361065]
Actions to choose Agent 1: dict_values([{'num_count': 386, 'sum_payoffs': 82.37753287963399, 'action': [1.0, 0]}, {'num_count': 214, 'sum_payoffs': 32.36399999406654, 'action': [0.0, 0]}])
Weights num count: [0.6422628951747088, 0.3560732113144759]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 323, 'sum_payoffs': 68.43374998745392, 'action': [1.0, 0]}, {'num_count': 277, 'sum_payoffs': 54.3149999900424, 'action': [0.0, 0]}])
Weights num count: [0.5374376039933444, 0.46089850249584025]
Actions to choose Agent 1: dict_values([{'num_count': 278, 'sum_payoffs': 54.82124998994959, 'action': [0.0, 0]}, {'num_count': 322, 'sum_payoffs': 68.33249998747247, 'action': [1.0, 0]}])
Weights num count: [0.46256239600665555, 0.5357737104825291]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.13543272018432617 s
