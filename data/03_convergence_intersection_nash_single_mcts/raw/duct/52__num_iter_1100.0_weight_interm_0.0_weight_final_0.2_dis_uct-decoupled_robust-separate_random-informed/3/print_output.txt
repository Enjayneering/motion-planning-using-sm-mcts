Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 309, 'sum_payoffs': 94.76999994313806, 'action': [0.0, 0]}, {'num_count': 791, 'sum_payoffs': 305.81549981650613, 'action': [1.0, 0]}])
Weights num count: [0.28065395095367845, 0.7184377838328792]
Actions to choose Agent 1: dict_values([{'num_count': 798, 'sum_payoffs': 309.8249998141005, 'action': [1.0, 0]}, {'num_count': 302, 'sum_payoffs': 92.21849994466905, 'action': [0.0, 0]}])
Weights num count: [0.7247956403269755, 0.2742960944595822]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 702, 'sum_payoffs': 295.8524998224888, 'action': [1.0, 0]}, {'num_count': 398, 'sum_payoffs': 149.24249991045386, 'action': [0.0, 0]}])
Weights num count: [0.6376021798365122, 0.3614895549500454]
Actions to choose Agent 1: dict_values([{'num_count': 695, 'sum_payoffs': 293.0174998241897, 'action': [1.0, 0]}, {'num_count': 405, 'sum_payoffs': 152.88749990826685, 'action': [0.0, 0]}])
Weights num count: [0.631244323342416, 0.3678474114441417]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.23051905632019043 s
