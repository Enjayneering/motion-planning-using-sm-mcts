Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 293, 'sum_payoffs': 87.84449998023473, 'action': [0.0, 0]}, {'num_count': 807, 'sum_payoffs': 312.1942499297589, 'action': [1.0, 0]}])
Weights num count: [0.26612170753860126, 0.7329700272479565]
Actions to choose Agent 1: dict_values([{'num_count': 798, 'sum_payoffs': 310.3717499301691, 'action': [1.0, 0]}, {'num_count': 302, 'sum_payoffs': 92.21849997925058, 'action': [0.0, 0]}])
Weights num count: [0.7247956403269755, 0.2742960944595822]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 709, 'sum_payoffs': 299.29499993265875, 'action': [1.0, 0]}, {'num_count': 391, 'sum_payoffs': 146.0024999671504, 'action': [0.0, 0]}])
Weights num count: [0.6439600363306085, 0.35513169845594916]
Actions to choose Agent 1: dict_values([{'num_count': 704, 'sum_payoffs': 297.26999993311443, 'action': [1.0, 0]}, {'num_count': 396, 'sum_payoffs': 148.43249996660367, 'action': [0.0, 0]}])
Weights num count: [0.6394187102633969, 0.35967302452316074]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.22885870933532715 s
