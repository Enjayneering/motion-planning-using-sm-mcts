Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 800, 'sum_payoffs': 308.91374993049726, 'action': [1.0, 0]}, {'num_count': 300, 'sum_payoffs': 90.39599997966073, 'action': [0.0, 0]}])
Weights num count: [0.7266121707538601, 0.2724795640326976]
Actions to choose Agent 1: dict_values([{'num_count': 790, 'sum_payoffs': 306.5444999310304, 'action': [1.0, 0]}, {'num_count': 310, 'sum_payoffs': 95.6812499784715, 'action': [0.0, 0]}])
Weights num count: [0.7175295186194369, 0.2815622161671208]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 389, 'sum_payoffs': 144.98999996737822, 'action': [0.0, 0]}, {'num_count': 711, 'sum_payoffs': 300.10499993247646, 'action': [1.0, 0]}])
Weights num count: [0.3533151680290645, 0.6457765667574932]
Actions to choose Agent 1: dict_values([{'num_count': 706, 'sum_payoffs': 298.2824999328866, 'action': [1.0, 0]}, {'num_count': 394, 'sum_payoffs': 147.62249996678594, 'action': [0.0, 0]}])
Weights num count: [0.6412352406902816, 0.3578564940962761]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2241978645324707 s
