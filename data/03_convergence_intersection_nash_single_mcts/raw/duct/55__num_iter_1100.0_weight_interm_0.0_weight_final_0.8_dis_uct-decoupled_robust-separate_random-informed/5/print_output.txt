Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 299, 'sum_payoffs': 90.2137499797017, 'action': [0.0, 0]}, {'num_count': 801, 'sum_payoffs': 309.2782499304149, 'action': [1.0, 0]}])
Weights num count: [0.27157129881925524, 0.7275204359673024]
Actions to choose Agent 1: dict_values([{'num_count': 315, 'sum_payoffs': 98.05049997793841, 'action': [0.0, 0]}, {'num_count': 785, 'sum_payoffs': 305.086499931358, 'action': [1.0, 0]}])
Weights num count: [0.28610354223433243, 0.7129881925522252]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 382, 'sum_payoffs': 141.54749996815275, 'action': [0.0, 0]}, {'num_count': 718, 'sum_payoffs': 303.5474999317018, 'action': [1.0, 0]}])
Weights num count: [0.3469573115349682, 0.6521344232515894]
Actions to choose Agent 1: dict_values([{'num_count': 386, 'sum_payoffs': 143.977499967606, 'action': [0.0, 0]}, {'num_count': 714, 'sum_payoffs': 302.3324999319752, 'action': [1.0, 0]}])
Weights num count: [0.3505903723887375, 0.6485013623978202]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.22442841529846191 s
