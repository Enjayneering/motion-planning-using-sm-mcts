Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 716, 'sum_payoffs': 146.5436249487102, 'action': [1.0, 0]}, {'num_count': 384, 'sum_payoffs': 58.97646708462129, 'action': [0.0, 0]}])
Weights num count: [0.6503178928247049, 0.34877384196185285]
Actions to choose Agent 1: dict_values([{'num_count': 381, 'sum_payoffs': 58.480342084795055, 'action': [0.0, 0]}, {'num_count': 719, 'sum_payoffs': 147.9509999482176, 'action': [1.0, 0]}])
Weights num count: [0.3460490463215259, 0.6530426884650318]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 450, 'sum_payoffs': 83.77874997067786, 'action': [0.0, 0]}, {'num_count': 650, 'sum_payoffs': 140.21999995092466, 'action': [1.0, 0]}])
Weights num count: [0.4087193460490463, 0.5903723887375113]
Actions to choose Agent 1: dict_values([{'num_count': 648, 'sum_payoffs': 139.8149999510664, 'action': [1.0, 0]}, {'num_count': 452, 'sum_payoffs': 84.38624997046526, 'action': [0.0, 0]}])
Weights num count: [0.5885558583106267, 0.410535876475931]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.22574543952941895 s
