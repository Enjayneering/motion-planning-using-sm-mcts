Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 761, 'sum_payoffs': 225.85076639655045, 'action': [1.0, 0]}, {'num_count': 339, 'sum_payoffs': 77.71162498251502, 'action': [0.0, 0]}])
Weights num count: [0.6911898274296094, 0.3079019073569482]
Actions to choose Agent 1: dict_values([{'num_count': 337, 'sum_payoffs': 77.16487498263808, 'action': [0.0, 0]}, {'num_count': 763, 'sum_payoffs': 226.94426639630444, 'action': [1.0, 0]}])
Weights num count: [0.3060853769300636, 0.6930063578564941]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 623, 'sum_payoffs': 195.30562495605497, 'action': [1.0, 0]}, {'num_count': 477, 'sum_payoffs': 139.3424999686471, 'action': [0.0, 0]}])
Weights num count: [0.5658492279745686, 0.4332425068119891]
Actions to choose Agent 1: dict_values([{'num_count': 622, 'sum_payoffs': 195.30562495605497, 'action': [1.0, 0]}, {'num_count': 478, 'sum_payoffs': 139.9499999685104, 'action': [0.0, 0]}])
Weights num count: [0.5649409627611263, 0.43415077202543145]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.22081542015075684 s
