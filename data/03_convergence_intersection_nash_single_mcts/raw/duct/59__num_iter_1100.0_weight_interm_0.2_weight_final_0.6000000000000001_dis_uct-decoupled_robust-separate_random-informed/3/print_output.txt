Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 774, 'sum_payoffs': 229.13126639581205, 'action': [1.0, 0]}, {'num_count': 326, 'sum_payoffs': 72.79593748362102, 'action': [0.0, 0]}])
Weights num count: [0.7029972752043597, 0.296094459582198]
Actions to choose Agent 1: dict_values([{'num_count': 337, 'sum_payoffs': 77.16993748263694, 'action': [0.0, 0]}, {'num_count': 763, 'sum_payoffs': 227.2176413962426, 'action': [1.0, 0]}])
Weights num count: [0.3060853769300636, 0.6930063578564941]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 681, 'sum_payoffs': 217.3274999510999, 'action': [1.0, 0]}, {'num_count': 419, 'sum_payoffs': 117.1687499736364, 'action': [0.0, 0]}])
Weights num count: [0.6185286103542235, 0.38056312443233425]
Actions to choose Agent 1: dict_values([{'num_count': 419, 'sum_payoffs': 117.32062497360224, 'action': [0.0, 0]}, {'num_count': 681, 'sum_payoffs': 217.47937495106572, 'action': [1.0, 0]}])
Weights num count: [0.38056312443233425, 0.6185286103542235]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.22100186347961426 s
