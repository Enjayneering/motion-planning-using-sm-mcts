Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 759, 'sum_payoffs': 238.3080631102297, 'action': [1.0, 0]}, {'num_count': 341, 'sum_payoffs': 84.25799998314841, 'action': [0.0, 0]}])
Weights num count: [0.6893732970027248, 0.3097184377838329]
Actions to choose Agent 1: dict_values([{'num_count': 762, 'sum_payoffs': 240.64086310976305, 'action': [1.0, 0]}, {'num_count': 338, 'sum_payoffs': 83.67479998326503, 'action': [0.0, 0]}])
Weights num count: [0.6920980926430518, 0.3069936421435059]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 696, 'sum_payoffs': 237.04199995259384, 'action': [1.0, 0]}, {'num_count': 404, 'sum_payoffs': 119.73599997605223, 'action': [0.0, 0]}])
Weights num count: [0.6321525885558583, 0.36693914623069934]
Actions to choose Agent 1: dict_values([{'num_count': 402, 'sum_payoffs': 118.92599997621426, 'action': [0.0, 0]}, {'num_count': 698, 'sum_payoffs': 238.1759999523671, 'action': [1.0, 0]}])
Weights num count: [0.3651226158038147, 0.633969118982743]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2572953701019287 s
