Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 342, 'sum_payoffs': 84.39707366733109, 'action': [0.0, 0]}, {'num_count': 758, 'sum_payoffs': 237.69066311035314, 'action': [1.0, 0]}])
Weights num count: [0.3106267029972752, 0.6884650317892824]
Actions to choose Agent 1: dict_values([{'num_count': 350, 'sum_payoffs': 87.50993682460329, 'action': [0.0, 0]}, {'num_count': 750, 'sum_payoffs': 235.7441999528478, 'action': [1.0, 0]}])
Weights num count: [0.3178928247048138, 0.6811989100817438]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 408, 'sum_payoffs': 121.3559999757283, 'action': [0.0, 0]}, {'num_count': 692, 'sum_payoffs': 235.58399995288528, 'action': [1.0, 0]}])
Weights num count: [0.37057220708446864, 0.628519527702089]
Actions to choose Agent 1: dict_values([{'num_count': 412, 'sum_payoffs': 122.97599997540429, 'action': [0.0, 0]}, {'num_count': 688, 'sum_payoffs': 234.28799995314446, 'action': [1.0, 0]}])
Weights num count: [0.37420526793823794, 0.6248864668483197]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.23026633262634277 s
