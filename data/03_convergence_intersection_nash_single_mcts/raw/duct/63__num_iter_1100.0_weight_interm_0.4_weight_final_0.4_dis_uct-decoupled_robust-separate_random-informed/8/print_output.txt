Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 374, 'sum_payoffs': 56.42099998730511, 'action': [0.0, 0]}, {'num_count': 726, 'sum_payoffs': 149.16653286117548, 'action': [1.0, 0]}])
Weights num count: [0.33969118982742963, 0.659400544959128]
Actions to choose Agent 1: dict_values([{'num_count': 376, 'sum_payoffs': 57.33224998710012, 'action': [0.0, 0]}, {'num_count': 724, 'sum_payoffs': 149.53103286109345, 'action': [1.0, 0]}])
Weights num count: [0.34150772025431425, 0.6575840145322435]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 443, 'sum_payoffs': 81.75374998160598, 'action': [0.0, 0]}, {'num_count': 657, 'sum_payoffs': 142.14374996801797, 'action': [1.0, 0]}])
Weights num count: [0.40236148955495005, 0.5967302452316077]
Actions to choose Agent 1: dict_values([{'num_count': 445, 'sum_payoffs': 82.46249998144651, 'action': [0.0, 0]}, {'num_count': 655, 'sum_payoffs': 141.83999996808632, 'action': [1.0, 0]}])
Weights num count: [0.40417801998183467, 0.594913714804723]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2614471912384033 s
