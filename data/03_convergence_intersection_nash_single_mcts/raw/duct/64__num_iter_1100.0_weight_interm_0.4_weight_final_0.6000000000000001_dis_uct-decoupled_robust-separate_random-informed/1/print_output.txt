Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 741, 'sum_payoffs': 179.51217627988862, 'action': [1.0, 0]}, {'num_count': 359, 'sum_payoffs': 65.18429998696291, 'action': [0.0, 0]}])
Weights num count: [0.6730245231607629, 0.3260672116257947]
Actions to choose Agent 1: dict_values([{'num_count': 742, 'sum_payoffs': 180.1682762797574, 'action': [1.0, 0]}, {'num_count': 358, 'sum_payoffs': 65.18429998696296, 'action': [0.0, 0]}])
Weights num count: [0.6739327883742052, 0.32515894641235243]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 548, 'sum_payoffs': 133.52399997329363, 'action': [0.0, 0]}, {'num_count': 552, 'sum_payoffs': 134.8559999730272, 'action': [1.0, 0]}])
Weights num count: [0.4977293369663942, 0.5013623978201635]
Actions to choose Agent 1: dict_values([{'num_count': 552, 'sum_payoffs': 134.8559999730272, 'action': [1.0, 0]}, {'num_count': 548, 'sum_payoffs': 133.52399997329363, 'action': [0.0, 0]}])
Weights num count: [0.5013623978201635, 0.4977293369663942]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.25241899490356445 s
