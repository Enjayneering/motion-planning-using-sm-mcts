Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 356, 'sum_payoffs': 64.57637367129496, 'action': [0.0, 0]}, {'num_count': 744, 'sum_payoffs': 180.27762627973564, 'action': [1.0, 0]}])
Weights num count: [0.32334241598546776, 0.6757493188010899]
Actions to choose Agent 1: dict_values([{'num_count': 365, 'sum_payoffs': 67.12337367078565, 'action': [0.0, 0]}, {'num_count': 735, 'sum_payoffs': 177.7306262802449, 'action': [1.0, 0]}])
Weights num count: [0.3315168029064487, 0.667574931880109]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 663, 'sum_payoffs': 170.5769999658825, 'action': [1.0, 0]}, {'num_count': 437, 'sum_payoffs': 97.68149998046292, 'action': [0.0, 0]}])
Weights num count: [0.6021798365122616, 0.39691189827429607]
Actions to choose Agent 1: dict_values([{'num_count': 440, 'sum_payoffs': 98.65349998026849, 'action': [0.0, 0]}, {'num_count': 660, 'sum_payoffs': 169.84799996602834, 'action': [1.0, 0]}])
Weights num count: [0.3996366939146231, 0.5994550408719346]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.22484159469604492 s
