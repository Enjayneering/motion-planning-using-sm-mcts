Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 754, 'sum_payoffs': 182.6873762792539, 'action': [1.0, 0]}, {'num_count': 346, 'sum_payoffs': 61.29179998774141, 'action': [0.0, 0]}])
Weights num count: [0.6848319709355132, 0.3142597638510445]
Actions to choose Agent 1: dict_values([{'num_count': 350, 'sum_payoffs': 62.49464998750079, 'action': [0.0, 0]}, {'num_count': 750, 'sum_payoffs': 181.70322627945058, 'action': [1.0, 0]}])
Weights num count: [0.3178928247048138, 0.6811989100817438]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 441, 'sum_payoffs': 99.01799998019565, 'action': [0.0, 0]}, {'num_count': 659, 'sum_payoffs': 169.36199996612552, 'action': [1.0, 0]}])
Weights num count: [0.40054495912806537, 0.5985467756584922]
Actions to choose Agent 1: dict_values([{'num_count': 441, 'sum_payoffs': 99.13949998017132, 'action': [0.0, 0]}, {'num_count': 659, 'sum_payoffs': 169.48349996610122, 'action': [1.0, 0]}])
Weights num count: [0.40054495912806537, 0.5985467756584922]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2631716728210449 s
