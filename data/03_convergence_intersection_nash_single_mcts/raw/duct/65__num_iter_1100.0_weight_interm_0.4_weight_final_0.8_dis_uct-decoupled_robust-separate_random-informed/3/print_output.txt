Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 346, 'sum_payoffs': 69.44699998726789, 'action': [0.0, 0]}, {'num_count': 754, 'sum_payoffs': 200.47160522640542, 'action': [1.0, 0]}])
Weights num count: [0.3142597638510445, 0.6848319709355132]
Actions to choose Agent 1: dict_values([{'num_count': 351, 'sum_payoffs': 71.51249998688924, 'action': [0.0, 0]}, {'num_count': 749, 'sum_payoffs': 200.1071052264722, 'action': [1.0, 0]}])
Weights num count: [0.3188010899182561, 0.6802906448683016]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 699, 'sum_payoffs': 200.66999996321056, 'action': [1.0, 0]}, {'num_count': 401, 'sum_payoffs': 96.95999998222388, 'action': [0.0, 0]}])
Weights num count: [0.6348773841961853, 0.3642143505903724]
Actions to choose Agent 1: dict_values([{'num_count': 406, 'sum_payoffs': 98.84999998187737, 'action': [0.0, 0]}, {'num_count': 694, 'sum_payoffs': 199.31999996345806, 'action': [1.0, 0]}])
Weights num count: [0.368755676657584, 0.6303360581289736]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.21945452690124512 s
