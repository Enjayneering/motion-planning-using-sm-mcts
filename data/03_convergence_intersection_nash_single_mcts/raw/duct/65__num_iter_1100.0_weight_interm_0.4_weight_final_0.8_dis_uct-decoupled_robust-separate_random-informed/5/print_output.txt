Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 358, 'sum_payoffs': 73.12949998659289, 'action': [0.0, 0]}, {'num_count': 742, 'sum_payoffs': 196.62749996395175, 'action': [1.0, 0]}])
Weights num count: [0.32515894641235243, 0.6739327883742052]
Actions to choose Agent 1: dict_values([{'num_count': 740, 'sum_payoffs': 196.50599996397386, 'action': [1.0, 0]}, {'num_count': 360, 'sum_payoffs': 73.97999998643691, 'action': [0.0, 0]}])
Weights num count: [0.6721162579473207, 0.32697547683923706]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 441, 'sum_payoffs': 110.72999997969936, 'action': [0.0, 0]}, {'num_count': 659, 'sum_payoffs': 186.899999965735, 'action': [1.0, 0]}])
Weights num count: [0.40054495912806537, 0.5985467756584922]
Actions to choose Agent 1: dict_values([{'num_count': 662, 'sum_payoffs': 188.24999996548752, 'action': [1.0, 0]}, {'num_count': 438, 'sum_payoffs': 109.91999997984786, 'action': [0.0, 0]}])
Weights num count: [0.6012715712988193, 0.3978201634877384]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.22635602951049805 s
