Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 405, 'sum_payoffs': 49.715810516372635, 'action': [0.0, 0]}, {'num_count': 695, 'sum_payoffs': 116.00208945048239, 'action': [1.0, 0]}])
Weights num count: [0.3678474114441417, 0.631244323342416]
Actions to choose Agent 1: dict_values([{'num_count': 395, 'sum_payoffs': 47.91761051673225, 'action': [0.0, 0]}, {'num_count': 705, 'sum_payoffs': 119.11248944986025, 'action': [1.0, 0]}])
Weights num count: [0.35876475930971846, 0.6403269754768393]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 600, 'sum_payoffs': 102.00599997959928, 'action': [1.0, 0]}, {'num_count': 500, 'sum_payoffs': 77.57099998448591, 'action': [0.0, 0]}])
Weights num count: [0.5449591280653951, 0.45413260672116257]
Actions to choose Agent 1: dict_values([{'num_count': 500, 'sum_payoffs': 77.8949999844211, 'action': [0.0, 0]}, {'num_count': 600, 'sum_payoffs': 102.16799997956686, 'action': [1.0, 0]}])
Weights num count: [0.45413260672116257, 0.5449591280653951]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2562530040740967 s
