Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 706, 'sum_payoffs': 143.93190786834867, 'action': [1.0, 0]}, {'num_count': 394, 'sum_payoffs': 61.57859209397352, 'action': [0.0, 0]}])
Weights num count: [0.6412352406902816, 0.3578564940962761]
Actions to choose Agent 1: dict_values([{'num_count': 714, 'sum_payoffs': 146.52390786787348, 'action': [1.0, 0]}, {'num_count': 386, 'sum_payoffs': 59.715592094315085, 'action': [0.0, 0]}])
Weights num count: [0.6485013623978202, 0.3505903723887375]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 673, 'sum_payoffs': 146.69999997310617, 'action': [1.0, 0]}, {'num_count': 427, 'sum_payoffs': 77.29874998582898, 'action': [0.0, 0]}])
Weights num count: [0.6112624886466849, 0.38782924613987285]
Actions to choose Agent 1: dict_values([{'num_count': 672, 'sum_payoffs': 146.4974999731433, 'action': [1.0, 0]}, {'num_count': 428, 'sum_payoffs': 77.70374998575473, 'action': [0.0, 0]}])
Weights num count: [0.6103542234332425, 0.3887375113533152]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.23340630531311035 s
