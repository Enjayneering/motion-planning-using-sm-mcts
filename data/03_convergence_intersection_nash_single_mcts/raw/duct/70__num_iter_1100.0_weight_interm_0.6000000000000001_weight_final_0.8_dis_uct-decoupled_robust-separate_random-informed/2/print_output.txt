Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 378, 'sum_payoffs': 67.00628570279919, 'action': [0.0, 0]}, {'num_count': 722, 'sum_payoffs': 166.46799245266476, 'action': [1.0, 0]}])
Weights num count: [0.34332425068119893, 0.6557674841053588]
Actions to choose Agent 1: dict_values([{'num_count': 380, 'sum_payoffs': 68.25599998829922, 'action': [0.0, 0]}, {'num_count': 720, 'sum_payoffs': 167.0928495954148, 'action': [1.0, 0]}])
Weights num count: [0.34514078110808355, 0.6539509536784741]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 392, 'sum_payoffs': 78.83999998648474, 'action': [0.0, 0]}, {'num_count': 708, 'sum_payoffs': 176.65714282686076, 'action': [1.0, 0]}])
Weights num count: [0.35603996366939145, 0.6430517711171662]
Actions to choose Agent 1: dict_values([{'num_count': 709, 'sum_payoffs': 177.23571425533302, 'action': [1.0, 0]}, {'num_count': 391, 'sum_payoffs': 78.72428570079029, 'action': [0.0, 0]}])
Weights num count: [0.6439600363306085, 0.35513169845594916]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.212019681930542 s
