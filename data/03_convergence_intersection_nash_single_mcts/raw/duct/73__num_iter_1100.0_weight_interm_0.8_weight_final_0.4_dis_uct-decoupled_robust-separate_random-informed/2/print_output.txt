Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 410, 'sum_payoffs': 41.154749992454896, 'action': [0.0, 0]}, {'num_count': 690, 'sum_payoffs': 98.59796050823965, 'action': [1.0, 0]}])
Weights num count: [0.3723887375113533, 0.6267029972752044]
Actions to choose Agent 1: dict_values([{'num_count': 686, 'sum_payoffs': 97.8689605083733, 'action': [1.0, 0]}, {'num_count': 414, 'sum_payoffs': 42.12674999227668, 'action': [0.0, 0]}])
Weights num count: [0.623069936421435, 0.3760217983651226]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 433, 'sum_payoffs': 49.87499999085618, 'action': [0.0, 0]}, {'num_count': 667, 'sum_payoffs': 100.22249998162607, 'action': [1.0, 0]}])
Weights num count: [0.3932788374205268, 0.6058128973660308]
Actions to choose Agent 1: dict_values([{'num_count': 432, 'sum_payoffs': 49.80749999086856, 'action': [0.0, 0]}, {'num_count': 668, 'sum_payoffs': 100.5599999815642, 'action': [1.0, 0]}])
Weights num count: [0.3923705722070845, 0.6067211625794732]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.23237919807434082 s
