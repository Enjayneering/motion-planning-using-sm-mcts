Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 420, 'sum_payoffs': 43.873499991956464, 'action': [0.0, 0]}, {'num_count': 680, 'sum_payoffs': 97.55771050843013, 'action': [1.0, 0]}])
Weights num count: [0.3814713896457766, 0.6176203451407811]
Actions to choose Agent 1: dict_values([{'num_count': 424, 'sum_payoffs': 44.8454999917782, 'action': [0.0, 0]}, {'num_count': 676, 'sum_payoffs': 96.95021050854139, 'action': [1.0, 0]}])
Weights num count: [0.3851044504995459, 0.6139872842870118]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 453, 'sum_payoffs': 54.2624999900518, 'action': [0.0, 0]}, {'num_count': 647, 'sum_payoffs': 96.0374999823932, 'action': [1.0, 0]}])
Weights num count: [0.4114441416893733, 0.5876475930971844]
Actions to choose Agent 1: dict_values([{'num_count': 646, 'sum_payoffs': 95.90249998241795, 'action': [1.0, 0]}, {'num_count': 454, 'sum_payoffs': 54.39749999002705, 'action': [0.0, 0]}])
Weights num count: [0.5867393278837421, 0.4123524069028156]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.20508575439453125 s
