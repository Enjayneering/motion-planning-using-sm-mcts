Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 388, 'sum_payoffs': 59.79262499028349, 'action': [0.0, 0]}, {'num_count': 712, 'sum_payoffs': 145.1165328711549, 'action': [1.0, 0]}])
Weights num count: [0.35240690281562215, 0.6466848319709355]
Actions to choose Agent 1: dict_values([{'num_count': 715, 'sum_payoffs': 146.70615787089656, 'action': [1.0, 0]}, {'num_count': 385, 'sum_payoffs': 59.478749990334514, 'action': [0.0, 0]}])
Weights num count: [0.6494096276112625, 0.3496821071752952]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 675, 'sum_payoffs': 147.20624997607834, 'action': [1.0, 0]}, {'num_count': 425, 'sum_payoffs': 76.79249998752131, 'action': [0.0, 0]}])
Weights num count: [0.6130790190735694, 0.3860127157129882]
Actions to choose Agent 1: dict_values([{'num_count': 674, 'sum_payoffs': 147.20624997607834, 'action': [1.0, 0]}, {'num_count': 426, 'sum_payoffs': 77.1974999874555, 'action': [0.0, 0]}])
Weights num count: [0.6121707538601272, 0.3869209809264305]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.20664143562316895 s
