Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 379, 'sum_payoffs': 57.65062499063147, 'action': [0.0, 0]}, {'num_count': 721, 'sum_payoffs': 148.25528287064535, 'action': [1.0, 0]}])
Weights num count: [0.3442325158946412, 0.6548592188919165]
Actions to choose Agent 1: dict_values([{'num_count': 383, 'sum_payoffs': 59.05799999040282, 'action': [0.0, 0]}, {'num_count': 717, 'sum_payoffs': 147.21240787081473, 'action': [1.0, 0]}])
Weights num count: [0.3478655767484105, 0.6512261580381471]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 480, 'sum_payoffs': 92.18249998502004, 'action': [0.0, 0]}, {'num_count': 620, 'sum_payoffs': 131.7149999785952, 'action': [1.0, 0]}])
Weights num count: [0.4359673024523161, 0.5631244323342416]
Actions to choose Agent 1: dict_values([{'num_count': 482, 'sum_payoffs': 92.78999998492131, 'action': [0.0, 0]}, {'num_count': 618, 'sum_payoffs': 131.30999997866098, 'action': [1.0, 0]}])
Weights num count: [0.43778383287920075, 0.5613079019073569]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2047255039215088 s
