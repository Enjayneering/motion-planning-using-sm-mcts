Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 404, 'sum_payoffs': 122.65424992640739, 'action': [0.0, 0]}, {'num_count': 1196, 'sum_payoffs': 459.45224972431896, 'action': [1.0, 0]}])
Weights num count: [0.2523422860712055, 0.7470331043098064]
Actions to choose Agent 1: dict_values([{'num_count': 1190, 'sum_payoffs': 458.72324972475644, 'action': [1.0, 0]}, {'num_count': 410, 'sum_payoffs': 125.93474992443925, 'action': [0.0, 0]}])
Weights num count: [0.7432854465958776, 0.2560899437851343]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1017, 'sum_payoffs': 426.059999744371, 'action': [1.0, 0]}, {'num_count': 583, 'sum_payoffs': 221.73749986695663, 'action': [0.0, 0]}])
Weights num count: [0.6352279825109307, 0.3641474078700812]
Actions to choose Agent 1: dict_values([{'num_count': 586, 'sum_payoffs': 223.35749986598464, 'action': [0.0, 0]}, {'num_count': 1014, 'sum_payoffs': 425.24999974485695, 'action': [1.0, 0]}])
Weights num count: [0.3660212367270456, 0.6333541536539663]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2715582847595215 s
