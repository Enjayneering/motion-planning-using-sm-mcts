Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 429, 'sum_payoffs': 133.0424999201745, 'action': [0.0, 0]}, {'num_count': 1171, 'sum_payoffs': 449.42849973033367, 'action': [1.0, 0]}])
Weights num count: [0.2679575265459088, 0.7314178638351031]
Actions to choose Agent 1: dict_values([{'num_count': 1154, 'sum_payoffs': 443.7787497337235, 'action': [1.0, 0]}, {'num_count': 446, 'sum_payoffs': 140.8792499154722, 'action': [0.0, 0]}])
Weights num count: [0.7207995003123048, 0.27857589006870703]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1027, 'sum_payoffs': 430.717499741577, 'action': [1.0, 0]}, {'num_count': 573, 'sum_payoffs': 217.0799998697511, 'action': [0.0, 0]}])
Weights num count: [0.641474078700812, 0.3579013116801999]
Actions to choose Agent 1: dict_values([{'num_count': 1026, 'sum_payoffs': 430.51499974169855, 'action': [1.0, 0]}, {'num_count': 574, 'sum_payoffs': 217.68749986938658, 'action': [0.0, 0]}])
Weights num count: [0.6408494690818238, 0.35852592129918803]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2704613208770752 s
