Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1167, 'sum_payoffs': 447.2414998434612, 'action': [1.0, 0]}, {'num_count': 433, 'sum_payoffs': 134.8649999527968, 'action': [0.0, 0]}])
Weights num count: [0.7289194253591506, 0.2704559650218613]
Actions to choose Agent 1: dict_values([{'num_count': 440, 'sum_payoffs': 138.5099999515212, 'action': [0.0, 0]}, {'num_count': 1160, 'sum_payoffs': 446.51249984371657, 'action': [1.0, 0]}])
Weights num count: [0.27482823235477827, 0.7245471580262336]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1030, 'sum_payoffs': 431.93249984883107, 'action': [1.0, 0]}, {'num_count': 570, 'sum_payoffs': 215.66249992452015, 'action': [0.0, 0]}])
Weights num count: [0.6433479075577764, 0.3560274828232355]
Actions to choose Agent 1: dict_values([{'num_count': 1026, 'sum_payoffs': 430.5149998493271, 'action': [1.0, 0]}, {'num_count': 574, 'sum_payoffs': 217.48499992388233, 'action': [0.0, 0]}])
Weights num count: [0.6408494690818238, 0.35852592129918803]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2832348346710205 s
