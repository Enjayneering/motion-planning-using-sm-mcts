Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1174, 'sum_payoffs': 450.7042498985966, 'action': [1.0, 0]}, {'num_count': 426, 'sum_payoffs': 132.13124997027012, 'action': [0.0, 0]}])
Weights num count: [0.7332916926920675, 0.2660836976889444]
Actions to choose Agent 1: dict_values([{'num_count': 1165, 'sum_payoffs': 447.97049989921186, 'action': [1.0, 0]}, {'num_count': 435, 'sum_payoffs': 136.32299996932704, 'action': [0.0, 0]}])
Weights num count: [0.7276702061211743, 0.2717051842598376]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 567, 'sum_payoffs': 214.24499995179636, 'action': [0.0, 0]}, {'num_count': 1033, 'sum_payoffs': 433.5524999024439, 'action': [1.0, 0]}])
Weights num count: [0.3541536539662711, 0.6452217364147408]
Actions to choose Agent 1: dict_values([{'num_count': 575, 'sum_payoffs': 218.29499995088514, 'action': [0.0, 0]}, {'num_count': 1025, 'sum_payoffs': 430.31249990317303, 'action': [1.0, 0]}])
Weights num count: [0.3591505309181761, 0.6402248594628357]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.35764193534851074 s
