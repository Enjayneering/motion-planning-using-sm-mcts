Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 531, 'sum_payoffs': 80.88749997168944, 'action': [0.0, 0]}, {'num_count': 1069, 'sum_payoffs': 215.55615781929035, 'action': [1.0, 0]}])
Weights num count: [0.3316677076826983, 0.6677076826983136]
Actions to choose Agent 1: dict_values([{'num_count': 524, 'sum_payoffs': 79.48012497218203, 'action': [0.0, 0]}, {'num_count': 1076, 'sum_payoffs': 218.0570328184151, 'action': [1.0, 0]}])
Weights num count: [0.32729544034978136, 0.6720799500312304]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 646, 'sum_payoffs': 121.03874995763795, 'action': [0.0, 0]}, {'num_count': 954, 'sum_payoffs': 204.20999992852953, 'action': [1.0, 0]}])
Weights num count: [0.40349781386633354, 0.5958775765146783]
Actions to choose Agent 1: dict_values([{'num_count': 952, 'sum_payoffs': 203.80499992867126, 'action': [1.0, 0]}, {'num_count': 648, 'sum_payoffs': 121.64624995742534, 'action': [0.0, 0]}])
Weights num count: [0.594628357276702, 0.4047470331043098]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2955145835876465 s
