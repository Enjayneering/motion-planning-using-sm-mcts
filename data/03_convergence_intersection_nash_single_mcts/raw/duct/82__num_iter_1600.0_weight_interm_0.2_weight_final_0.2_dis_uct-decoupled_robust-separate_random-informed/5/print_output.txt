Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1052, 'sum_payoffs': 212.01187492579473, 'action': [1.0, 0]}, {'num_count': 548, 'sum_payoffs': 85.34812497012828, 'action': [0.0, 0]}])
Weights num count: [0.6570893191755153, 0.3422860712054966]
Actions to choose Agent 1: dict_values([{'num_count': 554, 'sum_payoffs': 86.89162496958804, 'action': [0.0, 0]}, {'num_count': 1046, 'sum_payoffs': 210.650624926271, 'action': [1.0, 0]}])
Weights num count: [0.34603372891942535, 0.6533416614615865]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 995, 'sum_payoffs': 215.1449999247024, 'action': [1.0, 0]}, {'num_count': 605, 'sum_payoffs': 110.00249996150019, 'action': [0.0, 0]}])
Weights num count: [0.6214865708931917, 0.3778888194878201]
Actions to choose Agent 1: dict_values([{'num_count': 993, 'sum_payoffs': 214.8412499248087, 'action': [1.0, 0]}, {'num_count': 607, 'sum_payoffs': 110.71124996125215, 'action': [0.0, 0]}])
Weights num count: [0.6202373516552155, 0.3791380387257964]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3127021789550781 s
