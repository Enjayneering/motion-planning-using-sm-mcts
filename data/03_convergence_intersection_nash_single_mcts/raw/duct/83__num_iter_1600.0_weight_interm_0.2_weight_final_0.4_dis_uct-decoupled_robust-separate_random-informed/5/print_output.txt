Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 485, 'sum_payoffs': 98.25189471064068, 'action': [0.0, 0]}, {'num_count': 1115, 'sum_payoffs': 292.3631051851903, 'action': [1.0, 0]}])
Weights num count: [0.3029356652092442, 0.6964397251717677]
Actions to choose Agent 1: dict_values([{'num_count': 1116, 'sum_payoffs': 294.3641051846565, 'action': [1.0, 0]}, {'num_count': 484, 'sum_payoffs': 98.68089471052642, 'action': [0.0, 0]}])
Weights num count: [0.6970643347907558, 0.3023110555902561]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 590, 'sum_payoffs': 145.69499996114757, 'action': [0.0, 0]}, {'num_count': 1010, 'sum_payoffs': 287.0699999234462, 'action': [1.0, 0]}])
Weights num count: [0.3685196752029981, 0.6308557151780138]
Actions to choose Agent 1: dict_values([{'num_count': 1009, 'sum_payoffs': 286.93499992348217, 'action': [1.0, 0]}, {'num_count': 591, 'sum_payoffs': 146.09999996103957, 'action': [0.0, 0]}])
Weights num count: [0.6302311055590256, 0.36914428482198625]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3065757751464844 s
