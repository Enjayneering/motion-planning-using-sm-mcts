Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 479, 'sum_payoffs': 110.8659374750556, 'action': [0.0, 0]}, {'num_count': 1121, 'sum_payoffs': 327.52882887366957, 'action': [1.0, 0]}])
Weights num count: [0.29918800749531543, 0.7001873828856965]
Actions to choose Agent 1: dict_values([{'num_count': 484, 'sum_payoffs': 113.17218747453676, 'action': [0.0, 0]}, {'num_count': 1116, 'sum_payoffs': 327.40957887369643, 'action': [1.0, 0]}])
Weights num count: [0.3023110555902561, 0.6970643347907558]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1013, 'sum_payoffs': 321.8174999275892, 'action': [1.0, 0]}, {'num_count': 587, 'sum_payoffs': 164.2499999630427, 'action': [0.0, 0]}])
Weights num count: [0.6327295440349782, 0.36664584634603375]
Actions to choose Agent 1: dict_values([{'num_count': 591, 'sum_payoffs': 166.07249996263263, 'action': [0.0, 0]}, {'num_count': 1009, 'sum_payoffs': 320.90624992779425, 'action': [1.0, 0]}])
Weights num count: [0.36914428482198625, 0.6302311055590256]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3296656608581543 s
