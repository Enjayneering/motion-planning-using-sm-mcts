Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 451, 'sum_payoffs': 102.20118747700509, 'action': [0.0, 0]}, {'num_count': 1149, 'sum_payoffs': 337.9345163713281, 'action': [1.0, 0]}])
Weights num count: [0.2816989381636477, 0.7176764522173642]
Actions to choose Agent 1: dict_values([{'num_count': 1148, 'sum_payoffs': 338.20789137126656, 'action': [1.0, 0]}, {'num_count': 452, 'sum_payoffs': 102.7479374768823, 'action': [0.0, 0]}])
Weights num count: [0.7170518425983761, 0.28232354778263585]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 983, 'sum_payoffs': 310.8824999300497, 'action': [1.0, 0]}, {'num_count': 617, 'sum_payoffs': 175.33687496054807, 'action': [0.0, 0]}])
Weights num count: [0.6139912554653342, 0.3853841349156777]
Actions to choose Agent 1: dict_values([{'num_count': 977, 'sum_payoffs': 309.0599999304597, 'action': [1.0, 0]}, {'num_count': 623, 'sum_payoffs': 177.7668749600013, 'action': [0.0, 0]}])
Weights num count: [0.6102435977514054, 0.3891317926296065]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.299816370010376 s
