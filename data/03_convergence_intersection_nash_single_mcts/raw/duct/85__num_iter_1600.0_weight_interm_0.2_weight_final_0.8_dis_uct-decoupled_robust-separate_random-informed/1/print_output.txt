Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 457, 'sum_payoffs': 111.97799997760416, 'action': [0.0, 0]}, {'num_count': 1143, 'sum_payoffs': 356.11446308667263, 'action': [1.0, 0]}])
Weights num count: [0.28544659587757654, 0.7139287945034354]
Actions to choose Agent 1: dict_values([{'num_count': 463, 'sum_payoffs': 114.74819997705029, 'action': [0.0, 0]}, {'num_count': 1137, 'sum_payoffs': 355.38546308681856, 'action': [1.0, 0]}])
Weights num count: [0.2891942535915053, 0.7101811367895066]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 987, 'sum_payoffs': 332.45999993351296, 'action': [1.0, 0]}, {'num_count': 613, 'sum_payoffs': 186.31799996273747, 'action': [0.0, 0]}])
Weights num count: [0.6164896939412867, 0.3828856964397252]
Actions to choose Agent 1: dict_values([{'num_count': 611, 'sum_payoffs': 185.66999996286705, 'action': [0.0, 0]}, {'num_count': 989, 'sum_payoffs': 333.43199993331865, 'action': [1.0, 0]}])
Weights num count: [0.3816364772017489, 0.617738913179263]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3001425266265869 s
