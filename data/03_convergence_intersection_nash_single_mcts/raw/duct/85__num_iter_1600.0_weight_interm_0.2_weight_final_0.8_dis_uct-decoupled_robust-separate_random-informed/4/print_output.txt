Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1139, 'sum_payoffs': 354.0570630870843, 'action': [1.0, 0]}, {'num_count': 461, 'sum_payoffs': 113.1263999773744, 'action': [0.0, 0]}])
Weights num count: [0.7114303560274828, 0.28794503435352903]
Actions to choose Agent 1: dict_values([{'num_count': 1136, 'sum_payoffs': 355.239663086848, 'action': [1.0, 0]}, {'num_count': 464, 'sum_payoffs': 115.15139997696939, 'action': [0.0, 0]}])
Weights num count: [0.7095565271705184, 0.28981886321049344]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1020, 'sum_payoffs': 345.25799993095336, 'action': [1.0, 0]}, {'num_count': 580, 'sum_payoffs': 173.51999996529676, 'action': [0.0, 0]}])
Weights num count: [0.637101811367895, 0.3622735790131168]
Actions to choose Agent 1: dict_values([{'num_count': 580, 'sum_payoffs': 173.68199996526437, 'action': [0.0, 0]}, {'num_count': 1020, 'sum_payoffs': 345.41999993092094, 'action': [1.0, 0]}])
Weights num count: [0.3622735790131168, 0.637101811367895]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.30612683296203613 s
