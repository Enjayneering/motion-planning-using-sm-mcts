Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 456, 'sum_payoffs': 111.6011368197848, 'action': [0.0, 0]}, {'num_count': 1144, 'sum_payoffs': 356.38986308661765, 'action': [1.0, 0]}])
Weights num count: [0.2848219862585884, 0.7145534041224235]
Actions to choose Agent 1: dict_values([{'num_count': 1125, 'sum_payoffs': 350.78039992984475, 'action': [1.0, 0]}, {'num_count': 475, 'sum_payoffs': 118.66859997626624, 'action': [0.0, 0]}])
Weights num count: [0.7026858213616489, 0.2966895690193629]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 610, 'sum_payoffs': 185.02199996299635, 'action': [0.0, 0]}, {'num_count': 990, 'sum_payoffs': 333.4319999333187, 'action': [1.0, 0]}])
Weights num count: [0.3810118675827608, 0.6183635227982511]
Actions to choose Agent 1: dict_values([{'num_count': 613, 'sum_payoffs': 186.3179999627372, 'action': [0.0, 0]}, {'num_count': 987, 'sum_payoffs': 332.7839999334483, 'action': [1.0, 0]}])
Weights num count: [0.3828856964397252, 0.6164896939412867]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.31078004837036133 s
