Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 581, 'sum_payoffs': 58.23753945815355, 'action': [0.0, 0]}, {'num_count': 1019, 'sum_payoffs': 141.9922499621334, 'action': [1.0, 0]}])
Weights num count: [0.36289818863210493, 0.636477201748907]
Actions to choose Agent 1: dict_values([{'num_count': 589, 'sum_payoffs': 60.1807894576353, 'action': [0.0, 0]}, {'num_count': 1011, 'sum_payoffs': 141.14249996235998, 'action': [1.0, 0]}])
Weights num count: [0.36789506558401, 0.6314803247970019]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 760, 'sum_payoffs': 100.76999997312703, 'action': [0.0, 0]}, {'num_count': 840, 'sum_payoffs': 117.02999996879043, 'action': [1.0, 0]}])
Weights num count: [0.47470331043098063, 0.5246720799500312]
Actions to choose Agent 1: dict_values([{'num_count': 840, 'sum_payoffs': 117.09749996877242, 'action': [1.0, 0]}, {'num_count': 760, 'sum_payoffs': 100.83749997310903, 'action': [0.0, 0]}])
Weights num count: [0.5246720799500312, 0.47470331043098063]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2892730236053467 s
