Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 518, 'sum_payoffs': 95.78117366505391, 'action': [0.0, 0]}, {'num_count': 1082, 'sum_payoffs': 256.51079994870213, 'action': [1.0, 0]}])
Weights num count: [0.3235477826358526, 0.6758276077451593]
Actions to choose Agent 1: dict_values([{'num_count': 515, 'sum_payoffs': 95.63987366508226, 'action': [0.0, 0]}, {'num_count': 1085, 'sum_payoffs': 258.6203999482801, 'action': [1.0, 0]}])
Weights num count: [0.3216739537788882, 0.6777014366021237]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 614, 'sum_payoffs': 137.16899997256454, 'action': [0.0, 0]}, {'num_count': 986, 'sum_payoffs': 252.46799994950223, 'action': [1.0, 0]}])
Weights num count: [0.3835103060587133, 0.6158650843222986]
Actions to choose Agent 1: dict_values([{'num_count': 617, 'sum_payoffs': 138.3839999723215, 'action': [0.0, 0]}, {'num_count': 983, 'sum_payoffs': 251.98199994959944, 'action': [1.0, 0]}])
Weights num count: [0.3853841349156777, 0.6139912554653342]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.26840782165527344 s
