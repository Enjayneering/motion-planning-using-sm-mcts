Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 592, 'sum_payoffs': 74.87909998502366, 'action': [0.0, 0]}, {'num_count': 1008, 'sum_payoffs': 164.66219996706744, 'action': [1.0, 0]}])
Weights num count: [0.3697688944409744, 0.6296064959400375]
Actions to choose Agent 1: dict_values([{'num_count': 588, 'sum_payoffs': 74.32019998513539, 'action': [0.0, 0]}, {'num_count': 1012, 'sum_payoffs': 165.95009996680977, 'action': [1.0, 0]}])
Weights num count: [0.36727045596502184, 0.63210493441599]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 653, 'sum_payoffs': 96.52499998069571, 'action': [0.0, 0]}, {'num_count': 947, 'sum_payoffs': 164.13299996717546, 'action': [1.0, 0]}])
Weights num count: [0.40787008119925044, 0.5915053091817614]
Actions to choose Agent 1: dict_values([{'num_count': 654, 'sum_payoffs': 96.84899998063092, 'action': [0.0, 0]}, {'num_count': 946, 'sum_payoffs': 163.97099996720786, 'action': [1.0, 0]}])
Weights num count: [0.4084946908182386, 0.5908806995627732]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2711362838745117 s
