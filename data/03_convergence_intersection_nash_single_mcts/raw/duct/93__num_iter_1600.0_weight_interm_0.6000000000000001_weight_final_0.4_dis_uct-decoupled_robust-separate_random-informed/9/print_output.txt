Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 565, 'sum_payoffs': 69.33869998613194, 'action': [0.0, 0]}, {'num_count': 1035, 'sum_payoffs': 170.79858943952436, 'action': [1.0, 0]}])
Weights num count: [0.3529044347282948, 0.6464709556527171]
Actions to choose Agent 1: dict_values([{'num_count': 1029, 'sum_payoffs': 170.14248943965546, 'action': [1.0, 0]}, {'num_count': 571, 'sum_payoffs': 71.01539998579655, 'action': [0.0, 0]}])
Weights num count: [0.6427232979387882, 0.3566520924422236]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 622, 'sum_payoffs': 89.47799998210469, 'action': [0.0, 0]}, {'num_count': 978, 'sum_payoffs': 171.1799999657662, 'action': [1.0, 0]}])
Weights num count: [0.3885071830106184, 0.6108682073703935]
Actions to choose Agent 1: dict_values([{'num_count': 622, 'sum_payoffs': 89.55899998208847, 'action': [0.0, 0]}, {'num_count': 978, 'sum_payoffs': 171.26099996575002, 'action': [1.0, 0]}])
Weights num count: [0.3885071830106184, 0.6108682073703935]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.31374692916870117 s
