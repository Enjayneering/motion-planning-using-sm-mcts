Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1071, 'sum_payoffs': 215.69340785519202, 'action': [1.0, 0]}, {'num_count': 529, 'sum_payoffs': 80.20912498529468, 'action': [0.0, 0]}])
Weights num count: [0.6689569019362899, 0.33041848844472205]
Actions to choose Agent 1: dict_values([{'num_count': 525, 'sum_payoffs': 79.66799998539388, 'action': [0.0, 0]}, {'num_count': 1075, 'sum_payoffs': 217.69253285482552, 'action': [1.0, 0]}])
Weights num count: [0.3279200499687695, 0.6714553404122423]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 637, 'sum_payoffs': 118.6087499782564, 'action': [0.0, 0]}, {'num_count': 963, 'sum_payoffs': 206.639999962115, 'action': [1.0, 0]}])
Weights num count: [0.39787632729544037, 0.6014990630855716]
Actions to choose Agent 1: dict_values([{'num_count': 961, 'sum_payoffs': 206.23499996218925, 'action': [1.0, 0]}, {'num_count': 639, 'sum_payoffs': 119.21624997814504, 'action': [0.0, 0]}])
Weights num count: [0.6002498438475953, 0.39912554653341664]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3784751892089844 s
