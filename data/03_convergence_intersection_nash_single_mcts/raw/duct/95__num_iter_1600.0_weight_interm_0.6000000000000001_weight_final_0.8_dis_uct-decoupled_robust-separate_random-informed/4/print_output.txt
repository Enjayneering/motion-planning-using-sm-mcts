Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 518, 'sum_payoffs': 90.95914284155012, 'action': [0.0, 0]}, {'num_count': 1082, 'sum_payoffs': 246.34556386753997, 'action': [1.0, 0]}])
Weights num count: [0.3235477826358526, 0.6758276077451593]
Actions to choose Agent 1: dict_values([{'num_count': 523, 'sum_payoffs': 93.14614284117512, 'action': [0.0, 0]}, {'num_count': 1077, 'sum_payoffs': 246.86627815316484, 'action': [1.0, 0]}])
Weights num count: [0.3266708307307932, 0.6727045596502186]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 564, 'sum_payoffs': 115.28999998023701, 'action': [0.0, 0]}, {'num_count': 1036, 'sum_payoffs': 255.6899999561713, 'action': [1.0, 0]}])
Weights num count: [0.35227982510930667, 0.6470955652717052]
Actions to choose Agent 1: dict_values([{'num_count': 566, 'sum_payoffs': 116.09999998009816, 'action': [0.0, 0]}, {'num_count': 1034, 'sum_payoffs': 255.57428567047685, 'action': [1.0, 0]}])
Weights num count: [0.35352904434728294, 0.6458463460337289]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3087313175201416 s
