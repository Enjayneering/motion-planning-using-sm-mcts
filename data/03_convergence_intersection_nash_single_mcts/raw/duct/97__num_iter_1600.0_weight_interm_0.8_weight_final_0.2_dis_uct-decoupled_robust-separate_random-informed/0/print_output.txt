Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 652, 'sum_payoffs': 40.63679999187236, 'action': [0.0, 0]}, {'num_count': 948, 'sum_payoffs': 83.50690261487748, 'action': [1.0, 0]}])
Weights num count: [0.40724547158026236, 0.5921299188007495]
Actions to choose Agent 1: dict_values([{'num_count': 961, 'sum_payoffs': 85.57645261446366, 'action': [1.0, 0]}, {'num_count': 639, 'sum_payoffs': 38.93174999221339, 'action': [0.0, 0]}])
Weights num count: [0.6002498438475953, 0.39912554653341664]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 792, 'sum_payoffs': 64.70999998705884, 'action': [0.0, 0]}, {'num_count': 808, 'sum_payoffs': 67.04999998659089, 'action': [1.0, 0]}])
Weights num count: [0.4946908182386009, 0.504684572142411]
Actions to choose Agent 1: dict_values([{'num_count': 792, 'sum_payoffs': 64.75049998705073, 'action': [0.0, 0]}, {'num_count': 808, 'sum_payoffs': 67.0904999865828, 'action': [1.0, 0]}])
Weights num count: [0.4946908182386009, 0.504684572142411]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.31594204902648926 s
