Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 717, 'sum_payoffs': 50.57954998988357, 'action': [0.0, 0]}, {'num_count': 883, 'sum_payoffs': 74.787252616621, 'action': [1.0, 0]}])
Weights num count: [0.4478450968144909, 0.551530293566521]
Actions to choose Agent 1: dict_values([{'num_count': 718, 'sum_payoffs': 50.87114998982527, 'action': [0.0, 0]}, {'num_count': 882, 'sum_payoffs': 74.86015261660641, 'action': [1.0, 0]}])
Weights num count: [0.44846970643347905, 0.5509056839475328]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 693, 'sum_payoffs': 49.88699999002316, 'action': [0.0, 0]}, {'num_count': 907, 'sum_payoffs': 81.91349998361777, 'action': [1.0, 0]}])
Weights num count: [0.43285446595877575, 0.5665209244222361]
Actions to choose Agent 1: dict_values([{'num_count': 907, 'sum_payoffs': 81.95399998360968, 'action': [1.0, 0]}, {'num_count': 693, 'sum_payoffs': 49.84649999003126, 'action': [0.0, 0]}])
Weights num count: [0.5665209244222361, 0.43285446595877575]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.276263952255249 s
