Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 669, 'sum_payoffs': 43.75039735967082, 'action': [0.0, 0]}, {'num_count': 931, 'sum_payoffs': 81.93145261519248, 'action': [1.0, 0]}])
Weights num count: [0.4178638351030606, 0.5815115552779513]
Actions to choose Agent 1: dict_values([{'num_count': 912, 'sum_payoffs': 78.66805261584503, 'action': [1.0, 0]}, {'num_count': 688, 'sum_payoffs': 46.13899735919306, 'action': [0.0, 0]}])
Weights num count: [0.5696439725171768, 0.4297314178638351]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 889, 'sum_payoffs': 79.19999998416043, 'action': [1.0, 0]}, {'num_count': 711, 'sum_payoffs': 52.559999989488595, 'action': [0.0, 0]}])
Weights num count: [0.5552779512804498, 0.44409743910056215]
Actions to choose Agent 1: dict_values([{'num_count': 888, 'sum_payoffs': 79.07849998418472, 'action': [1.0, 0]}, {'num_count': 712, 'sum_payoffs': 52.6814999894643, 'action': [0.0, 0]}])
Weights num count: [0.5546533416614616, 0.4447220487195503]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.26173877716064453 s
