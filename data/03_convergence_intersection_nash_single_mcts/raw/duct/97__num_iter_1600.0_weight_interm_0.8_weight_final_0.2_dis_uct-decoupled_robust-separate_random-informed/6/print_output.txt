Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 916, 'sum_payoffs': 79.5703026156646, 'action': [1.0, 0]}, {'num_count': 684, 'sum_payoffs': 45.759149990847966, 'action': [0.0, 0]}])
Weights num count: [0.5721424109931293, 0.42723297938788257]
Actions to choose Agent 1: dict_values([{'num_count': 924, 'sum_payoffs': 81.09310261536012, 'action': [1.0, 0]}, {'num_count': 676, 'sum_payoffs': 44.81954999103588, 'action': [0.0, 0]}])
Weights num count: [0.5771392879450343, 0.42223610243597753]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 829, 'sum_payoffs': 70.24949998595085, 'action': [1.0, 0]}, {'num_count': 771, 'sum_payoffs': 61.55099998769052, 'action': [0.0, 0]}])
Weights num count: [0.5178013741411618, 0.4815740162398501]
Actions to choose Agent 1: dict_values([{'num_count': 829, 'sum_payoffs': 70.20899998595895, 'action': [1.0, 0]}, {'num_count': 771, 'sum_payoffs': 61.510499987698616, 'action': [0.0, 0]}])
Weights num count: [0.5178013741411618, 0.4815740162398501]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.28681230545043945 s
