Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1002, 'sum_payoffs': 139.11821050081068, 'action': [1.0, 0]}, {'num_count': 598, 'sum_payoffs': 61.62674998870165, 'action': [0.0, 0]}])
Weights num count: [0.6258588382261087, 0.3735165521549032]
Actions to choose Agent 1: dict_values([{'num_count': 595, 'sum_payoffs': 61.0799999888018, 'action': [0.0, 0]}, {'num_count': 1005, 'sum_payoffs': 139.66496050071024, 'action': [1.0, 0]}])
Weights num count: [0.3716427232979388, 0.6277326670830731]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 880, 'sum_payoffs': 125.19749997704722, 'action': [1.0, 0]}, {'num_count': 720, 'sum_payoffs': 92.53499998303593, 'action': [0.0, 0]}])
Weights num count: [0.5496564647095565, 0.4497189256714553]
Actions to choose Agent 1: dict_values([{'num_count': 721, 'sum_payoffs': 92.87249998297405, 'action': [0.0, 0]}, {'num_count': 879, 'sum_payoffs': 125.1299999770596, 'action': [1.0, 0]}])
Weights num count: [0.45034353529044346, 0.5490318550905684]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.3084726333618164 s
