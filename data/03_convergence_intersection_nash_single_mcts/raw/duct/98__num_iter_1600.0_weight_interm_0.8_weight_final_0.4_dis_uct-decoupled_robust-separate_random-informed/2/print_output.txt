Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1005, 'sum_payoffs': 140.10899997431332, 'action': [1.0, 0]}, {'num_count': 595, 'sum_payoffs': 61.36274998874988, 'action': [0.0, 0]}])
Weights num count: [0.6277326670830731, 0.3716427232979388]
Actions to choose Agent 1: dict_values([{'num_count': 588, 'sum_payoffs': 60.57974998889351, 'action': [0.0, 0]}, {'num_count': 1012, 'sum_payoffs': 142.34999997390233, 'action': [1.0, 0]}])
Weights num count: [0.36727045596502184, 0.63210493441599]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 880, 'sum_payoffs': 125.19749997704723, 'action': [1.0, 0]}, {'num_count': 720, 'sum_payoffs': 92.60249998302358, 'action': [0.0, 0]}])
Weights num count: [0.5496564647095565, 0.4497189256714553]
Actions to choose Agent 1: dict_values([{'num_count': 720, 'sum_payoffs': 92.66999998301121, 'action': [0.0, 0]}, {'num_count': 880, 'sum_payoffs': 125.26499997703485, 'action': [1.0, 0]}])
Weights num count: [0.4497189256714553, 0.5496564647095565]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2899787425994873 s
