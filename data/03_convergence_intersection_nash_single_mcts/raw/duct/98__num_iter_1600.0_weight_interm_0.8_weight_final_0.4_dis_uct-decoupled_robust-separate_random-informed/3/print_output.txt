Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1022, 'sum_payoffs': 143.30921050004196, 'action': [1.0, 0]}, {'num_count': 578, 'sum_payoffs': 58.16253946302079, 'action': [0.0, 0]}])
Weights num count: [0.6383510306058713, 0.3610243597751405]
Actions to choose Agent 1: dict_values([{'num_count': 575, 'sum_payoffs': 58.04778946304198, 'action': [0.0, 0]}, {'num_count': 1025, 'sum_payoffs': 144.63896049979826, 'action': [1.0, 0]}])
Weights num count: [0.3591505309181761, 0.6402248594628357]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 913, 'sum_payoffs': 131.8799999758224, 'action': [1.0, 0]}, {'num_count': 687, 'sum_payoffs': 85.91999998424805, 'action': [0.0, 0]}])
Weights num count: [0.5702685821361649, 0.429106808244847]
Actions to choose Agent 1: dict_values([{'num_count': 911, 'sum_payoffs': 131.60999997587186, 'action': [1.0, 0]}, {'num_count': 689, 'sum_payoffs': 86.3249999841738, 'action': [0.0, 0]}])
Weights num count: [0.5690193628981887, 0.43035602748282326]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.2732808589935303 s
