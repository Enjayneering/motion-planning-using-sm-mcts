Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 628, 'sum_payoffs': 67.95078946122615, 'action': [0.0, 0]}, {'num_count': 972, 'sum_payoffs': 134.37221050168088, 'action': [1.0, 0]}])
Weights num count: [0.39225484072454714, 0.6071205496564647]
Actions to choose Agent 1: dict_values([{'num_count': 636, 'sum_payoffs': 69.27978946098257, 'action': [0.0, 0]}, {'num_count': 964, 'sum_payoffs': 132.55721050201376, 'action': [1.0, 0]}])
Weights num count: [0.39725171767645223, 0.6021236727045597]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 880, 'sum_payoffs': 125.19749997704722, 'action': [1.0, 0]}, {'num_count': 720, 'sum_payoffs': 92.53499998303593, 'action': [0.0, 0]}])
Weights num count: [0.5496564647095565, 0.4497189256714553]
Actions to choose Agent 1: dict_values([{'num_count': 879, 'sum_payoffs': 125.1299999770596, 'action': [1.0, 0]}, {'num_count': 721, 'sum_payoffs': 92.87249998297405, 'action': [0.0, 0]}])
Weights num count: [0.5490318550905684, 0.45034353529044346]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.26441049575805664 s
