Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 556, 'sum_payoffs': 72.5326071304232, 'action': [0.0, 0]}, {'num_count': 1044, 'sum_payoffs': 182.27664470559313, 'action': [1.0, 0]}])
Weights num count: [0.3472829481574016, 0.6520924422236103]
Actions to choose Agent 1: dict_values([{'num_count': 1030, 'sum_payoffs': 179.77721613459332, 'action': [1.0, 0]}, {'num_count': 570, 'sum_payoffs': 75.81310712986088, 'action': [0.0, 0]}])
Weights num count: [0.6433479075577764, 0.3560274828232355]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 578, 'sum_payoffs': 86.17178569951336, 'action': [0.0, 0]}, {'num_count': 1022, 'sum_payoffs': 192.91178568121344, 'action': [1.0, 0]}])
Weights num count: [0.3610243597751405, 0.6383510306058713]
Actions to choose Agent 1: dict_values([{'num_count': 577, 'sum_payoffs': 86.08499998524252, 'action': [0.0, 0]}, {'num_count': 1023, 'sum_payoffs': 193.3457142525676, 'action': [1.0, 0]}])
Weights num count: [0.3603997501561524, 0.6389756402248594]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.27103400230407715 s
