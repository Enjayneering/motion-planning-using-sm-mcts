Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 150, 'sum_payoffs': 42.28199997463092, 'action': [0.0, 0]}, {'num_count': 1950, 'sum_payoffs': 715.8779995704556, 'action': [1.0, 0]}])
Weights num count: [0.07142857142857142, 0.9285714285714286]
Actions to choose Agent 1: dict_values([{'num_count': 213, 'sum_payoffs': 65.06324996096224, 'action': [0.0, 0]}, {'num_count': 1887, 'sum_payoffs': 707.676749575377, 'action': [1.0, 0]}])
Weights num count: [0.10142857142857142, 0.8985714285714286]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 155, 'sum_payoffs': 51.23249996926056, 'action': [0.0, 0]}, {'num_count': 1945, 'sum_payoffs': 795.01499952302, 'action': [1.0, 0]}])
Weights num count: [0.07380952380952381, 0.9261904761904762]
Actions to choose Agent 1: dict_values([{'num_count': 135, 'sum_payoffs': 47.99249997120457, 'action': [0.0, 0]}, {'num_count': 1965, 'sum_payoffs': 807.1649995157303, 'action': [1.0, 0]}])
Weights num count: [0.06428571428571428, 0.9357142857142857]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8074450492858887 s
