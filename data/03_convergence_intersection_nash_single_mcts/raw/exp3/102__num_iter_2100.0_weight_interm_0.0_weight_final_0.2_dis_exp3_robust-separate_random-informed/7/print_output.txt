Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 140, 'sum_payoffs': 41.00624997539636, 'action': [0.0, 0]}, {'num_count': 1960, 'sum_payoffs': 753.6037495478208, 'action': [1.0, 0]}])
Weights num count: [0.06666666666666667, 0.9333333333333333]
Actions to choose Agent 1: dict_values([{'num_count': 1909, 'sum_payoffs': 684.8954995890468, 'action': [1.0, 0]}, {'num_count': 191, 'sum_payoffs': 51.75899996894475, 'action': [0.0, 0]}])
Weights num count: [0.909047619047619, 0.09095238095238095]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 151, 'sum_payoffs': 63.17999996209218, 'action': [1.0, 0]}, {'num_count': 1949, 'sum_payoffs': 767.2724995396626, 'action': [0.0, 0]}])
Weights num count: [0.0719047619047619, 0.9280952380952381]
Actions to choose Agent 1: dict_values([{'num_count': 1926, 'sum_payoffs': 801.899999518887, 'action': [1.0, 0]}, {'num_count': 174, 'sum_payoffs': 68.2424999590546, 'action': [0.0, 0]}])
Weights num count: [0.9171428571428571, 0.08285714285714285]
Selected final action: [0.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 1.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8380889892578125 s
