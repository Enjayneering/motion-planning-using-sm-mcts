Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 132, 'sum_payoffs': 36.632249990231394, 'action': [0.0, 0]}, {'num_count': 1968, 'sum_payoffs': 741.0284998024146, 'action': [1.0, 0]}])
Weights num count: [0.06285714285714286, 0.9371428571428572]
Actions to choose Agent 1: dict_values([{'num_count': 1905, 'sum_payoffs': 691.4564998156311, 'action': [1.0, 0]}, {'num_count': 195, 'sum_payoffs': 61.78274998352449, 'action': [0.0, 0]}])
Weights num count: [0.9071428571428571, 0.09285714285714286]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1927, 'sum_payoffs': 786.3074997903142, 'action': [1.0, 0]}, {'num_count': 173, 'sum_payoffs': 59.33249998417786, 'action': [0.0, 0]}])
Weights num count: [0.9176190476190477, 0.08238095238095237]
Actions to choose Agent 1: dict_values([{'num_count': 180, 'sum_payoffs': 67.63499998196384, 'action': [0.0, 0]}, {'num_count': 1920, 'sum_payoffs': 787.724999789936, 'action': [1.0, 0]}])
Weights num count: [0.08571428571428572, 0.9142857142857143]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.9096770286560059 s
