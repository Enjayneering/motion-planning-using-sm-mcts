Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 225, 'sum_payoffs': 43.425394725261995, 'action': [0.0, 0]}, {'num_count': 1875, 'sum_payoffs': 465.58460513899263, 'action': [1.0, 0]}])
Weights num count: [0.10714285714285714, 0.8928571428571429]
Actions to choose Agent 1: dict_values([{'num_count': 166, 'sum_payoffs': 33.239999991136045, 'action': [0.0, 0]}, {'num_count': 1934, 'sum_payoffs': 485.4899998705259, 'action': [1.0, 0]}])
Weights num count: [0.07904761904761905, 0.920952380952381]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 142, 'sum_payoffs': 32.56499999131598, 'action': [0.0, 0]}, {'num_count': 1958, 'sum_payoffs': 535.6049998571863, 'action': [1.0, 0]}])
Weights num count: [0.06761904761904762, 0.9323809523809524]
Actions to choose Agent 1: dict_values([{'num_count': 129, 'sum_payoffs': 29.054999992251986, 'action': [0.0, 0]}, {'num_count': 1971, 'sum_payoffs': 538.5749998563945, 'action': [1.0, 0]}])
Weights num count: [0.06142857142857143, 0.9385714285714286]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8048291206359863 s
