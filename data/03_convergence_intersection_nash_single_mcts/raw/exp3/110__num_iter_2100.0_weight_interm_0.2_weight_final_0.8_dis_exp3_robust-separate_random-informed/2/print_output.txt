Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 129, 'sum_payoffs': 29.471399994105774, 'action': [0.0, 0]}, {'num_count': 1971, 'sum_payoffs': 574.5066630430043, 'action': [1.0, 0]}])
Weights num count: [0.06142857142857143, 0.9385714285714286]
Actions to choose Agent 1: dict_values([{'num_count': 1905, 'sum_payoffs': 579.4800630420101, 'action': [1.0, 0]}, {'num_count': 195, 'sum_payoffs': 46.659599990668205, 'action': [0.0, 0]}])
Weights num count: [0.9071428571428571, 0.09285714285714286]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 125, 'sum_payoffs': 34.68599999306276, 'action': [0.0, 0]}, {'num_count': 1975, 'sum_payoffs': 646.9019998706119, 'action': [1.0, 0]}])
Weights num count: [0.05952380952380952, 0.9404761904761905]
Actions to choose Agent 1: dict_values([{'num_count': 150, 'sum_payoffs': 42.62399999147507, 'action': [0.0, 0]}, {'num_count': 1950, 'sum_payoffs': 637.9919998723952, 'action': [1.0, 0]}])
Weights num count: [0.07142857142857142, 0.9285714285714286]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8888866901397705 s
