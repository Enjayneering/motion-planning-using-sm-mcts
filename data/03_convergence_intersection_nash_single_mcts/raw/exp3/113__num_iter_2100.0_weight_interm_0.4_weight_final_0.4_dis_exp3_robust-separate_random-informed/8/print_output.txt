Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 171, 'sum_payoffs': 21.793499995096457, 'action': [0.0, 0]}, {'num_count': 1929, 'sum_payoffs': 361.9434078132898, 'action': [1.0, 0]}])
Weights num count: [0.08142857142857143, 0.9185714285714286]
Actions to choose Agent 1: dict_values([{'num_count': 1891, 'sum_payoffs': 361.03215781349525, 'action': [1.0, 0]}, {'num_count': 209, 'sum_payoffs': 31.817249992841052, 'action': [0.0, 0]}])
Weights num count: [0.9004761904761904, 0.09952380952380953]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 242, 'sum_payoffs': 40.9499999907864, 'action': [0.0, 0]}, {'num_count': 1858, 'sum_payoffs': 382.8149999138716, 'action': [1.0, 0]}])
Weights num count: [0.11523809523809524, 0.8847619047619047]
Actions to choose Agent 1: dict_values([{'num_count': 1913, 'sum_payoffs': 396.48374991079714, 'action': [1.0, 0]}, {'num_count': 187, 'sum_payoffs': 32.95124999258599, 'action': [0.0, 0]}])
Weights num count: [0.910952380952381, 0.08904761904761904]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8300466537475586 s
