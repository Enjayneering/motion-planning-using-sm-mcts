Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1894, 'sum_payoffs': 185.79543745819055, 'action': [1.0, 0]}, {'num_count': 206, 'sum_payoffs': 18.830812495763066, 'action': [0.0, 0]}])
Weights num count: [0.9019047619047619, 0.0980952380952381]
Actions to choose Agent 1: dict_values([{'num_count': 343, 'sum_payoffs': 26.226325651993818, 'action': [0.0, 0]}, {'num_count': 1757, 'sum_payoffs': 169.3785493039917, 'action': [1.0, 0]}])
Weights num count: [0.16333333333333333, 0.8366666666666667]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 181, 'sum_payoffs': 16.014374996396768, 'action': [0.0, 0]}, {'num_count': 1919, 'sum_payoffs': 199.34437495515058, 'action': [1.0, 0]}])
Weights num count: [0.08619047619047619, 0.9138095238095238]
Actions to choose Agent 1: dict_values([{'num_count': 1859, 'sum_payoffs': 192.45937495669924, 'action': [1.0, 0]}, {'num_count': 241, 'sum_payoffs': 21.481874995166667, 'action': [0.0, 0]}])
Weights num count: [0.8852380952380953, 0.11476190476190476]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8284156322479248 s
