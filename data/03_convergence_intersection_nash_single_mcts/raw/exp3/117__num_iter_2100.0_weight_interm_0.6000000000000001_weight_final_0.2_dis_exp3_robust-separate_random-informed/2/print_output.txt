Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1791, 'sum_payoffs': 173.16867430313857, 'action': [1.0, 0]}, {'num_count': 309, 'sum_payoffs': 27.047249993914324, 'action': [0.0, 0]}])
Weights num count: [0.8528571428571429, 0.14714285714285713]
Actions to choose Agent 1: dict_values([{'num_count': 1866, 'sum_payoffs': 180.55654930147534, 'action': [1.0, 0]}, {'num_count': 234, 'sum_payoffs': 18.565874995822664, 'action': [0.0, 0]}])
Weights num count: [0.8885714285714286, 0.11142857142857143]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1870, 'sum_payoffs': 193.57312495644868, 'action': [1.0, 0]}, {'num_count': 230, 'sum_payoffs': 19.051874995713394, 'action': [0.0, 0]}])
Weights num count: [0.8904761904761904, 0.10952380952380952]
Actions to choose Agent 1: dict_values([{'num_count': 1887, 'sum_payoffs': 197.1168749556516, 'action': [1.0, 0]}, {'num_count': 213, 'sum_payoffs': 19.558124995599478, 'action': [0.0, 0]}])
Weights num count: [0.8985714285714286, 0.10142857142857142]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8473813533782959 s
