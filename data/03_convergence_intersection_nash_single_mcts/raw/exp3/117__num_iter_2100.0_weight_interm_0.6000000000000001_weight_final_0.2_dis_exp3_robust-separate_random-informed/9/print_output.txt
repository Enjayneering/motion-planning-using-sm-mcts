Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 217, 'sum_payoffs': 17.96512499595782, 'action': [0.0, 0]}, {'num_count': 1883, 'sum_payoffs': 181.2855493013111, 'action': [1.0, 0]}])
Weights num count: [0.10333333333333333, 0.8966666666666666]
Actions to choose Agent 1: dict_values([{'num_count': 216, 'sum_payoffs': 18.83081249576304, 'action': [0.0, 0]}, {'num_count': 1884, 'sum_payoffs': 182.60686180101385, 'action': [1.0, 0]}])
Weights num count: [0.10285714285714286, 0.8971428571428571]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 262, 'sum_payoffs': 22.696874994893314, 'action': [0.0, 0]}, {'num_count': 1838, 'sum_payoffs': 192.3581249567219, 'action': [1.0, 0]}])
Weights num count: [0.12476190476190477, 0.8752380952380953]
Actions to choose Agent 1: dict_values([{'num_count': 269, 'sum_payoffs': 23.000624994824967, 'action': [0.0, 0]}, {'num_count': 1831, 'sum_payoffs': 191.1431249569951, 'action': [1.0, 0]}])
Weights num count: [0.1280952380952381, 0.871904761904762]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8435416221618652 s
