Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 238, 'sum_payoffs': 26.384249995162865, 'action': [0.0, 0]}, {'num_count': 1862, 'sum_payoffs': 239.53796048239337, 'action': [1.0, 0]}])
Weights num count: [0.11333333333333333, 0.8866666666666667]
Actions to choose Agent 1: dict_values([{'num_count': 1817, 'sum_payoffs': 230.4247104840648, 'action': [1.0, 0]}, {'num_count': 283, 'sum_payoffs': 28.08599999485085, 'action': [0.0, 0]}])
Weights num count: [0.8652380952380953, 0.13476190476190475]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 368, 'sum_payoffs': 43.9349999919452, 'action': [0.0, 0]}, {'num_count': 1732, 'sum_payoffs': 239.06999995618168, 'action': [1.0, 0]}])
Weights num count: [0.17523809523809525, 0.8247619047619048]
Actions to choose Agent 1: dict_values([{'num_count': 241, 'sum_payoffs': 28.47749999477909, 'action': [0.0, 0]}, {'num_count': 1859, 'sum_payoffs': 259.25249995248333, 'action': [1.0, 0]}])
Weights num count: [0.11476190476190476, 0.8852380952380953]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8320753574371338 s
