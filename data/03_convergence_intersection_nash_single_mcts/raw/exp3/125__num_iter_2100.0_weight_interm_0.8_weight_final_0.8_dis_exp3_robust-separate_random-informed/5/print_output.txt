Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1887, 'sum_payoffs': 357.1137828367168, 'action': [1.0, 0]}, {'num_count': 213, 'sum_payoffs': 31.634999994859285, 'action': [0.0, 0]}])
Weights num count: [0.8985714285714286, 0.10142857142857142]
Actions to choose Agent 1: dict_values([{'num_count': 1858, 'sum_payoffs': 350.8261578377381, 'action': [1.0, 0]}, {'num_count': 242, 'sum_payoffs': 37.19362499395595, 'action': [0.0, 0]}])
Weights num count: [0.8847619047619047, 0.11523809523809524]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1812, 'sum_payoffs': 371.8799999395625, 'action': [1.0, 0]}, {'num_count': 288, 'sum_payoffs': 54.6187499911247, 'action': [0.0, 0]}])
Weights num count: [0.8628571428571429, 0.13714285714285715]
Actions to choose Agent 1: dict_values([{'num_count': 168, 'sum_payoffs': 29.81249999515558, 'action': [0.0, 0]}, {'num_count': 1932, 'sum_payoffs': 397.09124993546305, 'action': [1.0, 0]}])
Weights num count: [0.08, 0.92]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8309073448181152 s
