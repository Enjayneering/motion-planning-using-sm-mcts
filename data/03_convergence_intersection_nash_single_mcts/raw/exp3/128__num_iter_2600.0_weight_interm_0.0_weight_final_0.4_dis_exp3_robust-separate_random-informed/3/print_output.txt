Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2365, 'sum_payoffs': 796.7969997211395, 'action': [0.0, 0]}, {'num_count': 235, 'sum_payoffs': 86.38649996976429, 'action': [1.0, 0]}])
Weights num count: [0.9096153846153846, 0.09038461538461538]
Actions to choose Agent 1: dict_values([{'num_count': 158, 'sum_payoffs': 56.31524998028952, 'action': [0.0, 0]}, {'num_count': 2442, 'sum_payoffs': 955.9012496654683, 'action': [1.0, 0]}])
Weights num count: [0.06076923076923077, 0.9392307692307692]
Selected final action: [0.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2434, 'sum_payoffs': 965.1149996621726, 'action': [1.0, 0]}, {'num_count': 166, 'sum_payoffs': 23.28749999184937, 'action': [0.0, 0]}])
Weights num count: [0.9361538461538461, 0.06384615384615384]
Actions to choose Agent 1: dict_values([{'num_count': 2438, 'sum_payoffs': 1056.0374996303508, 'action': [1.0, 0]}, {'num_count': 162, 'sum_payoffs': 61.96499997831213, 'action': [0.0, 0]}])
Weights num count: [0.9376923076923077, 0.06230769230769231]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 1.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.0197489261627197 s
