Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 174, 'sum_payoffs': 48.113999989174374, 'action': [0.0, 0]}, {'num_count': 2426, 'sum_payoffs': 897.2167497980839, 'action': [1.0, 0]}])
Weights num count: [0.06692307692307692, 0.933076923076923]
Actions to choose Agent 1: dict_values([{'num_count': 2406, 'sum_payoffs': 893.2072497989868, 'action': [1.0, 0]}, {'num_count': 194, 'sum_payoffs': 56.861999987206104, 'action': [0.0, 0]}])
Weights num count: [0.9253846153846154, 0.07461538461538461]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 219, 'sum_payoffs': 84.03749998109184, 'action': [0.0, 0]}, {'num_count': 2381, 'sum_payoffs': 994.4774997762701, 'action': [1.0, 0]}])
Weights num count: [0.08423076923076923, 0.9157692307692308]
Actions to choose Agent 1: dict_values([{'num_count': 2437, 'sum_payoffs': 959.0399997842408, 'action': [0.0, 0]}, {'num_count': 163, 'sum_payoffs': 68.84999998450873, 'action': [1.0, 0]}])
Weights num count: [0.9373076923076923, 0.06269230769230769]
Selected final action: [1.0, 0, 0.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 1.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.0657286643981934 s
