Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 142, 'sum_payoffs': 40.094999990978636, 'action': [0.0, 0]}, {'num_count': 2458, 'sum_payoffs': 917.0819997936119, 'action': [1.0, 0]}])
Weights num count: [0.054615384615384614, 0.9453846153846154]
Actions to choose Agent 1: dict_values([{'num_count': 258, 'sum_payoffs': 73.81124998339236, 'action': [0.0, 0]}, {'num_count': 2342, 'sum_payoffs': 864.7762498053881, 'action': [1.0, 0]}])
Weights num count: [0.09923076923076923, 0.9007692307692308]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 157, 'sum_payoffs': 56.29499998733355, 'action': [0.0, 0]}, {'num_count': 2443, 'sum_payoffs': 997.3124997756347, 'action': [1.0, 0]}])
Weights num count: [0.060384615384615384, 0.9396153846153846]
Actions to choose Agent 1: dict_values([{'num_count': 135, 'sum_payoffs': 46.57499998952059, 'action': [0.0, 0]}, {'num_count': 2465, 'sum_payoffs': 1005.4124997738129, 'action': [1.0, 0]}])
Weights num count: [0.051923076923076926, 0.948076923076923]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.9836380481719971 s
