Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2432, 'sum_payoffs': 546.053376206562, 'action': [1.0, 0]}, {'num_count': 168, 'sum_payoffs': 27.92384999441524, 'action': [0.0, 0]}])
Weights num count: [0.9353846153846154, 0.06461538461538462]
Actions to choose Agent 1: dict_values([{'num_count': 2398, 'sum_payoffs': 538.0708262081594, 'action': [1.0, 0]}, {'num_count': 202, 'sum_payoffs': 35.68769999286243, 'action': [0.0, 0]}])
Weights num count: [0.9223076923076923, 0.07769230769230769]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 273, 'sum_payoffs': 61.595999987680855, 'action': [0.0, 0]}, {'num_count': 2327, 'sum_payoffs': 571.4054998857063, 'action': [1.0, 0]}])
Weights num count: [0.105, 0.895]
Actions to choose Agent 1: dict_values([{'num_count': 277, 'sum_payoffs': 61.59599998768087, 'action': [0.0, 0]}, {'num_count': 2323, 'sum_payoffs': 571.1624998857551, 'action': [1.0, 0]}])
Weights num count: [0.10653846153846154, 0.8934615384615384]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.9976320266723633 s
