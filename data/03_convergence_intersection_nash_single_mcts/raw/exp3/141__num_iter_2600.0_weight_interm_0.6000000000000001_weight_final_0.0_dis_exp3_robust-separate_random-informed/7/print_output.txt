Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1102, 'sum_payoffs': 2.5199999993280002, 'action': [0.0, 0]}, {'num_count': 1498, 'sum_payoffs': 5.759999998464, 'action': [1.0, 0]}])
Weights num count: [0.4238461538461539, 0.5761538461538461]
Actions to choose Agent 1: dict_values([{'num_count': 1370, 'sum_payoffs': 5.37631578804, 'action': [1.0, 0]}, {'num_count': 1230, 'sum_payoffs': 2.903684209752, 'action': [0.0, 0]}])
Weights num count: [0.5269230769230769, 0.47307692307692306]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 1381, 'sum_payoffs': 1.79999999952, 'action': [1.0, 0]}, {'num_count': 1219, 'sum_payoffs': 0.89999999976, 'action': [0.0, 0]}])
Weights num count: [0.5311538461538462, 0.46884615384615386]
Actions to choose Agent 1: dict_values([{'num_count': 1356, 'sum_payoffs': 1.79999999952, 'action': [1.0, 0]}, {'num_count': 1244, 'sum_payoffs': 0.89999999976, 'action': [0.0, 0]}])
Weights num count: [0.5215384615384615, 0.47846153846153844]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.9726669788360596 s
