Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2388, 'sum_payoffs': 230.4474867902434, 'action': [1.0, 0]}, {'num_count': 212, 'sum_payoffs': 16.605888154158382, 'action': [0.0, 0]}])
Weights num count: [0.9184615384615384, 0.08153846153846153]
Actions to choose Agent 1: dict_values([{'num_count': 326, 'sum_payoffs': 25.29988815220222, 'action': [0.0, 0]}, {'num_count': 2274, 'sum_payoffs': 217.47061179316543, 'action': [1.0, 0]}])
Weights num count: [0.12538461538461537, 0.8746153846153846]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 237, 'sum_payoffs': 20.87437499530335, 'action': [0.0, 0]}, {'num_count': 2363, 'sum_payoffs': 245.21062494483442, 'action': [1.0, 0]}])
Weights num count: [0.09115384615384615, 0.9088461538461539]
Actions to choose Agent 1: dict_values([{'num_count': 2328, 'sum_payoffs': 241.3124999457112, 'action': [1.0, 0]}, {'num_count': 272, 'sum_payoffs': 23.152499994790805, 'action': [0.0, 0]}])
Weights num count: [0.8953846153846153, 0.10461538461538461]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8875226974487305 s
