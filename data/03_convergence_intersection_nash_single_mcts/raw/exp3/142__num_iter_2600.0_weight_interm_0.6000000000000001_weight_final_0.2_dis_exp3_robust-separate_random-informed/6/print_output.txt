Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2238, 'sum_payoffs': 213.3455624519883, 'action': [1.0, 0]}, {'num_count': 362, 'sum_payoffs': 29.219062493425675, 'action': [0.0, 0]}])
Weights num count: [0.8607692307692307, 0.13923076923076924]
Actions to choose Agent 1: dict_values([{'num_count': 315, 'sum_payoffs': 25.93856249416378, 'action': [0.0, 0]}, {'num_count': 2285, 'sum_payoffs': 221.54681245014194, 'action': [1.0, 0]}])
Weights num count: [0.12115384615384615, 0.8788461538461538]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2318, 'sum_payoffs': 242.1224999455289, 'action': [1.0, 0]}, {'num_count': 282, 'sum_payoffs': 24.823124994414943, 'action': [0.0, 0]}])
Weights num count: [0.8915384615384615, 0.10846153846153846]
Actions to choose Agent 1: dict_values([{'num_count': 2283, 'sum_payoffs': 237.76874994650822, 'action': [1.0, 0]}, {'num_count': 317, 'sum_payoffs': 25.93687499416437, 'action': [0.0, 0]}])
Weights num count: [0.8780769230769231, 0.12192307692307693]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.9090783596038818 s
