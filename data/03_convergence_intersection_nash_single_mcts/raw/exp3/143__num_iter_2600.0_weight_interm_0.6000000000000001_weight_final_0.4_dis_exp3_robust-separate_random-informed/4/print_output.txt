Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 216, 'sum_payoffs': 25.933499994813367, 'action': [0.0, 0]}, {'num_count': 2384, 'sum_payoffs': 358.10298940207184, 'action': [1.0, 0]}])
Weights num count: [0.08307692307692308, 0.916923076923077]
Actions to choose Agent 1: dict_values([{'num_count': 2338, 'sum_payoffs': 353.5831894029755, 'action': [1.0, 0]}, {'num_count': 262, 'sum_payoffs': 32.93189999341371, 'action': [0.0, 0]}])
Weights num count: [0.8992307692307693, 0.10076923076923076]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2337, 'sum_payoffs': 384.1289999231619, 'action': [1.0, 0]}, {'num_count': 263, 'sum_payoffs': 37.07099999258559, 'action': [0.0, 0]}])
Weights num count: [0.8988461538461539, 0.10115384615384615]
Actions to choose Agent 1: dict_values([{'num_count': 196, 'sum_payoffs': 27.91799999441626, 'action': [0.0, 0]}, {'num_count': 2404, 'sum_payoffs': 396.3599999207139, 'action': [1.0, 0]}])
Weights num count: [0.07538461538461538, 0.9246153846153846]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.060394525527954 s
