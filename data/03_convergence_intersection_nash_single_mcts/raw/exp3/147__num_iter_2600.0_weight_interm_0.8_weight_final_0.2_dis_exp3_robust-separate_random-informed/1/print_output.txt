Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2314, 'sum_payoffs': 181.3031525953229, 'action': [1.0, 0]}, {'num_count': 286, 'sum_payoffs': 19.415699996116846, 'action': [0.0, 0]}])
Weights num count: [0.89, 0.11]
Actions to choose Agent 1: dict_values([{'num_count': 494, 'sum_payoffs': 32.10029999357977, 'action': [0.0, 0]}, {'num_count': 2106, 'sum_payoffs': 162.49495259908358, 'action': [1.0, 0]}])
Weights num count: [0.19, 0.81]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 295, 'sum_payoffs': 21.496499995700614, 'action': [0.0, 0]}, {'num_count': 2305, 'sum_payoffs': 191.78999996163589, 'action': [1.0, 0]}])
Weights num count: [0.11346153846153846, 0.8865384615384615]
Actions to choose Agent 1: dict_values([{'num_count': 305, 'sum_payoffs': 21.172499995765417, 'action': [0.0, 0]}, {'num_count': 2295, 'sum_payoffs': 191.06099996178182, 'action': [1.0, 0]}])
Weights num count: [0.11730769230769231, 0.8826923076923077]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.0031635761260986 s
