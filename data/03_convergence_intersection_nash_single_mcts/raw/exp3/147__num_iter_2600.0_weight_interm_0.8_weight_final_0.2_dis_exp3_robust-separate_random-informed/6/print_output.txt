Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 361, 'sum_payoffs': 22.86944999542603, 'action': [0.0, 0]}, {'num_count': 2239, 'sum_payoffs': 172.08130259716668, 'action': [1.0, 0]}])
Weights num count: [0.13884615384615384, 0.8611538461538462]
Actions to choose Agent 1: dict_values([{'num_count': 2294, 'sum_payoffs': 181.18480259534647, 'action': [1.0, 0]}, {'num_count': 306, 'sum_payoffs': 20.69144999586169, 'action': [0.0, 0]}])
Weights num count: [0.8823076923076923, 0.1176923076923077]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2321, 'sum_payoffs': 193.69349996125487, 'action': [1.0, 0]}, {'num_count': 279, 'sum_payoffs': 19.835999996032726, 'action': [0.0, 0]}])
Weights num count: [0.8926923076923077, 0.10730769230769231]
Actions to choose Agent 1: dict_values([{'num_count': 2256, 'sum_payoffs': 187.65899996246273, 'action': [1.0, 0]}, {'num_count': 344, 'sum_payoffs': 24.25049999514984, 'action': [0.0, 0]}])
Weights num count: [0.8676923076923077, 0.13230769230769232]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.9274296760559082 s
