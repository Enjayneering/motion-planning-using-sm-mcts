Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2230, 'sum_payoffs': 278.34971047527984, 'action': [1.0, 0]}, {'num_count': 370, 'sum_payoffs': 37.0215394668968, 'action': [0.0, 0]}])
Weights num count: [0.8576923076923076, 0.1423076923076923]
Actions to choose Agent 1: dict_values([{'num_count': 2419, 'sum_payoffs': 312.67346046899155, 'action': [1.0, 0]}, {'num_count': 181, 'sum_payoffs': 20.315289469959744, 'action': [0.0, 0]}])
Weights num count: [0.9303846153846154, 0.06961538461538462]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 226, 'sum_payoffs': 26.85749999507609, 'action': [0.0, 0]}, {'num_count': 2374, 'sum_payoffs': 326.8874999400907, 'action': [1.0, 0]}])
Weights num count: [0.08692307692307692, 0.9130769230769231]
Actions to choose Agent 1: dict_values([{'num_count': 2314, 'sum_payoffs': 318.1124999416983, 'action': [1.0, 0]}, {'num_count': 286, 'sum_payoffs': 33.87749999378908, 'action': [0.0, 0]}])
Weights num count: [0.89, 0.11]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.8834052085876465 s
