Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 212, 'sum_payoffs': 24.782464281465902, 'action': [0.0, 0]}, {'num_count': 2388, 'sum_payoffs': 384.2292518138184, 'action': [1.0, 0]}])
Weights num count: [0.08153846153846153, 0.9184615384615384]
Actions to choose Agent 1: dict_values([{'num_count': 199, 'sum_payoffs': 21.658178567715787, 'action': [0.0, 0]}, {'num_count': 2401, 'sum_payoffs': 393.4458946693806, 'action': [1.0, 0]}])
Weights num count: [0.07653846153846154, 0.9234615384615384]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2404, 'sum_payoffs': 425.3239284985002, 'action': [1.0, 0]}, {'num_count': 196, 'sum_payoffs': 27.764999995240274, 'action': [0.0, 0]}])
Weights num count: [0.9246153846153846, 0.07538461538461538]
Actions to choose Agent 1: dict_values([{'num_count': 2352, 'sum_payoffs': 415.86428564297967, 'action': [1.0, 0]}, {'num_count': 248, 'sum_payoffs': 36.70392856513641, 'action': [0.0, 0]}])
Weights num count: [0.9046153846153846, 0.09538461538461539]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.9988665580749512 s
