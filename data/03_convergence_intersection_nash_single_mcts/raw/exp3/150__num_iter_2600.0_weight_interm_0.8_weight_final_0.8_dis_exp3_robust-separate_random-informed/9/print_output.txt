Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2396, 'sum_payoffs': 450.42578282155944, 'action': [1.0, 0]}, {'num_count': 204, 'sum_payoffs': 26.12137499575524, 'action': [0.0, 0]}])
Weights num count: [0.9215384615384615, 0.07846153846153846]
Actions to choose Agent 1: dict_values([{'num_count': 187, 'sum_payoffs': 26.485874995696037, 'action': [0.0, 0]}, {'num_count': 2413, 'sum_payoffs': 455.89328282067083, 'action': [1.0, 0]}])
Weights num count: [0.07192307692307692, 0.928076923076923]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 202, 'sum_payoffs': 35.48249999423424, 'action': [0.0, 0]}, {'num_count': 2398, 'sum_payoffs': 492.3674999199715, 'action': [1.0, 0]}])
Weights num count: [0.07769230769230769, 0.9223076923076923]
Actions to choose Agent 1: dict_values([{'num_count': 242, 'sum_payoffs': 43.784999992885105, 'action': [0.0, 0]}, {'num_count': 2358, 'sum_payoffs': 484.06499992132154, 'action': [1.0, 0]}])
Weights num count: [0.09307692307692307, 0.9069230769230769]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.9102652072906494 s
