Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 187, 'sum_payoffs': 54.310499980991125, 'action': [0.0, 0]}, {'num_count': 2913, 'sum_payoffs': 1081.106999621644, 'action': [1.0, 0]}])
Weights num count: [0.06032258064516129, 0.9396774193548387]
Actions to choose Agent 1: dict_values([{'num_count': 2914, 'sum_payoffs': 1075.457249623622, 'action': [1.0, 0]}, {'num_count': 186, 'sum_payoffs': 49.38974998271342, 'action': [0.0, 0]}])
Weights num count: [0.94, 0.06]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 198, 'sum_payoffs': 72.29249997469746, 'action': [0.0, 0]}, {'num_count': 2902, 'sum_payoffs': 1184.8274995852921, 'action': [1.0, 0]}])
Weights num count: [0.06387096774193549, 0.9361290322580645]
Actions to choose Agent 1: dict_values([{'num_count': 180, 'sum_payoffs': 63.17999997788686, 'action': [0.0, 0]}, {'num_count': 2920, 'sum_payoffs': 1190.6999995832377, 'action': [1.0, 0]}])
Weights num count: [0.05806451612903226, 0.9419354838709677]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.1184794902801514 s
