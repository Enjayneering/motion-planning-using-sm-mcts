Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2907, 'sum_payoffs': 1082.5649996211334, 'action': [1.0, 0]}, {'num_count': 193, 'sum_payoffs': 49.02524998284099, 'action': [0.0, 0]}])
Weights num count: [0.937741935483871, 0.06225806451612903]
Actions to choose Agent 1: dict_values([{'num_count': 198, 'sum_payoffs': 49.571999982649615, 'action': [0.0, 0]}, {'num_count': 2902, 'sum_payoffs': 1078.737749622473, 'action': [1.0, 0]}])
Weights num count: [0.06387096774193549, 0.9361290322580645]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 150, 'sum_payoffs': 53.45999998128892, 'action': [0.0, 0]}, {'num_count': 2950, 'sum_payoffs': 1210.342499576367, 'action': [1.0, 0]}])
Weights num count: [0.04838709677419355, 0.9516129032258065]
Actions to choose Agent 1: dict_values([{'num_count': 2922, 'sum_payoffs': 1191.3074995830257, 'action': [1.0, 0]}, {'num_count': 178, 'sum_payoffs': 55.88999998043839, 'action': [0.0, 0]}])
Weights num count: [0.9425806451612904, 0.05741935483870968]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.0364019870758057 s
