Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2910, 'sum_payoffs': 1073.9992497582994, 'action': [1.0, 0]}, {'num_count': 190, 'sum_payoffs': 47.74949998925639, 'action': [0.0, 0]}])
Weights num count: [0.9387096774193548, 0.06129032258064516]
Actions to choose Agent 1: dict_values([{'num_count': 2904, 'sum_payoffs': 1080.1957497569085, 'action': [1.0, 0]}, {'num_count': 196, 'sum_payoffs': 57.9554999869601, 'action': [0.0, 0]}])
Weights num count: [0.9367741935483871, 0.06322580645161291]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 174, 'sum_payoffs': 70.06499998423547, 'action': [0.0, 0]}, {'num_count': 2926, 'sum_payoffs': 1218.8474997257563, 'action': [1.0, 0]}])
Weights num count: [0.056129032258064517, 0.9438709677419355]
Actions to choose Agent 1: dict_values([{'num_count': 2891, 'sum_payoffs': 1135.6199997444928, 'action': [0.0, 0]}, {'num_count': 209, 'sum_payoffs': 86.06249998063615, 'action': [1.0, 0]}])
Weights num count: [0.9325806451612904, 0.06741935483870967]
Selected final action: [1.0, 0, 0.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 1.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.1442053318023682 s
