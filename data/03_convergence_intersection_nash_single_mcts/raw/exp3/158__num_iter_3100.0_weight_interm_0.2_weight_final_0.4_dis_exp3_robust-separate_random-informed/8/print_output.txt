Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 191, 'sum_payoffs': 34.72499999074001, 'action': [0.0, 0]}, {'num_count': 2909, 'sum_payoffs': 723.1439998071921, 'action': [1.0, 0]}])
Weights num count: [0.061612903225806454, 0.9383870967741935]
Actions to choose Agent 1: dict_values([{'num_count': 258, 'sum_payoffs': 44.91039472486595, 'action': [0.0, 0]}, {'num_count': 2842, 'sum_payoffs': 711.2576050735171, 'action': [1.0, 0]}])
Weights num count: [0.0832258064516129, 0.9167741935483871]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2903, 'sum_payoffs': 792.6449997886593, 'action': [1.0, 0]}, {'num_count': 197, 'sum_payoffs': 44.17499998821988, 'action': [0.0, 0]}])
Weights num count: [0.9364516129032258, 0.0635483870967742]
Actions to choose Agent 1: dict_values([{'num_count': 225, 'sum_payoffs': 52.81499998591578, 'action': [0.0, 0]}, {'num_count': 2875, 'sum_payoffs': 785.6249997905303, 'action': [1.0, 0]}])
Weights num count: [0.07258064516129033, 0.9274193548387096]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.2890479564666748 s
