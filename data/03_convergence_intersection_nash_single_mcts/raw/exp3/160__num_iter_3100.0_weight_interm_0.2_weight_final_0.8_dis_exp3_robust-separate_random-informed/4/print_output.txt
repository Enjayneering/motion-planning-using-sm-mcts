Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 189, 'sum_payoffs': 40.254536834054456, 'action': [0.0, 0]}, {'num_count': 2911, 'sum_payoffs': 869.8794629839438, 'action': [1.0, 0]}])
Weights num count: [0.06096774193548387, 0.9390322580645162]
Actions to choose Agent 1: dict_values([{'num_count': 212, 'sum_payoffs': 41.71253683376286, 'action': [0.0, 0]}, {'num_count': 2888, 'sum_payoffs': 859.673462985984, 'action': [1.0, 0]}])
Weights num count: [0.06838709677419355, 0.9316129032258065]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2904, 'sum_payoffs': 949.8419998099811, 'action': [1.0, 0]}, {'num_count': 196, 'sum_payoffs': 56.39399998872093, 'action': [0.0, 0]}])
Weights num count: [0.9367741935483871, 0.06322580645161291]
Actions to choose Agent 1: dict_values([{'num_count': 2891, 'sum_payoffs': 945.1439998109213, 'action': [1.0, 0]}, {'num_count': 209, 'sum_payoffs': 58.823999988234895, 'action': [0.0, 0]}])
Weights num count: [0.9325806451612904, 0.06741935483870967]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.0385584831237793 s
