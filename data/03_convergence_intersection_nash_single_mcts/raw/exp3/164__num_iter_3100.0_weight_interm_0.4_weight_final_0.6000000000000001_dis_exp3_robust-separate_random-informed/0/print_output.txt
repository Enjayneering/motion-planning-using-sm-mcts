Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2921, 'sum_payoffs': 652.3897498694972, 'action': [1.0, 0]}, {'num_count': 179, 'sum_payoffs': 31.43114999371378, 'action': [0.0, 0]}])
Weights num count: [0.942258064516129, 0.057741935483870965]
Actions to choose Agent 1: dict_values([{'num_count': 2875, 'sum_payoffs': 638.8915261879873, 'action': [1.0, 0]}, {'num_count': 225, 'sum_payoffs': 42.086273675793215, 'action': [0.0, 0]}])
Weights num count: [0.9274193548387096, 0.07258064516129033]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 232, 'sum_payoffs': 50.660999989867875, 'action': [0.0, 0]}, {'num_count': 2868, 'sum_payoffs': 705.7844998588278, 'action': [1.0, 0]}])
Weights num count: [0.07483870967741936, 0.9251612903225807]
Actions to choose Agent 1: dict_values([{'num_count': 2865, 'sum_payoffs': 702.7469998594349, 'action': [1.0, 0]}, {'num_count': 235, 'sum_payoffs': 49.56749999008656, 'action': [0.0, 0]}])
Weights num count: [0.9241935483870968, 0.07580645161290323]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.1337826251983643 s
