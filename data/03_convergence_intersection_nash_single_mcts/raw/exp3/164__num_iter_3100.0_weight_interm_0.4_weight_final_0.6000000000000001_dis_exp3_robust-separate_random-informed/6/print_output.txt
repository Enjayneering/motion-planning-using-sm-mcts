Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2905, 'sum_payoffs': 649.170426185931, 'action': [1.0, 0]}, {'num_count': 195, 'sum_payoffs': 31.256573677959214, 'action': [0.0, 0]}])
Weights num count: [0.9370967741935484, 0.06290322580645161]
Actions to choose Agent 1: dict_values([{'num_count': 209, 'sum_payoffs': 36.72407367686568, 'action': [0.0, 0]}, {'num_count': 2891, 'sum_payoffs': 647.8582261861935, 'action': [1.0, 0]}])
Weights num count: [0.06741935483870967, 0.9325806451612904]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 217, 'sum_payoffs': 47.62349999047537, 'action': [0.0, 0]}, {'num_count': 2883, 'sum_payoffs': 709.4294998580988, 'action': [1.0, 0]}])
Weights num count: [0.07, 0.93]
Actions to choose Agent 1: dict_values([{'num_count': 2908, 'sum_payoffs': 712.831499857418, 'action': [1.0, 0]}, {'num_count': 192, 'sum_payoffs': 39.118499992176346, 'action': [0.0, 0]}])
Weights num count: [0.9380645161290323, 0.06193548387096774]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.123502492904663 s
