Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2877, 'sum_payoffs': 716.9111051317371, 'action': [1.0, 0]}, {'num_count': 223, 'sum_payoffs': 45.089999991733514, 'action': [0.0, 0]}])
Weights num count: [0.9280645161290323, 0.07193548387096774]
Actions to choose Agent 1: dict_values([{'num_count': 2919, 'sum_payoffs': 720.3131051311127, 'action': [1.0, 0]}, {'num_count': 181, 'sum_payoffs': 30.26699999445111, 'action': [0.0, 0]}])
Weights num count: [0.9416129032258065, 0.05838709677419355]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2908, 'sum_payoffs': 793.3199998546123, 'action': [1.0, 0]}, {'num_count': 192, 'sum_payoffs': 47.00999999138144, 'action': [0.0, 0]}])
Weights num count: [0.9380645161290323, 0.06193548387096774]
Actions to choose Agent 1: dict_values([{'num_count': 237, 'sum_payoffs': 56.59499998962418, 'action': [0.0, 0]}, {'num_count': 2863, 'sum_payoffs': 779.1449998572099, 'action': [1.0, 0]}])
Weights num count: [0.0764516129032258, 0.9235483870967742]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.1036715507507324 s
