Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2812, 'sum_payoffs': 272.72948678072976, 'action': [1.0, 0]}, {'num_count': 288, 'sum_payoffs': 21.29962499520756, 'action': [0.0, 0]}])
Weights num count: [0.9070967741935484, 0.09290322580645162]
Actions to choose Agent 1: dict_values([{'num_count': 2772, 'sum_payoffs': 262.9335492829325, 'action': [1.0, 0]}, {'num_count': 328, 'sum_payoffs': 24.807937494418187, 'action': [0.0, 0]}])
Weights num count: [0.8941935483870967, 0.10580645161290322]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 397, 'sum_payoffs': 34.7962499921711, 'action': [0.0, 0]}, {'num_count': 2703, 'sum_payoffs': 279.4331249371309, 'action': [1.0, 0]}])
Weights num count: [0.12806451612903225, 0.8719354838709678]
Actions to choose Agent 1: dict_values([{'num_count': 298, 'sum_payoffs': 26.443124994050468, 'action': [0.0, 0]}, {'num_count': 2802, 'sum_payoffs': 291.22874993447454, 'action': [1.0, 0]}])
Weights num count: [0.09612903225806452, 0.9038709677419355]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.2056682109832764 s
