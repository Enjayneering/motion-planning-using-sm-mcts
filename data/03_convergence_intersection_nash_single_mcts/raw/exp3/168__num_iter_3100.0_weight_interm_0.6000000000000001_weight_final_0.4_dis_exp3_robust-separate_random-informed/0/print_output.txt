Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2861, 'sum_payoffs': 433.6030893869747, 'action': [1.0, 0]}, {'num_count': 239, 'sum_payoffs': 25.74449999485117, 'action': [0.0, 0]}])
Weights num count: [0.9229032258064516, 0.07709677419354839]
Actions to choose Agent 1: dict_values([{'num_count': 2838, 'sum_payoffs': 427.9897893880968, 'action': [1.0, 0]}, {'num_count': 262, 'sum_payoffs': 30.920399993816012, 'action': [0.0, 0]}])
Weights num count: [0.915483870967742, 0.08451612903225807]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 273, 'sum_payoffs': 39.662999992067185, 'action': [0.0, 0]}, {'num_count': 2827, 'sum_payoffs': 464.2379999071287, 'action': [1.0, 0]}])
Weights num count: [0.08806451612903225, 0.9119354838709678]
Actions to choose Agent 1: dict_values([{'num_count': 2881, 'sum_payoffs': 473.4719999052807, 'action': [1.0, 0]}, {'num_count': 219, 'sum_payoffs': 30.428999993914037, 'action': [0.0, 0]}])
Weights num count: [0.9293548387096774, 0.07064516129032258]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.4465620517730713 s
