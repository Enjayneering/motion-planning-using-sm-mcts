Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 256, 'sum_payoffs': 32.397299993520626, 'action': [0.0, 0]}, {'num_count': 2844, 'sum_payoffs': 430.1470893876661, 'action': [1.0, 0]}])
Weights num count: [0.08258064516129032, 0.9174193548387096]
Actions to choose Agent 1: dict_values([{'num_count': 2829, 'sum_payoffs': 423.61578938897213, 'action': [1.0, 0]}, {'num_count': 271, 'sum_payoffs': 31.20119999375985, 'action': [0.0, 0]}])
Weights num count: [0.9125806451612903, 0.08741935483870968]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 240, 'sum_payoffs': 33.5069999932984, 'action': [0.0, 0]}, {'num_count': 2860, 'sum_payoffs': 469.5029999060752, 'action': [1.0, 0]}])
Weights num count: [0.07741935483870968, 0.9225806451612903]
Actions to choose Agent 1: dict_values([{'num_count': 199, 'sum_payoffs': 27.836999994432468, 'action': [0.0, 0]}, {'num_count': 2901, 'sum_payoffs': 476.79299990461624, 'action': [1.0, 0]}])
Weights num count: [0.06419354838709677, 0.9358064516129032]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.2386000156402588 s
