Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 241, 'sum_payoffs': 26.929799994614115, 'action': [0.0, 0]}, {'num_count': 2859, 'sum_payoffs': 432.2422893872474, 'action': [1.0, 0]}])
Weights num count: [0.07774193548387097, 0.922258064516129]
Actions to choose Agent 1: dict_values([{'num_count': 2835, 'sum_payoffs': 426.0457893884861, 'action': [1.0, 0]}, {'num_count': 265, 'sum_payoffs': 29.189699994162144, 'action': [0.0, 0]}])
Weights num count: [0.9145161290322581, 0.08548387096774193]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 306, 'sum_payoffs': 43.307999991338164, 'action': [0.0, 0]}, {'num_count': 2794, 'sum_payoffs': 457.5959999084583, 'action': [1.0, 0]}])
Weights num count: [0.09870967741935484, 0.9012903225806451]
Actions to choose Agent 1: dict_values([{'num_count': 228, 'sum_payoffs': 34.559999993087835, 'action': [0.0, 0]}, {'num_count': 2872, 'sum_payoffs': 472.0139999055726, 'action': [1.0, 0]}])
Weights num count: [0.07354838709677419, 0.9264516129032258]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.1360790729522705 s
