Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 294, 'sum_payoffs': 59.741999989047095, 'action': [1.0, 0]}, {'num_count': 2806, 'sum_payoffs': 472.62374991335423, 'action': [0.0, 0]}])
Weights num count: [0.09483870967741935, 0.9051612903225806]
Actions to choose Agent 1: dict_values([{'num_count': 247, 'sum_payoffs': 47.161124991353674, 'action': [0.0, 0]}, {'num_count': 2853, 'sum_payoffs': 562.8431248968262, 'action': [1.0, 0]}])
Weights num count: [0.07967741935483871, 0.9203225806451613]
Selected final action: [0.0, 0, 1.0, 0]
Total payoff list: [0.4999999999083333, 0.4999999999083333]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 227, 'sum_payoffs': 17.90999999671648, 'action': [0.0, 0]}, {'num_count': 2873, 'sum_payoffs': 564.007499896601, 'action': [1.0, 0]}])
Weights num count: [0.0732258064516129, 0.9267741935483871]
Actions to choose Agent 1: dict_values([{'num_count': 2916, 'sum_payoffs': 637.0537498832239, 'action': [1.0, 0]}, {'num_count': 184, 'sum_payoffs': 39.431249992770944, 'action': [0.0, 0]}])
Weights num count: [0.9406451612903226, 0.05935483870967742]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.5263157893771929, 0.5263157893771929]
Terminal state: [1.0, 1.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.1525015830993652 s
