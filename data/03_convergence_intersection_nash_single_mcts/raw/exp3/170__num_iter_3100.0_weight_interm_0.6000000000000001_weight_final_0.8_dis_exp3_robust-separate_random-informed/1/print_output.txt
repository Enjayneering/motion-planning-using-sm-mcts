Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2857, 'sum_payoffs': 608.936278091073, 'action': [1.0, 0]}, {'num_count': 243, 'sum_payoffs': 42.251142849899885, 'action': [0.0, 0]}])
Weights num count: [0.9216129032258065, 0.07838709677419355]
Actions to choose Agent 1: dict_values([{'num_count': 260, 'sum_payoffs': 43.466142849691614, 'action': [0.0, 0]}, {'num_count': 2840, 'sum_payoffs': 606.2632780915314, 'action': [1.0, 0]}])
Weights num count: [0.08387096774193549, 0.9161290322580645]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 193, 'sum_payoffs': 36.025714279538334, 'action': [0.0, 0]}, {'num_count': 2907, 'sum_payoffs': 681.2871427403234, 'action': [1.0, 0]}])
Weights num count: [0.06225806451612903, 0.937741935483871]
Actions to choose Agent 1: dict_values([{'num_count': 234, 'sum_payoffs': 46.671428563427625, 'action': [0.0, 0]}, {'num_count': 2866, 'sum_payoffs': 673.1871427417137, 'action': [1.0, 0]}])
Weights num count: [0.07548387096774194, 0.9245161290322581]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.3845009803771973 s
