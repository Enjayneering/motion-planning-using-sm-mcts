Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2777, 'sum_payoffs': 214.47175258869032, 'action': [1.0, 0]}, {'num_count': 323, 'sum_payoffs': 19.584047364504226, 'action': [0.0, 0]}])
Weights num count: [0.8958064516129032, 0.10419354838709677]
Actions to choose Agent 1: dict_values([{'num_count': 2768, 'sum_payoffs': 212.39500258910545, 'action': [1.0, 0]}, {'num_count': 332, 'sum_payoffs': 21.369197364147183, 'action': [0.0, 0]}])
Weights num count: [0.8929032258064517, 0.10709677419354839]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 2774, 'sum_payoffs': 230.5484999538789, 'action': [1.0, 0]}, {'num_count': 326, 'sum_payoffs': 22.265999995546743, 'action': [0.0, 0]}])
Weights num count: [0.8948387096774194, 0.10516129032258065]
Actions to choose Agent 1: dict_values([{'num_count': 2784, 'sum_payoffs': 232.0064999535873, 'action': [1.0, 0]}, {'num_count': 316, 'sum_payoffs': 21.779999995643934, 'action': [0.0, 0]}])
Weights num count: [0.8980645161290323, 0.10193548387096774]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.2021734714508057 s
