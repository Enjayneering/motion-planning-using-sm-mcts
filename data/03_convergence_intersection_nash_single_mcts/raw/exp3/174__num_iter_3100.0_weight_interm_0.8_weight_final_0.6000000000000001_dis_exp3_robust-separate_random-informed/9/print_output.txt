Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 2832, 'sum_payoffs': 458.4635018010881, 'action': [1.0, 0]}, {'num_count': 268, 'sum_payoffs': 35.04117856542157, 'action': [0.0, 0]}])
Weights num count: [0.9135483870967742, 0.08645161290322581]
Actions to choose Agent 1: dict_values([{'num_count': 300, 'sum_payoffs': 36.13467856523415, 'action': [0.0, 0]}, {'num_count': 2800, 'sum_payoffs': 449.4030732312134, 'action': [1.0, 0]}])
Weights num count: [0.0967741935483871, 0.9032258064516129]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 250, 'sum_payoffs': 40.08857142169902, 'action': [0.0, 0]}, {'num_count': 2850, 'sum_payoffs': 501.78214277110277, 'action': [1.0, 0]}])
Weights num count: [0.08064516129032258, 0.9193548387096774]
Actions to choose Agent 1: dict_values([{'num_count': 2867, 'sum_payoffs': 503.17071419943613, 'action': [1.0, 0]}, {'num_count': 233, 'sum_payoffs': 34.1871428512822, 'action': [0.0, 0]}])
Weights num count: [0.9248387096774193, 0.07516129032258065]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.2213881015777588 s
