Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3248, 'sum_payoffs': 249.06280258177392, 'action': [1.0, 0]}, {'num_count': 352, 'sum_payoffs': 22.36724999552651, 'action': [0.0, 0]}])
Weights num count: [0.9022222222222223, 0.09777777777777778]
Actions to choose Agent 1: dict_values([{'num_count': 361, 'sum_payoffs': 21.536999995692575, 'action': [0.0, 0]}, {'num_count': 3239, 'sum_payoffs': 248.50795258188475, 'action': [1.0, 0]}])
Weights num count: [0.10027777777777777, 0.8997222222222222]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 3281, 'sum_payoffs': 272.3444999455192, 'action': [1.0, 0]}, {'num_count': 319, 'sum_payoffs': 22.630499995473805, 'action': [0.0, 0]}])
Weights num count: [0.9113888888888889, 0.08861111111111111]
Actions to choose Agent 1: dict_values([{'num_count': 482, 'sum_payoffs': 34.82099999303578, 'action': [0.0, 0]}, {'num_count': 3118, 'sum_payoffs': 257.56199994847293, 'action': [1.0, 0]}])
Weights num count: [0.1338888888888889, 0.8661111111111112]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.388965368270874 s
