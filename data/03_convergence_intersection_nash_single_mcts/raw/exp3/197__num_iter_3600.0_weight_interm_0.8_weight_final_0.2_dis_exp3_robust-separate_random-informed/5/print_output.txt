Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 333, 'sum_payoffs': 21.027599995794457, 'action': [0.0, 0]}, {'num_count': 3267, 'sum_payoffs': 251.53330258127954, 'action': [1.0, 0]}])
Weights num count: [0.0925, 0.9075]
Actions to choose Agent 1: dict_values([{'num_count': 359, 'sum_payoffs': 21.820499995635846, 'action': [0.0, 0]}, {'num_count': 3241, 'sum_payoffs': 247.02250258218154, 'action': [1.0, 0]}])
Weights num count: [0.09972222222222223, 0.9002777777777777]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 358, 'sum_payoffs': 25.991999994801493, 'action': [0.0, 0]}, {'num_count': 3242, 'sum_payoffs': 269.4689999460943, 'action': [1.0, 0]}])
Weights num count: [0.09944444444444445, 0.9005555555555556]
Actions to choose Agent 1: dict_values([{'num_count': 3261, 'sum_payoffs': 269.46899994609396, 'action': [1.0, 0]}, {'num_count': 339, 'sum_payoffs': 22.58999999548192, 'action': [0.0, 0]}])
Weights num count: [0.9058333333333334, 0.09416666666666666]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.4070103168487549 s
