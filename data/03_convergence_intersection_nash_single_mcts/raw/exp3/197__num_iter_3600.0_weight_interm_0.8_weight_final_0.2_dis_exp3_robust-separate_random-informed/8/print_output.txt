Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3172, 'sum_payoffs': 244.0691525827722, 'action': [1.0, 0]}, {'num_count': 428, 'sum_payoffs': 26.728247363075308, 'action': [0.0, 0]}])
Weights num count: [0.8811111111111111, 0.11888888888888889]
Actions to choose Agent 1: dict_values([{'num_count': 370, 'sum_payoffs': 24.139397363593105, 'action': [0.0, 0]}, {'num_count': 3230, 'sum_payoffs': 245.78320258242937, 'action': [1.0, 0]}])
Weights num count: [0.10277777777777777, 0.8972222222222223]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 327, 'sum_payoffs': 22.549499995490013, 'action': [0.0, 0]}, {'num_count': 3273, 'sum_payoffs': 271.16999994575406, 'action': [1.0, 0]}])
Weights num count: [0.09083333333333334, 0.9091666666666667]
Actions to choose Agent 1: dict_values([{'num_count': 3238, 'sum_payoffs': 268.33499994632047, 'action': [1.0, 0]}, {'num_count': 362, 'sum_payoffs': 25.546499994890592, 'action': [0.0, 0]}])
Weights num count: [0.8994444444444445, 0.10055555555555555]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.4796411991119385 s
