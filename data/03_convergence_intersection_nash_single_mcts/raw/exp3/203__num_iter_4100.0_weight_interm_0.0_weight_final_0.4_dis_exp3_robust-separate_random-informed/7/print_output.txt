Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3811, 'sum_payoffs': 1408.427999507015, 'action': [1.0, 0]}, {'num_count': 289, 'sum_payoffs': 75.99824997340026, 'action': [0.0, 0]}])
Weights num count: [0.9295121951219513, 0.07048780487804877]
Actions to choose Agent 1: dict_values([{'num_count': 240, 'sum_payoffs': 69.0727499758242, 'action': [0.0, 0]}, {'num_count': 3860, 'sum_payoffs': 1435.4009994975697, 'action': [1.0, 0]}])
Weights num count: [0.05853658536585366, 0.9414634146341463]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 239, 'sum_payoffs': 88.28999996909828, 'action': [0.0, 0]}, {'num_count': 3861, 'sum_payoffs': 1574.0324994491366, 'action': [1.0, 0]}])
Weights num count: [0.05829268292682927, 0.9417073170731707]
Actions to choose Agent 1: dict_values([{'num_count': 211, 'sum_payoffs': 75.1274999737052, 'action': [0.0, 0]}, {'num_count': 3889, 'sum_payoffs': 1583.1449994459492, 'action': [1.0, 0]}])
Weights num count: [0.05146341463414634, 0.9485365853658536]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.4674129486083984 s
