Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3795, 'sum_payoffs': 707.5496247523399, 'action': [1.0, 0]}, {'num_count': 305, 'sum_payoffs': 47.12062498350776, 'action': [0.0, 0]}])
Weights num count: [0.925609756097561, 0.07439024390243902]
Actions to choose Agent 1: dict_values([{'num_count': 3758, 'sum_payoffs': 697.2524997559459, 'action': [1.0, 0]}, {'num_count': 342, 'sum_payoffs': 54.50174998092434, 'action': [0.0, 0]}])
Weights num count: [0.9165853658536586, 0.08341463414634147]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 277, 'sum_payoffs': 50.66999998226538, 'action': [0.0, 0]}, {'num_count': 3823, 'sum_payoffs': 780.4237497268748, 'action': [1.0, 0]}])
Weights num count: [0.0675609756097561, 0.932439024390244]
Actions to choose Agent 1: dict_values([{'num_count': 300, 'sum_payoffs': 54.82124998081241, 'action': [0.0, 0]}, {'num_count': 3800, 'sum_payoffs': 777.0824997280439, 'action': [1.0, 0]}])
Weights num count: [0.07317073170731707, 0.926829268292683]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.5053160190582275 s
