Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 329, 'sum_payoffs': 61.153874978596065, 'action': [0.0, 0]}, {'num_count': 3771, 'sum_payoffs': 715.4172826443191, 'action': [1.0, 0]}])
Weights num count: [0.08024390243902439, 0.9197560975609756]
Actions to choose Agent 1: dict_values([{'num_count': 3665, 'sum_payoffs': 644.5766247743898, 'action': [0.0, 0]}, {'num_count': 435, 'sum_payoffs': 85.52078286480486, 'action': [1.0, 0]}])
Weights num count: [0.8939024390243903, 0.10609756097560975]
Selected final action: [1.0, 0, 0.0, 0]
Total payoff list: [0.499999999825, 0.499999999825]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 3813, 'sum_payoffs': 827.4037497104388, 'action': [1.0, 0]}, {'num_count': 287, 'sum_payoffs': 66.36374997677268, 'action': [0.0, 0]}])
Weights num count: [0.93, 0.07]
Actions to choose Agent 1: dict_values([{'num_count': 3823, 'sum_payoffs': 748.5862497380139, 'action': [1.0, 0]}, {'num_count': 277, 'sum_payoffs': 20.64374999277464, 'action': [0.0, 0]}])
Weights num count: [0.932439024390244, 0.0675609756097561]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.5263157892894736, 0.5263157892894736]
Terminal state: [1.0, 2.0, 1.5707963267948966, 1.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.5694026947021484 s
