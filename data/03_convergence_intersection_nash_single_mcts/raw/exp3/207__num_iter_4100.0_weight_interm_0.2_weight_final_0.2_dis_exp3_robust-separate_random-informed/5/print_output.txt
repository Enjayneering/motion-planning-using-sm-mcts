Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 309, 'sum_payoffs': 48.23384208838119, 'action': [0.0, 0]}, {'num_count': 3791, 'sum_payoffs': 714.5971576446074, 'action': [1.0, 0]}])
Weights num count: [0.07536585365853658, 0.9246341463414635]
Actions to choose Agent 1: dict_values([{'num_count': 388, 'sum_payoffs': 53.92409208638967, 'action': [0.0, 0]}, {'num_count': 3712, 'sum_payoffs': 688.4949076537495, 'action': [1.0, 0]}])
Weights num count: [0.09463414634146342, 0.9053658536585366]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 3827, 'sum_payoffs': 785.18249972521, 'action': [1.0, 0]}, {'num_count': 273, 'sum_payoffs': 49.556249982655174, 'action': [0.0, 0]}])
Weights num count: [0.9334146341463415, 0.06658536585365854]
Actions to choose Agent 1: dict_values([{'num_count': 274, 'sum_payoffs': 46.51874998371835, 'action': [0.0, 0]}, {'num_count': 3826, 'sum_payoffs': 781.7399997264145, 'action': [1.0, 0]}])
Weights num count: [0.06682926829268293, 0.9331707317073171]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.8361403942108154 s
