Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3823, 'sum_payoffs': 707.7684076470003, 'action': [1.0, 0]}, {'num_count': 277, 'sum_payoffs': 38.71634209171231, 'action': [0.0, 0]}])
Weights num count: [0.932439024390244, 0.0675609756097561]
Actions to choose Agent 1: dict_values([{'num_count': 274, 'sum_payoffs': 40.36221709113618, 'action': [0.0, 0]}, {'num_count': 3826, 'sum_payoffs': 716.510782643938, 'action': [1.0, 0]}])
Weights num count: [0.06682926829268293, 0.9331707317073171]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 307, 'sum_payoffs': 56.54249998020999, 'action': [0.0, 0]}, {'num_count': 3793, 'sum_payoffs': 776.3737497282916, 'action': [1.0, 0]}])
Weights num count: [0.0748780487804878, 0.9251219512195122]
Actions to choose Agent 1: dict_values([{'num_count': 265, 'sum_payoffs': 47.328749983434854, 'action': [0.0, 0]}, {'num_count': 3835, 'sum_payoffs': 782.75249972606, 'action': [1.0, 0]}])
Weights num count: [0.06463414634146342, 0.9353658536585366]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.666851282119751 s
