Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3848, 'sum_payoffs': 1146.4962629285862, 'action': [1.0, 0]}, {'num_count': 252, 'sum_payoffs': 55.90553683092431, 'action': [0.0, 0]}])
Weights num count: [0.9385365853658536, 0.06146341463414634]
Actions to choose Agent 1: dict_values([{'num_count': 247, 'sum_payoffs': 49.052936832294805, 'action': [0.0, 0]}, {'num_count': 3853, 'sum_payoffs': 1144.0176629290836, 'action': [1.0, 0]}])
Weights num count: [0.06024390243902439, 0.9397560975609756]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 244, 'sum_payoffs': 71.94599998561037, 'action': [0.0, 0]}, {'num_count': 3856, 'sum_payoffs': 1260.2339997479412, 'action': [1.0, 0]}])
Weights num count: [0.05951219512195122, 0.9404878048780487]
Actions to choose Agent 1: dict_values([{'num_count': 250, 'sum_payoffs': 70.9739999858048, 'action': [0.0, 0]}, {'num_count': 3850, 'sum_payoffs': 1255.0499997489774, 'action': [1.0, 0]}])
Weights num count: [0.06097560975609756, 0.9390243902439024]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.5844342708587646 s
