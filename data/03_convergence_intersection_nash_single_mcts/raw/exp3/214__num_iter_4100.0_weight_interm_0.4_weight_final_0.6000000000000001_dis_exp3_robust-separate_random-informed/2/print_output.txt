Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 333, 'sum_payoffs': 60.40889998791791, 'action': [0.0, 0]}, {'num_count': 3767, 'sum_payoffs': 835.1135998329382, 'action': [1.0, 0]}])
Weights num count: [0.08121951219512195, 0.9187804878048781]
Actions to choose Agent 1: dict_values([{'num_count': 297, 'sum_payoffs': 52.750349989449724, 'action': [0.0, 0]}, {'num_count': 3803, 'sum_payoffs': 853.2697498293056, 'action': [1.0, 0]}])
Weights num count: [0.0724390243902439, 0.9275609756097561]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 266, 'sum_payoffs': 67.42349998651535, 'action': [1.0, 0]}, {'num_count': 3834, 'sum_payoffs': 908.4509998182949, 'action': [0.0, 0]}])
Weights num count: [0.06487804878048781, 0.9351219512195122]
Actions to choose Agent 1: dict_values([{'num_count': 3877, 'sum_payoffs': 969.5609998060709, 'action': [1.0, 0]}, {'num_count': 223, 'sum_payoffs': 49.32449999013514, 'action': [0.0, 0]}])
Weights num count: [0.9456097560975609, 0.054390243902439024]
Selected final action: [0.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 1.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.556474208831787 s
