Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3738, 'sum_payoffs': 354.74198676228997, 'action': [1.0, 0]}, {'num_count': 362, 'sum_payoffs': 27.19237499388166, 'action': [0.0, 0]}])
Weights num count: [0.9117073170731708, 0.08829268292682926]
Actions to choose Agent 1: dict_values([{'num_count': 338, 'sum_payoffs': 26.09887499412769, 'action': [0.0, 0]}, {'num_count': 3762, 'sum_payoffs': 356.9289867617982, 'action': [1.0, 0]}])
Weights num count: [0.0824390243902439, 0.9175609756097561]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 399, 'sum_payoffs': 34.188749992307756, 'action': [0.0, 0]}, {'num_count': 3701, 'sum_payoffs': 382.45499991393285, 'action': [1.0, 0]}])
Weights num count: [0.09731707317073171, 0.9026829268292683]
Actions to choose Agent 1: dict_values([{'num_count': 3743, 'sum_payoffs': 387.6693749127579, 'action': [1.0, 0]}, {'num_count': 357, 'sum_payoffs': 29.986874993253174, 'action': [0.0, 0]}])
Weights num count: [0.9129268292682927, 0.08707317073170731]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.576636791229248 s
