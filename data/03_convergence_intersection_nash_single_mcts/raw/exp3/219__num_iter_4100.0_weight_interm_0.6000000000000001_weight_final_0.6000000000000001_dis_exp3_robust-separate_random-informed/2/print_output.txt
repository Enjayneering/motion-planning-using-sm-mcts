Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 335, 'sum_payoffs': 44.534249991835246, 'action': [0.0, 0]}, {'num_count': 3765, 'sum_payoffs': 699.44512487182, 'action': [1.0, 0]}])
Weights num count: [0.08170731707317073, 0.9182926829268293]
Actions to choose Agent 1: dict_values([{'num_count': 3823, 'sum_payoffs': 719.5781248681355, 'action': [1.0, 0]}, {'num_count': 277, 'sum_payoffs': 39.892499992686275, 'action': [0.0, 0]}])
Weights num count: [0.932439024390244, 0.0675609756097561]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 261, 'sum_payoffs': 46.72124999143443, 'action': [0.0, 0]}, {'num_count': 3839, 'sum_payoffs': 785.1824998561044, 'action': [1.0, 0]}])
Weights num count: [0.06365853658536585, 0.9363414634146342]
Actions to choose Agent 1: dict_values([{'num_count': 238, 'sum_payoffs': 41.759999992343964, 'action': [0.0, 0]}, {'num_count': 3862, 'sum_payoffs': 789.73874985527, 'action': [1.0, 0]}])
Weights num count: [0.05804878048780488, 0.9419512195121951]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.5527591705322266 s
