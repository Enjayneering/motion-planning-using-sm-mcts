Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3874, 'sum_payoffs': 724.1247827620406, 'action': [1.0, 0]}, {'num_count': 226, 'sum_payoffs': 33.01199999394773, 'action': [0.0, 0]}])
Weights num count: [0.9448780487804878, 0.055121951219512196]
Actions to choose Agent 1: dict_values([{'num_count': 3794, 'sum_payoffs': 704.3000327656688, 'action': [1.0, 0]}, {'num_count': 306, 'sum_payoffs': 41.7194999923513, 'action': [0.0, 0]}])
Weights num count: [0.9253658536585366, 0.07463414634146341]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 3823, 'sum_payoffs': 782.3474998566237, 'action': [1.0, 0]}, {'num_count': 277, 'sum_payoffs': 46.82249999141589, 'action': [0.0, 0]}])
Weights num count: [0.932439024390244, 0.0675609756097561]
Actions to choose Agent 1: dict_values([{'num_count': 3761, 'sum_payoffs': 771.5137498586071, 'action': [1.0, 0]}, {'num_count': 339, 'sum_payoffs': 62.71874998850158, 'action': [0.0, 0]}])
Weights num count: [0.9173170731707317, 0.0826829268292683]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.5476226806640625 s
