Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3851, 'sum_payoffs': 720.0337498680519, 'action': [1.0, 0]}, {'num_count': 249, 'sum_payoffs': 36.1451249933733, 'action': [0.0, 0]}])
Weights num count: [0.9392682926829268, 0.06073170731707317]
Actions to choose Agent 1: dict_values([{'num_count': 291, 'sum_payoffs': 40.61024999255467, 'action': [0.0, 0]}, {'num_count': 3809, 'sum_payoffs': 708.4608748701716, 'action': [1.0, 0]}])
Weights num count: [0.07097560975609755, 0.9290243902439025]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 296, 'sum_payoffs': 53.30249999022788, 'action': [0.0, 0]}, {'num_count': 3804, 'sum_payoffs': 776.8799998576247, 'action': [1.0, 0]}])
Weights num count: [0.0721951219512195, 0.9278048780487805]
Actions to choose Agent 1: dict_values([{'num_count': 3804, 'sum_payoffs': 778.6012498573098, 'action': [1.0, 0]}, {'num_count': 296, 'sum_payoffs': 54.61874998998656, 'action': [0.0, 0]}])
Weights num count: [0.9278048780487805, 0.0721951219512195]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.5717096328735352 s
