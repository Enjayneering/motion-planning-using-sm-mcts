Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3815, 'sum_payoffs': 814.9308494843193, 'action': [1.0, 0]}, {'num_count': 285, 'sum_payoffs': 46.82957142054363, 'action': [0.0, 0]}])
Weights num count: [0.9304878048780488, 0.06951219512195123]
Actions to choose Agent 1: dict_values([{'num_count': 276, 'sum_payoffs': 42.38999999273323, 'action': [0.0, 0]}, {'num_count': 3824, 'sum_payoffs': 811.4555637706297, 'action': [1.0, 0]}])
Weights num count: [0.06731707317073171, 0.9326829268292683]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 271, 'sum_payoffs': 56.62285713315017, 'action': [0.0, 0]}, {'num_count': 3829, 'sum_payoffs': 896.7471427033402, 'action': [1.0, 0]}])
Weights num count: [0.06609756097560976, 0.9339024390243903]
Actions to choose Agent 1: dict_values([{'num_count': 3713, 'sum_payoffs': 866.8928569941788, 'action': [1.0, 0]}, {'num_count': 387, 'sum_payoffs': 79.9971428434292, 'action': [0.0, 0]}])
Weights num count: [0.905609756097561, 0.09439024390243902]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.4854021072387695 s
