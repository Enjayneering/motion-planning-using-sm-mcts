Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 322, 'sum_payoffs': 18.88514999622297, 'action': [0.0, 0]}, {'num_count': 3778, 'sum_payoffs': 288.6394025738484, 'action': [1.0, 0]}])
Weights num count: [0.07853658536585366, 0.9214634146341464]
Actions to choose Agent 1: dict_values([{'num_count': 353, 'sum_payoffs': 20.953799995809206, 'action': [0.0, 0]}, {'num_count': 3747, 'sum_payoffs': 284.38375257470085, 'action': [1.0, 0]}])
Weights num count: [0.08609756097560975, 0.9139024390243903]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 3639, 'sum_payoffs': 301.30199993973383, 'action': [1.0, 0]}, {'num_count': 461, 'sum_payoffs': 31.98599999360272, 'action': [0.0, 0]}])
Weights num count: [0.8875609756097561, 0.1124390243902439]
Actions to choose Agent 1: dict_values([{'num_count': 472, 'sum_payoffs': 34.01099999319776, 'action': [0.0, 0]}, {'num_count': 3628, 'sum_payoffs': 301.3019999397336, 'action': [1.0, 0]}])
Weights num count: [0.1151219512195122, 0.8848780487804878]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 2.4673843383789062 s
