Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 3642, 'sum_payoffs': 274.76095257662865, 'action': [1.0, 0]}, {'num_count': 458, 'sum_payoffs': 30.004649993998946, 'action': [0.0, 0]}])
Weights num count: [0.8882926829268293, 0.11170731707317073]
Actions to choose Agent 1: dict_values([{'num_count': 322, 'sum_payoffs': 21.48434999570311, 'action': [0.0, 0]}, {'num_count': 3778, 'sum_payoffs': 290.5712525734615, 'action': [1.0, 0]}])
Weights num count: [0.07853658536585366, 0.9214634146341464]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 418, 'sum_payoffs': 28.502999994299294, 'action': [0.0, 0]}, {'num_count': 3682, 'sum_payoffs': 304.784999939038, 'action': [1.0, 0]}])
Weights num count: [0.10195121951219512, 0.8980487804878049]
Actions to choose Agent 1: dict_values([{'num_count': 404, 'sum_payoffs': 29.029499994194015, 'action': [0.0, 0]}, {'num_count': 3696, 'sum_payoffs': 306.2024999387547, 'action': [1.0, 0]}])
Weights num count: [0.09853658536585366, 0.9014634146341464]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 2.0046353340148926 s
