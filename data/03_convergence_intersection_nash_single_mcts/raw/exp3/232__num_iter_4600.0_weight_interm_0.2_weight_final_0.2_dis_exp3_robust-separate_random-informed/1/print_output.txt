Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 4207, 'sum_payoffs': 786.5606247246689, 'action': [1.0, 0]}, {'num_count': 393, 'sum_payoffs': 57.46387497988758, 'action': [0.0, 0]}])
Weights num count: [0.9145652173913044, 0.08543478260869565]
Actions to choose Agent 1: dict_values([{'num_count': 4263, 'sum_payoffs': 794.9384997217351, 'action': [1.0, 0]}, {'num_count': 337, 'sum_payoffs': 48.356999983074985, 'action': [0.0, 0]}])
Weights num count: [0.9267391304347826, 0.07326086956521739]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 289, 'sum_payoffs': 51.58124998194642, 'action': [0.0, 0]}, {'num_count': 4311, 'sum_payoffs': 882.0787496913129, 'action': [1.0, 0]}])
Weights num count: [0.06282608695652174, 0.9371739130434783]
Actions to choose Agent 1: dict_values([{'num_count': 308, 'sum_payoffs': 55.631249980528935, 'action': [0.0, 0]}, {'num_count': 4292, 'sum_payoffs': 876.4087496932965, 'action': [1.0, 0]}])
Weights num count: [0.06695652173913043, 0.9330434782608695]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.6778528690338135 s
