Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 323, 'sum_payoffs': 42.94012498497082, 'action': [0.0, 0]}, {'num_count': 4277, 'sum_payoffs': 793.9670326168128, 'action': [1.0, 0]}])
Weights num count: [0.07021739130434783, 0.9297826086956522]
Actions to choose Agent 1: dict_values([{'num_count': 4307, 'sum_payoffs': 806.325157612485, 'action': [1.0, 0]}, {'num_count': 293, 'sum_payoffs': 43.339499984831065, 'action': [0.0, 0]}])
Weights num count: [0.936304347826087, 0.06369565217391304]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 359, 'sum_payoffs': 64.13624997755217, 'action': [0.0, 0]}, {'num_count': 4241, 'sum_payoffs': 866.587499696732, 'action': [1.0, 0]}])
Weights num count: [0.07804347826086956, 0.9219565217391305]
Actions to choose Agent 1: dict_values([{'num_count': 300, 'sum_payoffs': 55.023749980741556, 'action': [0.0, 0]}, {'num_count': 4300, 'sum_payoffs': 879.9524996920566, 'action': [1.0, 0]}])
Weights num count: [0.06521739130434782, 0.9347826086956522]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.699145793914795 s
