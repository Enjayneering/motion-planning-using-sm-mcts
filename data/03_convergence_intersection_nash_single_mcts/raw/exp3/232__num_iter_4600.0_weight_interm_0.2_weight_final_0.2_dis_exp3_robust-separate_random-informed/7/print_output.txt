Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 345, 'sum_payoffs': 50.138999982451224, 'action': [0.0, 0]}, {'num_count': 4255, 'sum_payoffs': 773.6152497292017, 'action': [1.0, 0]}])
Weights num count: [0.075, 0.925]
Actions to choose Agent 1: dict_values([{'num_count': 4203, 'sum_payoffs': 801.4449076141925, 'action': [1.0, 0]}, {'num_count': 397, 'sum_payoffs': 61.49309208374041, 'action': [0.0, 0]}])
Weights num count: [0.913695652173913, 0.08630434782608695]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 4322, 'sum_payoffs': 883.3949996908523, 'action': [1.0, 0]}, {'num_count': 278, 'sum_payoffs': 50.16374998244255, 'action': [0.0, 0]}])
Weights num count: [0.9395652173913044, 0.06043478260869565]
Actions to choose Agent 1: dict_values([{'num_count': 4331, 'sum_payoffs': 883.7999996907105, 'action': [1.0, 0]}, {'num_count': 269, 'sum_payoffs': 48.13874998315135, 'action': [0.0, 0]}])
Weights num count: [0.9415217391304348, 0.058478260869565216]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.9197356700897217 s
