Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 391, 'sum_payoffs': 35.94228946409938, 'action': [0.0, 0]}, {'num_count': 4209, 'sum_payoffs': 528.8894603853187, 'action': [1.0, 0]}])
Weights num count: [0.085, 0.915]
Actions to choose Agent 1: dict_values([{'num_count': 351, 'sum_payoffs': 33.14703946484486, 'action': [0.0, 0]}, {'num_count': 4249, 'sum_payoffs': 533.0212103842155, 'action': [1.0, 0]}])
Weights num count: [0.07630434782608696, 0.923695652173913]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 375, 'sum_payoffs': 45.757499987797985, 'action': [0.0, 0]}, {'num_count': 4225, 'sum_payoffs': 578.5949998457073, 'action': [1.0, 0]}])
Weights num count: [0.08152173913043478, 0.9184782608695652]
Actions to choose Agent 1: dict_values([{'num_count': 331, 'sum_payoffs': 38.669999989687916, 'action': [0.0, 0]}, {'num_count': 4269, 'sum_payoffs': 582.4424998446805, 'action': [1.0, 0]}])
Weights num count: [0.07195652173913043, 0.9280434782608695]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.6674458980560303 s
