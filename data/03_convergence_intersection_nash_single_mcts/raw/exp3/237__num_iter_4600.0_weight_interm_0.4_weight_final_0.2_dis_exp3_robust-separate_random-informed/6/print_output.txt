Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 343, 'sum_payoffs': 32.32424999138007, 'action': [0.0, 0]}, {'num_count': 4257, 'sum_payoffs': 535.5044603835539, 'action': [1.0, 0]}])
Weights num count: [0.07456521739130435, 0.9254347826086956]
Actions to choose Agent 1: dict_values([{'num_count': 306, 'sum_payoffs': 28.982999992271104, 'action': [0.0, 0]}, {'num_count': 4294, 'sum_payoffs': 538.3597103827911, 'action': [1.0, 0]}])
Weights num count: [0.06652173913043478, 0.9334782608695652]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 399, 'sum_payoffs': 56.95499998481215, 'action': [1.0, 0]}, {'num_count': 4201, 'sum_payoffs': 550.9949998530752, 'action': [0.0, 0]}])
Weights num count: [0.08673913043478261, 0.9132608695652173]
Actions to choose Agent 1: dict_values([{'num_count': 307, 'sum_payoffs': 38.19749998981394, 'action': [0.0, 0]}, {'num_count': 4293, 'sum_payoffs': 599.3174998401738, 'action': [1.0, 0]}])
Weights num count: [0.0667391304347826, 0.9332608695652174]
Selected final action: [0.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 1.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.6159493923187256 s
