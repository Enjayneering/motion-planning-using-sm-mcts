Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 4287, 'sum_payoffs': 794.3917498544429, 'action': [1.0, 0]}, {'num_count': 313, 'sum_payoffs': 42.979499992120296, 'action': [0.0, 0]}])
Weights num count: [0.9319565217391305, 0.06804347826086957]
Actions to choose Agent 1: dict_values([{'num_count': 285, 'sum_payoffs': 45.17609209698073, 'action': [0.0, 0]}, {'num_count': 4315, 'sum_payoffs': 804.7704077472794, 'action': [1.0, 0]}])
Weights num count: [0.06195652173913044, 0.9380434782608695]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 4315, 'sum_payoffs': 882.5849998382712, 'action': [1.0, 0]}, {'num_count': 285, 'sum_payoffs': 49.65749999089612, 'action': [0.0, 0]}])
Weights num count: [0.9380434782608695, 0.06195652173913044]
Actions to choose Agent 1: dict_values([{'num_count': 4302, 'sum_payoffs': 881.0662498385493, 'action': [1.0, 0]}, {'num_count': 298, 'sum_payoffs': 52.39124999039491, 'action': [0.0, 0]}])
Weights num count: [0.9352173913043478, 0.06478260869565218]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 2.40513277053833 s
