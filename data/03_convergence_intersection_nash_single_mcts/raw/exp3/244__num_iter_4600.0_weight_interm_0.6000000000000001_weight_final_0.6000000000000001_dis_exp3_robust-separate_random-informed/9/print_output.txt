Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 304, 'sum_payoffs': 47.403592096572346, 'action': [0.0, 0]}, {'num_count': 4296, 'sum_payoffs': 799.5256577482386, 'action': [1.0, 0]}])
Weights num count: [0.06608695652173913, 0.9339130434782609]
Actions to choose Agent 1: dict_values([{'num_count': 279, 'sum_payoffs': 39.566842098009126, 'action': [0.0, 0]}, {'num_count': 4321, 'sum_payoffs': 802.6239077476721, 'action': [1.0, 0]}])
Weights num count: [0.06065217391304348, 0.9393478260869565]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 4139, 'sum_payoffs': 845.7299998450192, 'action': [1.0, 0]}, {'num_count': 461, 'sum_payoffs': 86.81624998408445, 'action': [0.0, 0]}])
Weights num count: [0.8997826086956522, 0.10021739130434783]
Actions to choose Agent 1: dict_values([{'num_count': 326, 'sum_payoffs': 59.174999989151274, 'action': [0.0, 0]}, {'num_count': 4274, 'sum_payoffs': 873.776249839884, 'action': [1.0, 0]}])
Weights num count: [0.07086956521739131, 0.9291304347826087]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 2.011701822280884 s
