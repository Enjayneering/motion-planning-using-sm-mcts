Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 4158, 'sum_payoffs': 313.952802568778, 'action': [1.0, 0]}, {'num_count': 442, 'sum_payoffs': 27.94409999441105, 'action': [0.0, 0]}])
Weights num count: [0.9039130434782608, 0.09608695652173913]
Actions to choose Agent 1: dict_values([{'num_count': 389, 'sum_payoffs': 22.23854999555226, 'action': [0.0, 0]}, {'num_count': 4211, 'sum_payoffs': 321.7724525672114, 'action': [1.0, 0]}])
Weights num count: [0.08456521739130435, 0.9154347826086957]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 4177, 'sum_payoffs': 344.75849993105163, 'action': [1.0, 0]}, {'num_count': 423, 'sum_payoffs': 28.90799999421831, 'action': [0.0, 0]}])
Weights num count: [0.9080434782608696, 0.09195652173913044]
Actions to choose Agent 1: dict_values([{'num_count': 424, 'sum_payoffs': 30.446999993910502, 'action': [0.0, 0]}, {'num_count': 4176, 'sum_payoffs': 345.4064999309218, 'action': [1.0, 0]}])
Weights num count: [0.09217391304347826, 0.9078260869565218]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.6499919891357422 s
