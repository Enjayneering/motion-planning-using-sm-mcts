Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 4274, 'sum_payoffs': 795.7490327654614, 'action': [1.0, 0]}, {'num_count': 326, 'sum_payoffs': 44.539874992762165, 'action': [0.0, 0]}])
Weights num count: [0.9291304347826087, 0.07086956521739131]
Actions to choose Agent 1: dict_values([{'num_count': 275, 'sum_payoffs': 35.47237499423565, 'action': [0.0, 0]}, {'num_count': 4325, 'sum_payoffs': 809.5550327632184, 'action': [1.0, 0]}])
Weights num count: [0.059782608695652176, 0.9402173913043478]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 4221, 'sum_payoffs': 864.6637498595363, 'action': [1.0, 0]}, {'num_count': 379, 'sum_payoffs': 67.27499998906795, 'action': [0.0, 0]}])
Weights num count: [0.917608695652174, 0.08239130434782609]
Actions to choose Agent 1: dict_values([{'num_count': 4267, 'sum_payoffs': 875.4974998577778, 'action': [1.0, 0]}, {'num_count': 333, 'sum_payoffs': 58.46624999049947, 'action': [0.0, 0]}])
Weights num count: [0.927608695652174, 0.07239130434782609]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.7191598415374756 s
