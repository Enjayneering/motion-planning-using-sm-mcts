Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 4330, 'sum_payoffs': 807.7269077635143, 'action': [1.0, 0]}, {'num_count': 270, 'sum_payoffs': 39.84637499352489, 'action': [0.0, 0]}])
Weights num count: [0.941304347826087, 0.058695652173913045]
Actions to choose Agent 1: dict_values([{'num_count': 293, 'sum_payoffs': 39.0262499936581, 'action': [0.0, 0]}, {'num_count': 4307, 'sum_payoffs': 799.7990327648015, 'action': [1.0, 0]}])
Weights num count: [0.06369565217391304, 0.936304347826087]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 292, 'sum_payoffs': 48.138749992177615, 'action': [0.0, 0]}, {'num_count': 4308, 'sum_payoffs': 882.8887498565783, 'action': [1.0, 0]}])
Weights num count: [0.06347826086956522, 0.9365217391304348]
Actions to choose Agent 1: dict_values([{'num_count': 281, 'sum_payoffs': 48.13874999217764, 'action': [0.0, 0]}, {'num_count': 4319, 'sum_payoffs': 886.3312498560196, 'action': [1.0, 0]}])
Weights num count: [0.06108695652173913, 0.9389130434782609]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 1.6277744770050049 s
