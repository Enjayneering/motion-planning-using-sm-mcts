Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 93, 'sum_payoffs': 19.204714282422042, 'action': [0.0, 0]}, {'num_count': 507, 'sum_payoffs': 105.44027817741366, 'action': [1.0, 0]}])
Weights num count: [0.155, 0.845]
Actions to choose Agent 1: dict_values([{'num_count': 496, 'sum_payoffs': 116.68770674691417, 'action': [1.0, 0]}, {'num_count': 104, 'sum_payoffs': 19.829571425172066, 'action': [0.0, 0]}])
Weights num count: [0.8266666666666667, 0.17333333333333334]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 509, 'sum_payoffs': 122.15571426477435, 'action': [1.0, 0]}, {'num_count': 91, 'sum_payoffs': 18.437142853982184, 'action': [0.0, 0]}])
Weights num count: [0.8483333333333334, 0.15166666666666667]
Actions to choose Agent 1: dict_values([{'num_count': 132, 'sum_payoffs': 27.23142856676026, 'action': [0.0, 0]}, {'num_count': 468, 'sum_payoffs': 111.97285712366255, 'action': [1.0, 0]}])
Weights num count: [0.22, 0.78]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.23758149147033691 s
