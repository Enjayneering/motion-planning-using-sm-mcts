Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 160, 'sum_payoffs': 18.722999996567466, 'action': [0.0, 0]}, {'num_count': 440, 'sum_payoffs': 58.764749989225976, 'action': [1.0, 0]}])
Weights num count: [0.26666666666666666, 0.7333333333333333]
Actions to choose Agent 1: dict_values([{'num_count': 117, 'sum_payoffs': 15.388499997178803, 'action': [0.0, 0]}, {'num_count': 483, 'sum_payoffs': 67.20224998767917, 'action': [1.0, 0]}])
Weights num count: [0.195, 0.805]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 441, 'sum_payoffs': 63.164999988419666, 'action': [1.0, 0]}, {'num_count': 159, 'sum_payoffs': 18.68999999657348, 'action': [0.0, 0]}])
Weights num count: [0.735, 0.265]
Actions to choose Agent 1: dict_values([{'num_count': 461, 'sum_payoffs': 67.34999998765241, 'action': [1.0, 0]}, {'num_count': 139, 'sum_payoffs': 16.25999999701898, 'action': [0.0, 0]}])
Weights num count: [0.7683333333333333, 0.23166666666666666]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.28320813179016113 s
