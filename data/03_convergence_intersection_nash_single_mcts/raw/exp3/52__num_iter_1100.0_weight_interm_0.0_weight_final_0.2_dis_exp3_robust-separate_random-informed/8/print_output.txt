Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 991, 'sum_payoffs': 387.2812497676236, 'action': [1.0, 0]}, {'num_count': 109, 'sum_payoffs': 32.2582499806451, 'action': [0.0, 0]}])
Weights num count: [0.9009090909090909, 0.09909090909090909]
Actions to choose Agent 1: dict_values([{'num_count': 207, 'sum_payoffs': 82.19474995068336, 'action': [1.0, 0]}, {'num_count': 893, 'sum_payoffs': 300.1657498198953, 'action': [0.0, 0]}])
Weights num count: [0.18818181818181817, 0.8118181818181818]
Selected final action: [1.0, 0, 0.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 108, 'sum_payoffs': 54.06749996755959, 'action': [1.0, 0]}, {'num_count': 992, 'sum_payoffs': 406.4174997561557, 'action': [0.0, 0]}])
Weights num count: [0.09818181818181818, 0.9018181818181819]
Actions to choose Agent 1: dict_values([{'num_count': 67, 'sum_payoffs': 13.769999991738011, 'action': [0.0, 0]}, {'num_count': 1033, 'sum_payoffs': 416.3399997502026, 'action': [1.0, 0]}])
Weights num count: [0.060909090909090906, 0.9390909090909091]
Selected final action: [0.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 1.0, 1.5707963267948966, 1.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4278225898742676 s
