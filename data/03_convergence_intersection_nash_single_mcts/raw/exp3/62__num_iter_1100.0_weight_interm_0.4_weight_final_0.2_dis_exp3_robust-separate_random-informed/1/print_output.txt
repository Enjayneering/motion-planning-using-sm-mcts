Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 195, 'sum_payoffs': 19.513499994796387, 'action': [0.0, 0]}, {'num_count': 905, 'sum_payoffs': 119.0774604945598, 'action': [1.0, 0]}])
Weights num count: [0.17727272727272728, 0.8227272727272728]
Actions to choose Agent 1: dict_values([{'num_count': 167, 'sum_payoffs': 18.9809999949384, 'action': [0.0, 0]}, {'num_count': 933, 'sum_payoffs': 123.25496049344571, 'action': [1.0, 0]}])
Weights num count: [0.15181818181818182, 0.8481818181818181]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 944, 'sum_payoffs': 133.4999999643982, 'action': [1.0, 0]}, {'num_count': 156, 'sum_payoffs': 18.959999994943974, 'action': [0.0, 0]}])
Weights num count: [0.8581818181818182, 0.14181818181818182]
Actions to choose Agent 1: dict_values([{'num_count': 253, 'sum_payoffs': 30.09749999197389, 'action': [0.0, 0]}, {'num_count': 847, 'sum_payoffs': 118.04249996852026, 'action': [1.0, 0]}])
Weights num count: [0.23, 0.77]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4406585693359375 s
