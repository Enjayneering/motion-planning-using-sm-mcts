Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 146, 'sum_payoffs': 25.15814999496838, 'action': [0.0, 0]}, {'num_count': 954, 'sum_payoffs': 216.1484762725632, 'action': [1.0, 0]}])
Weights num count: [0.13272727272727272, 0.8672727272727273]
Actions to choose Agent 1: dict_values([{'num_count': 980, 'sum_payoffs': 226.20462627055244, 'action': [1.0, 0]}, {'num_count': 120, 'sum_payoffs': 22.756499995448717, 'action': [0.0, 0]}])
Weights num count: [0.8909090909090909, 0.10909090909090909]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 125, 'sum_payoffs': 26.36099999472783, 'action': [0.0, 0]}, {'num_count': 975, 'sum_payoffs': 245.90699995081422, 'action': [1.0, 0]}])
Weights num count: [0.11363636363636363, 0.8863636363636364]
Actions to choose Agent 1: dict_values([{'num_count': 899, 'sum_payoffs': 222.82199995543183, 'action': [1.0, 0]}, {'num_count': 201, 'sum_payoffs': 41.91299999161743, 'action': [0.0, 0]}])
Weights num count: [0.8172727272727273, 0.18272727272727274]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.40839338302612305 s
