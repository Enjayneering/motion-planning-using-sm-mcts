Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 208, 'sum_payoffs': 17.31206249610476, 'action': [0.0, 0]}, {'num_count': 892, 'sum_payoffs': 88.65698682215832, 'action': [1.0, 0]}])
Weights num count: [0.1890909090909091, 0.8109090909090909]
Actions to choose Agent 1: dict_values([{'num_count': 210, 'sum_payoffs': 18.04106249594074, 'action': [0.0, 0]}, {'num_count': 890, 'sum_payoffs': 93.12211182115367, 'action': [1.0, 0]}])
Weights num count: [0.19090909090909092, 0.8090909090909091]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 188, 'sum_payoffs': 16.723124996237317, 'action': [0.0, 0]}, {'num_count': 912, 'sum_payoffs': 98.1449999779161, 'action': [1.0, 0]}])
Weights num count: [0.1709090909090909, 0.8290909090909091]
Actions to choose Agent 1: dict_values([{'num_count': 852, 'sum_payoffs': 90.6018749796138, 'action': [1.0, 0]}, {'num_count': 248, 'sum_payoffs': 21.329999995200836, 'action': [0.0, 0]}])
Weights num count: [0.7745454545454545, 0.22545454545454546]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.4184861183166504 s
