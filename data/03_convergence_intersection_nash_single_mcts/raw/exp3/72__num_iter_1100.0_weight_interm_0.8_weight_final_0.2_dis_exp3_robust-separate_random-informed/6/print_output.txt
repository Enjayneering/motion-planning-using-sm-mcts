Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 817, 'sum_payoffs': 69.57490261766327, 'action': [1.0, 0]}, {'num_count': 283, 'sum_payoffs': 19.988999996002207, 'action': [0.0, 0]}])
Weights num count: [0.7427272727272727, 0.25727272727272726]
Actions to choose Agent 1: dict_values([{'num_count': 887, 'sum_payoffs': 73.43860261689085, 'action': [1.0, 0]}, {'num_count': 213, 'sum_payoffs': 16.635599996672916, 'action': [0.0, 0]}])
Weights num count: [0.8063636363636364, 0.19363636363636363]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 763, 'sum_payoffs': 65.95649998680952, 'action': [1.0, 0]}, {'num_count': 337, 'sum_payoffs': 23.278499995344223, 'action': [0.0, 0]}])
Weights num count: [0.6936363636363636, 0.30636363636363634]
Actions to choose Agent 1: dict_values([{'num_count': 849, 'sum_payoffs': 74.8259999850358, 'action': [1.0, 0]}, {'num_count': 251, 'sum_payoffs': 18.45899999630813, 'action': [0.0, 0]}])
Weights num count: [0.7718181818181818, 0.22818181818181818]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5208883285522461 s
