Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 211, 'sum_payoffs': 15.768899996846264, 'action': [0.0, 0]}, {'num_count': 889, 'sum_payoffs': 75.07885261656274, 'action': [1.0, 0]}])
Weights num count: [0.1918181818181818, 0.8081818181818182]
Actions to choose Agent 1: dict_values([{'num_count': 259, 'sum_payoffs': 17.94779999641046, 'action': [0.0, 0]}, {'num_count': 841, 'sum_payoffs': 69.54655261766882, 'action': [1.0, 0]}])
Weights num count: [0.23545454545454544, 0.7645454545454545]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 255, 'sum_payoffs': 18.215999996356725, 'action': [0.0, 0]}, {'num_count': 845, 'sum_payoffs': 72.43649998551376, 'action': [1.0, 0]}])
Weights num count: [0.2318181818181818, 0.7681818181818182]
Actions to choose Agent 1: dict_values([{'num_count': 207, 'sum_payoffs': 15.05699999698853, 'action': [0.0, 0]}, {'num_count': 893, 'sum_payoffs': 76.81049998463901, 'action': [1.0, 0]}])
Weights num count: [0.18818181818181817, 0.8118181818181818]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.49672842025756836 s
