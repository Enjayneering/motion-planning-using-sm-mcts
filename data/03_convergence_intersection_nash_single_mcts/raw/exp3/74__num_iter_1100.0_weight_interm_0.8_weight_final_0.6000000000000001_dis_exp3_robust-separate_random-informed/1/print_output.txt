Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 965, 'sum_payoffs': 159.67121613804127, 'action': [1.0, 0]}, {'num_count': 135, 'sum_payoffs': 17.194499997052397, 'action': [0.0, 0]}])
Weights num count: [0.8772727272727273, 0.12272727272727273]
Actions to choose Agent 1: dict_values([{'num_count': 154, 'sum_payoffs': 22.037142853365072, 'action': [0.0, 0]}, {'num_count': 946, 'sum_payoffs': 158.4215018525413, 'action': [1.0, 0]}])
Weights num count: [0.14, 0.86]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 140, 'sum_payoffs': 20.64857142503169, 'action': [0.0, 0]}, {'num_count': 960, 'sum_payoffs': 171.04178568496286, 'action': [1.0, 0]}])
Weights num count: [0.12727272727272726, 0.8727272727272727]
Actions to choose Agent 1: dict_values([{'num_count': 139, 'sum_payoffs': 21.516428567740057, 'action': [0.0, 0]}, {'num_count': 961, 'sum_payoffs': 171.90964282767126, 'action': [1.0, 0]}])
Weights num count: [0.12636363636363637, 0.8736363636363637]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5819611549377441 s
