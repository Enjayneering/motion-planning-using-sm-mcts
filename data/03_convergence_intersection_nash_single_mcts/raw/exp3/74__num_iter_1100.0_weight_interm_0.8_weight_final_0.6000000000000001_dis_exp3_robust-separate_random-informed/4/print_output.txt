Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 166, 'sum_payoffs': 18.956892853893144, 'action': [0.0, 0]}, {'num_count': 934, 'sum_payoffs': 153.84564471046878, 'action': [1.0, 0]}])
Weights num count: [0.1509090909090909, 0.8490909090909091]
Actions to choose Agent 1: dict_values([{'num_count': 106, 'sum_payoffs': 13.801821426205418, 'action': [0.0, 0]}, {'num_count': 994, 'sum_payoffs': 166.96764470821853, 'action': [1.0, 0]}])
Weights num count: [0.09636363636363636, 0.9036363636363637]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 188, 'sum_payoffs': 30.281785709094592, 'action': [0.0, 0]}, {'num_count': 912, 'sum_payoffs': 163.83857140048394, 'action': [1.0, 0]}])
Weights num count: [0.1709090909090909, 0.8290909090909091]
Actions to choose Agent 1: dict_values([{'num_count': 116, 'sum_payoffs': 15.528214283052334, 'action': [0.0, 0]}, {'num_count': 984, 'sum_payoffs': 175.2942856842336, 'action': [1.0, 0]}])
Weights num count: [0.10545454545454545, 0.8945454545454545]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5860245227813721 s
