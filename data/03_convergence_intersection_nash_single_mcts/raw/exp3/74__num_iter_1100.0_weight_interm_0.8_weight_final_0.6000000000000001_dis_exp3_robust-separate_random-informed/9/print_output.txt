Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 971, 'sum_payoffs': 168.4973232793844, 'action': [1.0, 0]}, {'num_count': 129, 'sum_payoffs': 18.756642853927453, 'action': [0.0, 0]}])
Weights num count: [0.8827272727272727, 0.11727272727272728]
Actions to choose Agent 1: dict_values([{'num_count': 210, 'sum_payoffs': 26.95789285252152, 'action': [0.0, 0]}, {'num_count': 890, 'sum_payoffs': 143.11250185516633, 'action': [1.0, 0]}])
Weights num count: [0.19090909090909092, 0.8090909090909091]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 955, 'sum_payoffs': 170.52107139933796, 'action': [1.0, 0]}, {'num_count': 145, 'sum_payoffs': 21.863571424823398, 'action': [0.0, 0]}])
Weights num count: [0.8681818181818182, 0.1318181818181818]
Actions to choose Agent 1: dict_values([{'num_count': 182, 'sum_payoffs': 28.97999999503202, 'action': [0.0, 0]}, {'num_count': 918, 'sum_payoffs': 163.57821425767136, 'action': [1.0, 0]}])
Weights num count: [0.16545454545454547, 0.8345454545454546]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.5153861045837402 s
