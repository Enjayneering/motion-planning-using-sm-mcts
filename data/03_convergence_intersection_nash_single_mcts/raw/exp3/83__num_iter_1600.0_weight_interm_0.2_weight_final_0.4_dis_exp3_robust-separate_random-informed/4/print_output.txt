Searching game tree in timestep 0...
Max timehorizon: 2
Actions to choose Agent 0: dict_values([{'num_count': 1480, 'sum_payoffs': 359.75810516721543, 'action': [1.0, 0]}, {'num_count': 120, 'sum_payoffs': 22.99649999386763, 'action': [0.0, 0]}])
Weights num count: [0.925, 0.075]
Actions to choose Agent 1: dict_values([{'num_count': 1455, 'sum_payoffs': 374.82410516319794, 'action': [1.0, 0]}, {'num_count': 145, 'sum_payoffs': 28.34249999244204, 'action': [0.0, 0]}])
Weights num count: [0.909375, 0.090625]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Searching game tree in timestep 1...
Max timehorizon: 1
Actions to choose Agent 0: dict_values([{'num_count': 150, 'sum_payoffs': 35.39999999055996, 'action': [0.0, 0]}, {'num_count': 1450, 'sum_payoffs': 398.1749998938252, 'action': [1.0, 0]}])
Weights num count: [0.09375, 0.90625]
Actions to choose Agent 1: dict_values([{'num_count': 1424, 'sum_payoffs': 390.74999989580476, 'action': [1.0, 0]}, {'num_count': 176, 'sum_payoffs': 41.47499998893989, 'action': [0.0, 0]}])
Weights num count: [0.89, 0.11]
Selected final action: [1.0, 0, 1.0, 0]
Total payoff list: [0.0, 0.0]
Terminal state: [1.0, 2.0, 1.5707963267948966, 2.0, 1.0, 0.0, 2]
Timestep: 2
Runtime: 0.6206772327423096 s
