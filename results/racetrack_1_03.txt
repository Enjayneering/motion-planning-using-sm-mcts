delta_t: 1
agents: [0, 1]
state_space: ['x0', 'y0', 'theta0', 'x1', 'y1', 'theta1', 'timestep']
action_space: ['x0', 'y0', 'x1', 'y1']
interm_payoffs: {'penalty_collision_0': {'pos': 0, 'weight': -100, 'agent': 0}, 'penalty_collision_1': {'pos': 1, 'weight': -100, 'agent': 1}, 'reward_progress_0': {'pos': 2, 'weight': 1, 'agent': 0}, 'reward_progress_1': {'pos': 3, 'weight': 1, 'agent': 1}}
final_payoffs: {'penalty_timestep_0': {'pos': 0, 'weight': -0.05, 'agent': 0}, 'penalty_timestep_1': {'pos': 1, 'weight': -0.05, 'agent': 1}, 'reward_lead_0': {'pos': 2, 'weight': 5, 'agent': 0}, 'reward_lead_1': {'pos': 3, 'weight': 5, 'agent': 1}}
len_interm_payoffs: 4
len_final_payoffs: 4
num_iter: 1000
c_param: 1.4142135623730951
penalty_collision_init: 0.1
penalty_collision_delay: 1
penalty_stuck_in_env: -1
action_set_0: {'velocity_0': [0.0, 1.0, 2.0], 'ang_velocity_0': [-1.5707963267948966, 0.0, 1.5707963267948966]}
action_set_1: {'velocity_1': [0.0, 1.0], 'ang_velocity_1': [-1.5707963267948966, 0.0, 1.5707963267948966]}
Environment trigger: [(0, 'racetrack_1')]
Payoff Agent 0: [(0, 0), (1, -80.0), (2, -80.68779256603858), (3, -75.64054023128818), (4, -71.25081295870558), (5, -72.87022729062184), (6, -70.1471666847059), (7, -67.51821024943811), (7, -37.868210249438114)]
Payoff Agent 1: [(0, 0), (1, -90.0), (2, -100.68779256603858), (3, -102.30720689795484), (4, -100.41747962537224), (5, -100.0368939572885), (6, -98.98050001803922), (7, -97.78011501134287), (7, -128.13011501134287)]
Duration: 38.374698877334595

Content of global_state.csv:
x0,y0,theta0,x1,y1,theta1,timestep
1,3,0,2,3,0,0
3.0,3.0,0.0,3.0,3.0,4.71238898038469,1
5.0,3.0,0.0,3.0,2.0,4.71238898038469,2
7.0,3.0,0.0,3.0,2.0,0.0,3
9.0,3.0,0.0,4.0,2.0,0.0,4
9.0,3.0,0.0,5.0,2.0,0.0,5
11.0,3.0,0.0,6.0,2.0,0.0,6
13.0,3.0,4.71238898038469,7.0,2.0,4.71238898038469,7
