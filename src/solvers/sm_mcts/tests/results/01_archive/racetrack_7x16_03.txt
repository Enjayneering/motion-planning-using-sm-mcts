state_space: ['x0', 'y0', 'theta0', 'x1', 'y1', 'theta1', 'timestep']
delta_t: 1
collision_distance: 0.5
agents: [0, 1]
interm_payoffs: {'penalty_distance_0': {'pos': 0, 'weight': -2.0, 'agent': 0}, 'penalty_distance_1': {'pos': 1, 'weight': -2.0, 'agent': 1}, 'reward_progress_0': {'pos': 2, 'weight': 0.5, 'agent': 0}, 'reward_progress_1': {'pos': 3, 'weight': 0.5, 'agent': 1}}
final_payoffs: {'penalty_timestep_0': {'pos': 0, 'weight': 0, 'agent': 0}, 'penalty_timestep_1': {'pos': 1, 'weight': 0, 'agent': 1}, 'reward_lead_0': {'pos': 2, 'weight': 1, 'agent': 0}, 'reward_lead_1': {'pos': 3, 'weight': 1, 'agent': 1}}
len_interm_payoffs: 4
len_final_payoffs: 4
num_iter: 2000
c_param: 20
action_set_0: {'velocity_0': [0.0, 1.0, 2.0], 'ang_velocity_0': [-1.5707963267948966, 0.0, 1.5707963267948966]}
action_set_1: {'velocity_1': [0.0, 1.0], 'ang_velocity_1': [-1.5707963267948966, 0.0, 1.5707963267948966]}
Environment trigger: racetrack_7x16
penalty_distance_0: [-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]
penalty_distance_1: [-0.0, -0.0, -0.0, -0.0, -0.0, -0.0, -0.0]
reward_progress_0: [0.0, 0.5, 0.5, 0.0, 0.0, 0.0, 0.0]
reward_progress_1: [0.0, 0.0, 0.0, 0.5, 0.0, 0.0, 0.0]
penalty_timestep_0: [0.0]
penalty_timestep_1: [0.0]
reward_lead_0: [1.0]
reward_lead_1: [-1.0]
payoff_total_0: [0.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0]
payoff_total_1: [0.0, 0.0, 0.0, 0.5, 0.5, 0.5, 0.5, -0.5]
Config: {'alpha_rollout': 1, 'alpha_terminal': 1, 'num_sim': 5, 'num_iter': 2000, 'delta_t': 1, 'c_param': 20, 'collision_distance': 0.5, 'goal_distance': 0.5, 'collision_ignorance': 0.5, 'discount_factor': 1, 'penalty_distance': -4, 'reward_progress_0': 0.5, 'reward_progress_1': 0.5, 'penalty_timestep_0': 0, 'penalty_timestep_1': 0, 'reward_lead_0': 1, 'reward_lead_1': 1, 'velocity_0': [0.0, 1.0, 2.0], 'ang_velocity_0': [-1.5707963267948966, 0.0, 1.5707963267948966], 'velocity_1': [0.0, 1.0], 'ang_velocity_1': [-1.5707963267948966, 0.0, 1.5707963267948966], 'standard_dev_vel_0': 2, 'standard_dev_ang_vel_0': 0.2617993877991494, 'standard_dev_vel_1': 1, 'standard_dev_ang_vel_1': 0.7853981633974483, 'feature_flags': {'final_move': {'robust-joint': False, 'robust-separate': True, 'max': False, 'uct-decoupled': False}, 'collision_handling': {'punishing': True, 'pruning': False}, 'selection_policy': {'uct-decoupled': True, 'max': False, 'regret-matching': False}, 'rollout_policy': {'random-uniform': True, 'random-informed': False}, 'payoff_weights': {'fixed': True, 'adaptive': False}, 'expansion_policy': {'every-child': False, 'random-informed': True}, 'strategy': {'pure': False, 'mixed': True}}}
Duration: 96.9076509475708

Content of global_state.csv:
x0,y0,theta0,x1,y1,theta1,timestep
1,3,0.0,3,3,0.0,0
1,3,0.0,3,3,0.0,0
1.0,3.0,0.0,3.0,3.0,-1.5707963267948966,1
3.0,3.0,0.0,3.0,2.0,-1.5707963267948966,2
5.0,3.0,1.5707963267948966,3.0,1.0,0.0,3
5.0,3.0,1.5707963267948966,4.0,1.0,1.5707963267948966,4
5.0,4.0,0.0,4.0,1.0,1.5707963267948966,5
7.0,4.0,0.0,4.0,2.0,1.5707963267948966,6
8.0,4.0,0.0,4.0,3.0,1.5707963267948966,7
