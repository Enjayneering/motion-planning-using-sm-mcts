agents: [0, 1]
state_space: ['x0', 'y0', 'theta0', 'x1', 'y1', 'theta1', 'timestep']
action_space: ['x0', 'y0', 'x1', 'y1']
payoff_vector: {'intermediate_penalties': {'penalty_collision_0': {'weight': 1, 'pos': 0, 'agent': 0}, 'penalty_collision_1': {'weight': 1, 'pos': 1, 'agent': 1}}, 'intermediate_rewards': {'reward_progress_0': {'weight': 0.1, 'pos': 2, 'agent': 0}, 'reward_progress_1': {'weight': 0.1, 'pos': 3, 'agent': 1}}, 'final_rewards': {'reward_lead_0': {'weight': 1, 'pos': 4, 'agent': 0}, 'reward_lead_1': {'weight': 1, 'pos': 5, 'agent': 1}}}
len_payoff_vector: 6
num_iter: 800
c_param: 1.4142135623730951
penalty_collision_init: 0.1
penalty_collision_delay: 1
penalty_stuck_in_env: -1
action_set_0: {'velocity_0': [0.0, 1.0, 2.0], 'ang_velocity_0': [-1.5707963267948966, 0.0, 1.5707963267948966]}
action_set_1: {'velocity_1': [0.0, 1.0, 2.0], 'ang_velocity_1': [-1.5707963267948966, 0.0, 1.5707963267948966]}
Environment trigger: [(0, 'benchmark_dynamic_small2'), (2, 'benchmark_dynamic_small2_1'), (5, 'benchmark_dynamic_small2_2')]
Payoff Agent 0: [(0, 0), (1, -0.10687792566038574), (2, -0.14920714528359075), (3, -0.1654012886027533), (4, -0.18159543192191585), (5, -0.18387729614066645), (6, -0.18615916035941704), (7, -7.186848270193165)]
Payoff Agent 1: [(0, 0), (1, -0.10687792566038574), (2, -0.14920714528359075), (3, -0.1654012886027533), (4, -0.18159543192191585), (5, -0.18387729614066645), (6, -0.18615916035941704), (7, 6.813151729806835)]
Duration: 35.9285044670105

Content of global_state.csv:
x0,y0,theta0,x1,y1,theta1,timestep
1,2,0,1,4,0,0
2.0,2.0,1.5707963267948966,3.0,4.0,0.0,1
2.0,3.0,3.141592653589793,5.0,4.0,0.0,2
2.0,3.0,4.71238898038469,6.0,4.0,0.0,3
2.0,3.0,3.141592653589793,6.0,4.0,0.0,4
2.0,3.0,4.71238898038469,8.0,4.0,0.0,5
2.0,3.0,4.71238898038469,8.0,4.0,0.0,6
1.9999999999999998,2.0,0.0,9.0,4.0,4.71238898038469,7
