delta_t: 0.1
agents: [0, 1]
state_space: ['x0', 'y0', 'theta0', 'x1', 'y1', 'theta1', 'timestep']
action_space: ['x0', 'y0', 'x1', 'y1']
interm_payoffs: {'penalty_distance_0': {'pos': 0, 'weight': -1, 'agent': 0}, 'penalty_distance_1': {'pos': 1, 'weight': -1, 'agent': 1}, 'reward_progress_0': {'pos': 2, 'weight': 1, 'agent': 0}, 'reward_progress_1': {'pos': 3, 'weight': 1, 'agent': 1}}
final_payoffs: {'penalty_timestep_0': {'pos': 0, 'weight': -1, 'agent': 0}, 'penalty_timestep_1': {'pos': 1, 'weight': -1, 'agent': 1}, 'reward_lead_0': {'pos': 2, 'weight': 10, 'agent': 0}, 'reward_lead_1': {'pos': 3, 'weight': 10, 'agent': 1}}
len_interm_payoffs: 4
len_final_payoffs: 4
num_iter: 1000
c_param: 1.4142135623730951
action_set_0: {'velocity_0': [0.0, 1.0, 2.0], 'ang_velocity_0': [-1.5707963267948966, 0.0, 1.5707963267948966]}
action_set_1: {'velocity_1': [0.0, 1.0], 'ang_velocity_1': [-1.5707963267948966, 0.0, 1.5707963267948966]}
Environment trigger: [(0, 'benchmark_dynamic_small2'), (1, 'benchmark_dynamic_small2_1'), (5, 'benchmark_dynamic_small2_2')]
Payoff Agent 0: [(0, 0), (0.1, -0.10687792566038574), (0.2, 119.83401632777766), (0.30000000000000004, 199.80684386660542), (0.4, 199.74773812004346), (0.5, 223.6886323734815), (0.5, 243.1886323734815)]
Payoff Agent 1: [(0, 0), (0.1, 119.89312207433962), (0.2, 179.83401632777765), (0.30000000000000004, 219.80684386660542), (0.4, 249.74773812004346), (0.5, 273.6886323734815), (0.5, 253.18863237348148)]
Duration: 84.78950381278992

Content of global_state.csv:
x0,y0,theta0,x1,y1,theta1,timestep
4,2,1.5707963267948966,2,4,0,0
4.0,2.0,0.0,3.0,4.0,0.0,0.1
6.0,2.0,0.0,4.0,4.0,0.0,0.2
8.0,2.0,1.5707963267948966,5.0,4.0,0.0,0.30000000000000004
8.0,2.0,0.0,6.0,4.0,0.0,0.4
9.0,2.0,4.71238898038469,7.0,4.0,0.0,0.5
